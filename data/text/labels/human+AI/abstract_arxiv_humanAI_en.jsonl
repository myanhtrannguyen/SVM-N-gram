{"solution_id": "llama-33-70b-instruct-turbo-0001", "problem_id": "00010001", "content": "Numerous image super-resolution algorithms utilizing sparse coding have been developed, with several achieving multi-frame super-resolution capabilities. To accomplish multi-frame super-resolution via sparse coding, it is essential to perform precise image registration and sparse coding. Traditionally, research in this area has employed a two-step approach, where block matching is used for image registration, followed by sparse coding to enhance image resolution. In contrast, this paper presents a unified approach that addresses both challenges by optimizing a single objective function, and the outcomes of numerical experiments demonstrate the efficacy of the proposed method, as shown in Figure A, B, C (References [1], [2], and citations therein).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0002", "problem_id": "00020001", "content": "This study presents a novel application of deep learning for generating realistic river images, leveraging a generative adversarial network (GAN) model to produce high-resolution images that can facilitate modeling and analysis in various hydrological research areas, including surface water estimation, river meandering, and wetland loss. To achieve this, a comprehensive dataset of overhead river images was compiled for training purposes, and the Progressive Growing GAN (PGGAN) architecture was utilized, which enables the iterative training of smaller-resolution GANs to ultimately generate high-quality (1024x1024) synthetic river images. The PGGAN approach effectively mitigated issues associated with simpler GAN architectures, such as exponentially increased training time and vanishing or exploding gradients. The findings of this research demonstrate significant potential for generating high-quality images that accurately capture river structure and flow details, thereby supporting hydrological research that often relies on extensive imagery for optimal model performance.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0003", "problem_id": "00030001", "content": "Text generation, a crucial task in Natural Language Processing, finds utility across numerous applications. Current evaluation metrics for text generation methods possess individual limitations; for instance, BLEU, a prevalent metric, assesses generated sentence quality but overlooks diversity. This can lead to inflated scores when a single, high-quality sentence is generated repeatedly. Conversely, Self-BLEU, a newer metric focusing on diversity, disregards the quality aspect. This paper introduces metrics designed to simultaneously evaluate both quality and diversity by estimating the distance between the learned generative model and the actual data distribution. We first present an n-gram-based metric for approximating this distance. Subsequently, we introduce a feature-based measure leveraging BERT, a deep learning model trained on a substantial text corpus. Finally, for oracle training, we propose distribution distance measures. We evaluate popular and recent text generation models using both existing and proposed metrics to determine the advantages of our approach.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0004", "problem_id": "00040001", "content": "Product embeddings have been extensively studied in recent years, forming the foundation for numerous machine learning applications in e-commerce. While these embeddings have demonstrated practical success, their underlying mechanisms remain poorly understood from a theoretical perspective. Existing insights from natural language processing (NLP) often depend on domain-specific characteristics that do not apply to e-commerce, and the target tasks typically emphasize different embedding aspects. This work adopts an e-commerce-centric approach to product embeddings, offering a comprehensive theoretical framework that integrates representation learning and learning theory. We demonstrate that embeddings generated by the widely-used skip-gram negative sampling algorithm and its variations achieve sufficient dimension reduction for a key product relatedness metric. The generalization capability in downstream machine learning tasks depends on how well the embeddings align with this product relatedness measure. Building on these theoretical findings, we perform exploratory experiments that validate our theoretical conclusions about product embeddings.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0005", "problem_id": "00050001", "content": "On graph-structured domains, Graph Gaussian Processes (GGPs) offer a data-efficient solution, but their applicability is restricted by their focus on static structures, whereas numerous real-world graph datasets exhibit dynamic structures. To address this limitation, we introduce evolving-Graph Gaussian Processes (e-GGPs), a novel approach that enables learning the temporal evolution of graph vertices using a neighbourhood kernel to capture changes in connectivity and interactions between vertices over time. The efficacy of our proposed method is evaluated on time-series regression tasks involving evolving graphs, and the results highlight the advantages of e-GGPs over conventional static graph Gaussian Process approaches.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0006", "problem_id": "00060001", "content": "While pretrained language models achieve top performance across diverse language tasks when fine-tuned, the underlying mechanisms, particularly in low-data scenarios, remain poorly understood. How can simple gradient descent methods (without heavy regularization) effectively adjust models with hundreds of millions of parameters using only small labeled datasets? This paper explores fine-tuning through intrinsic dimension analysis, offering empirical and theoretical insights into this phenomenon. Our findings reveal that pretrained models inherently possess a low intrinsic dimension, meaning a reduced parameter subspace suffices for fine-tuning as effectively as the full space. For instance, optimizing just 200 parameters—randomly projected into the full space—enables a RoBERTa model to reach 90% of full-parameter performance on MRPC. Additionally, pretraining inherently reduces intrinsic dimension, and notably, larger models exhibit even lower intrinsic dimensions after fixed pretraining steps, partially explaining their superior performance. Finally, we link intrinsic dimension to low-dimensional task representations and compression-based generalization bounds, deriving generalization bounds based on intrinsic dimension that are independent of total parameter count.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0007", "problem_id": "00070001", "content": "Pose tracking involves recognizing distinct human pose instances and linking them across video frames over time. Current approaches struggle to effectively capture temporal dependencies and often demand extensive computational resources, typically processing tracks offline. We introduce KeyTrack, a real-time Multi-person Pose Tracking solution that efficiently tracks human keypoints using only keypoint data, eliminating the need for RGB or optical flow inputs. Our Pose Entailment technique tracks keypoints by sampling and tokenizing pose pairs from different frames, then employing a Transformer-based network to classify whether one pose succeeds another temporally. Additionally, we enhance our top-down pose estimation with a novel, parameter-free keypoint refinement method, improving accuracy for Pose Entailment. Our approach delivers state-of-the-art performance on the PoseTrack'17 and PoseTrack'18 datasets while requiring significantly less computation than existing tracking methods.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0008", "problem_id": "00080001", "content": "The detailed characterization of human faces by face attributes has garnered significant interest, prompting a shift in focus from attribute prediction to the more complex task of face attribute manipulation, which involves altering a face image in accordance with a specified attribute value. To efficiently achieve this, we introduce a novel approach that learns the residual image, defined as the difference between the original and manipulated images, thereby enabling subtle pixel modifications. Our framework, grounded in the Generative Adversarial Network, comprises two image transformation networks responsible for attribute manipulation and its inverse operation, as well as a discriminative network that distinguishes between generated and real images. By incorporating dual learning, the transformation networks can mutually inform each other, leading to more effective attribute manipulation. As demonstrated by our experiments, the residual images can be successfully learned and utilized for attribute manipulation, resulting in generated images that preserve most details in areas unrelated to the manipulated attribute, as shown in Figure A, B, C (References [1], [2], [3] and citations [4] remain unchanged).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0009", "problem_id": "00090001", "content": "Most existing tone mapping operators (TMOs) neglect color management, solely compressing luminance values, which can result in undesirable alterations to image chroma and hues that are typically addressed through a subsequent post-processing step. Nevertheless, current tone reproduction post-processing techniques fail to account for the target display's color gamut explicitly. In contrast, gamut mapping, which involves transferring images between different color gamuts, often to a smaller one, has traditionally concentrated on modest, chromatic adjustments. We introduce a new framework for managing tone and gamut that enables accurate color reproduction of high dynamic range (HDR) images, characterized by its simplicity, lack of parameters, and compatibility with existing TMOs, as seen in Figure A, B, C (References: [citation]). By compressing chroma within the CIE LCh color space to match the output color space's gamut, our approach prevents shifts in hue and luminance while respecting gamut boundaries. Additionally, we propose a lightness compression scheme that minimizes color space conversions, as discussed in References [citation]. Our findings demonstrate that the proposed gamut management method effectively compresses the chroma of tone-mapped images, adhering to the target gamut without compromising image quality.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0010", "problem_id": "00100001", "content": "Recent advancements in RGB-D saliency detection have demonstrated notable capabilities in addressing challenging scenarios, thanks to the spatial cues inherent in depth images. Nevertheless, two significant limitations persist: the pooling and upsampling operations in fully convolutional networks (FCNs) can result in blurred object boundaries, and the use of an additional depth network to extract depth features can incur substantial computational and storage costs. Furthermore, the dependence on depth inputs during testing restricts the practical applicability of existing RGB-D models. To address these issues, this paper presents a novel collaborative learning framework that leverages edge, depth, and saliency information in a more efficient manner. By explicitly extracting edge information and combining it with saliency, the model emphasizes salient regions and object boundaries more effectively. Additionally, the framework integrates depth and saliency learning into the high-level feature learning process in a mutually beneficial way, eliminating the need for extra depth networks and depth inputs during inference. This approach enables the development of a more lightweight, faster, and versatile model, as evidenced by its superior performance on seven benchmark datasets, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0011", "problem_id": "00110001", "content": "Weakly Supervised Object Localization (WSOL) methods rely solely on image-level labels to determine object locations, omitting the need for location annotations. However, a prevalent issue with these methods is their tendency to focus only on the most distinctive portion of the object, rather than its entirety. To overcome this limitation, we introduce an Attention-based Dropout Layer (ADL) that leverages self-attention mechanisms to process the model's feature maps. Our proposed approach consists of two primary components: firstly, it conceals the most distinctive part of the object from the model to facilitate the capture of the object's complete extent, and secondly, it emphasizes the informative regions to enhance the model's recognition capabilities. Through comprehensive experiments, we show that our method significantly improves WSOL accuracy, achieving a new state-of-the-art localization accuracy on the CUB-200-2011 dataset, as seen in Figure A, and outperforming existing techniques in terms of parameter and computational efficiency, as demonstrated in Figure B, and discussed in References [citation].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0012", "problem_id": "00120001", "content": "Convolutional Neural Networks (CNNs) are susceptible to noise, where minor image perturbations can significantly alter the output. To mitigate the impact of noise on final predictions, we propose enhancing CNNs by substituting max-pooling, strided-convolution, and average-pooling operations with Discrete Wavelet Transform (DWT). We introduce versatile DWT and Inverse DWT (IDWT) layers applicable to a range of wavelets, such as Haar, Daubechies, and Cohen, and construct wavelet-integrated CNNs (WaveCNets) utilizing these layers for image classification. In WaveCNets, feature maps are decomposed into low-frequency and high-frequency components during down-sampling. The low-frequency component, which retains primary information including fundamental object structures, is passed to subsequent layers for the extraction of resilient high-level features. Conversely, the high-frequency components, which primarily contain data noise, are discarded during inference to bolster the noise resilience of WaveCNets. Empirical evaluations conducted on ImageNet and ImageNet-C (the noisy version of ImageNet) demonstrate that WaveCNets, the wavelet-integrated counterparts of VGG, ResNets, and DenseNet, attain superior accuracy and improved noise robustness compared to their standard counterparts.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0013", "problem_id": "00130001", "content": "Estimating optical flow is a core challenge in computer vision, yet deep learning approaches often struggle with non-rigid motions like facial movements due to insufficient training data capturing subtle facial dynamics. We propose that training optical flow models specifically on facial motion data can enhance flow prediction accuracy for faces. This study focuses on three objectives: (1) investigating self-supervised methods to create ground truth optical flow for facial images; (2) establishing baseline outcomes by training Convolutional Neural Networks (CNNs) on facial data for optical flow estimation; and (3) validating the utility of the learned flow in micro-expression recognition. Using facial key-points from the BP4D-Spontaneous dataset, we generate optical flow ground truth and train the FlowNetS model, which outperforms other CNN-based flow architectures when tested on facial data. Additionally, our optical flow features are evaluated with the STSTNet micro-expression classifier, showing their potential for advancing facial expression analysis applications.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0014", "problem_id": "00140001", "content": "The analysis and prediction of complex dynamical systems, which are prevalent in fields such as financial markets, power grid management, climate modeling, and molecular dynamics, can be significantly enhanced through the use of time series data. Fortunately, many of these nonlinear systems can be transformed into a feature space where their dynamics can be accurately approximated by linear Markovian models, facilitating a more straightforward analysis. Furthermore, the collective behavior of system variables on large scales enables a reduction in dimensionality, allowing for a more efficient analysis in feature space. This paper presents a variational approach for Markov processes, referred to as VAMP, which enables the identification of optimal feature mappings and Markovian models from time series data. The core idea behind VAMP is that the optimal linear model can be derived from the top singular components of the Koopman operator, giving rise to a family of score functions, known as VAMP-r, that can be computed from data and used to optimize Markovian models. Additionally, a new score function, VAMP-E, is introduced, which leverages the relationship between variational scores and approximation errors of Koopman operators to perform cross-validation for hyper-parameter optimization and model selection in VAMP, with the approach being applicable to both reversible and nonreversible processes, as well as stationary and non-stationary processes or realizations.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0015", "problem_id": "00150001", "content": "A potential approach for depth imaging of dynamic scenes involves projecting a static pattern onto the object and reconstructing the scene from a single image. However, rapid object motion relative to the camera's exposure time causes pattern blurring, leading to reconstruction failure. This paper introduces a method that embeds multiple projection patterns into each captured image to achieve temporal super resolution in depth image sequences. By projecting patterns at a higher frame rate than the camera can capture, the observed pattern changes based on object depth and motion, enabling temporal scene information extraction from a single image. A learning-based decoding process eliminates the need for geometric calibration. Experimental results demonstrate the method's success in reconstructing sequential shapes from individual images, supported by quantitative assessments and comparisons with existing techniques.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0016", "problem_id": "00160001", "content": "Skeleton data, being efficient and compact, serves as an excellent input for action recognition on resource-constrained edge devices. Contemporary approaches in skeleton-based action recognition leverage 3D joint coordinates to derive spatial-temporal features, integrating them through graph neural networks to enhance classification accuracy. While first- and second-order features, such as joint and bone representations, have improved performance, existing models often struggle with actions sharing similar motion patterns. To overcome this limitation, we introduce third-order angular encoding features into current frameworks, enabling better modeling of joint and body part relationships. This straightforward integration with widely-used spatial-temporal graph neural networks achieves superior accuracy on major benchmarks like NTU60 and NTU120, while maintaining lower computational costs and fewer parameters. Our implementation is accessible at: https://github.com/ZhenyueQin/Angular-Skeleton-Encoding.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0017", "problem_id": "00170001", "content": "Action recognition using 3D skeleton data has recently become a significant area of research. Current methodologies predominantly rely on either manually designed descriptors or supervised learning approaches, the latter necessitating substantial amounts of labeled data to derive action representations. This paper introduces AS-CAL, a novel contrastive action learning paradigm that, for the first time, uses diverse augmentations of unlabeled skeleton data to learn action representations without supervision. The proposed method contrasts the similarity between augmented instances (query and key) of the input skeleton sequence, transformed using multiple novel augmentation strategies, to ascertain inherent action patterns or \"pattern-invariance\" across different skeleton transformations. To foster the learning of pattern-invariance with enhanced action representation consistency, a momentum LSTM is proposed, implemented as a momentum-based moving average of an LSTM-based query encoder, to capture long-term action dynamics within the key sequence. Furthermore, a queue is utilized to store encoded keys, enabling flexible reuse of prior keys and establishing a more coherent dictionary to refine contrastive learning. Finally, a novel representation, Contrastive Action Encoding (CAE), is introduced, generated by temporally averaging the hidden states of actions learned by the query encoder, to effectively represent human actions. Comprehensive experiments demonstrate that this approach typically enhances existing hand-crafted methods by 10-50% in top-1 accuracy and achieves performance comparable to or exceeding that of several supervised learning methods.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0018", "problem_id": "00180001", "content": "The Single Shot Multibox Detector (SSD) is a highly effective object detection model, renowned for its exceptional accuracy and rapid processing speed. Nevertheless, the features extracted from the shallow layers of the SSD, primarily Conv4_3, are deficient in semantic information, which compromises its performance in detecting small objects. To address this limitation, this paper introduces the Dilation and Deconvolution Single Shot Multibox Detector (DDSSD), an advanced SSD variant that incorporates a novel feature fusion module to enhance small object detection capabilities. By leveraging a dilation convolution module to expand the receptive field of shallow layer features and a deconvolution module to increase the size of high-layer feature maps, the proposed network achieves a mean average precision (mAP) of 79.7% on the PASCAL VOC2007 test set and a mmAP of 28.3% on the MS COCO test-dev set, operating at 41 FPS with a 300x300 input resolution using a single Nvidia 1080 GPU. Notably, DDSSD demonstrates superior performance on small objects, yielding 10.5% on MS COCO and 22.8% on the FLIR thermal dataset, surpassing numerous state-of-the-art object detection algorithms in terms of both accuracy and speed.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0019", "problem_id": "00190001", "content": "Training models with limited data presents significant difficulties, as insufficient data often results in poor generalization. Traditional approaches relying on global pooled representations tend to overlook valuable local details. Recent few-shot learning techniques attempt to mitigate this issue by employing deep descriptors and pixel-level metrics, yet these may fail to capture broader contextual information. Additionally, many existing methods process each support set class in isolation, missing opportunities to leverage discriminative features and task-specific embeddings effectively. To address these limitations, we introduce SSFormers, a Transformer-based architecture that identifies task-relevant features while filtering out irrelevant ones. Our approach begins by partitioning input images into multi-scale patches to extract dense local features, preserving both contextual and localized details. A sparse spatial transformer layer then establishes spatial relationships between query images and the entire support set, selectively emphasizing relevant patches. Finally, a patch matching module computes distances between dense local representations to classify query images within the support set. Comprehensive evaluations on standard few-shot benchmarks demonstrate our method's superior performance. Our implementation is accessible at \\url.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0020", "problem_id": "00200001", "content": "This study proposes a pre-processing technique and a distance metric that enhance the accuracy of machine learning algorithms applied to independent and identically distributed stochastic processes. A new non-parametric method is introduced for representing random variables, which effectively separates dependency and distribution without incurring any information loss. An accompanying metric is also developed, utilizing this representation and its statistical estimation. The efficacy of this contribution is demonstrated not only through experiments on synthetic datasets, but also by applying it to the clustering of financial time series, such as credit default swaps market prices, with results and an IPython Notebook tutorial available at www.datagrapple.com and www.datagrapple.com/Tech, respectively, to facilitate reproducible research.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0021", "problem_id": "00210001", "content": "This paper introduces a new weighted nonlocal total variation (WNTV) approach. Unlike traditional nonlocal total variation methods, the proposed technique incorporates a weighting factor into the energy functional to achieve equilibrium between labeled and unlabeled datasets. The effectiveness and efficiency of WNTV are demonstrated through comprehensive numerical experiments in semi-supervised clustering, image inpainting, and image colorization, showcasing its utility across various image processing and machine learning challenges.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0022", "problem_id": "00220001", "content": "The concept of autonomous restoration of deep-sea coral reefs has recently been introduced to bolster marine ecosystems critical for commercial fishing, tourism, and biodiversity. This approach involves the deployment of numerous small autonomous underwater vehicles (AUVs) paired with swarm intelligence strategies to identify and replace broken coral fragments, thereby facilitating regrowth and preserving habitats. The project's objective is to create machine vision algorithms that enable an underwater robot to detect coral reefs and coral pieces on the ocean floor, guiding the robot to retrieve them. While there is little existing research on this specific issue, studies related to fish counting may provide valuable perspectives. The primary technical obstacles stem from potential water turbidity, maintaining platform stability, and dealing with distracting elements such as rocks, fish, and crabs. We introduce an effective sparse classification for coral species utilizing a supervised deep learning approach known as Convolutional Neural Networks (CNNs). To extract shape and texture feature descriptors, we calculate the Weber Local Descriptor (WLD), Phase Congruency (PC), and apply Zero Component Analysis (ZCA) Whitening, which serve as additional channels (feature-based maps) alongside basic spatial color channels (spatial-based maps) in the coral input images. Furthermore, we assess cutting-edge preprocessing underwater algorithms for image enhancement, color normalization, and adjustment of color conversion. Our coral classification methodology is developed on the MATLAB platform and tested against two distinct coral datasets: the University of California San Diego's Moorea Labeled Corals and Heriot-Watt University's Atlantic Deep Sea.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0023", "problem_id": "00230001", "content": "The creation of High Definition (HD) maps, which provide detailed descriptions of road lanes and associated traffic rules, is essential for various critical components of autonomous driving systems, such as motion forecasting and planning. Nevertheless, the limited availability of real-world road topologies and geometries restricts the ability to comprehensively test and generalize self-driving systems to novel, unseen scenarios. To mitigate this limitation, this study introduces the task of generating HD maps, exploring various autoregressive models that utilize different data representations, including sequences, plain graphs, and hierarchical graphs. The proposed HDMapGen model, a hierarchical graph generation approach, employs a coarse-to-fine methodology to produce diverse, high-quality HD maps. As demonstrated by experiments conducted on the Argoverse dataset and an in-house dataset, HDMapGen exhibits superior performance compared to baseline methods, while also achieving high scalability and efficiency, as shown in Figure A, B, C (References [1], [2], and [3] provide further details).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0024", "problem_id": "00240001", "content": "Text-to-image generation in the broader domain has remained a challenging issue, necessitating a robust generative model along with cross-modal comprehension. We introduce CogView, a 4-billion-parameter Transformer utilizing a VQ-VAE tokenizer to tackle this challenge. Additionally, we illustrate finetuning techniques for a variety of downstream applications, including style learning, super-resolution, text-image ranking, and fashion design, as well as approaches to stabilize pretraining, such as addressing NaN losses. CogView (zero-shot) sets a new benchmark for FID on blurred MS COCO, surpassing previous GAN-based models and a recent comparable project, DALL-E.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0025", "problem_id": "00250001", "content": "This study investigates a self-supervised approach for learning amodal 3D feature representations using posed RGB and RGB-D images and videos, without relying on object or scene semantic information. The generated scene representations are then assessed on visual correspondence, object tracking, and object detection tasks. The proposed model deduces a latent 3D scene representation as 3D feature points, mapping each continuous 3D point in the world to a corresponding feature vector. Training involves contrastive view prediction, where 3D feature clouds are rendered from queried viewpoints and matched against a 3D feature point cloud predicted from the query view. Significantly, the representation allows querying for any 3D location, regardless of its visibility from the input view. Our model integrates three key concepts from recent research: 3D feature grids as a neural bottleneck for view prediction, implicit functions to address resolution limitations of 3D grids, and contrastive learning for unsupervised feature representation training. The results demonstrate that the resulting 3D visual feature representations effectively generalize across objects and scenes, infer occluded or missing information, track objects over time, align semantically related objects in 3D, and enhance 3D object detection. The proposed method surpasses several state-of-the-art techniques for 3D feature learning and view prediction, which are often constrained by 3D grid spatial resolution, fail to construct amodal 3D representations, or struggle with combinatorial scene variability due to their non-convolutional bottlenecks.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0026", "problem_id": "00260001", "content": "This paper examines adversarial attacks targeting monocular depth estimation (MDE) systems built on convolutional neural networks (CNNs). The study has dual objectives: first, to assess the security of MDE systems, an area that has received limited attention, and second, to enhance comprehension of how CNNs process MDE tasks. To achieve this, we adapt a recently introduced MDE visualization technique for defense purposes, training an auxiliary CNN to generate saliency maps from input images. This ensures the primary MDE CNN maintains accurate depth estimation even when non-salient regions are masked. Our key observations reveal that IFGSM (or PGD) attacks successfully disrupt depth predictions, yet masking non-salient pixels mitigates these attacks, suggesting perturbations primarily affect less critical regions. However, saliency map prediction itself proves susceptible to adversarial manipulation, despite not being the primary target. We demonstrate that employing a robust CNN-trained saliency map can effectively counter these attacks. These findings not only propose a viable defense strategy but also offer insights into the computational workings of CNNs in MDE.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0027", "problem_id": "00270001", "content": "Smart home systems enhance residents' quality of life by utilizing sensors and actuators to automate tasks, with many actions relying on activity recognition models. However, imperfect recognition accuracy can cause unexpected behaviors, prompting users to question system decisions. This research leverages Explainable AI (XAI) principles to develop methods for clarifying activity classifications in smart homes. We propose four computational techniques for producing natural language explanations from smart home data and assess their effectiveness. A user study reveals that SHAP-based explanations achieve 92% accuracy, while 84% of participants favor detailed explanations over basic activity labels, highlighting the demand for transparency. Additionally, certain XAI methods influence user trust—some diminish confidence in the recognition model, while others strengthen it. Based on these findings, we identify the most suitable XAI approach for smart home automation and outline directions for future research.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0028", "problem_id": "00280001", "content": "This study presents an innovative methodology for executing transfer learning across different domains and tasks, conceptualizing it as a clustering learning challenge. The principal observation is that, alongside feature transfer, it is feasible to transfer similarity information, which is adequate for learning a similarity function and a clustering network that facilitate both domain adaptation and transfer learning across tasks. We start by transforming categorical data into pairwise constraints, focusing solely on whether pairs of instances are from the same class. This category-agnostic similarity can be derived from data in the source domain via a similarity network. We then introduce two original methods for conducting transfer learning based on this similarity function. Firstly, for unsupervised domain adaptation, we establish a new loss function that combines classification regularization with a constrained clustering loss, enabling the learning of a clustering network that utilizes the transferred similarity metric to generate training data. Secondly, for cross-task learning (i.e., unsupervised clustering of previously unseen categories), we suggest a framework for reconstructing and assessing the number of semantic clusters, again employing the clustering network. Given the inherent noise in the similarity network, the crucial aspect is the application of a robust clustering algorithm, and we demonstrate that our approach is more resilient than conventional constrained and unconstrained clustering methods. Utilizing this technique, we achieve state-of-the-art results on the challenging cross-task scenario with applications to Omniglot and ImageNet, illustrating our capability to accurately reconstruct semantic clusters. Furthermore, we assess cross-domain transfer performance with images from the Office-31 and SVHN-MNIST tasks, achieving top accuracy on both datasets. Notably, our method does not specifically address domain discrepancy, but when combined with a domain adaptation loss, it yields additional enhancements.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0029", "problem_id": "00290001", "content": "The linear relaxation based perturbation analysis (LiRPA) technique, which calculates verifiable linear bounds for output neurons under specific input perturbations, has emerged as a crucial element in robustness verification and certified defense for neural networks. While existing LiRPA-based methods primarily focus on straightforward feed-forward networks, requiring customized derivations and implementations for other architectures, this paper presents an automated framework that extends perturbation analysis to any neural network structure by adapting algorithms like CROWN to operate on general computational graphs. Our framework's flexibility, differentiability, and user-friendliness enable state-of-the-art performance in LiRPA-based certified defense for complex networks, including DenseNet, ResNeXt, and Transformer, which were previously unsupported. Additionally, our framework facilitates loss fusion, significantly reducing the computational complexity of LiRPA for certified defense, and demonstrates the first successful application of LiRPA-based certified defense on Tiny ImageNet and Downscaled ImageNet, which were inaccessible to prior approaches due to their large number of classes. Our work also results in an open-source library, available at https://github.com/KaidiXu/auto_LiRPA, allowing the community to apply LiRPA to various areas beyond certified defense without requiring extensive LiRPA expertise, such as creating neural networks with potentially flat optimization landscapes by applying LiRPA to network parameters.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0030", "problem_id": "00300001", "content": "The edge computing paradigm involves deploying compute-capable devices, known as edge servers, at the network edge to facilitate the execution of data analysis tasks on mobile devices. Offloading computationally intensive tasks to edge servers can potentially reduce execution times, but may be hindered by poor wireless channel conditions between mobile devices and edge servers, leading to increased capture-to-output delays. This study explores the application of edge computing to remote object detection using Deep Neural Networks (DNNs) and proposes a framework to minimize data transmission over the wireless link. By dividing DNNs into two sections, namely head and tail models, executed on mobile devices and edge servers respectively, the proposed approach transmits the output of the head model's last layer instead of the DNN input. Unlike previous work, which focuses on classification tasks with unaltered DNN structures, this study concentrates on object detection tasks with complex DNN structures and introduces modifications to achieve in-network compression and prefiltering of irrelevant images. The results demonstrate that the proposed technique offers a viable intermediate solution between local and edge computing, particularly in scenarios where extreme point solutions fail to deliver satisfactory performance, as shown in Figure A, B, C (see References [citation] for details).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0031", "problem_id": "00310001", "content": "Although generative adversarial networks (GANs) have become increasingly popular, the optimization processes within them remain largely unclear. This study examines GAN optimization through the lens of gradient descent, a common approach involving simultaneous, incremental adjustments to generator and discriminator parameters. We demonstrate that, even when GAN optimization deviates from a convex-concave game scenario (even with basic parameterizations), its equilibrium points, under specific conditions, align with the conventional GAN framework. Conversely, we reveal that the more recently developed Wasserstein GAN may exhibit non-convergent cyclic behavior near equilibrium. Inspired by this stability analysis, we introduce a novel regularization term to enhance gradient descent GAN updates. This term ensures local stability for both the WGAN and the traditional GAN, while also demonstrating potential for accelerating convergence and mitigating mode collapse.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0032", "problem_id": "00320001", "content": "The detection of objects from images captured by Unmanned Aerial Vehicles (UAVs) has gained significant importance, yet applying conventional object detection methods, which are typically trained on ground-to-ground images, results in substantial performance degradation due to various UAV-specific challenges. These challenges, including diverse flying altitudes, unfavorable weather conditions, and dynamic viewing angles, create a multitude of fine-grained domains that require the detection model to maintain robustness. However, UAVs often record metadata that describe these varying attributes, which can be readily obtained or are freely available alongside the UAV images. To address this issue, we introduce the Nuisance Disentangled Feature Transform (NDFT) framework, an adversarial training approach that leverages the available metadata in conjunction with UAV images to learn robust features, thereby enhancing the model's resilience to UAV-specific nuisances. Our proposed algorithm achieves state-of-the-art performance on two existing UAV-based object detection benchmarks, demonstrating its effectiveness, and the code is available at https://github.com/TAMU-VITA/UAV-NDFT.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0033", "problem_id": "00330001", "content": "Many machine learning tasks require a significant level of invariance, meaning that the data distribution remains unchanged when the data is subjected to specific transformation groups. For example, the labels of images remain invariant when the images are translated. It is commonly thought that certain neural network architectures, such as convolutional networks, gain their effectiveness from leveraging these invariance properties. To assess the benefits conferred by invariant architectures, we propose two categories of models: invariant random features and invariant kernel methods, with the latter encompassing the neural tangent kernel as a specific case for convolutional networks utilizing global average pooling. Our focus includes uniform distributions of covariates on both the sphere and hypercube, alongside a general invariant target function. We analyze the test error of invariant methods within a high-dimensional context where both sample size and the count of hidden units grow polynomially with the dimension, specifically for a set of groups termed 'degeneracy α', where α is less than or equal to 1. Our findings indicate that by incorporating invariance in the architecture, a reduction factor of d^α (where d denotes the dimension) in sample size and hidden units is achieved while maintaining the same test error level as unstructured architectures. Additionally, we demonstrate that symmetrizing the output of an unstructured kernel estimator does not significantly enhance statistical performance; conversely, employing data augmentation with an unstructured kernel estimator is equivalent to using an invariant kernel estimator, thus benefiting from similar enhancements in statistical efficiency.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0034", "problem_id": "00340001", "content": "This study addresses the challenge of reconstructing high-resolution light field (LF) images using hybrid lenses, which consist of a high-resolution camera complemented by several low-resolution cameras. To solve this issue, we introduce a cutting-edge end-to-end learning-based methodology that effectively leverages the distinct features of the input captured from two complementary, parallel views. One component of our approach focuses on deriving a spatially consistent intermediate estimate through the learning of deep multidimensional and cross-domain feature representations, while the other module generates an intermediate estimate that preserves high-frequency textures by disseminating information from the high-resolution perspective. We ultimately combine the strengths of the two intermediate estimates using the learned attention maps, resulting in the final high-resolution LF image. Comprehensive experiments validate the marked superiority of our approach compared to existing state-of-the-art methods, with our technique achieving over a 2 dB improvement in PSNR and significantly better preservation of LF structure. To our knowledge, this represents the first end-to-end deep learning technique for reconstructing high-resolution LF images from a hybrid input. We believe that our framework may reduce the costs associated with acquiring high-resolution LF data and can also enhance LF data storage and transmission efficiency. The code is accessible at https://github.com/jingjin25/LFhybridSR-Fusion.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0035", "problem_id": "00350001", "content": "We introduce a video story question-answering (QA) framework known as Multimodal Dual Attention Memory (MDAM). The central concept involves employing a dual attention mechanism alongside late fusion. MDAM utilizes self-attention to capture the underlying concepts within scene frames and their corresponding captions. When presented with a question, it applies a secondary attention mechanism to these identified concepts. Multimodal fusion occurs subsequent to the dual attention steps (late fusion). This processing approach enables MDAM to derive a comprehensive vision-language joint representation from an abstraction of the entire video content. We assess MDAM's performance on the PororoQA and MovieQA datasets, which feature extensive QA annotations for animated videos and films, respectively. MDAM achieves groundbreaking results on both datasets, outperforming competing models by substantial margins. Our ablation studies validate the effectiveness of the dual attention mechanism in conjunction with late fusion. Furthermore, we conduct a qualitative analysis by visualizing the inference processes of MDAM.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0036", "problem_id": "00360001", "content": "Generative adversarial networks (GANs) have become a prominent framework for image and video synthesis, facilitating the creation of visual content both unconditionally and based on specific inputs. GANs now allow for the generation of photorealistic, high-resolution images and videos, overcoming limitations of previous approaches and opening doors to novel content creation applications. This paper presents a comprehensive review of GANs, emphasizing algorithms and applications related to visual synthesis. We explore key methodologies for stabilizing GAN training, addressing its known challenges, and examine applications in image translation, image processing, video synthesis, and neural rendering.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0037", "problem_id": "00370001", "content": "This paper introduces Detective, an attentive object detection system designed to sequentially identify objects within images. The architecture comprises a convolutional neural network encoder and a convolutional recurrent neural network decoder enhanced with an attention mechanism. The decoder iteratively concentrates on pertinent image regions via the attention mechanism, subsequently predicting the object's class and bounding box coordinates. Unlike current dense prediction models that necessitate post-processing for duplicate removal, Detective is a sparse detector, producing a unique bounding box for each object instance. To address the training challenges inherent in sparse detection, which demands instance-level reasoning, we introduce a Hungarian algorithm-based training mechanism alongside a loss function that harmonizes localization and classification. Detective demonstrates encouraging performance on the PASCAL VOC dataset. Experimental results validate the feasibility of sparse object detection and highlight its potential for future applications where prediction order is significant.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0038", "problem_id": "00380001", "content": "Current reinforcement learning algorithms achieve exceptional performance in various board and video games, surpassing human capabilities, but they often suffer from sample inefficiency, requiring substantially more experience to attain comparable performance levels to humans. Enhancing sample efficiency can be accomplished by enabling an agent to construct an environmental model and leverage planning techniques to refine its policy. This article presents Variational State Tabulation (VaST), a novel approach that transforms complex, high-dimensional state spaces, such as those involving visual inputs, into abstract tabular representations. By utilizing prioritized sweeping with small backups, a highly efficient planning method, VaST facilitates the updating of state-action values. The effectiveness of VaST is demonstrated through its rapid learning and adaptation capabilities in tasks such as 3D navigation, as well as its ability to efficiently respond to sudden changes in reward structures or transition probabilities.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0039", "problem_id": "00390001", "content": "Bayesian optimisation has proven effective in numerous reinforcement learning applications. Nevertheless, standard methods for acquiring optimal policies in simulated environments fail to leverage adjustable environment variables—state features that are typically unobservable and stochastic in real-world scenarios but can be manipulated within a simulator—to enhance learning. This study addresses the challenge of identifying a resilient policy by explicitly modelling the influence of these environment variables. To this end, we introduce Alternating Optimisation and Quadrature (ALOQ), a technique that combines Bayesian optimisation and Bayesian quadrature. ALOQ demonstrates robustness against significant, infrequent events, which are often undetectable through random sampling yet critically affect the determination of the optimal policy. Empirical evaluations across diverse domains demonstrate that ALOQ achieves superior learning efficiency and robustness compared to existing approaches.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0040", "problem_id": "00400001", "content": "The performance of recent reinforcement learning algorithms is hindered by fragile training dynamics, including result regression and heightened sensitivity to initialization and parameter settings. We attribute some of this fragility to discrepancies in variance across different environmental states and actions, which can lead to two key issues. Firstly, the \"Boring Areas Trap\" affects algorithms like Q-learning, where transitions between areas are influenced by the current area's variance, making it difficult to escape areas with low variance. Secondly, the \"Manipulative Consultant\" problem arises in DQN and Actor-Critic algorithms, where value-estimation functions prioritize precision over reward maximization, causing agents to prefer areas with low variance regardless of the potential rewards. This highlights the crucial role of exploration in training, as it can mitigate both challenges. Interestingly, cognitive experiments in humans have shown that introducing noise into reward signals can counterintuitively enhance performance, which can be explained by the aforementioned problems. We propose the Adaptive Symmetric Reward Noising (ASRN) method, which involves adding Gaussian noise to rewards based on their estimated state variance, thereby addressing the two problems without altering the environment's mean reward behavior. Our experiments, conducted on a Multi Armed Bandit problem with variance differences, demonstrate the brittleness effect in Q-learning and the significant improvement achieved with ASRN. Furthermore, ASRN is shown to enhance the training process of a DQN algorithm in an end-to-end autonomous driving task using the AirSim driving simulator, Figure A, B, C, as seen in References [citation].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0041", "problem_id": "00410001", "content": "Explainable artificial intelligence has increasingly become a focus of research in recent years. Current approaches often rely on gradients or intermediate features that do not directly influence the classifier's decision-making. This paper introduces SCOUTER, a slot attention-based classifier designed for both transparency and high accuracy. Unlike other attention-based techniques, SCOUTER differs in two key aspects: (a) its explanations contribute to the final confidence scores for each class, enabling clearer interpretation, and (b) it provides both positive and negative explanations for all categories, clarifying why an image belongs to or does not belong to a specific class. A specialized loss function is developed for SCOUTER to regulate the model's ability to toggle between positive and negative explanations while managing the extent of explanatory regions. Evaluations demonstrate that SCOUTER delivers superior visual explanations across multiple metrics while maintaining strong performance on small and medium-sized datasets.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0042", "problem_id": "00420001", "content": "Learning a joint embedding for 3D shapes and text is crucial for various tasks related to shape understanding, including shape-text matching, retrieval, and captioning. Existing multi-view approaches develop mappings from multiple rendered perspectives to text; however, they struggle with 3D shape analysis due to self-occlusion and constraints in manifold learning. To address this challenge, we propose a novel method that learns a joint embedding of point clouds and text by correlating segments of shapes with words in a shared space. Initially, we extract segmentation to divide point clouds into meaningful parts. Subsequently, we position these parts and words into an optimized space that facilitates their mutual matching. Within this optimized environment, we characterize a part by aggregating the features of all its constituent points, while each word is represented by its contextual information. Our network is trained to minimize triplet ranking loss to enhance this representation. Additionally, we incorporate cross-modal attention mechanisms to better capture the part-word relationships during the matching process, thereby improving joint embedding learning. Our experimental findings surpass the current state-of-the-art in multi-modal retrieval on widely recognized benchmarks.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0043", "problem_id": "00430001", "content": "A typical approach in data exploration involves obtaining a low-dimensional representation of the data, recognizing clusters of points within that representation, and analyzing the distinctions among these clusters to understand their meanings. We frame this approach as a problem in interpretable machine learning by utilizing the model that generated the low-dimensional representation to assist in pinpointing the significant differences between the clusters. To address this issue, we propose a novel type of explanation known as a Global Counterfactual Explanation (GCE), along with our algorithm, Transitive Global Translations (TGT), for deriving GCEs. TGT determines the variations between each group pair through compressed sensing while ensuring that these pairwise differences remain consistent across all groups. Through empirical evaluation, we show that TGT can effectively identify explanations that accurately represent the model, are relatively sparse, and correspond to genuine patterns in the data.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0044", "problem_id": "00440001", "content": "Latent factor models serve as the foundation for cutting-edge recommender systems, offering a crucial advantage by transforming raw input features into dense embeddings. Typically, the dimensions of various feature embeddings are predetermined to be the same based on empirical considerations, which can limit the predictive capabilities of these models. Previous research has attempted to address this through heuristic or reinforcement learning methods that explore mixed feature embedding dimensions. However, these approaches often restrict embedding dimensions to a narrow set of candidates for efficiency, resulting in reduced flexibility in dimension selection and ultimately yielding suboptimal search outcomes. In this paper, we introduce Differentiable Neural Input Search (DNIS), which enables a more flexible exploration of mixed feature embedding dimensions using continuous relaxation and differentiable optimization techniques. Central to our approach is a soft selection layer that regulates the importance of each embedding dimension, which is optimized based on the model's validation performance. DNIS is compatible with a variety of existing latent factor models, allowing for easy integration. We perform experiments with diverse architectures of latent factor models on three publicly available real-world datasets, focusing on rating prediction, Click-Through-Rate (CTR) prediction, and top-k item recommendation. Our findings indicate that DNIS outperforms existing neural input search methods in terms of predictive accuracy while utilizing fewer embedding parameters and requiring less computational time.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0045", "problem_id": "00450001", "content": "Partial monitoring serves as a broad framework for sequential decision-making under limited feedback, covering diverse problems like dueling bandits, learning with expert advice, dynamic pricing, dark pools, and label-efficient prediction. We analyze the utility-based dueling bandit problem as a case of partial monitoring, demonstrating its classification as an easy instance with a Theta (sqrt) regret bound within the time-regret hierarchy. Additionally, we review existing partial monitoring algorithms and explore their potential for efficiently addressing dueling bandit scenarios. Keywords: Online learning, Dueling Bandits, Partial Monitoring, Partial Feedback, Multiarmed Bandits.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0046", "problem_id": "00460001", "content": "Event cameras, inspired by biological vision, offer benefits like data sparsity, high temporal resolution, low latency, and reduced power consumption due to their activity-driven nature. Since event cameras possess a unique sensing modality and conventional vision provides high quality, event processing typically involves converting sparse, asynchronous events into 2D grids before applying standard vision pipelines. Although supervised learning methods show promise in 2D grid generation, they require labeled task-specific ground truth event data, which is difficult to obtain. To address this, we introduce Event-LSTM, an unsupervised Auto-Encoder architecture using LSTM layers, as a potential method for learning 2D grid representations from event sequences. Unlike supervised methods, our task-agnostic approach is well-suited for event data, where labeled data is limited. Our solution is designed to leverage the asynchronous nature of event streams, enabling speed invariant and energy-efficient 2D grid generation. Furthermore, we enhance event de-noising by incorporating memory into the process. Evaluations on activity recognition and gesture recognition reveal that our approach outperforms existing methods and offers the flexibility to learn from unlabeled data.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0047", "problem_id": "00470001", "content": "In autonomous driving, three-dimensional perception using sensors adhering to vehicle industry standards is essential. Micro-Electro-Mechanical Systems (MEMS) LiDAR is becoming increasingly popular due to its cost-effectiveness, robustness, and suitability for mass production. However, its limited field of view (FoV) hinders its widespread adoption. This paper introduces LiDAR Extender for Autonomous Driving (LEAD), a method to enhance MEMS LiDAR capabilities by integrating image data to expand both FoV and range. We present a multi-stage propagation approach based on depth distributions and uncertainty maps, demonstrating effective propagation capabilities. Furthermore, our depth outpainting/propagation network employs a teacher-student training methodology, enabling the transfer of depth estimation skills to the depth completion network without introducing scale errors. To assess the LiDAR extension quality, we created a ground-truth dataset using a high-precision laser scanner. Quantitative and qualitative evaluations demonstrate that our approach significantly surpasses state-of-the-art methods. We anticipate that the proposed LEAD, along with the dataset, will be valuable to the research community in the field of depth perception.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0048", "problem_id": "00480001", "content": "Zero-shot learning, which focuses on identifying unseen classes without prior training examples, has been extensively studied for 2D image classification. However, the analogous 3D point cloud classification task, despite the growing prevalence of 3D sensors, remains relatively unexplored and presents unique difficulties. This paper elucidates these challenges and evaluates the efficacy of existing 2D Zero-Shot Learning (ZSL) techniques when applied to 3D data. Furthermore, we introduce a novel method tailored to address the specific issues in 3D ZSL. Initially, an inductive ZSL process is presented, which is subsequently expanded to transductive ZSL and Generalized ZSL (GZSL) scenarios for 3D point cloud classification. A new loss function is developed to simultaneously align semantic information with point cloud characteristics and leverages unlabeled test data to mitigate domain adaptation, hubness, and data bias issues. Although designed for 3D point cloud classification, the method's applicability extends to 2D image classification. Comprehensive experiments demonstrate state-of-the-art ZSL and GZSL performance on both synthetic (ModelNet40, ModelNet10, McGill) and real (ScanObjectNN) 3D point cloud datasets.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0049", "problem_id": "00490001", "content": "Stochastic Neighbor Embedding (SNE) is a dimensionality reduction technique rooted in manifold learning that utilizes a probabilistic framework. SNE operates by treating each data point as a potential neighbor to all others, assigning probabilities that it then seeks to maintain within the lower-dimensional embedding space. The original SNE formulation employs Gaussian distributions to model these probabilities in both the original high-dimensional space and the resulting embedding space. In contrast, t-SNE utilizes a Student-t distribution in the embedding space while retaining the Gaussian distribution in the original space. This paper serves as both a tutorial and survey, detailing SNE, symmetric SNE, t-SNE (or Cauchy-SNE), and t-SNE with varying degrees of freedom. Furthermore, it explores out-of-sample extensions and acceleration techniques applicable to these methods. Visual embedding examples are provided through illustrative simulations.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0050", "problem_id": "00500001", "content": "We present a comprehensive probabilistic framework designed to address sequential decision-making challenges, including Bayesian optimization, contextual bandits, and reinforcement learning. This approach employs a model-based probabilistic method that interprets observed data while accounting for predictive uncertainty throughout decision-making. A key feature is the integration of a Meta-Learning system, enabling adaptation across a distribution of related tasks for efficient data utilization in target scenarios. Neural processes are investigated as a viable implementation of this framework due to their statistical and computational advantages. The framework is tested across diverse applications, such as control tasks, recommendation systems, and adversarial attacks on RL agents, showcasing its versatility and effectiveness as a generalized black-box learning solution.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0051", "problem_id": "00510001", "content": "This paper introduces Pix2Seq, a straightforward and versatile framework for object detection. Diverging from current methodologies that incorporate task-specific prior knowledge, we formulate object detection as a language modeling problem based on pixel input. Object descriptions, such as bounding boxes and class labels, are represented as discrete token sequences, and a neural network is trained to interpret the image and produce the corresponding sequence. Our method is predicated on the idea that if a neural network possesses knowledge of object location and identity, the remaining task is to train it to articulate this information. With the exception of task-specific data augmentation, our approach relies on minimal assumptions, yet it attains competitive performance on the demanding COCO dataset when benchmarked against highly specialized and optimized detection algorithms.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0052", "problem_id": "00520001", "content": "The availability of sufficient data is crucial for the performance of deep learning systems, and this issue is particularly pronounced in the medical imaging domain, especially when dealing with pathologies, due to the limited number of cases and significant variations in terms of location, scale, and appearance. This study explores the potential of enhancing dataset robustness for pathological lung segmentation in CT scans by augmenting it with artificially generated lung nodules, with a focus on improving the progressive holistically nested network (P-HNN) model. To accomplish this, a 3D generative adversarial network (GAN) is developed to learn the distribution of lung nodule properties in 3D space, conditioned on a volume of interest with the central nodule region removed, and incorporating a novel multi-mask reconstruction loss to enhance realism and integration with the background. The method is trained on over 1000 nodules from the LIDC dataset, and qualitative results show its effectiveness compared to state-of-the-art approaches. Furthermore, the GAN is utilized to generate simulated training images featuring nodules at the lung border, a challenging scenario for the published P-HNN model, and both qualitative and quantitative results demonstrate that the P-HNN model's segmentation capabilities are improved when trained with these simulated images, thereby offering a promising solution to address the common issue of data scarcity in medical imaging.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0053", "problem_id": "00530001", "content": "Detecting curved text presents difficulties because of its diverse sizes and non-uniform shapes. This study examines the limitations of current curve detection techniques and introduces a novel Conditional Spatial Expansion (CSE) approach to enhance curve text detection accuracy. Rather than framing curve text detection as a segmentation task or polygon regression, we approach it as a progressive region expansion process. The CSE method begins with a randomly placed seed within a text area and incrementally combines adjacent regions by leveraging CNN-extracted local features and contextual data from merged regions. Highly adaptable, the CSE mechanism can be effortlessly incorporated into existing object detection frameworks. By utilizing this data-driven technique, our system achieves reliable instance-level text extraction with minimal post-processing. Experimental analysis demonstrates that the CSE effectively processes text of varying shapes, sizes, and orientations while reducing false positives caused by text-like patterns or unintended text within the same region of interest. Our approach outperforms existing curve text detection methods in robustness and simplicity, achieving a state-of-the-art F-score of 78.4% on benchmark datasets.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0054", "problem_id": "00540001", "content": "Current methodologies for graph neural networks frequently encounter the oversmoothing challenge, irrespective of the neighborhood aggregation techniques employed. Additionally, many of these methods concentrate on transductive settings with static graphs, resulting in limited generalization capabilities for previously unseen graphs. To counter these challenges, we introduce a novel graph neural network that incorporates both edge-oriented neighborhood dynamics and node-centric entity characteristics, referred to as Graph Entities with Step Mixture via random walk (GESM). GESM utilizes a combination of various steps in random walks to reduce the oversmoothing issue, applies attention mechanisms that adaptively reflect interconnections based on node data, and introduces structure-based regularization to improve embedding quality. Through extensive experimental evaluation, we demonstrate that GESM achieves either state-of-the-art or comparable results across eight benchmark graph datasets that include both transductive and inductive learning tasks. Moreover, we provide empirical evidence highlighting the importance of integrating global information.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0055", "problem_id": "00550001", "content": "The presence of nondeterminism in neural network optimization leads to performance uncertainty, which can obscure the detection of minor improvements amidst variability between runs. Although training multiple models can mitigate this uncertainty, it is a time-consuming and costly approach that compromises reproducibility. This study introduces an experimental framework to investigate the impact of optimization nondeterminism on model diversity, enabling the isolation of various nondeterminism sources. Notably, our findings indicate that different sources of nondeterminism exert similar influences on model diversity metrics. We attribute this phenomenon to the inherent instability of the model training process, where even minimal changes in initial parameters can result in drastically different convergence values, as seen in Figure A (Reference [1]). To address this issue, we propose two methods for reducing the effects of instability on run-to-run variability, as illustrated in Figure B (Reference [2]), and discussed in detail in Figure C (Reference [3]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0056", "problem_id": "00560001", "content": "Self-supervised learning utilizes inherent data characteristics to acquire representations; however, datasets may include information detrimental to subsequent tasks. For example, gender-related data can introduce bias in tasks where gender is irrelevant. This paper introduces conditional contrastive learning as a means of eliminating unwanted information from self-supervised representations. Our method conditions on the undesirable variable during contrastive learning—effectively neutralizing its variations—to mitigate its influence. Specifically, drawing inspiration from the InfoNCE contrastive objective, we present Conditional InfoNCE (C-InfoNCE) and its computationally optimized version, Weak-Conditional InfoNCE (WeaC-InfoNCE), for conditional contrastive learning. Empirical results demonstrate that our techniques effectively learn self-supervised representations suitable for downstream tasks, while significantly reducing the influence of undesirable variables. We examine three distinct scenarios, each involving a different type of undesirable variable: task-irrelevant meta-information in self-supervised speech representation learning, sensitive attributes in fair representation learning, and domain specification in multi-domain visual representation learning.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0057", "problem_id": "00570001", "content": "The field of media forensics has garnered significant attention in recent years, largely due to growing concerns surrounding DeepFakes. The evolution of DeepFake databases, from the first generation, including UADFV and FaceForensics++, to the second generation, such as Celeb-DF and DFDC, has led to substantial visual enhancements, rendering fake videos nearly imperceptible to the human eye. This research undertakes a comprehensive examination of both first and second-generation DeepFakes, focusing on facial regions and fake detection efficacy. Our experimental framework employs two approaches: the conventional method, which utilizes the entire face as input for fake detection, and a novel method, which selects specific facial regions as input. Notably, our experiments reveal that even the most advanced state-of-the-art fake detectors yield poor results on the latest second-generation DeepFake databases, with Equal Error Rate results spanning 15% to 30%, underscoring the need for further research into developing more sophisticated fake detection systems, as seen in Figure A, B, C (References [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0058", "problem_id": "00580001", "content": "We introduce a new group collaborative learning approach (GCoNet) that efficiently detects co-salient objects in real time (16ms) by jointly extracting consensus representations at the group level based on two essential principles: 1) intra-group compactness, which enhances consistency modeling among co-salient objects through a novel group affinity module that captures their shared characteristics, and 2) inter-group separability, which minimizes noise interference using a proposed group collaborating module that conditions inconsistent consensus. To improve the embedding space without additional computational cost, auxiliary classification supervision is explicitly integrated. Comprehensive evaluations on three challenging datasets—CoCA, CoSOD3k, and Cosal2015—show that our straightforward GCoNet surpasses 10 state-of-the-art methods, setting a new benchmark. Additionally, we highlight the framework’s novel technical contributions in key downstream applications such as content-aware co-segmentation and co-localization-based automatic thumbnail generation.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0059", "problem_id": "00590001", "content": "Following the remarkable achievements of deep learning in visual applications, the deep features extracted from the intermediate layers of trained models have garnered significant interest among researchers. Prior empirical investigations indicate that these features can encapsulate relevant semantic information. Consequently, when utilizing a model that has been trained on a large-scale benchmark dataset (such as ImageNet), the extracted features tend to perform effectively on other tasks. This study explores this phenomenon and reveals that deep features may be suboptimal because they are derived from minimizing empirical risk. In scenarios where the data distribution for the target task diverges from that of the benchmark dataset, the effectiveness of deep features can diminish. To address this issue, we introduce a hierarchically robust optimization approach aimed at learning more generalizable features. By simultaneously considering both example-level and concept-level robustness, we frame the problem as a distributionally robust optimization challenge using Wasserstein ambiguity set constraints, along with proposing an efficient algorithm integrated into the standard training pipeline. Experiments on benchmark datasets validate the efficacy of these robust deep representations.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0060", "problem_id": "00600001", "content": "Image classification models often rely on various semantic attributes within an image, necessitating explanations that identify and illustrate these features. We introduce StylEx, an approach that trains a generative model to explicitly reveal multiple attributes influencing classifier decisions. While StyleGAN's StyleSpace inherently captures meaningful semantic dimensions, standard GAN training may overlook classifier-relevant attributes, leading to irrelevant StyleSpace dimensions. To address this, we develop a classifier-guided StyleGAN training method to create a classifier-specific StyleSpace, from which explanatory attributes are selected. These attributes enable visualization of how altering multiple features per image affects classification, offering tailored explanations. Evaluated across domains such as animals, leaves, faces, and retinal images, StylEx demonstrates the ability to modify images in diverse ways to shift classifier predictions. The approach identifies semantically aligned attributes, produces interpretable image-specific explanations, and achieves human-understandable results, as validated through user studies.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0061", "problem_id": "00610001", "content": "We introduce an Auto-Parsing Network (APN) designed to identify and leverage latent tree structures within input data to enhance the performance of Transformer-based vision-language models. The approach integrates a Probabilistic Graphical Model (PGM), parameterized by attention operations, into each self-attention layer to enforce sparsity constraints. This PGM softly partitions an input sequence into clusters, treating each cluster as a parent node for its constituent elements. Through stacking these PGM-constrained self-attention layers, lower-level clusters merge into a new sequence, which higher layers further segment. This iterative process implicitly constructs a sparse tree, embedding its hierarchical structure into the transformed representations for downstream vision-language tasks. We demonstrate APN’s effectiveness in improving Transformer models for key tasks such as Captioning and Visual Question Answering. Additionally, we develop a PGM probability-based parsing algorithm to uncover the underlying input structure during inference.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0062", "problem_id": "00620001", "content": "Numerous historical figures are represented solely through old, faded black-and-white photographs, which have suffered from the constraints of early photographic technology and the effects of time. This study simulates the experience of revisiting the past with a contemporary camera to rephotograph well-known figures. In contrast to traditional image restoration techniques that perform distinct processes such as denoising, colorization, and superresolution independently, we utilize the StyleGAN2 framework to map vintage photographs into the realm of modern high-resolution images, effectively integrating all these enhancements within a single framework. A distinct challenge posed by this method is maintaining the identity and pose of the subject rather than focusing on the various imperfections present in low-quality antique images. Our evaluations against existing leading restoration filters demonstrate marked enhancements and impressive results for a range of significant historical individuals.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0063", "problem_id": "00630001", "content": "Recent approaches in tracking-by-detection leverage deep object detectors as a foundation for target detection, capitalizing on their strong performance with static images. For successful video object tracking, these detection methods are combined with a data association component, which can either be a specially designed inference architecture or a cohesive end-to-end training process aimed at tracking. In this study, we utilize the former method by employing the pre-trained Mask R-CNN as our foundational detector. We propose a new inference architecture built on the FPN-ResNet101 backbone of Mask R-CNN, enabling simultaneous detection and tracking without necessitating additional training for tracking purposes. The single object tracker we present, TDIOT, utilizes a temporal matching strategy based on appearance similarity for data association. To address issues of tracking interruptions, we integrate a local search and matching module in the inference head that leverages SiamFC for short-term tracking. Furthermore, to enhance robustness against scale variations, we introduce a scale adaptive region proposal network that dynamically searches for the target within an adaptively enlarged area defined by the target's trajectory. To fulfill the needs of long-term tracking, we add a low-cost verification layer to the inference architecture to assess the target's presence using its LBP histogram model. Evaluation results on videos from the VOT2016, VOT2018, and VOT-LT2018 datasets indicate that TDIOT surpasses the accuracy of leading short-term trackers, while providing competitive performance in long-term tracking scenarios.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0064", "problem_id": "00640001", "content": "Bagging, or bootstrap aggregation, is a widely used ensemble technique in machine learning (ML), where multiple hypotheses are merged to create a unified predictive model. This approach trains diverse classifiers on various subsets of a dataset and integrates them into a single robust classifier. Financial institutions, particularly in retail banking, leverage ML techniques like decision trees and random forests to enhance operational efficiency. However, regulatory compliance and governance impose strict requirements, making the implementation of reliable ML solutions complex. The process involves internal validation by the bank's governance team, deployment in a live environment, and external scrutiny by financial authorities, necessitating transparent justification for algorithmic decisions. To address this, we introduce XtracTree, a method that transforms bagging classifiers, such as random forests, into interpretable \"if-then\" rules compliant with validation standards. Testing on a Kaggle loan dataset confirms that XtracTree simplifies model validation for regulators and banks while cutting AI solution deployment time by 50% in our institution.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0065", "problem_id": "00650001", "content": "In computational color constancy, deriving the illumination of a scene from object appearance representations is a commonly used approach. However, challenges persist due to inherent ambiguities in appearance and labeling that arise from unknown light sources, varying material reflection properties, and extrinsic imaging variables, such as differing camera sensors. This paper presents an innovative algorithm known as Cascading Convolutional Color Constancy (C4), designed to enhance the resilience of regression learning and facilitate stable generalization across diverse datasets encompassing various cameras and scenes within a cohesive framework. The C4 method integrates a sequence of interdependent illumination hypotheses at each cascade level by employing a weighted multiply-accumulate loss function, which adeptly captures multiple illumination modes and imposes a coarse-to-fine optimization strategy on the network. Experimental findings on the public Color Checker and NUS 8-Camera benchmarks reveal that the proposed algorithm outperforms state-of-the-art methods, particularly in challenging scene scenarios.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0066", "problem_id": "00660001", "content": "The use of screening mammograms is widely regarded as the most effective method for early detection of breast cancer. Although significant research has been conducted on the classification of mammography images, particularly using deep neural networks, there is a notable lack of investigation into quantifying the confidence or uncertainty associated with these classifications. This paper introduces a novel evaluation metric for breast cancer screening, based on confidence measurement, which utilizes a modular network architecture comprising a traditional neural network for feature extraction via transfer learning, followed by a simple Bayesian neural network. By adopting a two-stage approach, the proposed framework achieves reduced computational complexity, rendering it more suitable for widespread implementation. The framework's adaptability is further enhanced by allowing medical practitioners to adjust two key hyperparameters of the Bayesian neural network, specifically the fraction of sampled networks and the minimum probability. Rather than relying solely on accuracy as an evaluation metric, the proposed framework employs a tuple comprising accuracy, coverage, sampled networks, and minimum probability. Experimental results on the CBIS-DDSM dataset demonstrate the tradeoff between accuracy and coverage when tuning these hyperparameters, and also show that confidence tuning yields increased accuracy with a reduced set of high-confidence images compared to baseline transfer learning. To facilitate deployment, the paper includes anonymized source code with reproducible results, available at https://git.io/JvRqE, Figure A, B, C, and as reported in References [citation].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0067", "problem_id": "00670001", "content": "A new area of scientific computing has developed, treating numerical error as a type of epistemic uncertainty that can be statistically modeled. This development introduces statistical challenges, particularly in creating methods for consistent probability propagation through computational workflows, even deterministic ones. This paper investigates the applicability of probabilistic numerical methods in standard statistical computations, focusing on numerical integration. Specifically, a probabilistic integrator is used to produce a complete distribution over its output, accounting for the inherent unknown numerical error. The primary technical achievement is the initial determination of posterior contraction rates for these methods, demonstrating the potential of probabilistic integrators to combine the sampling efficiency of Monte Carlo methods with a structured approach to evaluating the influence of numerical error on scientific findings. Several significant applications, including examples from statistical modeling, computer graphics, and an oil reservoir computer model, are presented to illustrate and critically assess the proposed methods.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0068", "problem_id": "00680001", "content": "This paper introduces an innovative image inpainting approach that leverages frequency domain data. Existing techniques primarily rely on spatial domain information to train neural networks for predicting missing pixels, yet they often fail to accurately restore high-frequency details in complex real-world scenes, resulting in color inconsistencies, boundary artifacts, pattern distortions, and blurred textures. To address these limitations, we explore whether incorporating frequency domain information—specifically through the Discrete Fourier Transform—alongside spatial data can enhance performance. We propose a frequency-aware deconvolution module designed to capture global context while selectively restoring high-frequency elements. Our method is tested on the CelebA, Paris Streetview, and DTD texture datasets, demonstrating superior qualitative and quantitative results compared to leading image inpainting techniques.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0069", "problem_id": "00690001", "content": "In this study, we applied the GrowCut segmentation algorithm, which is publicly accessible in three-dimensional Slicer, for the segmentation of vertebral bodies in three dimensions. To our knowledge, this marks the inaugural investigation of the GrowCut method for vertebral body segmentation. In summary, our findings indicated that the segmentation times using GrowCut were consistently shorter than those for manual segmentation. Therefore, GrowCut offers a viable alternative to traditional manual segmentation performed slice by slice.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0070", "problem_id": "00700001", "content": "The rapid growth of automatic image analysis has underscored the need for rigorous algorithm validation, yet recent meta-research has exposed significant shortcomings in this area. Performance metrics are crucial for evaluating the objective, transparent, and comparative performance of image analysis algorithms, but the practical challenges associated with selecting appropriate metrics for specific tasks have received insufficient attention. To address this issue, several international initiatives have been launched with the goal of providing researchers with problem-aware guidelines and tools for choosing performance metrics. This living document aims to highlight the key limitations of commonly used performance metrics in image analysis, with its current version being the result of a Delphi process on metrics undertaken by a global consortium of experts in the field, as reported in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0071", "problem_id": "00710001", "content": "Reinforcement learning (RL) has shown its capacity to tackle complex, high-dimensional tasks by utilizing non-linear function approximators, albeit primarily through 'black-box' policies in simulated environments. However, when transitioning to real-world applications, concerns arise regarding the opacity of such policies. To enhance the transparency of learned policies, this paper proposes a policy iteration framework that maintains a sophisticated function approximator for internal value predictions while imposing a concise, hierarchical, and human-readable structure on the policy itself, achieved through a mixture of interpretable experts. Each expert determines a primitive action based on its proximity to a prototypical state, with these states selected from trajectory data to ensure interpretability. The primary technical contribution of this work lies in addressing the challenges posed by the non-differentiable nature of this prototypical state selection process. Through experimental evaluation, we demonstrate that our algorithm can learn effective policies on continuous action deep RL benchmarks, rivaling the performance of neural network-based policies while yielding policies that are more susceptible to human interpretation than those based on neural networks or linear-in-feature policies.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0072", "problem_id": "00720001", "content": "In recent years, Small Sample Learning (SSL) has emerged as a promising area of research in artificial intelligence, garnering significant attention from the academic community. This paper provides a comprehensive survey of the current techniques in SSL, which can be broadly categorized into two main approaches. The first approach, termed \"concept learning\", focuses on acquiring new concepts from limited observations, with the primary goal of mimicking human learning behaviors such as recognition, generation, and analysis. In contrast, the second approach, referred to as \"experience learning\", often co-exists with traditional machine learning methods that rely on large datasets, and is also known as small data learning in some literature. This survey delves into both categories of SSL techniques, providing a detailed overview and supporting neuroscience evidence to validate the SSL framework and its connection to human learning processes, as well as discussing key challenges and potential future research directions, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0073", "problem_id": "00730001", "content": "We introduce a meta-algorithm designed to learn posterior-inference techniques for constrained probabilistic programs. This method utilizes a training dataset consisting of probabilistic programs that represent models with observations, aiming to derive an efficient approach for estimating the posterior distribution of analogous programs. A distinctive aspect of our technique involves employing a white-box inference algorithm, which directly extracts insights from program-based model descriptions. Specifically, this algorithm incorporates multiple neural networks—each dedicated to a distinct type of atomic command—and approximates the posterior of a given probabilistic program by analyzing its atomic commands using these networks. Our meta-algorithm learns the parameters of these networks from the training data. Empirical evaluations indicate that the learned inference method generalizes effectively to new programs, both within and beyond the training scope, and in certain scenarios outperforms advanced alternatives like HMC. The findings highlight the potential of our approach while also identifying areas for further improvement.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0074", "problem_id": "00740001", "content": "Efficient exploration remains a difficult issue in reinforcement learning, particularly in scenarios where external rewards from the environment are scarce or entirely absent. Notable progress driven by intrinsic motivation demonstrates encouraging outcomes in straightforward environments, yet often falters in those exhibiting multimodal and stochastic characteristics. In this study, we introduce a variational dynamic model informed by conditional variational inference to effectively capture the multimodal and stochastic nature of such environments. We interpret the state-action transition of the environment as a conditional generative process by predicting the subsequent state based on the current state, action, and a latent variable. We establish an upper limit on the negative log-likelihood of the environmental transition and employ this upper bound as an intrinsic reward for exploration, enabling the agent to acquire skills through self-supervised exploration without the necessity of extrinsic rewards. We assess the proposed approach across multiple image-based simulation tasks as well as a practical robotic manipulation task, demonstrating that our method surpasses various leading environment model-based exploration strategies.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0075", "problem_id": "00750001", "content": "In recent years, time series analysis has garnered significant attention, with notable advancements in Time Series Classification (TSC) and Time Series Forecasting (TSF), largely facilitated by the University of California Riverside and University of East Anglia (UCR/UEA) Time Series Archives and competitions like the Makridakis competitions, NN3 and NN5 Neural Network competitions, and several Kaggle competitions. Although these archives have enabled the development of numerous algorithms for TSC and TSF, with thousands of papers proposing new methods annually, they may not be directly applicable to tasks such as predicting heart rate from photoplethysmogram (PPG) and accelerometer data. This limitation highlights the need for a more general approach, referred to as Time Series Extrinsic Regression (TSER), which involves predicting a continuous value from univariate or multivariate time series, regardless of whether the prediction is related to the same time series or not, and without necessarily depending on recent values. Despite its importance, TSER has received relatively little attention, with most models being developed for specific problems, and no general models available for TSER. To address this gap, we introduce the first benchmarking archive for TSER, comprising 19 datasets from diverse domains with varying dimensions, lengths, and missing values, and provide an initial evaluation of existing models on these datasets, aiming to stimulate research in this area, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0076", "problem_id": "00760001", "content": "Advances in training deep neural networks have spurred interest in analyzing the features developed within their hidden layers. This exploration presents challenges due to the complexity of interpreting non-linear transformations involving millions of parameters, yet it remains crucial for enhancing model comprehension and facilitating improvements. Our study examines whether neural networks demonstrate convergent learning—a phenomenon where independently trained networks develop either comparable individual features or feature subsets occupying analogous low-dimensional spaces. We introduce an analytical framework involving the comparison of learned representations across multiple networks, assessing similarities at both single-neuron and neuron-group levels. Our investigation employs three alignment methods: bipartite matching for neuron-to-neuron correspondence, sparse prediction for one-to-many relationships, and spectral clustering for many-to-many mappings. These approaches uncover novel network characteristics, suggesting that further study of convergent learning will yield additional discoveries. Key findings include: (1) certain features are consistently learned across networks while others vary; (2) neurons collectively form low-dimensional subspaces that recur across networks, though their specific basis vectors differ; (3) the representation scheme combines elements of local coding with partially distributed coding across units.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0077", "problem_id": "00770001", "content": "Despite the significant need for automated techniques, modern deep learning segmentation methods are not widely used in clinical practice. This is largely due to their unreliability, as these models can fail without warning and often generate anatomically unrealistic results that clinicians would not produce. This paper introduces an automated image segmentation approach using (Bayesian) dilated convolutional networks (DCNN) that simultaneously creates segmentation masks and spatial uncertainty maps for a given input image. The method's performance was assessed by segmenting the left ventricle (LV) cavity, right ventricle (RV) endocardium, and myocardium (Myo) at end-diastole (ED) and end-systole (ES) in 100 cardiac 2D MR scans from the MICCAI 2017 Challenge (ACDC). By integrating segmentations with uncertainty maps within a human-in-the-loop framework, we demonstrate that regions identified as highly uncertain regarding the segmentation predominantly correspond to areas of segmentation errors. This integrated information can be used to enhance segmentation accuracy. Our findings indicate that DCNNs can generate useful spatial uncertainty maps with minimal computational cost.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0078", "problem_id": "00780001", "content": "The application of multitask learning and transfer learning has demonstrated significant benefits in machine learning when supplementary knowledge is leveraged to enhance predictive capabilities. This study focuses on developing methodologies grounded in these paradigms for autotuning purposes, where the objective is to optimize the performance parameters of an application modeled as a black-box function. A comparative analysis with existing state-of-the-art autotuning techniques, such as OpenTuner and HpBandSter, reveals an average 1.5x reduction in application runtime, underscoring the efficacy of our approaches. Furthermore, we discuss the advantages of our methods over certain state-of-the-art autotuners, highlighting their suitability for tuning a broad range of applications, with a particular emphasis on expensive exascale applications.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0079", "problem_id": "00790001", "content": "Convolutional Neural Networks (CNNs) are highly effective deep learning architectures widely used in computer vision applications. To optimize CNN performance for Human Action Recognition (HAR) with inertial sensor data, this study employs four spatial domain techniques—Signal Images (SI), Gramian Angular Field (GAF) Images, Markov Transition Field (MTF) Images, and Recurrence Plot (RP) Images—to convert sensor data into activity images within an innovative fusion framework. These activity images are further enhanced into multimodal representations by applying Prewitt and High-boost spatial filters. A ResNet-18 CNN model extracts deep features from these modalities, with features pooled from the final layer and fused using canonical correlation-based fusion (CCF) to enhance HAR accuracy. The resulting discriminative features are then classified using a multiclass Support Vector Machine (SVM). Evaluations on three public inertial datasets demonstrate the proposed method's superiority over existing state-of-the-art approaches.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0080", "problem_id": "00800001", "content": "Current methods for removing atmospheric particles in learning-based approaches, particularly those aimed at rainy and hazy images, are founded on strong assumptions about spatial frequency, trajectory, and translucency. In contrast, the challenge of removing snow particles is heightened due to additional factors such as particle size and shape, which can differ within a single image. At present, the predominant approach for snow removal relies on hand-crafted features, making it challenging to achieve widespread generalization. To address this, we developed a multistage network named DesnowNet specifically for the effective removal of both translucent and opaque snow particles. Our method categorizes snow based on translucency and chromatic aberration to enhance estimation accuracy. Furthermore, we calculate the residual components of snow-free images separately to restore details hidden by opaque snow. A multi-scale architecture is consistently applied throughout the network to capture the variety of snow. Experimental results reveal that our approach surpasses existing state-of-the-art learning-based methods for atmospheric phenomena removal and a semantic segmentation baseline on the Snow100K dataset in both qualitative and quantitative analyses. These findings suggest that our network could significantly enhance applications in computer vision and graphics.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0081", "problem_id": "00810001", "content": "Manipulation involves physical interaction with the environment through hand movements, characterized by contact and motion between objects. This paper proposes a novel, bottom-up approach for detecting contacts between actors and objects, as well as extracting and tracking the motion of manipulated objects in RGBD videos of manipulation tasks. The method relies on non-rigid registration, which involves continuously deforming a point cloud model of the scene to match the current video frame, resulting in a set of dense 3D point trajectories. By applying point cloud segmentation techniques under relaxed assumptions, the actor is identified and contacts with the environment are detected based on the estimated trajectories. For each interaction, the detected contact serves as an attention mechanism to initialize a motion segment for the manipulated object by clustering nearby trajectories, and then the object segment and its 6DOF pose are refined jointly across all observed frames. The generality and informative output of this approach make it suitable for various perception and planning tasks, as demonstrated through qualitative evaluation on multiple input sequences and a comprehensive example of robot imitation learning, where the method's outputs play a crucial role in developing action representations and plans from observations, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0082", "problem_id": "00820001", "content": "This study seeks to create a framework for extreme learning machines (ELMs) utilizing general hypercomplex algebras. Hypercomplex neural networks are advanced machine learning models that utilize higher-dimensional numbers for parameters, inputs, and outputs. We begin by reviewing a variety of hypercomplex algebras and demonstrate a method for effectively performing operations within these algebras using real-valued linear algebra techniques. Subsequently, we examine several notable four-dimensional examples. We then introduce hypercomplex-valued ELMs and derive their learning process through a hypercomplex-valued least-squares problem. Finally, we conduct experiments comparing the performance of real ELM models against hypercomplex-valued counterparts in time-series prediction and color image auto-encoding tasks. The computational results underscore the superior capability of hypercomplex-valued ELMs in handling high-dimensional data, including those based on unconventional hypercomplex algebras.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0083", "problem_id": "00830001", "content": "The effectiveness of generative zero-shot approaches primarily hinges on the caliber of the generated features and the model's capability to facilitate knowledge transfer between the visual and semantic domains. The caliber of these features directly correlates with the model's proficiency in capturing various modes of the underlying data distribution. To address these challenges, we introduce a novel two-level joint maximization approach designed to enhance the generative network with an inference network during training, enabling our model to effectively capture multiple modes of the data and produce features that more accurately represent the underlying distribution. This enhances cross-modal interaction, facilitating efficient knowledge transfer between visual and semantic domains. Moreover, existing techniques typically train the zero-shot classifier using either synthetic image features or latent embeddings derived from representation learning. In this study, we integrate these approaches into a cohesive model that not only synthesizes image features but also leverages the representation learning strengths of the inference network to generate discriminative features for the final zero-shot recognition task. We assess our method on four benchmark datasets, namely CUB, FLO, AWA1, and AWA2, comparing its performance with several leading-edge methods. Additionally, we conduct ablation studies to thoroughly analyze and gain insights into our method for the Generalized Zero-shot Learning task.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0084", "problem_id": "00840001", "content": "Meta-learning offers a potential solution to the difficulties presented by few-shot learning. The central principle involves utilizing numerous analogous few-shot tasks to learn how to effectively adapt a base-learner to a novel task with limited labeled data. Given the propensity of deep neural networks (DNNs) to overfit with scarce data, meta-learning models typically employ shallow networks, which can restrict their performance. Recent studies aiming for optimal results have explored pre-trained DNNs from large datasets, often using them in basic ways, such as (1) initializing meta-training weights and (2) freezing convolutional layers to serve as feature extractors for base-learners. This paper introduces a novel meta-transfer learning (MTL) method designed to learn the transfer of weights from a deep NN to facilitate few-shot learning. Meta-learning is achieved via training across various tasks, and transfer is realized by learning scaling and shifting functions for DNN weights tailored to each task. Furthermore, we present a hard task (HT) meta-batch scheme, an effective learning strategy that enhances MTL's learning efficiency. Few-shot learning experiments on miniImageNet, tieredImageNet and Fewshot-CIFAR100 (FC100) demonstrate state-of-the-art performance in five-class few-shot recognition. Comparative analyses with related research affirm that our MTL approach, when combined with the HT meta-batch scheme, achieves superior results. An ablation study confirms that both components contribute to both rapid convergence and high accuracy.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0085", "problem_id": "00850001", "content": "The incorporation of Non-Local (NL) blocks in mobile neural networks has been largely unexplored, primarily due to two significant challenges: the high computational cost associated with NL blocks, which poses difficulties in resource-constrained applications, and the lack of an optimal configuration for integrating NL blocks into mobile neural networks. To address these challenges, we introduce AutoNL, a novel approach that firstly proposes a Lightweight Non-Local (LightNL) block, which reduces computational costs by 400 times compared to conventional NL blocks without compromising performance, achieved through the optimization of transformation operations and the utilization of compact features. Secondly, by rendering the LightNL block's structure differentiable during training, we develop an efficient neural architecture search algorithm that enables the end-to-end learning of an optimal LightNL block configuration. Notably, our AutoNL model, which was searched using only 32 GPU hours, achieves a top-1 accuracy of 77.7% on ImageNet under typical mobile settings (350M FLOPs), substantially outperforming existing mobile models, including MobileNetV2 (+5.7%), FBNet (+2.8%), and MnasNet (+2.1%), with code and models available at https://github.com/LiYingwei/AutoNL.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0086", "problem_id": "00860001", "content": "Background: The shift towards extensive studies, including population imaging, introduces new challenges regarding quality control (QC). This issue is particularly pronounced when utilizing automated processing tools, such as image segmentation techniques, to generate quantitative metrics or biomarkers for future analysis. Conducting manual checks and visual QC for each segmentation is impractical on a large scale. Nevertheless, it is crucial to automatically identify instances when a segmentation method fails to prevent erroneous measurements from affecting subsequent analyses, which can lead to faulty conclusions. Methods: To address this challenge, we investigate a method for predicting segmentation quality through Reverse Classification Accuracy, allowing us to differentiate between successful and unsuccessful segmentations on an individual case basis. We validate this method on a new, extensive manually-annotated dataset comprising 4,800 cardiac magnetic resonance images. Subsequently, we apply our approach to a large cohort of 7,250 cardiac MRIs that have undergone manual QC. Results: We present findings related to predicting segmentation quality metrics, including Dice Similarity Coefficient (DSC) and surface-distance measurements. For initial validation, we provide data from 400 scans, achieving 99% accuracy in classifying low and high-quality segmentations based on predicted DSC scores. Further validation demonstrates a strong correlation between actual and predicted scores, with 95% classification accuracy on 4,800 scans for which manual segmentations were available. We simulate real-world application of the method on 7,250 cardiac MRIs, revealing good concordance between predicted quality metrics and manual visual QC scores. Conclusions: Our findings indicate that RCA holds significant promise for precise and entirely automated segmentation QC on a case-by-case basis within the framework of large-scale population imaging, such as the UK Biobank Imaging Study.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0087", "problem_id": "00870001", "content": "Isolated trees beyond dense, closed-canopy forests play a crucial role in carbon storage, sustaining livelihoods, preserving ecosystem stability, and aiding climate change adaptation and mitigation. Unlike trees within dense forests, the global spatial coverage and distribution of scattered trees remain poorly understood. High-resolution satellite data is often prohibitively expensive, leading global monitoring systems to depend on medium-resolution satellites for land-use assessment. This study introduces a globally applicable approach for detecting trees with canopy diameters exceeding three meters using medium-resolution optical and radar imagery. A fully convolutional network—incorporating a convolutional gated recurrent unit layer and a feature pyramid attention layer—is trained using biweekly, cloud-free, pan-sharpened 10-meter Sentinel-2 optical and Sentinel-1 radar data. Evaluated across over 215,000 Sentinel-1 and Sentinel-2 pixels spanning latitudes from -60 to +60, the model achieves over 75% user's and producer's accuracy in areas with low to moderate tree cover (below 40%) and 95% accuracy in densely covered regions (above 40%). The method improves tree detection accuracy in sparse and scattered tree cover (below 40%) by up to 20% and reduces misclassification errors in mountainous and highly cloudy regions by nearly 50%. When applied to large, varied landscapes, the results indicate the potential for high-precision tree mapping across diverse global environments. Such data is vital for assessing current land cover and detecting changes, including agroforestry practices, buffer zones near biodiversity hotspots, and forest expansion or encroachment.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0088", "problem_id": "00880001", "content": "Content-based image retrieval (CBIR) remains a prominent area of investigation within multimedia information retrieval, focusing on the retrieval of relevant images from a database based on a given query image. Low-level image attributes such as color, texture, and shape features are crucial components in CBIR systems. Integrating these feature vectors can improve the performance of such systems. This paper introduces a new CBIR framework that employs multiclass SVM for indexing and automatically determines optimal weights for individual features using the relevance ratio and mean difference. Four feature descriptors are utilized to represent color, texture, and shape characteristics. During retrieval, the query image's feature vectors are combined, weighted, and compared against the database images' feature vectors to rank the results. The proposed framework's effectiveness is validated through experiments conducted on four benchmark datasets, with performance comparisons against existing techniques.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0089", "problem_id": "00890001", "content": "The advancement of Natural Language Processing has yielded numerous State-of-the-Art pretrained models that can be fine-tuned for specific tasks, with large models boasting billions of parameters and requiring extensive computational resources, including multiple GPUs/TPUs and weeks of training time, dominating benchmark leaderboards. However, there is a growing need for a benchmark that accommodates smaller, more efficient models that can be trained on a single GPU, thereby enabling researchers with limited resources to explore innovative ideas related to tokenization, pretraining tasks, architecture, and fine-tuning methods. To address this, we introduce Small-Bench NLP, a benchmark designed for small neural language models trained on a single GPU, comprising eight NLP tasks based on the publicly available GLUE datasets and a leaderboard to monitor community progress, as shown in Figure A, B, C. Notably, our proposed ELECTRA-DeBERTa small model architecture, with 15M parameters, achieves an average score of 81.53, comparable to the 82.20 score of BERT-Base, which has 110M parameters, and our models, code, and leaderboard are accessible at https://github.com/smallbenchnlp (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0090", "problem_id": "00900001", "content": "Numerical experiments in large-scale machine learning have revealed an unexpected phase transition, characterized by the relationship between sample size and model parameters. When the number of parameters approaches the sample size, the generalization error initially increases, but then decreases after surpassing the threshold where the number of parameters equals the sample size. This phenomenon, first highlighted in \\cite, has been extensively studied, particularly for simpler models like linear models with minimum norm solutions to least-square problems, typically in the asymptotic regime as parameters and sample size tend to infinity; see e.g. \\cite. This paper presents a finite sample analysis of non-linear models, examining the double descent phenomenon for both the  and the  problem. Our findings offer a detailed examination of the distance between the optimal estimator and the true parameter, as well as a generalization bound that complements recent research by \\cite and \\cite, using efficient and elementary techniques related to the continuous Newton method \\cite.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0091", "problem_id": "00910001", "content": "Computer-aided design (CAD) represents the predominant methodology for technical modeling. Typically, the process begins with 2D sketches, which can subsequently be extruded and combined to create intricate three-dimensional assemblies. These sketches generally consist of parametric primitives, including points, lines, and circular arcs, enhanced by geometric constraints that connect the primitives, such as coincidence, parallelism, or orthogonality. They can also be viewed as graphs, where the primitives serve as nodes and the constraints function as edges. Creating a model that can autonomously generate CAD sketches facilitates various innovative workflows, yet poses challenges due to the intricacies of the graphs and the diversity of primitives and constraints. Specifically, each primitive and constraint type may necessitate different record sizes and parameter types. To tackle the issue of heterogeneity, we introduce SketchGen, a generative model founded on transformer architecture, which strategically crafts a sequential language for the primitives and constraints. This language enables the distinction of various primitive or constraint types along with their parameters while promoting the model’s ability to leverage information across related parameters, thus encoding common structures. A notable feature of our work is its capacity to generate primitives interconnected through constraints, allowing the final output to undergo additional regulation through a constraint solver. We assess our model by illustrating the prediction of constraints for specified sets of primitives and the complete generation of sketches from the ground up, demonstrating that our method significantly surpasses current state-of-the-art techniques in CAD sketch generation.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0092", "problem_id": "00920001", "content": "Encoded manuscripts, also known as ciphered texts, represent a unique category of historical documents that feature encrypted content. The automatic identification of such documents poses significant challenges due to: 1) variations in cipher alphabets across different documents, 2) an absence of annotated datasets for training, and 3) the intricacy of symbol segmentation caused by overlapping symbols. To address these challenges, we introduce an innovative approach for the recognition of handwritten ciphers utilizing few-shot object detection techniques. Our method initially detects all symbols from a specified alphabet within a line image, followed by a decoding phase that translates symbol similarity scores into a cohesive string of transcribed symbols. Through training on synthetic data, we demonstrate that the proposed framework can recognize handwritten ciphers with previously unseen alphabets. Furthermore, by employing a limited number of labeled pages featuring the same alphabet for fine-tuning, our method outperforms existing unsupervised and supervised methods for ciphers recognition.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0093", "problem_id": "00930001", "content": "This study introduces a framework designed to separate appearance and geometric representations in face recognition. Supervision for this objective is achieved by generating faces with identical geometry through spatial transformations. The approach improves deep face recognition models by aiding training in two key aspects: it encourages early and intermediate convolutional layers to learn more discriminative features aligned with disentangled embedding properties, and it expands the training dataset via geometric face variations. Comprehensive experiments confirm that incorporating this method into leading face recognition techniques boosts their accuracy on demanding benchmarks like LFW, YTF, and MegaFace. The method's theoretical and practical implications are thoroughly examined through ablation studies and knowledge transfer evaluations. Additionally, the learned representations prove beneficial for other facial tasks, including attribute prediction.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0094", "problem_id": "00940001", "content": "Three-dimensional multi-object tracking (MOT) is essential for autonomous systems. Contemporary approaches frequently employ a tracking-by-detection framework, where an affinity matrix is computed based on features extracted separately for each object. This matrix is subsequently used by the Hungarian algorithm to perform data association. A critical aspect of this framework involves learning distinctive features for different objects to minimize data association errors. To address this, we introduce two novel techniques: (1) a feature interaction mechanism using Graph Neural Networks, which moves beyond independent object feature acquisition; and (2) a joint feature extractor that learns appearance and motion features from both 2D and 3D space, unlike prior work that extracts features from either 2D or 3D space. Experimental results on the KITTI dataset demonstrate that our method achieves state-of-the-art 3D MOT performance. Additional project details are available at http://www.xinshuoweng.com/projects/GNN3DMOT.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0095", "problem_id": "00950001", "content": "This study introduces a novel method for compactly representing time-series data using reduced-order Markov modeling. The approach employs symbolic dynamics-based techniques to derive an approximate generative Markov model, wherein the continuous signal measurement space is partitioned and symbolized, and the resulting discrete sequential data are modeled using symbolic dynamics. The temporal memory size of the symbol sequence is estimated from the spectral properties of the stochastic matrix associated with a first-order Markov model, and hierarchical clustering is applied to construct a reduced-order Markov model with a non-deterministic algebraic structure by representing the states of the full-state Markov model. The parameters of the reduced-order model are identified from the original model using a Bayesian inference rule, and the final model is selected based on information-theoretic criteria. The efficacy of the proposed concept is demonstrated through two example applications: analyzing pressure data from a swirl-stabilized combustor, where the complexity of the derived Markov model reflects changes in the system's operating condition from stable to unstable combustion regimes, and prognostic analysis of bearings on rotating shafts using a data set from NASA's repository, which shows that the reduced-order models can achieve comparable performance even with a small state-space, offering flexibility in model selection for representation and learning.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0096", "problem_id": "00960001", "content": "Parameter estimation in differential equation models can be accomplished through the application of learning algorithms to quantitative time-series data, but in cases where only qualitative system changes are measurable in response to controlled conditions, alternative approaches are necessary. In the context of dynamical systems theory, these qualitative changes, referred to as bifurcations, are characterized by a bifurcation diagram, which is a function of the controlled condition. This study presents a semi-supervised, gradient-based method for inferring parameters of differential equations that yield a predefined bifurcation diagram. The proposed cost function incorporates a supervised error term, which is minimized when model bifurcations align with target specifications, and an unsupervised bifurcation measure with gradients that guide optimizers towards parameter regimes exhibiting bifurcations, all without requiring differentiation through the solver operations used to compute the diagram, as seen in Figure A, B, C (citations). The effectiveness of this approach is demonstrated through the inference of parameters in minimal models that generate saddle-node and pitchfork diagrams, as well as the genetic toggle switch model from synthetic biology, and the resulting cost landscape enables the organization of models based on topological and geometric equivalence, as discussed in References.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0097", "problem_id": "00970001", "content": "Non-negative Matrix Factorization (NMF) is recognized as a robust unsupervised learning technique for revealing hidden features within intricate and noisy data sets, applicable in areas such as data mining, text recognition, dimensionality reduction, face recognition, anomaly detection, and blind source separation, among others. A crucial parameter for NMF is the latent dimensionality, or the number of hidden features, K, that exist in the analyzed data set; however, this value is seldom known beforehand. We adopt a supervised machine learning framework alongside a novel model determination technique referred to as NMFk, which allows for the automatic identification of the number of hidden features. NMFk conducts a series of NMF simulations on various matrices generated by bootstrapping the original data set, identifying which K yields stable clusters of latent features effectively reconstructing the initial data. Subsequently, a Multi-Layer Perceptron (MLP) classifier network is trained to ascertain the accurate number of latent features by leveraging the statistics and attributes of the NMF outputs obtained from NMFk. To facilitate the training of the MLP classifier, a set of 58,660 matrices with known latent features was factorized using NMFk. The combination of the MLP classifier and NMFk achieves more than a 95% success rate on a separate test set. Furthermore, when tested on two established benchmark data sets, the swimmer and MIT face data, the NMFk/MLP framework successfully retrieved the known number of hidden features. Finally, we evaluated the performance of our approach against ARD, AIC, and stability-based methods.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0098", "problem_id": "00980001", "content": "Deep reinforcement learning (RL) agents often struggle to generalize to new environments, even when the underlying dynamics are consistent with the training environments, posing a significant challenge to modern machine learning. To address this, we examine policy learning within Partially Observable Markov Decision Processes (POMDPs), framing the dynamics of training levels as specific instances. Our analysis reveals that, regardless of the exploration strategy used, repeated use of training instances significantly alters the observed Markov dynamics during training. The pursuit of maximizing expected rewards leads to a learned belief state that favors undesirable, instance-specific speedrunning policies over more generalizable policies, even if the generalizable policies are suboptimal within the training set. We present generalization bounds, based on the number of training instances, for the value gap between training and testing environments, and utilize these insights to enhance performance in unseen environments. Our approach involves training a shared belief representation across an ensemble of specialized policies, deriving a consensus policy for data collection that prevents instance-specific exploitation. The theoretical results, observations, and the proposed computational solution are validated through experiments conducted on the CoinRun benchmark.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0099", "problem_id": "00990001", "content": "The task of LiDAR odometry (LO) involves determining the alignment between consecutive LiDAR point clouds, which can be utilized to estimate the motion of the platform equipped with the LiDAR sensor. Notably, state-of-the-art algorithms on the KITTI Vision Benchmark Suite currently employ non-learning approaches. In this work, a novel network architecture is proposed that directly processes 3D point clouds to learn LO in an end-to-end manner, eliminating the need for predefined point correspondences, and is trained on the KITTI dataset. An evaluation on the KITTI Vision Benchmark Suite reveals that the proposed model achieves comparable performance to DeepCLR [1], despite using only approximately 3.56% of the network parameters. Additionally, applying plane point extraction results in a slight decrease in performance, while reducing the input size by up to 50%.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0100", "problem_id": "01000001", "content": "The integration of Artificial Intelligence with society is facilitated by explainability, which is a crucial aspect given the inherent weakness of popular deep learning models in providing insights into their decision-making processes and predictive outcomes. A relatively new approach, Local Interpretable Model-agnostic Explanation (LIME), has emerged as a means to faithfully explain the predictions of any classifier by locally learning an interpretable model around the prediction. Nevertheless, the standard implementation of LIME is flawed due to its sampling operation, which generates perturbed samples from a uniform distribution without considering the complex correlations between features. To address this limitation, this paper introduces a novel Modified Perturbed Sampling operation for LIME, denoted as MPS-LIME, which is formulated as a clique set construction problem. In the context of image classification, MPS-LIME involves converting a superpixel image into an undirected graph, and experimental results demonstrate that the explanations provided by MPS-LIME for black-box models exhibit significant improvements in terms of understandability, fidelity, and efficiency, as shown in Figure A, B, C (References [1], [2], and [3] provide further details).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0101", "problem_id": "01010001", "content": "The healthcare sector represents a highly promising area for advancements in data mining and machine learning. The widespread implementation of electronic health records (EHRs) has led to a substantial increase in accessible digital clinical data for analysis. However, assessing progress in machine learning applications for healthcare research has been challenging due to the lack of openly available benchmark datasets. To tackle this issue, we introduce four clinical prediction benchmarks derived from the publicly accessible Medical Information Mart for Intensive Care (MIMIC-III) database. These benchmarks address diverse clinical challenges such as predicting mortality risk, estimating length of stay, identifying physiological deterioration, and classifying phenotypes. For each task, we establish robust linear and neural baselines and examine how deep supervision, multitask learning, and architecture adaptations tailored to the data influence neural model performance.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0102", "problem_id": "01020001", "content": "This paper introduces an innovative locally statistical active contour model (ACM) for the segmentation of images characterized by intensity inhomogeneity. In this approach, the inhomogeneous objects are represented as Gaussian distributions with varying means and variances. A moving window technique is employed to transform the original image into an alternative domain, where the intensity distributions of the inhomogeneous objects remain Gaussian but exhibit improved separation. The means of these Gaussian distributions in the transformed domain are adaptively estimated by applying a bias field to the original signal within the window. Subsequently, a statistical energy functional is established for each local region, integrating the bias field, the level set function, and a constant that approximates the actual signal of the relevant object. Experimental results on both synthetic and real images confirm that our proposed algorithm outperforms leading state-of-the-art methods.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0103", "problem_id": "01030001", "content": "The remarkable advancements achieved by CNN-based models in human pose estimation have not provided clear insights into the spatial dependencies that facilitate keypoint localization. This study proposes a novel model, termed \\textbf, which leverages the Transformer architecture for human pose estimation, enabling the efficient capture of long-range relationships and revealing the dependencies that underlie keypoint predictions. By utilizing the attention layers in the Transformer, the model can effectively aggregate contributions from image features to predict keypoint heatmaps, with the final attention layer serving as an aggregator that identifies the maximum keypoint positions. This heatmap-based localization approach, facilitated by the Transformer, aligns with the Activation Maximization principle~\\cite, and the revealed dependencies are both image-specific and fine-grained, offering valuable insights into the model's handling of special cases, such as occlusion. Experimental results demonstrate that the proposed model, TransPose, achieves impressive performance, with 75.8 AP and 75.0 AP on the COCO validation and test-dev sets, respectively, while being more lightweight and faster than conventional CNN architectures, and also exhibits superior performance on the MPII benchmark with minimal fine-tuning costs, with code and pre-trained models made publicly available\\footnote.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0104", "problem_id": "01040001", "content": "A crucial aspect of human intelligence involves comprehending the environment and anticipating future outcomes for informed decision-making. The capacity of autonomous systems to reason about future events is paramount to their advancement. Semantic anticipation, such as predicting pedestrian movement, represents a relatively untapped area of potential benefit for autonomous vehicles. Driven by the necessity for real-time prediction in autonomous systems, we introduce a method that divides the complex task of semantic forecasting into two sub-problems: segmenting the current frame and predicting future optical flow. Utilizing this decomposition, we developed a streamlined and effective model with minimal overhead, comprising three primary elements: a flow prediction network, a feature-flow aggregation LSTM, and an end-to-end learnable warp layer. Our approach attains state-of-the-art precision in short-term and moving object semantic forecasting, concurrently decreasing model parameters by as much as 95% and improving efficiency by over 40 times.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0105", "problem_id": "01050001", "content": "The early identification of lung cancer has been shown to significantly reduce mortality rates. A recent advancement in computed tomography (CT), known as spectral CT, has the potential to enhance diagnostic precision by providing more detailed information per scan compared to conventional CT. Nevertheless, the significant effort required to analyze numerous scans highlights the necessity for automated diagnostic techniques. Consequently, we put forward a system for the detection and classification of lung nodules in CT images. Additionally, we intend to assess whether the use of spectral images can enhance the performance of classifiers. To detect nodules, we trained a VGG-like 3D convolutional neural network (CNN). We pre-trained a 3D CNN with a similar structure on nodule malignancies from a large publicly available dataset, the LIDC-IDRI dataset, to develop a primary tumor classifier for our own dataset. We then employed this pre-trained network as a feature extractor for the nodules in our dataset, leading to the classification of the resulting feature vectors into two (benign/malignant) and three (benign/primary lung cancer/metastases) categories using support vector machine (SVM), with classifications executed at both the nodule and scan levels. We achieved state-of-the-art results for detection and malignancy regression on the LIDC-IDRI database. Our dataset demonstrated superior classification performance at the scan level compared to the nodule level, with a scan-level accuracy of 78% in the three-class classification. Although spectral features did enhance classifier performance, the improvement was not substantial. Our findings indicate that a pre-trained feature extractor can effectively function as a classifier for the primary tumor origin of lung nodules, negating the requirement for extensive fine-tuning of new networks and large datasets. The code is accessible at \\url.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0106", "problem_id": "01060001", "content": "In the context of nanopore sequencing, the translation of electrical signals into DNA bases, known as base calling, poses a significant challenge, with a substantial impact on sequencing accuracy. To address this, convolutional neural networks (CNNs) have emerged as the most effective approach, leveraging their capabilities to interpret complex signal patterns. However, conventional CNN architectures, which typically employ filters with fixed window sizes, are optimized for analyzing signals with uniform velocities. Given that nanopore sequencing speeds can vary considerably within reads and across different sequencing runs, a novel neural network component, termed dynamic pooling, has been developed to adaptively adjust the pooling ratio, thereby enhancing the accuracy of base calling. The efficacy of dynamic pooling is demonstrated through the development of two base callers, Heron and Osprey, with Heron surpassing the accuracy of the high-accuracy base caller Bonito, and Osprey achieving near real-time speeds on standard desktop CPUs while maintaining accuracy comparable to Guppy's high-accuracy mode without requiring GPU acceleration, as shown in Figure A, B, C (References [citation]), and made available at https://github.com/fmfi-compbio/osprey, https://github.com/fmfi-compbio/heron, with keywords including nanopore sequencing, base calling, convolutional neural networks, and pooling.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0107", "problem_id": "01070001", "content": "Reinforcement learning often requires agents to learn solely from a predefined dataset without additional data collection opportunities. This study reveals that conventional off-policy deep reinforcement learning methods, including DQN and DDPG, struggle in such fixed-batch scenarios due to extrapolation errors, particularly when the data distribution deviates from the current policy. To address this, we propose batch-constrained reinforcement learning, a new approach that limits the action space to align the agent’s behavior more closely with a subset of the available data. We develop the first deep reinforcement learning algorithm for continuous control capable of learning efficiently from any fixed dataset and validate its performance through empirical evaluations across multiple tasks.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0108", "problem_id": "01080001", "content": "The remarkable accomplishments of deep neural networks in fields such as medical image segmentation have been somewhat overshadowed by their opaque nature, which hinders the interpretation, comprehension, and modification of architectures due to the lack of a unified design theory. To address this issue, precision learning was introduced as a means of integrating deep architectures with traditional methods, resulting in networks that leverage known operators, possess fewer parameters, and exhibit enhanced interpretability, although they may not always achieve optimal performance. This paper presents an alternative approach, wherein deep networks are analyzed using known operators through a divide-and-conquer strategy that replaces network components while preserving performance, with the task of retinal vessel segmentation serving as a case study. By commencing with a high-performance U-Net and incrementally converting it, we demonstrate the feasibility of dividing the network into modules comprising known operators, with the results showing that a combination of a trainable guided filter and a trainable Frangi filter achieves comparable performance to U-Net (AUC 0.974 vs. 0.972) while significantly reducing parameters (111,536 vs. 9,575), and allowing the trained layers to be mapped back to their original algorithmic interpretation for analysis using standard signal processing tools.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0109", "problem_id": "01090001", "content": "The presence of transparent objects, such as glass windows and bottles, is ubiquitous in real-world environments, posing a significant challenge for segmentation tasks due to their tendency to blend in with their surroundings, resulting in similar appearances. The complexity of this task is further exacerbated by the limited availability of specialized datasets, with existing ones often suffering from limitations such as small sample sizes, lack of manual annotations, or being generated using computer graphics rather than real images. To overcome these hurdles, we introduce Trans10K, a large-scale dataset comprising 10,428 manually annotated images of real-world scenarios, representing a substantial increase in size compared to existing datasets. The transparent objects in Trans10K exhibit high diversity in terms of scale, viewpoint, and occlusion, as illustrated in Fig. 1, making them particularly challenging to segment. We also propose a novel boundary-aware segmentation approach, termed TransLab, which leverages boundary information to enhance the segmentation of transparent objects. Through extensive experiments and ablation studies, we demonstrate the efficacy of Trans10K and the practicality of TransLab, with the latter outperforming 20 recent deep learning-based object segmentation methods, highlighting the significance of this task and the potential of Trans10K and TransLab to contribute meaningfully to both academic and industrial pursuits.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0110", "problem_id": "01100001", "content": "Effective reinforcement learning relies heavily on the reuse of prior experience, with a central challenge being how such experience is encoded and maintained. Traditionally, experience has been stored as features, individual models, or an averaged model, each operating at distinct granularities. Yet, novel tasks often demand experience spanning multiple levels of granularity. To address this, we introduce the policy residual representation (PRR) network, designed to capture and retain experience at various hierarchical levels. The PRR network employs a multi-level architecture trained across diverse tasks, with each module corresponding to specific task subsets, thereby organizing experience in a continuous spectrum. When applied to new tasks, PRR facilitates learning by supplying relevant experience at appropriate granularities. Evaluations on grid world navigation, locomotion, and video game combat tasks demonstrate that the PRR network enhances experience reuse and surpasses several leading methods.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0111", "problem_id": "01110001", "content": "The surge in machine learning applications for satellite imagery frequently depends on visible light images, which limits data availability at night. To address this limitation, infrared observations can be utilized to synthesize visible images. This study demonstrates the effective application of deep learning, specifically U-Net based architectures, for creating such images. The proposed techniques yield encouraging outcomes, attaining a structural similarity index (SSIM) of up to 86\\% on a separate test dataset and producing visually compelling images derived from infrared data.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0112", "problem_id": "01120001", "content": "Estimating human pose using deep neural networks involves mapping input images with significant variations to multiple body keypoints, which must adhere to the geometric constraints and interdependencies defined by the human body model, presenting a complex nonlinear manifold learning challenge in a high-dimensional feature space. However, the inherent algebraic nature of deep neural networks may not be the most efficient means of capturing intricate human knowledge, such as the highly coupled geometric characteristics and interdependencies between keypoints in human poses. To address this, we propose incorporating external knowledge into deep neural networks through learned projections that impose appropriate priors, guiding the training process. Our approach utilizes a fractal network, constructed from stacked hourglass and inception-resnet modules, to regress human pose images into heatmaps without explicit graphical modeling, and encodes external knowledge as visual features that characterize human body model constraints and evaluate intermediate network output fitness. These external features are injected into the neural network via a projection matrix learned using an auxiliary cost function, as shown in Figure A, B, C (see References [citation]). The efficacy of the proposed inception-resnet module and knowledge projection-guided learning is demonstrated on two widely used benchmarks, yielding state-of-the-art performance on both datasets, as discussed in References [citation].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0113", "problem_id": "01130001", "content": "Enhancing symmetry equivariance in deep neural networks has led to gains in data efficiency and generalization capabilities. In this study, we apply the principles of scale and translation equivariance to address the challenge of learning from raw time-series waveforms. Consequently, we achieve representations similar to those produced by the wavelet transform in the initial layer, which develop into more intricate forms with increased depth. Our empirical findings affirm the effectiveness of our Wavelet Networks, which, despite having a straightforward architectural design, consistently outperform CNNs on raw waveforms and perform comparably to methods based on spectrograms.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0114", "problem_id": "01140001", "content": "In recent years, there have been substantial advancements in reinforcement learning (RL), which has achieved notable success in addressing various sequential decision-making challenges within the machine learning field. Many prominent applications of RL, such as the games of Go and Poker, robotics, and autonomous driving, involve multiple agents, leading to the field of multi-agent RL (MARL), which has a considerable history and has recently been revitalized by progress in single-agent RL methodologies. While MARL has seen empirical success, its theoretical underpinnings are less developed in existing literature. This chapter offers a curated overview of MARL, concentrating on algorithms supported by theoretical frameworks. We specifically examine the theoretical findings of MARL algorithms primarily within two key contexts: Markov/stochastic games and extensive-form games, categorized by the nature of their tasks, including fully cooperative, fully competitive, and hybrid scenarios. Additionally, we present various important but complex applications of these algorithms. Distinct from existing reviews on MARL, we emphasize new perspectives and classifications of MARL theory, such as learning in extensive-form games, decentralized MARL with interconnected agents, MARL in mean-field contexts, and the (non-)convergence of policy-based methods in gaming scenarios, among others. Some of these fresh perspectives draw from our research interests and initiatives. The overarching aim of this chapter is not only to evaluate the current state of the field but also to highlight promising avenues for future theoretical research in MARL. We hope this chapter will inspire researchers eager to explore this captivating yet challenging subject.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0115", "problem_id": "01150001", "content": "The challenge of feature extraction in image classification tasks is exacerbated by intra-class variability, which complicates the design of effective extractors, and the limitations of hand-crafted feature extractors in adapting to new situations. The advent of deep learning has sparked significant interest in automatic feature learning from data. This research proposes a novel multi-subspace neural network (MSNN) architecture, which combines the key elements of convolutional neural networks (CNNs), including receptive fields, with subspace concepts. By integrating subspace with deep networks, this innovative design provides diverse perspectives on the data, utilizing basis vectors trained by adaptive subspace self-organization map (ASSOM) to span the subspace, acting as a transfer function to access axial components and define the receptive field for extracting fundamental patterns without topological distortion in visual tasks. The multiple-subspace strategy is implemented through parallel blocks, enabling adaptation to real-world data and facilitating various interpretations to enhance robustness in addressing intra-class variability issues. The proposed MSNN architecture is validated using handwritten digit and object image datasets, such as MNIST and COIL-20, for classification tasks, and experimental results demonstrate its competitiveness with other state-of-the-art approaches, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0116", "problem_id": "01160001", "content": "This paper addresses the challenge of detecting outlier arms within multi-armed bandit frameworks, a problem relevant to numerous critical fields, including finance, healthcare, and online advertising. The goal is to enable a learning agent to pinpoint arms with expected rewards that differ substantially from the majority. Departing from prior research, this study focuses on identifying generic outlier arms or groups of arms, where the expected rewards can be higher, lower, or intermediate relative to normal arms. Initially, a thorough definition of generic outlier arms and groups is presented. Subsequently, a new pulling algorithm, GOLD, is introduced to identify these outliers by constructing a dynamic neighborhood graph based on upper confidence bounds, thereby capturing the behavioral distinctions between outliers and normal arms. A comprehensive performance analysis is also provided. Empirical evaluations using both synthetic and real-world datasets demonstrate that the proposed algorithm achieves 98% accuracy while reducing exploration costs by an average of 83% compared to existing state-of-the-art methods.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0117", "problem_id": "01170001", "content": "This study presents a innovative approach to 3D face recognition, combining a deep convolutional neural network (DCNN) with a 3D augmentation method. While the accuracy of 2D face recognition algorithms has improved substantially through the utilization of deep neural networks and large-scale labeled training data, 3D face recognition poses a unique challenge due to the scarcity of extensive 3D face datasets, making it difficult to train discriminative deep features. However, we demonstrate that transfer learning from a CNN trained on 2D face images can be effectively applied to 3D face recognition by fine-tuning the CNN with a limited number of 3D facial scans. Additionally, a 3D face augmentation technique is proposed, which generates multiple facial expressions from a single 3D face scan. Our method yields outstanding recognition results on the Bosphorus, BU-3DFE, and 3D-TEC datasets without relying on hand-crafted features, and the 3D identification using our deep features exhibits scalability for large databases.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0118", "problem_id": "01180001", "content": "The significance of explainability in machine learning is increasingly recognized as neural network structures and the corresponding data they handle grow in complexity. When a model has high-dimensional input features, distinct challenges emerge; while principled model-agnostic explainability methods can become prohibitively demanding in terms of computation, more efficient algorithms often fail to provide clear interpretations for general users. In this study, we propose a framework for achieving human-interpretable explainability within high-dimensional datasets, which comprises two components. Initially, we employ a semantically meaningful latent representation to both decrease the raw data dimensionality and enhance its interpretability for humans. These latent features may be learned explicitly as disentangled representations or implicitly via image-to-image translation, or they may rely on any computable attributes selected by the user. Subsequently, we modify the Shapley framework for model-agnostic explainability to function on these latent features, resulting in model explanations that are both theoretically sound and computationally feasible. We evaluate our method using synthetic data and showcase its effectiveness across various image classification tasks.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0119", "problem_id": "01190001", "content": "Object detection has remained a prominent area of research in computer vision, with recent efforts primarily concentrated on advancing the state-of-the-art in the general-purpose COCO benchmark. Nevertheless, the application of these detection frameworks to specific domains, such as autonomous driving, still requires attention. This research introduces an improved 2D object detector, built upon Faster R-CNN, tailored to the autonomous vehicle context. The study focuses on enhancing two key aspects: the anchor generation process and the performance decline in minority classes. Given the limitations of the standard uniform anchor configuration due to the perspective projection of vehicle cameras, a perspective-aware approach is proposed, which clusters the image into distinct regions and utilizes evolutionary algorithms to optimize base anchors for each region. Additionally, a module is incorporated to enhance the precision of the second-stage header network by integrating spatial information from candidate regions proposed in the first stage. Various re-weighting strategies are explored to address foreground-foreground class imbalance, revealing that a modified version of focal loss can substantially improve the detection of challenging and underrepresented objects in two-stage detectors. An ensemble model is also designed to combine the strengths of different learning strategies. The proposed approach is evaluated using the Waymo Open Dataset, the most comprehensive and diverse to date, yielding an average accuracy improvement of 6.13% mAP with the best single model and 9.69% mAP with the ensemble, without increasing computational cost, and can be easily extended to optimize other anchor-based detection frameworks, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0120", "problem_id": "01200001", "content": "Acquiring information about an unknown state is most effectively achieved through informative measurements. A novel, first-principles-based dynamic programming algorithm is presented, which generates a sequence of such measurements by iteratively maximizing the entropy of potential outcomes. This approach enables autonomous agents or robots to determine the optimal location for their next measurement, thereby planning an optimal path. The algorithm's versatility allows it to accommodate both continuous and discrete states and controls, as well as stochastic or deterministic agent dynamics, including Markov decision processes. By leveraging recent advances in approximate dynamic programming and reinforcement learning, such as on-line methods like rollout and Monte Carlo tree search, agents can solve measurement tasks in real-time, yielding near-optimal solutions that often surpass traditional greedy heuristics, like maximizing individual measurement entropy. For instance, in a global search problem, on-line planning combined with extended local search reduced the required number of measurements by half, as seen in Figure A (Reference [1], Figure B, [2], and Figure C [3]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0121", "problem_id": "01210001", "content": "Time series data often result from the interplay of continuous deterministic dynamics and discrete stochastic events, yet the underlying equations governing these processes are typically unknown. To address this challenge, we propose Neural Jump Stochastic Differential Equations, a data-driven framework for capturing both continuous and discrete dynamics in hybrid systems characterized by flows and jumps. By incorporating a stochastic process term into the Neural Ordinary Differential Equations framework, our approach effectively models the impact of discrete events on continuous behavior. Furthermore, we represent temporal point processes using a piecewise-continuous latent trajectory, where stochastic events with latent state-dependent conditional intensity induce discontinuities. The efficacy of our model is demonstrated through its predictive performance on various synthetic and real-world datasets, including marked point processes (e.g., Hawkes processes), Stack Overflow awards, medical records, and earthquake monitoring data, as shown in Figure A, B, C (see References [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0122", "problem_id": "01220001", "content": "For edge computing applications like medical devices and IoT, classifiers requiring minimal on-chip computational and memory resources are crucial. This paper presents an oblique decision tree-based machine learning model designed for resource-efficient classification on neural implants. Our approach integrates model compression with probabilistic routing and cost-aware learning, substantially decreasing memory usage and hardware costs compared to existing models, without sacrificing classification accuracy. We evaluated the performance, memory footprint, and hardware needs of our resource-efficient oblique tree with power-efficient regularization (ResOT-PE) across three neural classification tasks. In seizure detection, using intracranial EEG data from 10 epilepsy patients, we achieved a 3.4X reduction in model size and a 14.6X decrease in feature extraction cost compared to boosted tree ensembles. In tremor detection for Parkinson's disease, employing local field potentials from 12 deep-brain stimulation (DBS) patients, the ResOT-PE model matched the classification performance of state-of-the-art boosted tree ensembles while reducing model size by 10.6X and feature extraction cost by 6.8X. Furthermore, in a 6-class finger movement detection task using ECoG recordings from 9 subjects, we observed a 17.6X reduction in model size and a 5.1X decrease in feature computation cost. These results suggest the proposed model facilitates the creation of low-power, memory-efficient classifiers for real-time neurological disease detection and motor decoding.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0123", "problem_id": "01230001", "content": "Capturing features across various scales is crucial for many vision-related tasks. Recent developments in backbone convolutional neural networks (CNNs) have showcased enhanced abilities to represent multi-scale features, resulting in significant performance improvements across diverse applications. Nevertheless, the majority of current approaches acquire multi-scale features in a layered fashion. In this paper, we introduce a novel component for CNNs called Res2Net, which establishes hierarchical residual-like connections within a single residual block. This architecture enables the representation of multi-scale features at a more detailed level and expands the receptive fields of individual layers within the network. The Res2Net block can be incorporated into advanced backbone CNN architectures such as ResNet, ResNeXt, and DLA. We assess the performance of the Res2Net block within these models and display consistent enhancements over baseline models on popular datasets, including CIFAR-100 and ImageNet. Additional ablation studies and experimental findings on key computer vision tasks, such as object detection, class activation mapping, and salient object detection, further confirm the superiority of Res2Net compared to leading baseline methods. The source code and trained models can be accessed at https://mmcheng.net/res2net/.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0124", "problem_id": "01240001", "content": "Objective: The aim is to create an effective and scalable method for predicting individual patient costs by automatically discovering hidden temporal patterns within multivariate time series data from patient insurance claims, utilizing a convolutional neural network (CNN) framework. Methods: We analyzed three years' worth of medical and pharmacy claims data covering 2013 to 2016 from a healthcare insurer, employing the first two years of data to develop a model that forecasts costs for the third year. This data comprised multivariate time series encompassing costs, visits, and medical features, structured as images representing patients' health status (i.e., matrices with time windows along one dimension and medical, visit, and cost features along the other). The CNN method with a proposed architecture received these multivariate time series images. Following hyper-parameter optimization, the architecture was structured with three blocks of convolution and pooling layers featuring an LReLU activation function and customized kernel sizes at each layer tailored for healthcare data. The temporal patterns identified by the CNN served as inputs for a fully connected layer. Conclusions: The feature learning achieved through the suggested CNN configuration notably enhanced individual-level healthcare cost predictions. The proposed CNN outperformed traditional temporal pattern detection techniques that rely on a predefined set of pattern shapes, as it can capture a variable number of patterns with diverse shapes. The temporal patterns derived from medical, visit, and cost data significantly boosted prediction accuracy. Hyper-parameter tuning indicated that employing three-month data patterns resulted in the highest predictive accuracy. Our findings highlighted that images generated from multivariate time series data differ from standard images, necessitating specialized CNN architecture designs.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0125", "problem_id": "01250001", "content": "Graph representation learning aims to map each node in a graph to a low-dimensional vector. Current approaches fall into two groups: generative models that capture the graph's connectivity patterns and discriminative models that estimate edge formation probabilities. This paper introduces GraphGAN, a novel framework combining both methods through a minimax game, where the generative model approximates the true connectivity distribution and generates synthetic samples to mislead the discriminative model, while the latter distinguishes real from generated samples. This adversarial interaction enhances both models iteratively. Additionally, we present a graph-aware softmax alternative to address traditional softmax limitations, ensuring normalization, structural awareness, and efficiency. Experiments on real-world datasets show GraphGAN outperforms existing methods in tasks like link prediction, node classification, and recommendation.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0126", "problem_id": "01260001", "content": "Pedestrian detection is a crucial component in various applications, including autonomous driving, and its performance can be substantially enhanced by leveraging semantic segmentation results as self-attention cues. To this end, we present a multi-task network that simultaneously learns semantic segmentation and pedestrian detection from image datasets annotated with weak box-wise labels. By concatenating semantic segmentation feature maps with corresponding convolutional feature maps, our approach generates more discriminative features for detecting and classifying pedestrians. The joint learning of segmentation and detection enables our proposed pedestrian self-attention mechanism to accurately identify pedestrian regions while suppressing background noise. Furthermore, we integrate semantic attention information from multiple scales into a deep convolutional neural network to enhance pedestrian detection capabilities. As demonstrated by our experimental results, the proposed method yields state-of-the-art detection performance, achieving a miss rate of 6.27% on the Caltech dataset and competitive results on the CityPersons dataset, all while maintaining high computational efficiency, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0127", "problem_id": "01270001", "content": "The Mixture-of-Expert (MoE) approach has shown great promise in scaling language models to trillions of parameters, but training such large-scale MoE models necessitates a carefully designed distributed training system that optimizes both algorithms and hardware. Currently, the only available platform capable of supporting trillion-scale MoE training relies heavily on Google's proprietary TPU hardware and Mesh Tensorflow software, limiting accessibility for the broader research community, particularly those utilizing GPUs and PyTorch. To address this, we introduce FastMoE, a PyTorch-based distributed training system for MoE models that leverages common accelerators, offering a hierarchical interface that facilitates flexible model design and seamless adaptation to various applications, including Transformer-XL and Megatron-LM, as seen in Figure A. Unlike straightforward PyTorch implementations of MoE models, FastMoE achieves significantly optimized training speeds through advanced high-performance acceleration techniques, and it allows for the distribution of experts across multiple GPUs and nodes, enabling a linear increase in the number of experts with the number of GPUs, as demonstrated in Figure B (cited in [1]). The FastMoE system is openly available at https://github.com/laekov/fastmoe under the Apache-2 license.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0128", "problem_id": "01280001", "content": "The editing of portrait images is a significant and widely researched topic, with numerous applications, and ideally requires an intuitive control system akin to those used in computer animation. However, most existing methods lack fine-grained control or only allow for coarse adjustments of isolated parameters. Although recent advances have demonstrated high-quality semantically controlled editing, this has been limited to synthetically generated StyleGAN images. This work presents a novel approach for embedding real portrait images into the StyleGAN latent space, enabling intuitive and detailed editing of head pose, facial expression, and scene illumination. By leveraging StyleRig, a pretrained neural network that maps a 3D morphable face model's control space to the GAN's latent space, semantic editing is achieved in the parameter space. A hierarchical non-linear optimization problem is formulated to obtain the embedding, incorporating an identity preservation energy term to ensure spatially coherent edits that maintain facial integrity. The resulting method operates at interactive frame rates, allowing users to explore various edits, and its effectiveness is validated through a comprehensive evaluation on a diverse set of portrait photos, comparison to state-of-the-art methods, and an ablation study, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0129", "problem_id": "01290001", "content": "This paper addresses the challenge of acquiring collaborative behavior through communication in multi-agent systems via deep reinforcement learning. We introduce a connectivity-driven communication (CDC) algorithm that focuses on three essential components: determining which agents should participate in communication, deciding what information to convey, and establishing the frequency of these exchanges. The multi-agent system is conceptualized as a weighted graph where nodes symbolize agents. The unspecified edge weights indicate the level of interaction between agent pairs, influenced by a diffusion mechanism on the graph known as the heat kernel. An optimal communication approach is developed in conjunction with the agents' policy to enhance future expected returns, considering the overall topology of the graph. Experimental findings demonstrate that CDC outperforms alternative algorithms across various cooperative navigation tasks and that the resulting graph structures are interpretable.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0130", "problem_id": "01300001", "content": "This paper introduces a hierarchical model designed for pixel clustering and image segmentation. The model structures an image hierarchically, treating the original image as a collection of nested images that can be merged reversibly. An object is defined as a structural element within the image, with the image itself considered a maximal object. The study explores the simulation of non-hierarchical optimal pixel clustering through hierarchical clustering methods. Furthermore, the conversion of any approximation hierarchy into a hierarchy related to the number of intensity levels, using a convex sequence of total squared errors, is proposed to produce a hierarchy of optimized piecewise constant image approximations, estimated via the standard deviation of approximation from the image.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0131", "problem_id": "01310001", "content": "Deep neural networks (DNNs) have recently demonstrated exceptional results in various low-level vision applications. Nevertheless, top-performing models often rely on extremely deep architectures, comprising dozens of layers and millions of parameters. To enhance the feasibility of deploying DNNs on resource-constrained devices, there is a need to improve the balance between accuracy and computational efficiency. This work introduces a novel activation unit specifically designed for image restoration tasks. Unlike conventional per-pixel activation functions such as ReLUs and sigmoids, our proposed unit incorporates learnable nonlinear operations with spatial dependencies, allowing the network to learn more intricate features while requiring fewer layers to achieve comparable performance. We validate the efficacy of our approach through experiments on cutting-edge networks for tasks like denoising, de-raining, and super-resolution, which are already compact. Our method reduces these models by approximately 50% while maintaining their original performance levels.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0132", "problem_id": "01320001", "content": "The advancement of deep learning in video-based human action recognition is hampered by the need for extensive, manually labeled video data, which is a costly endeavor. This study explores the use of synthetic training data for action recognition, building on its success in other computer vision applications. We present an interpretable parametric generative model for human action videos, leveraging procedural generation and computer graphics techniques found in contemporary game engines. The resulting dataset, named PHAV (\"Procedural Human Action Videos\"), comprises 39,982 videos exhibiting diversity, realism, and physical plausibility, with over 1,000 instances for each of the 35 action categories. Our methodology extends beyond existing motion capture data by procedurally defining 14 synthetic actions. We introduce a deep multi-task representation learning framework to integrate both synthetic and real videos, even when action categories vary. Experimental results on the UCF101 and HMDB51 benchmarks demonstrate that supplementing small real-world datasets with our substantial synthetic video dataset improves recognition accuracy, surpassing the performance achieved by fine-tuning advanced unsupervised generative video models.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0133", "problem_id": "01330001", "content": "In developing a semantic segmentation system for real-world applications like autonomous vehicles, assessing the module's resilience to diverse image distortions is essential. Although recent research has explored robustness in full-image classification, this work provides the first comprehensive analysis for semantic segmentation using the advanced DeepLabv3+ model. To enhance the study's realism, we employ nearly 400,000 images derived from Cityscapes, PASCAL VOC 2012, and ADE20K. Our benchmark analysis reveals key findings: first, unlike full-image classification, model robustness generally improves alongside performance. Second, certain architectural features, such as the Dense Prediction Cell—originally optimized for clean data—have a notable impact on robustness.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0134", "problem_id": "01340001", "content": "Temporal action localization focuses on identifying the start and end times associated with specific action categories. Due to limitations in GPU memory, prevailing approaches rely on pre-extracting features for each video, making the quality of these features crucial for detection performance. In this technical report, we examined traditional convolutional backbones alongside the recent advancements in transformer-based backbones. Our findings indicate that transformer-based techniques can provide superior classification performance compared to their convolutional counterparts; however, they struggle to produce accurate action proposals. Furthermore, utilizing features with a higher frame resolution helps mitigate the loss of spatial information, thereby enhancing the effectiveness of temporal action localization. Ultimately, we achieved a mean Average Precision (mAP) of 42.42% on the validation set using a straightforward combination of BMN+TCANet with a single SlowFast feature, surpassing the 2020 multi-model ensemble result by 1.87%. As a result, we secured the 1st Rank at the CVPR2021 HACS supervised Temporal Action Localization Challenge.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0135", "problem_id": "01350001", "content": "FPN-based detectors have achieved notable advancements in general object detection tasks such as MS COCO and PASCAL VOC. Nonetheless, these detectors encounter challenges in specific scenarios, particularly in detecting small objects. In this study, we contend that the top-down connections among neighboring layers in FPN exert both beneficial and adverse effects on tiny object detection. We introduce an innovative concept termed the fusion factor, which regulates the information transmitted from deeper layers to shallower ones, thereby tailoring FPN for tiny object detection. Through a series of experiments and analyses, we investigate a statistical approach to estimate an optimal value for the fusion factor tailored to a specific dataset. This estimation hinges on the distribution of objects across each layer. We conduct extensive experiments on datasets focused on tiny object detection, such as TinyPerson and Tiny CityPersons. Our findings indicate that configuring FPN with an appropriate fusion factor enables the network to achieve substantial performance enhancements over the baseline in tiny object detection tasks. Codes and models will be made available.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0136", "problem_id": "01360001", "content": "Several essential graph learning tasks, including identifying interpretable subgraphs, graph denoising, and graph compression, can be framed as recognizing a subgraph within the original graph that is both highly informative and minimally redundant. This task aligns with the information bottleneck (IB) principle, which has received limited attention in the context of irregular graph data and graph neural networks (GNNs). In this paper, we introduce a Graph Information Bottleneck (GIB) framework for subgraph recognition in deep graph learning, enabling the identification of an IB-subgraph that is both maximally informative and compressive. To address the challenges of optimizing the GIB objective, primarily due to the intractability of mutual information in irregular graph data and optimization instability, we propose: i) a GIB objective based on a mutual information estimator for irregular graph data; ii) a bi-level optimization approach to maximize the GIB objective; and iii) a connectivity loss to stabilize the optimization process. We assess the IB-subgraph's properties across three applications: enhancing graph classification, graph interpretation, and graph denoising. Extensive experiments confirm the superior properties of the information-theoretic IB-subgraph.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0137", "problem_id": "01370001", "content": "Deep metric learning, particularly in the context of large-scale image data, has seen significant advancements. However, in various practical applications, we often encounter only vectorized input data. Furthermore, the availability of well-labeled data is frequently restricted due to the considerable costs associated with annotation. Additionally, real-world data is typically dynamic, necessitating online processing. Under these circumstances, traditional deep metric learning methods become inadequate. To address this issue, we reevaluate conventional shallow online metric learning and introduce a novel online progressive deep metric learning (ODML) framework that integrates a metric-algorithm-based deep network. Specifically, we implement an online metric learning algorithm as a metric layer, followed by a nonlinear layer, and organize these layers similarly to deep learning architectures. Unlike shallow online metric learning, which is limited to one metric space, our proposed ODML can learn multiple hierarchical metric spaces. Moreover, ODML demonstrates a superior learning capability in a progressive and nonlinear manner, particularly when training data is scarce. To enhance the clarity and theoretical foundation of the learning process, we also present a theoretical analysis. The ODML framework possesses several advantageous attributes and effectively learns metrics progressively while demonstrating improved performance on benchmark datasets. Comprehensive experiments across various configurations have been conducted to validate these characteristics of the proposed ODML.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0138", "problem_id": "01380001", "content": "Model-free reinforcement learning has shown great potential for addressing continuous control robotic tasks, largely fueled by recent simulation-based advancements. Open-source algorithms and benchmark simulations have enabled rapid progress in reproducing, analyzing, and expanding upon these findings. Transitioning these successes to real-world applications necessitates avoiding reliance on simulation-specific advantages and engaging in direct experimentation with physical robots. However, the absence of standardized benchmark tasks and readily available source code presents a significant obstacle to reinforcement learning research using physical robots. This paper introduces several reinforcement learning tasks, designed with varying difficulty, setup requirements, and repeatability, using several commercially available robots. We evaluate the learning performance of four readily available reinforcement learning algorithms on these tasks, examining their hyper-parameter sensitivity to assess their suitability for diverse real-world applications. Our findings indicate that with careful task interface and computation design, certain implementations can be directly applied to physical robots. We observe that state-of-the-art learning algorithms exhibit high sensitivity to hyper-parameters, and their relative performance varies across tasks, highlighting the need for task-specific re-tuning to achieve optimal results. Conversely, optimal hyper-parameter configurations from one task can often facilitate effective learning on held-out tasks, even with different robots, offering a viable default starting point. To foster reproducibility in real-world reinforcement learning, we are releasing these benchmark tasks to the public.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0139", "problem_id": "01390001", "content": "Object detection remains a formidable challenge in the field of computer vision. Despite substantial advancements achieved through deep neural networks, the development of the attention mechanism is still lagging. This paper introduces a hybrid attention mechanism tailored for single-stage object detection. Initially, we detail the components of spatial attention, channel attention, and aligned attention. Notably, we create stacked dilated convolution layers with symmetrically fixed rates to facilitate the learning of spatial attention. For channel attention, we integrate cross-level group normalization with a squeeze-and-excitation module. Aligned attention is developed using organized deformable filters. Subsequently, the three forms of attention are amalgamated to form the hybrid attention mechanism. We then incorporate this hybrid attention into Retina-Net and introduce the efficient single-stage HAR-Net for object detection. Both the attention modules and the proposed HAR-Net undergo evaluation on the COCO detection dataset. Experimental results indicate that hybrid attention significantly enhances detection accuracy, and HAR-Net achieves a state-of-the-art 45.8% mAP, surpassing existing single-stage object detectors.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0140", "problem_id": "01400001", "content": "This paper introduces Georeference Contrastive Learning of visual Representation (GeoCLR), a technique designed for the efficient training of deep-learning Convolutional Neural Networks (CNNs). The approach utilizes georeferencing data to construct similar image pairs from images of proximate locations, contrasting them with pairs from distant locations. This relies on the assumption that closely located images are more likely to exhibit visual similarities, an assumption valid in seafloor robotic imaging where image footprints are limited to a few meters and overlap along a vehicle's path, while seafloor substrates and habitats have considerably larger patch sizes. A significant benefit of this technique is its self-supervised nature, eliminating the need for human input during CNN training. The method is computationally efficient, enabling the generation of results between dives during multi-day AUV missions using typical oceanic field trial computational resources. GeoCLR is applied to habitat classification on a dataset of approximately 86,000 images collected by an Autonomous Underwater Vehicle (AUV). The study illustrates how GeoCLR-generated latent representations can effectively guide human annotation, with the semi-supervised framework enhancing classification accuracy by an average of 11.8 % when compared to current transfer learning methods, utilizing the same CNN and an equivalent number of human annotations for training.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0141", "problem_id": "01410001", "content": "Estimating networks from multivariate point process or time series data is a crucial problem. Earlier research has predominantly employed parametric methods that necessitate a predefined parametric model, which can lead to less resilient estimation processes when confronted with model mis-specification, non-linearities, and heterogeneities. This paper introduces a semi-parametric method based on the monotone single-index multivariate autoregressive model (SIMAM) that effectively tackles these issues. We offer theoretical assurances for dependent data alongside an alternating projected gradient descent algorithm. Notably, our approach does not explicitly impose mixing conditions on the process, although we do require conditions similar to restricted strong convexity. We achieve rates of the form O(T^}) (which is optimal in the independent design scenario), where s denotes the threshold for the maximum in-degree of the network indicating sparsity, M refers to the number of actors, and T is the total number of time points. Furthermore, we illustrate the enhanced performance of our SIMAM method, surpassing leading parametric techniques in terms of both prediction and network estimation in tests conducted on simulated data as well as two real-world data cases.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0142", "problem_id": "01420001", "content": "We introduce the Compressive Transformer, an attention-based sequence model that condenses historical information for long-range sequence learning. Our results indicate that the Compressive Transformer achieves leading language modeling performance on the WikiText-103 and Enwik8 benchmarks, with scores of 17.1 perplexity and 0.97 bits per character, respectively. Additionally, we demonstrate its effectiveness in modeling high-frequency speech and its applicability as a memory system for reinforcement learning, validated through an object matching task. To advance the field of long-range sequence learning, we present a new open-vocabulary language modeling benchmark based on literature, termed PG-19.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0143", "problem_id": "01430001", "content": "Universal adversarial perturbations (UAPs), also known as input-agnostic perturbations, have been shown to exist and can deceive state-of-the-art deep learning models across a majority of data samples. Most existing UAP techniques have concentrated on attacking image classification models; however, there has been insufficient focus on inflicting harm on image retrieval systems. This paper represents the initial attempt to target image retrieval systems. Specifically, an image retrieval attack aims to manipulate the system into presenting unrelated images at the top of the ranking list in response to a query. This process significantly disrupts the neighborhood relationships among features in image retrieval. To achieve this, we introduce a novel approach to generate retrieval-targeted UAPs that undermine the neighborhood relationships of image features by degrading the associated ranking metric. Furthermore, to adapt the attack method for situations involving varying input sizes or fixed network parameters, we propose a multi-scale random resizing strategy and a ranking distillation technique. We assess the efficacy of our proposed method across four commonly used image retrieval datasets, reporting a notable decline in performance across various metrics, including mAP and mP@10. Finally, we apply our attack methods to the real-world visual search engine, Google Images, highlighting the practical applicability of our techniques.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0144", "problem_id": "01440001", "content": "Creating accurate 3D maps is difficult and resource-intensive, demanding precise sensors and meticulous scanning. This work aims to lower the costs of generating reliable reconstructions by refining existing, substandard models after creation, leveraging learned priors for surface geometry and appearance. A convolutional neural network (CNN) is trained to estimate the disparity in inverse depth between multiple views of two meshes: the low-quality mesh targeted for correction and a high-quality reference mesh. Unlike prior methods, this approach specifically addresses the issue of over-smoothing in corrected meshes, mitigated through a tailored network architecture and a loss-weighting strategy that accentuates edges in the predicted output. Furthermore, to resolve geometric inconsistencies arising from smooth predictions, a loss function is introduced to penalize re-projection discrepancies not caused by occlusions. The proposed model achieves a reduction in significant errors ranging from 45.3% to 77.5%, representing an improvement of up to fivefold compared to previous research.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0145", "problem_id": "01450001", "content": "This paper presents a novel approach to monocular 3D object detection, wherein a deep neural network is trained to evaluate the fitting degree between proposed 3D objects and actual objects, enabling accurate localization without relying on stringent constraints. Unlike existing frameworks that typically employ tight constraints to determine 3D locations, our method assesses the visual consistency between projected 3D proposals and objects to achieve high-precision localization. The process begins with regressing object dimensions and orientation using an anchor-based method to construct suitable 3D proposals. We introduce FQNet, a network capable of estimating the 3D Intersection over Union (IoU) between 3D proposals and objects based solely on 2D cues. During detection, numerous 3D candidate proposals are sampled, projected onto the 2D image, and evaluated using FQNet's 3D IoU score, allowing for the selection of the best candidate by examining spatial overlaps. The efficacy of our framework is validated through experiments conducted on the KITTI dataset, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0146", "problem_id": "01460001", "content": "Unmanned vehicles increasingly depend on advanced collision avoidance systems (CAS) to achieve autonomy. Detecting obstacles, particularly at night, remains difficult because traditional cameras struggle in low-light environments. To address this, we use the capabilities of event-based cameras for obstacle detection under dim lighting. These cameras asynchronously generate events at a high temporal rate and possess a high dynamic range reaching 120 dB. Our algorithm filters out background noise and identifies objects using a robust Hough transform method. Object depth is then calculated by triangulating 2D features extracted via LC-Harris. Finally, an asynchronous adaptive collision avoidance (AACA) algorithm is implemented for effective obstacle avoidance. A qualitative assessment compares the performance of the event camera with that of a traditional camera.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0147", "problem_id": "01470001", "content": "Recent developments in attention-based networks have demonstrated that Vision Transformers can yield state-of-the-art or close to state-of-the-art performance in various image classification tasks, positioning them as a viable alternative to conventional convolutional neural networks (CNNs). While CNNs have been extensively examined concerning adversarial attacks, Vision Transformers have not received the same level of scrutiny. In this paper, we investigate the resilience of Vision Transformers against adversarial examples. Our examination of transformer security is organized into three segments. Initially, we evaluate the transformer’s performance under standard white-box and black-box attacks. Next, we explore the transferability of adversarial examples between CNNs and transformers, revealing that such examples do not easily transfer between the two architectures. Following this discovery, we assess the security of a simple ensemble comprising CNNs and transformers. By introducing a novel attack, termed the self-attention blended gradient attack, we demonstrate that this ensemble lacks security against a white-box adversary. Nonetheless, against a black-box adversary, we find that the ensemble can achieve exceptional robustness without compromising clean accuracy. Our analysis involves six types of white-box attacks and two types of black-box attacks, and encompasses various Vision Transformers, Big Transfer Models, and CNN architectures trained on CIFAR-10, CIFAR-100, and ImageNet.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0148", "problem_id": "01480001", "content": "Despite the increasing availability of healthcare data, a limited amount, primarily structured data, is leveraged for predicting patient outcomes. Unstructured clinical notes, containing valuable subjective details, could potentially improve predictive accuracy, particularly in mortality prediction, but are often underused. This study investigates the performance improvements achievable by using multiple, minimally preprocessed notes as predictive inputs. A hierarchical architecture, incorporating convolutional and recurrent layers, is employed to simultaneously model diverse notes from a single hospital admission. The methodology is assessed by predicting in-hospital mortality using the MIMIC-III dataset. Compared to methods relying on structured data, the proposed approach attained superior results with less data cleaning and preprocessing. These findings highlight the potential of unstructured data to improve mortality prediction and emphasize the importance of integrating more raw unstructured data into existing clinical prediction techniques.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0149", "problem_id": "01490001", "content": "Covariance kernels are fundamental in spatial statistics, enabling the transformation of data into high-dimensional spaces and extending linear methods to nonlinear approaches with higher-order interactions. Traditionally, models have predominantly relied on stationary kernels like the Matern or squared exponential, which limits expressiveness. Recent advances in machine learning have explored spectral representations for arbitrary stationary kernels and more general representations encompassing nonstationary kernels. This paper leverages the relationships between Fourier feature representations, Gaussian processes, and neural networks to create a simplified, efficient framework for learning complex, nonstationary kernel functions directly from data. This framework employs advanced deep learning techniques to mitigate overfitting and facilitates the creation of a diverse range of kernel classes. We demonstrate the framework's applicability using a time series dataset and a remote sensing problem focused on land surface temperature in Eastern Africa. The results indicate that nonstationary kernels enhance generalisation performance and improve interpretability without increasing computational or storage demands.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0150", "problem_id": "01500001", "content": "The majority of learning algorithms lack scale invariance, meaning their performance is affected by the magnitude of the function being approximated. To address this limitation, we introduce an adaptive target normalization approach, which is particularly beneficial in value-based reinforcement learning where the optimal value approximation scales can vary over time as the policy is updated. Our approach is motivated by previous research on learning to play Atari games, where rewards were clipped to a fixed range, as seen in Figure A (Reference [1]), to enable a single learning algorithm to be applied across multiple games, although this clipping can lead to distinct behavioral outcomes, as illustrated in Figure B (Reference [2]). By utilizing adaptive normalization, we can eliminate the need for this domain-specific clipping heuristic without compromising overall performance, as demonstrated in Figure C (Reference [3]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0151", "problem_id": "01510001", "content": "Modern deep learning techniques for graph representation learning often rely on neighborhood aggregation methods. This study examines key characteristics of such models and introduces an approach to address their limitations. Specifically, the extent of influence from neighboring nodes in a node's representation varies with graph topology, similar to random walk dynamics. To enhance adaptability to local structural features and task requirements, we present jumping knowledge (JK) networks—an architecture that dynamically selects appropriate neighborhood ranges for each node, improving structure-aware representations. Experimental results across social networks, bioinformatics datasets, and citation graphs show our model outperforms existing methods. Additionally, integrating the JK framework with established models such as Graph Convolutional Networks, GraphSAGE, and Graph Attention Networks yields consistent performance gains across these architectures.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0152", "problem_id": "01520001", "content": "Significant attention has been given to exploring spatiotemporal correlations to enhance the accuracy of traffic speed predictions. Nevertheless, existing approaches usually focus on modeling these correlations solely based on the observed traffic state (e.g., traffic speed), overlooking the fact that different correlation measurements of the traffic data may reveal a variety of patterns in distinct traffic conditions. Moreover, these approaches generally assume a uniform sampling frequency for all road segments, which is not realistic. This paper introduces new measurements for modeling the spatial correlations among traffic data, demonstrating that the resulting correlation patterns differ significantly across various traffic scenarios. We present a Heterogeneous Spatial Correlation (HSC) model to capture spatial correlations tailored to specific measurements, allowing for heterogeneity in traffic data across different road segments (i.e., gathered with varying sampling frequencies). Additionally, we propose a Multi-fold Correlation Attention Network (MCAN), which leverages the HSC model to investigate multi-fold spatial correlations and employs LSTM networks to capture multi-fold temporal correlations, resulting in distinct features that facilitate accurate traffic predictions. The learned multi-fold spatiotemporal correlations, along with contextual factors, are integrated using an attention mechanism for final predictions. Experiments conducted on real-world datasets show that the MCAN model significantly outperforms contemporary baselines.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0153", "problem_id": "01530001", "content": "MeanShift, a widely utilized mode-seeking clustering algorithm in machine learning, is hindered by its slow performance, characterized by a quadratic runtime per iteration. To address this limitation, we introduce MeanShift++, a significantly accelerated mode-seeking algorithm that builds upon MeanShift by leveraging a grid-based approach to expedite the mean shift step. This is achieved by substituting the computationally intensive neighbors search with a density-weighted mean of adjacent grid cells, which also provides theoretical guarantees for density estimation. The resulting algorithm exhibits a runtime that is linear in the number of points and exponential in dimension, rendering MeanShift++ particularly suited for low-dimensional applications, such as image segmentation and object tracking, as demonstrated through extensive experimental analysis, where MeanShift++ is shown to be over 10,000 times faster than MeanShift while maintaining competitive clustering results on benchmark datasets and yielding nearly identical image segmentations, with promising results also obtained for object tracking.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0154", "problem_id": "01540001", "content": "This study introduces a method for automated theorem proving that leverages a substantial knowledge base of potential premises, circumventing the need to learn from human-generated proofs. We propose an exploration strategy that incorporates supplementary premises chosen via a tf-idf-based retrieval method within a deep reinforcement learning framework. This approach facilitates exploration and learning to identify pertinent premises for proving new theorems. Empirical results indicate that the theorem prover trained with this exploration strategy surpasses the performance of provers trained solely on human proofs, achieving a level of effectiveness comparable to that of a prover trained using a combination of imitation and reinforcement learning. Furthermore, we conduct several experiments to assess the significance of the core assumptions underpinning our exploration method, thereby elucidating our design decisions.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0155", "problem_id": "01550001", "content": "For the practical deployment of reinforcement learning (RL), ensuring safe exploration is paramount. Prior research has framed this challenge as a Constrained Markov Decision Process (CMDP), focusing on policy optimization within predefined constraints. However, humans typically halt actions upon perceiving potential risks, limiting their ability to learn safe behaviors in hazardous situations. Inspired by this human tendency, we present a novel methodology for safe RL, utilizing the Early Terminated MDP (ET-MDP) framework. Initially, we formulate the ET-MDP as an unconstrained MDP, demonstrating its equivalence in optimal value function to the original CMDP. Subsequently, we introduce an off-policy algorithm leveraging context models to effectively solve the ET-MDP, leading to a solution for the corresponding CMDP with enhanced asymptotic performance and learning efficiency. Experimental results across diverse CMDP tasks demonstrate significant performance gains compared to conventional CMDP-solving techniques.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0156", "problem_id": "01560001", "content": "This paper presents comprehensive proofs for the lemmas concerning the characteristics of the regularized loss function employed in second-order methods designed for learning time-series data exhibiting structural breaks, as described in Osogami (2021). Furthermore, empirical evidence is provided to corroborate the effectiveness of these techniques.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0157", "problem_id": "01570001", "content": "This research presents an examination of modeling water consumption across the city of London, Canada. Various modeling strategies were analyzed for univariate time series forecasting related to water usage, such as linear regression, Facebook's Prophet method, recurrent neural networks, and convolutional neural networks. The Prophet method emerged as the preferred model, demonstrating a mean absolute percentage error of 2.51% when averaged over a 5-fold cross-validation. Additionally, Prophet was recognized for its other beneficial features valuable to stakeholders in water demand management, such as built-in interpretability and effective management of missing data. The implementations of the methods discussed in this paper have been made open source, allowing for potential adaptation by other municipalities.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0158", "problem_id": "01580001", "content": "Significant advancements have been made in machine learning and computer vision tasks, such as classification, detection, and segmentation, through the application of Domain Adaptation (DA) approaches. Nevertheless, direct domain adaptation on 3D point cloud data remains a relatively unexplored area, despite the unique challenges posed by the spatial geometric information and regional geometric structures that contribute to the semantics of objects. Conventional DA methods, which focus on global feature alignment while neglecting local geometric information, are ill-suited for 3D domain alignment. This paper introduces a novel 3D Domain Adaptation Network, termed PointDAN, which simultaneously aligns global and local features across multiple levels. To achieve local alignment, a Self-Adaptive (SA) node module is proposed, featuring an adjustable receptive field to model discriminative local structures and facilitate domain alignment. Additionally, a node-attention module is employed to weigh the relationships between SA nodes across objects and domains, representing hierarchically scaled features. For global alignment, an adversarial training strategy is utilized to learn and align global features across domains. Due to the lack of a standard evaluation benchmark for 3D point cloud DA, a new benchmark, PointDA-10, is established by extracting data from three popular 3D object/scene datasets (ModelNet, ShapeNet, and ScanNet) for cross-domain 3D object classification. Comprehensive experiments conducted on PointDA-10 demonstrate the superiority of the proposed PointDAN model over state-of-the-art general-purpose DA methods (Figure A, B, C) as shown in References [citation].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0159", "problem_id": "01590001", "content": "We introduce a novel deep autoencoder architecture that incorporates graph topology inference and filtering to generate compact representations of unstructured 3D point clouds in an unsupervised setting. Unlike previous approaches that rely on discretizing 3D points into voxels and applying lattice-based methods, which inherently introduce discretization errors, our method processes raw 3D points directly. The proposed network adheres to the autoencoder framework, with a particular emphasis on decoder design, where the encoder draws inspiration from PointNet architectures. The decoder comprises three innovative modules: a folding module that maps a canonical 2D lattice onto the underlying surface of a 3D point cloud to achieve initial reconstruction, a graph-topology-inference module that learns a graph topology to capture pairwise relationships between 3D points and preserves both point coordinates and relationships in the latent code, and a graph-filtering module that integrates the preceding modules to refine the reconstruction through a learned graph topology. By leveraging a learnable graph topology, the proposed decoder enhances the preservation of salient features in the codeword, thereby improving unsupervised learning performance. Theoretical analyses of the proposed architecture are also provided. Experimental validation of the proposed networks is conducted across three tasks: 3D point cloud reconstruction, visualization, and transfer classification, yielding results that demonstrate (1) superior performance compared to state-of-the-art methods, (2) the ability to infer graph topology as auxiliary information without explicit supervision, and (3) the effectiveness of graph filtering in refining reconstructions and enhancing overall performance.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0160", "problem_id": "01600001", "content": "Time series data are essential in numerous applications, and their analysis offers critical insights into the mechanisms driving the data. Within time series learning, this study emphasizes semi-supervised learning using a graph-based data representation, which involves selecting an appropriate distance measure for assessing time series similarities and employing a learning technique to generate predictions based on these distances. The interplay between these elements, however, remains largely unexplored within graph-based learning. This work presents four distance measures, including (Soft) DTW and MPDist, which utilizes the Matrix Profile. It also uses four effective semi-supervised learning algorithms, including the graph Allen--Cahn method and a Graph Convolutional Neural Network. The algorithms' performance is then compared using binary classification datasets. The results demonstrate considerable variability in accuracy across different distance measures for the graph-based methods examined. In line with the \"no free lunch\" theorem, no single combination consistently outperforms others. This research establishes a replicable framework to support future semi-supervised time series learning research, focusing on graph representations.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0161", "problem_id": "01610001", "content": "The process of preparing and scanning histopathology slides involves multiple steps with numerous parameters that can differ significantly between pathology laboratories and even within the same laboratory over time, leading to substantial variability in tissue appearance and hindering the applicability of automated image analysis techniques. To mitigate this issue, methods such as staining normalization are often employed to reduce variability in appearance, but these approaches are typically ad-hoc. This paper presents a systematic approach utilizing domain-adversarial neural networks, which is based on the hypothesis that eliminating domain information from model representations enhances generalizability. An experimental evaluation of this hypothesis was conducted on the task of mitosis detection in breast cancer histopathology images, and a comparative analysis was performed with two alternative approaches, Figure A, B, C (as seen in References [citation]). The results demonstrate that combining color augmentation with domain-adversarial training yields better generalization performance for deep learning methods compared to conventional approaches.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0162", "problem_id": "01620001", "content": "Self-supervised image learning aims to create semantically relevant image representations using pretext tasks on extensive unlabeled image datasets, thus avoiding the need for semantic annotations. Recognizing that existing pretext tasks often produce representations that vary with image transformations, we propose that semantic representations should instead remain invariant to these transformations. To this end, we introduce Pretext-Invariant Representation Learning (PIRL), designed to learn invariant representations through pretext tasks. Applying PIRL to the common jigsaw puzzle pretext task significantly enhances the semantic quality of the resulting image representations. Our method achieves state-of-the-art performance in self-supervised image learning across multiple standard benchmarks. Notably, PIRL, despite its unsupervised nature, surpasses supervised pre-training in learning image representations for object detection. These findings highlight the promise of self-supervised learning in generating image representations with desirable invariance characteristics.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0163", "problem_id": "01630001", "content": "Network quantization is a crucial technique for minimizing neural network size, enabling deployment on devices with limited resources by decreasing the bit-lengths of network weights and activations. Recent work, Relaxed Quantization (RQ) [Louizos et al. 2019], effectively utilizes the Gumbel-Softmax technique to address the challenge of converting continuous activations and weights into discrete values, facilitating efficient gradient-based optimization. Nevertheless, RQ's Gumbel-Softmax relaxation is still affected by the bias-variance trade-off, which is dependent on the Gumbel-Softmax temperature parameter. To mitigate this, we introduce Semi-Relaxed Quantization (SRQ), a novel approach employing a multi-class straight-through estimator to substantially decrease bias and variance. Additionally, we propose DropBits, a novel regularization method that replaces dropout regularization by randomly dropping bits instead of neurons, further reducing the bias of the multi-class straight-through estimator in SRQ. Extending DropBits, we present a method for learning heterogeneous quantization levels, enabling the determination of suitable bit-lengths for each layer. Our method is experimentally validated on diverse benchmark datasets and network architectures, supporting the quantized lottery ticket hypothesis: training with heterogeneous quantization levels from scratch outperforms using identical, fixed quantization levels.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0164", "problem_id": "01640001", "content": "Utilizing synthetic images is a promising approach to mitigate the high costs associated with creating annotated datasets for training supervised convolutional neural networks (CNN), but achieving generalizability from synthetic to real images requires domain adaptation techniques. Unsupervised domain adaptation (UDA) methods are applied to an anchorless object detector in this study, as these detectors have garnered significant attention in object detection due to their comparable performance to anchor-based methods and substantially faster processing speeds. CenterNet, a recent anchorless architecture, is employed for a domain adaptation task involving synthetic images, and two UDA methods, entropy minimization and maximum squares loss, initially designed for segmentation tasks, are adapted for object detection. The proposed UDA methods yield a notable improvement, increasing the mAP from 61% to 69% compared to direct transfer on the anchorless detector, demonstrating their effectiveness. The code is available: https://github.com/scheckmedia/centernet-uda.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0165", "problem_id": "01650001", "content": "The application of mutual information in learning latent representations of observations is well-established, yet its role in classification neural networks requires further clarification. Our research demonstrates that optimising the parameters of classification neural networks using softmax cross-entropy is essentially equivalent to maximising the mutual information between inputs and labels, assuming a balanced dataset. Experimental results on both synthetic and real datasets reveal that softmax cross-entropy can serve as a proxy for estimating mutual information. By leveraging this relationship, we can approximate the point-wise mutual information between an input image and its corresponding label without altering the network architecture, particularly in image classification tasks. To exploit this insight, we introduce infoCAM, an informative class activation map that identifies the most informative regions of an input image with respect to a given label by quantifying differences in information. This activation map facilitates the localisation of target objects within input images, and its effectiveness is validated through semi-supervised object localisation experiments conducted on two real-world datasets, as shown in Figure A, B, C (see References [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0166", "problem_id": "01660001", "content": "The acquisition of imaging data across multiple wavelengths of the electromagnetic spectrum simultaneously poses significant technical difficulties, typically necessitating the use of sophisticated and expensive multispectral image sensors. This research presents a novel, comprehensive approach to achieving simultaneous multispectral imaging utilizing standard image sensors equipped with color filter arrays, through the application of numerical demultiplexing techniques to the measurements obtained from these sensors. By developing a numerical forward model that describes how sensor measurements are generated from incident light spectra, based on an in-depth spectral characterization of the sensor, a numerical demultiplexing process can be learned using non-linear random forest modeling. This learned demultiplexer enables the decomposition of simultaneously captured sensor measurements into reflectance intensities at specific, selectable wavelengths, yielding a higher resolution reflectance spectrum. The effectiveness of this methodology for simultaneous multispectral imaging is validated through both simulated and real-world experimental results, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0167", "problem_id": "01670001", "content": "A significant hurdle in training deep neural networks for visual detection and recognition tasks is the necessity of extensive annotated image datasets. This paper introduces a new image synthesis method designed to produce a substantial quantity of annotated scene text images for training precise and resilient scene text detection and recognition models. The proposed method incorporates three novel elements. First, it achieves \"semantic coherent\" synthesis by positioning text within semantically appropriate areas of the background image, utilizing semantic annotations of objects and image regions derived from existing semantic segmentation research. Second, it uses visual saliency to identify embedding locations within each semantically suitable region, mirroring the common practice of placing text near homogeneous areas for enhanced visibility in real-world scenes. Third, it employs an adaptive text appearance model, which learns from the characteristics of real scene text images to determine the color and brightness of the embedded text. Evaluations conducted on five public datasets demonstrate that the proposed technique excels in training accurate and robust scene text detection and recognition models.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0168", "problem_id": "01680001", "content": "The problem of ranking items based on pairwise comparisons is revisited, with a focus on leveraging associated feature information to improve ranking accuracy. While previous research has shown that O(n\\log(n)) samples are required for effective ranking in the absence of feature information, the presence of such features may enable more efficient ranking methods. To address this, a novel probabilistic preference model, the feature-Bradley-Terry-Luce (f-BTL) model, is introduced, extending the standard BTL model to incorporate feature information. A corresponding least squares-based algorithm, fBTL-LS, is presented, which achieves good ranking performance with a significantly reduced sample complexity of O(\\alpha\\log \\alpha), where \\alpha represents the number of 'independent items' in the set, typically much smaller than n. The analysis, which utilizes tools from classical graph matching theory, provides tighter bounds that reveal the true complexity of the ranking problem by capturing item dependencies through their feature representations, outperforming earlier matrix completion-based approaches. Furthermore, an information theoretic lower bound is established, demonstrating the tightness of the proposed algorithm, and extensive experiments on synthetic and real-world datasets validate its efficacy, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0169", "problem_id": "01690001", "content": "Numerous applications in robotics and human-computer interaction require accurate 3D motion estimation of points in dynamic scenes, commonly referred to as scene flow. Although existing approaches predominantly utilize stereo or RGB-D images, limited research explores direct scene flow estimation from point clouds. This paper introduces FlowNet3D, an end-to-end deep neural network designed to predict scene flow from point cloud data. The architecture incorporates two novel learning layers for point sets, enabling joint learning of hierarchical point cloud features and motion-representing flow embeddings. Evaluations on synthetic data from FlyingThings3D and real-world Lidar scans from KITTI demonstrate that our network, trained solely on synthetic data, generalizes effectively to real scans, surpassing baseline methods and achieving performance comparable to state-of-the-art techniques. Additionally, we showcase practical applications of our scene flow predictions in scan registration and motion segmentation, highlighting its broad applicability.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0170", "problem_id": "01700001", "content": "The utilization of neural networks for analyzing physiological time-series data has gained significant traction, with the most effective deep learning systems leveraging a combination of convolutional and recurrent layers to capture temporal relationships and extract relevant features. However, these recurrent models are often cumbersome to optimize and fine-tune, typically necessitating task-specific adjustments that can be daunting for non-experts. To address this challenge, we introduce U-Time, a novel, fully feed-forward deep learning approach for segmenting physiological time-series data, specifically designed for sleep data analysis. By adapting the U-Net architecture, originally developed for image segmentation, U-Time functions as a temporal fully convolutional network, capable of mapping input sequences of varying lengths to sequences of class labels at a user-defined temporal resolution. This is achieved through the implicit classification of individual time-points within the input signal, which are then aggregated over fixed intervals to yield the final predictions, as shown in Figure A, B, C (see References [citation]). Our evaluation of U-Time for sleep stage classification using a large collection of sleep EEG datasets revealed that it either matches or surpasses the performance of current state-of-the-art deep learning models, while exhibiting enhanced robustness during training and eliminating the need for task-specific architectural or hyperparameter adjustments.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0171", "problem_id": "01710001", "content": "Siamese trackers frame object tracking as a similarity assessment between a template and potential target regions within a frame. A crucial factor in the success of the similarity function is its equivariance to translation. Architectures lacking translation equivariance can introduce positional biases during training, hindering the recovery of the target's location from the feature space. Considering real-world scenarios where objects experience transformations like rotation or scaling, the similarity function's performance may decline without a mechanism to address these variations. This paper centers on scaling and aims to enhance Siamese networks with inherent scale equivariance, enabling them to capture natural target variations. We establish the theoretical foundation for scale-equivariant Siamese trackers and offer a straightforward method for adapting existing trackers to achieve scale equivariance. We introduce SE-SiamFC, a scale-equivariant adaptation of SiamFC constructed using this method. Experimental results on OTB and VOT benchmarks, as well as the synthetic T-MNIST and S-MNIST datasets, demonstrate the benefits of incorporating additional scale equivariance for visual object tracking.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0172", "problem_id": "01720001", "content": "This study presents a novel approach to reconstructing three-dimensional shapes from two-dimensional line drawings. The proposed technique accepts one or multiple sketches as input and generates a dense point cloud that represents the 3D reconstruction of the input sketch(es), which is subsequently converted into a polygon mesh. A deep encoder-decoder network is the core component of this method, where the encoder extracts a compact representation of the sketch that encapsulates shape information, and the decoder transforms this representation into depth and normal maps that capture the underlying surface from multiple viewpoints. These multi-view maps are then combined into a 3D point cloud by solving an optimization problem that integrates depth and normals across all viewpoints. As demonstrated by our experimental results, our architecture outperforms other methods, such as volumetric networks, in terms of reconstruction fidelity, surface resolution, and preservation of topology and shape structure, as shown in Figure A, B, C (References [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0173", "problem_id": "01730001", "content": "This study introduces an unsupervised learning model designed for nonlinear disentanglement of latent factors within naturalistic videos. Prior research indicates that disentangled representations are achievable when most environmental factors remain constant, leading to existing algorithms being evaluated primarily on meticulously designed datasets. Consequently, their applicability to natural scenes remains uncertain. We observe that objects in segmented natural videos exhibit predominantly small transitions punctuated by infrequent, large changes, indicative of a temporally sparse distribution. Utilizing this observation, we introduce SlowVAE, an unsupervised representation learning model employing a sparse prior on temporally adjacent observations to disentangle generative factors, independent of the number of changing factors. We present an identifiability proof and demonstrate the model's ability to reliably learn disentangled representations on standard benchmark datasets, frequently exceeding state-of-the-art performance. Furthermore, we showcase transferability to video datasets with natural dynamics, Natural Sprites and KITTI Masks, which are introduced as benchmarks to direct disentanglement research toward more realistic data domains.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0174", "problem_id": "01740001", "content": "In this paper, we investigate a transfer reinforcement learning challenge where the transitions and rewards are influenced by the environmental context. We specifically examine a demonstrator agent that utilizes a context-aware policy to generate data regarding transitions and rewards, which forms the experience of the demonstrator. The objective is to transfer this experience—excluding the associated contextual information—to a learner agent that lacks access to the environmental context, enabling it to develop a control policy with fewer samples. It is widely recognized that neglecting the causal influence of contextual factors can lead to biases in the transition and reward models estimated by the learner, ultimately resulting in a suboptimal policy. To tackle this issue, we introduce a method to derive causal bounds on the transition and reward functions based on the demonstrator's data, which are then used to establish causal bounds on the value functions. Utilizing these value function bounds, we propose novel Q learning and UCB-Q learning algorithms that converge to the true value function without bias. We present numerical experiments related to robot motion planning problems that confirm the proposed value function bounds and illustrate that the suggested algorithms effectively leverage the demonstrator's data to enhance the learner's learning process.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0175", "problem_id": "01750001", "content": "This study re-examines the conventional method of representing 3D point clouds as linear shape models. The central idea is to employ deep learning to portray a group of shapes as affine transformations of simplified linear shape models. Each linear model is defined by a shape prototype, a reduced shape basis, and a pair of neural networks. These networks receive a point cloud as input and predict both the shape coordinates within the linear basis and the affine transformation that most closely approximates the input. The linear models and neural networks are trained jointly, using a singular reconstruction loss function. The primary benefit of our method is its explicitness, with all operations occurring in 3D space, unlike many contemporary deep learning techniques that acquire complex, feature-based shape representations. Consequently, our linear shape models are easily visualized and annotated, facilitating the visual understanding of failure modes. Although the primary objective is to present a condensed and interpretable representation of shape collections, we demonstrate that this approach yields leading performance in few-shot segmentation.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0176", "problem_id": "01760001", "content": "The application of phase contrast microscopy (PCM) in biomedicine research has become increasingly prevalent, enabling the observation of specimens without the need for staining or fixation. A significant area of research involves utilizing PCM for the monitoring of live cells, with a key challenge being the segmentation of cell populations in the resulting images, a crucial step that precedes various downstream analyses, including cell tracking and classification, as seen in Figure A, B, C (References: [citation]). As this problem has garnered significant attention, numerous studies have been conducted to address it from diverse perspectives. This paper seeks to provide an exhaustive review of the advancements made in the segmentation of cell populations in PCM images, as discussed in [citations].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0177", "problem_id": "01770001", "content": "This paper introduces an effective post-processing technique designed to minimize artifacts present in cone-beam CT (CBCT) images reconstructed from sparse data. The methodology leverages advanced image-to-image generative models, incorporating a perceptual loss for regularization. In contrast to conventional CT artifact reduction strategies, the proposed approach employs adversarial training, resulting in outputs that are both more realistic in perception and preserve anatomical integrity. To specifically target streak artifacts, which are inherently local and manifest at different scales, a novel discriminator architecture is introduced, utilizing feature pyramid networks and a differentially modulated focus map to guide the adversarial training process. Experimental evaluations demonstrate that the proposed method significantly reduces cone-beam artifacts in clinical CBCT images reconstructed with 1/3 projections, achieving superior quantitative and qualitative performance compared to established baseline methods.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0178", "problem_id": "01780001", "content": "Transformers possess the capability to model long-term dependencies, yet their effectiveness in language modeling is constrained by fixed-length contexts. We introduce Transformer-XL, a new neural architecture that extends dependency learning beyond fixed contexts while maintaining temporal consistency through a segment-level recurrence mechanism and an innovative positional encoding approach. This approach not only enhances the capture of long-range dependencies but also addresses the issue of context fragmentation. Consequently, Transformer-XL achieves dependencies 80% longer than RNNs and 450% longer than standard Transformers, delivers superior performance across both short and long sequences, and operates up to 1,800+ times faster than conventional Transformers during evaluation. Our model sets new benchmarks with bpc/perplexity scores of 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). Additionally, when trained solely on WikiText-103, Transformer-XL produces coherent, novel text articles spanning thousands of tokens. The implementation, pretrained models, and hyperparameters are accessible in Tensorflow and PyTorch.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0179", "problem_id": "01790001", "content": "Recent developments in deep learning have led to significant achievements in super-resolution (SR) techniques for images and videos that utilize convolutional neural networks (CNNs). In the context of video SR, innovative algorithms have been designed to take advantage of the temporal relationships among low-resolution (LR) video frames, and/or to enhance a frame using several LR frames. These approaches aim to produce higher quality super-resolved frames, typically evaluated on a frame-by-frame basis, for instance, using PSNR. However, assessing quality for each frame individually may not adequately reflect the coherence between frames. When an algorithm processes each frame separately, as is common in many prior methods, it can introduce temporal inconsistencies, such as flickering. Thus, it is essential to enhance both frame-wise fidelity and consistency between frames, referred to as spatial quality and temporal quality, respectively. This leads to the question: does optimizing for spatial quality inherently optimize for temporal quality as well? Moreover, can both quality metrics be optimized simultaneously?", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0180", "problem_id": "01800001", "content": "Image segmentation can be achieved through two primary approaches, semantic and instance segmentation, both of which leverage Convolutional Neural Networks. Although semantic segmentation thrives under end-to-end training, instance segmentation is often treated as a multi-stage process, relying on learning-based discrimination and subsequent clustering. However, optimizing each stage independently can lead to the propagation of segmentation errors. To address this, we introduce FCRNet, an embedding learning framework that integrates prior clustering information to facilitate one-stage instance segmentation. By incorporating the number of clustering groups into the embedding space, FCRNet simplifies the post-processing step, and its efficacy is demonstrated through comparisons with other methods on the nucleus dataset BBBC006, showcasing its superior performance.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0181", "problem_id": "01810001", "content": "Gesture recognition enables intuitive human-machine interaction, offering service robots an additional communication channel to respond to visual cues like directing attention. Identifying and categorizing gestures from video sequences remains complex, with numerous techniques developed over time. This study introduces an approach for recognizing gestures in RGB video by employing OpenPose for human pose estimation and combining Dynamic Time Warping (DTW) with One-Nearest-Neighbor (1NN) for time-series analysis. Key advantages include hardware independence and adaptability, as new gestures can be incorporated with minimal training samples. The method leverages OpenPose's deep learning capabilities without requiring extensive neural network training. Its classification accuracy is validated using an existing dataset.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0182", "problem_id": "01820001", "content": "We introduce a new attention mechanism designed to effectively focus on target objects with diverse sizes and forms within images. This model learns to progressively filter out unimportant areas through a multi-layer attentive process in a convolutional neural network. At each layer, the attention mechanism selectively transmits or blocks spatial features for use in deeper layers. The progressive attention approach demonstrates particularly strong performance when integrated with hard attention. Additionally, we utilize local context information to capture neighboring features at each position, improving the accuracy of attention probability maps. Experimental results on both synthetic and real-world datasets indicate that our attention networks surpass conventional attention approaches in visual attribute prediction tasks.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0183", "problem_id": "01830001", "content": "Recent studies on the lottery ticket hypothesis (LTH) and single-shot network pruning (SNIP) have drawn significant interest toward post-training pruning (iterative magnitude pruning) and before-training pruning (pruning at initialization). While the former approach incurs high computational overhead, the latter often underperforms. In contrast, during-training pruning—a class of methods that balances training/inference efficiency and competitive performance—remains relatively underexplored. To investigate this, we analyze pruning plasticity (the capacity of pruned networks to regain original performance) throughout training. This concept clarifies various empirical findings on neural network pruning. We also discover that neuroregeneration—replenishing pruned connections—significantly enhances pruning plasticity. Leveraging these insights, we introduce gradual magnitude pruning (GMP) with zero-cost neuroregeneration (GraNet) and its dynamic sparse training (DST) variant (GraNet-ST), both achieving state-of-the-art results. Notably, GraNet-ST surpasses dense-to-sparse methods by a wide margin in sparse-to-sparse training with ResNet-50 on ImageNet, a first in the field. All codes will be made publicly available.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0184", "problem_id": "01840001", "content": "This study introduces a memory-enhanced algorithm designed for anomaly detection. While traditional approaches primarily model normal data without robust guarantees for identifying anomalies, the proposed Memory Augmented Generative Adversarial Networks (MEMGAN) incorporates a memory module to enhance both encoding and generation. The algorithm ensures that normal data predominantly lie within the convex hull of memory units, whereas anomalies remain isolated outside, enabling reliable reconstruction of normal data and poor reconstruction of anomalies. This distinctive feature strengthens anomaly detection guarantees. Additionally, MEMGAN’s decoded memory units exhibit greater interpretability and disentanglement compared to prior methods, highlighting the efficacy of the memory mechanism. Evaluations across twenty anomaly detection datasets from CIFAR-10 and MNIST confirm MEMGAN’s superior performance over existing techniques.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0185", "problem_id": "01850001", "content": "Orthogonality is a prevalent technique in deep neural network (DNN) training because it preserves the Jacobian's singular values near unity and minimizes representational redundancy. This study introduces a computationally efficient and numerically robust orthogonalization method, Newton's Iteration for Orthogonalization (ONI), designed to learn orthogonal weight matrices within DNN layers. ONI functions by iteratively driving the weight matrix's singular values towards 1, allowing for controlled orthogonality based on the number of iterations. Empirical results demonstrate that our method enhances image classification network performance by effectively managing orthogonality, thus achieving a balance between optimization advantages and reduced representational capacity. Furthermore, we demonstrate that ONI stabilizes the training of generative adversarial networks (GANs) by upholding the network's Lipschitz continuity, akin to spectral normalization (SN), and surpasses SN through its capacity for controllable orthogonality.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0186", "problem_id": "01860001", "content": "Neural networks have demonstrated significant advancements in various machine learning applications, recently expanding into graph learning. Nevertheless, the theoretical underpinnings of their efficacy, particularly when compared to conventional statistical methods like spectral embedding, remain underexplored. In this paper, we introduce a basic generative model illustrating a scenario where an unsupervised graph convolutional network underperforms, while adjacency spectral embedding achieves success. The limitation of the unsupervised graph convolutional network lies in its inability to effectively utilize information beyond the primary eigenvector in specific, nearly regular graphs, thereby overlooking crucial inference signals present in subsequent eigenvectors. This behavior is supported by visual illustrations and extensive simulations.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0187", "problem_id": "01870001", "content": "Determining the relative rigid pose between two RGB-D scans of the same environment is a crucial challenge in the fields of computer vision, robotics, and computer graphics. Many current methods impose limitations on the maximum changes in relative pose, as they rely on significant overlap between the scans. We present a new deep neural network that broadens the applicability to extreme relative poses, accommodating cases with little to no overlap between the scans. The central concept is to extract more comprehensive scene information about the environment and perform matching on the enhanced scans. Specifically, instead of merely executing scene completion for each individual scan, our method alternates between estimating the relative pose and completing the scene. This iterative process enables us to leverage details from both scans for scene completion during later stages, improving the outcomes for both tasks. Our experimental results on benchmark datasets demonstrate that our method substantially enhances relative pose estimation performance compared to existing state-of-the-art techniques, yielding promising estimates even in scenarios involving non-overlapping scans.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0188", "problem_id": "01880001", "content": "The vulnerability of ensemble stumps and trees to minor input perturbations has been highlighted in recent studies, underscoring the need for robustness verification and defense mechanisms for these models. Existing research has focused on \\ell_\\infty norm perturbations due to the inherent structure of decision trees, where decisions are made based on individual feature values. However, examining robustness in relation to general \\ell_p norm perturbations requires considering the interplay between perturbations across different features, a challenge that previous algorithms have not addressed. This paper investigates the robustness verification and certified defense of ensemble decision stumps and trees against general \\ell_p norm perturbations. The results show that complete verification for ensemble stumps is NP-complete for p\\in(0, \\infty), while polynomial time algorithms are available for p=0 or \\infty; a dynamic programming-based algorithm is developed for sound verification of ensemble stumps when p\\in(0, \\infty). Furthermore, the multi-level robustness verification algorithm is extended to accommodate \\ell_p norm for ensemble trees, and a certified defense method is proposed for training ensemble stumps and trees against \\ell_p norm perturbations, with empirical evaluations on real datasets demonstrating its effectiveness, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0189", "problem_id": "01890001", "content": "In this study, we introduce a method to adapt CNN models, originally designed for semantic segmentation of still images, for processing video data. We outline a warping technique that enhances current architectures with minimal additional computational expense. This component, termed NetWarp, is demonstrated across various network frameworks. The central design concept involves utilizing optical flow from successive frames to warp the internal representations of the network over time. A significant finding of this research is that rapid optical flow techniques can be integrated with numerous CNN architectures to enhance performance and facilitate end-to-end training. Experimental results confirm that our proposed method adds only a slight computational burden while boosting performance when video streams are utilized. Our approach achieves new state-of-the-art results on the CamVid and Cityscapes benchmark datasets, consistently outpacing various baseline networks. Our code and models will be accessible at http://segmentation.is.tue.mpg.de.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0190", "problem_id": "01900001", "content": "The problem of zero-shot video object segmentation (VOS) is addressed, which involves automatically segmenting and tracking multiple moving objects in a video without manual initialization. This challenge is approached as a grouping problem, leveraging object proposals to make joint inferences about grouping in both spatial and temporal domains. A proposed network architecture enables efficient proposal selection and joint grouping, and is trained using reinforcement learning to optimize the sequence of grouping decisions for segmenting the entire video. This approach allows for direct optimization of non-differentiable overlap-based metrics used in VOS evaluation, unlike traditional supervised methods. The resulting method, termed ALBA, achieves superior performance compared to the previous state-of-the-art on three benchmarks: DAVIS 2017 [2], FBMS [20], and Youtube-VOS [27].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0191", "problem_id": "01910001", "content": "Class imbalance poses a major challenge in practical deep learning applications, particularly in critical domains like healthcare, transportation, and finance, where models are used for decision-making and automation. Developing effective deep learning models from imbalanced datasets remains difficult, with current approaches often being data-specific and predominantly tailored for image-based tasks. However, real-world classification problems involving imbalanced data span various formats, requiring a versatile solution applicable to tabular, image, and textual data. This paper introduces ReMix, a training method that combines batch resampling, instance mixing, and soft-labeling to create robust deep learning models for imbalanced datasets. Experimental results demonstrate that ReMix-trained dense nets and CNNs achieve superior performance in terms of g-mean and exhibit better calibration as measured by the balanced Brier score compared to other methods.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0192", "problem_id": "01920001", "content": "Dimensionality reduction (DR) is commonly employed to analyze and visualize high-dimensional data, offering an initial overview. However, interpreting DR results to derive meaningful insights requires further analysis, such as identifying clusters and understanding their properties. Although numerous automated techniques exist for cluster identification (e.g., density-based clustering methods), effective approaches for characterizing clusters remain limited. A cluster's feature value distribution largely defines its characteristics, but reviewing original feature values becomes challenging with a high number of features. To overcome this, we introduce a visual analytics method that effectively emphasizes the key features of a cluster within a DR result. We utilize an enhanced application of contrastive principal component analysis (cPCA) to extract these essential features. Our method, named ccPCA (contrasting clusters in PCA), quantifies each feature's relative contribution to the contrast between a specific cluster and the remaining clusters. Leveraging ccPCA, we developed an interactive system featuring a scalable visualization of cluster feature contributions. We validate the efficacy of our method and system through case studies utilizing several publicly available datasets.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0193", "problem_id": "01930001", "content": "This study introduces projection pursuit random forest (PPF), a novel ensemble learning technique designed for classification tasks. PPF builds upon the PPtree algorithm from Lee et al. (2013), creating decision trees by partitioning data based on linear combinations of randomly selected features. The approach employs projection pursuit to identify optimal variable projections that maximize class separation. By considering variable correlations through these linear combinations, PPF demonstrates superior performance compared to conventional random forests, particularly when class distinctions emerge from feature interactions. The method is applicable to multi-class scenarios and has been implemented in the R package PPforest (R Core Team, 2018), available on CRAN with developmental versions hosted at https://github.com/natydasilva/PPforest.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0194", "problem_id": "01940001", "content": "Low-light photography on mobile devices often suffers from noise due to their limited sensor size and aperture dimensions. Extending the camera's shutter time can reduce noise by accumulating more light, but this approach has drawbacks: (a) overexposure in bright areas and (b) motion-induced blur from camera or scene movement. An alternative solution involves capturing a burst of short, noisy exposures and intelligently combining them to circumvent these issues. This paper employs a burst-capture approach with a recurrent fully convolutional neural network (CNN) for effective integration. Our architecture is designed as a modular extension to existing single-frame denoising models, capable of processing any number of input frames. The method outperforms leading multi-frame techniques like VBM4D and FlexISP, achieving state-of-the-art results on burst datasets. Additionally, we demonstrate its versatility by applying it to image super-resolution, showing strong generalization across enhancement tasks.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0195", "problem_id": "01950001", "content": "This work introduces a new approach to energy minimization in low-level vision by substituting the traditional heuristic regularization with a learnable subspace constraint, while retaining the data term to leverage task-specific domain knowledge. This learning subspace minimization (LSM) framework allows for unified network architectures and shared parameters across various low-level vision tasks. Consequently, a single network can be trained for multiple tasks concurrently with completely shared parameters and can be generalized to new, unseen tasks provided their data term can be formulated. The LSM framework is evaluated on four low-level tasks—interactive image segmentation, video segmentation, stereo matching, and optical flow—and validated across different datasets. Experimental results demonstrate that the proposed LSM achieves state-of-the-art performance, while also offering advantages such as a reduced model size, accelerated training convergence, and real-time inference capabilities.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0196", "problem_id": "01960001", "content": "Comprehending physical phenomena frequently involves understanding the dynamical system that dictates observed measurements. Although black box systems can yield precise predictions, they often lack interpretability and are not easily examined by experts. Conversely, symbolic regression can be employed to analyze the dynamics. This work expands upon the AIFeynman approach (Udrescu et al., 2020) to accommodate dynamic systems, enabling symbolic regression on systems of ordinary differential equations (ODEs) using trajectory data. An empirical comparison against current symbolic regression techniques was conducted using various dynamical systems with known equations of increasing complexity. While the proposed method demonstrated superior performance on this benchmark, all symbolic regression approaches encountered challenges with more intricate systems, such as Cart-Pole.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0197", "problem_id": "01970001", "content": "Monocular depth estimation and semantic segmentation are crucial for scene understanding, prompting research into joint task learning algorithms that exploit task interaction. Nevertheless, current methodologies often underutilize semantic labels, neglecting contextual structures and primarily using them for segmentation split supervision, which restricts the performance of both tasks. To address this, we introduce a Contextual Information Network (CI-Net) that incorporates contextual information. Specifically, we employ a self-attention block in the encoder to produce an attention map. By supervising this map with an ideal attention map derived from semantic labels, the network gains contextual understanding, enabling it to leverage correlated features for precise predictions. Furthermore, a feature sharing module is developed to deeply fuse task-specific features, and a consistency loss is designed to facilitate mutual guidance between these features. Evaluations of CI-Net on the NYU-Depth-v2 and SUN-RGBD datasets demonstrate that it effectively enhances the accuracy of both semantic segmentation and depth estimation.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0198", "problem_id": "01980001", "content": "We introduce an innovative boundary-aware loss component for semantic segmentation, employing an inverse-transformation network to effectively learn parametric transformation differences between predicted and ground truth boundaries. This additional loss term enhances cross-entropy loss by better modeling boundary transformations, leading to notable and consistent performance gains in segmentation models without adding computational overhead or increasing model size. We evaluate our approach quantitatively and qualitatively across three indoor and outdoor segmentation benchmarks—Cityscapes, NYU-Depth-v2, and PASCAL—by incorporating it into various backbone networks during training in both single-task and multi-task configurations. Comprehensive experiments demonstrate that our method consistently surpasses baseline approaches and achieves state-of-the-art results on two datasets.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0199", "problem_id": "01990001", "content": "As mobile devices and the Internet of Things expand, the necessity for well-founded privacy solutions in time series applications has grown significantly. Although differential privacy is regarded as the gold standard for safeguarding database privacy, various time series applications necessitate alternative assurances, leading many recent studies to explore forms of inferential privacy to tackle these challenges. Nonetheless, a significant challenge in the practical implementation of inferential privacy is its inability to ensure robust composition; even when the same or related sensitive data is utilized in several releases that are individually secure, their combined release may compromise privacy. In this study, we investigate the compositional characteristics of a specific type of inferential privacy known as Pufferfish in the context of time series data. Our findings indicate that while general Pufferfish mechanisms may not exhibit graceful composition, a particular variant known as the Markov Quilt Mechanism, recently proposed, demonstrates strong composition properties that are comparable to those of pure differential privacy when used with time series data.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0200", "problem_id": "02000001", "content": "This paper presents a novel approach to specifying conditional random fields (CRFs) using probabilistic logic programs in a generative manner, rather than relying on graphical models. The primary objectives are to provide a unified framework for complex CRF modeling using a Turing complete language, and to facilitate the realization of generative-discriminative pairs in machine learning, enabling the comparison and selection of the most suitable model. To achieve this, we developed the D-PRISM language by extending PRISM, a logic-based probabilistic modeling language, and leveraging its dynamic programming mechanism for efficient probability computation. Experimental evaluations of D-PRISM using logistic regression, linear-chain CRF, and CRF-CFG demonstrated superior discriminative performance compared to their generative counterparts, including naive Bayes, HMM, and PCFG. Additionally, we introduced CRF-BNCs and CRF-LCGs, which are CRF variants of Bayesian network classifiers and probabilistic left-corner grammars, respectively, and can be easily implemented in D-PRISM, with empirical results showing that they outperform their generative counterparts as expected.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0201", "problem_id": "02010001", "content": "We introduce nonparametric deconvolution models (NDMs), a Bayesian nonparametric framework designed for datasets where each observation represents an aggregate of features from diverse particles. Such data arise in contexts like elections, where precinct-level vote counts (observations) reflect individual voter choices (particles) across candidates or ballot measures (features), with voters belonging to distinct demographic groups (factors). Similar to the hierarchical Dirichlet process, NDMs employ a dual-layer Dirichlet process structure to capture latent factors of unknown quantity, modeling each observation as a weighted combination of these factors. However, unlike prior approaches, NDMs identify localized variations in factor distributions for individual observations. This distinctive capability enables NDMs to both decompose observations into their underlying factors and characterize how observation-specific factor distributions differ from global trends. We develop variational inference methods for NDMs and evaluate their performance on synthetic data and California voting records. Our results demonstrate that incorporating local factors enhances global factor estimation and offers a new framework for data exploration.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0202", "problem_id": "02020001", "content": "This research investigates value function approximation in reinforcement learning (RL) for high-dimensional state or action spaces using an extended form of representation policy iteration (RPI). We analyze the shortcomings of proto-value functions (PVFs) in achieving precise low-dimensional value function approximations and emphasize the role of feature learning for enhanced performance. To address this, we employ various graph-based representation learning techniques to identify optimal basis functions for value function representation. Experimental results demonstrate that node2vec, a scalable network feature learning method, and the Variational Graph Auto-Encoder consistently surpass traditional smooth proto-value functions in low-dimensional feature spaces.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0203", "problem_id": "02030001", "content": "Instance segmentation, a crucial task in computer vision, involves identifying targets in an image and segmenting each target at the pixel level. Although Mask R-CNN is a seminal approach to instance segmentation, its predicted masks often suffer from ambiguity and inaccuracy near object boundaries. To address this limitation, we introduce a novel loss function, termed contour loss, which leverages the concept of contour matching based on distance transformation images. By specifically optimizing the contour regions of predicted masks, contour loss enables more precise instance segmentation. To facilitate joint training with modern neural networks, we develop a differentiable module for calculating k-step distance transformation images, allowing for online computation of truncated distance transformation images for both predicted and ground-truth masks. The proposed contour loss can be seamlessly integrated into existing instance segmentation frameworks, such as Mask R-CNN, without modifying the inference network architecture, thereby demonstrating strong adaptability. As evidenced by experiments on COCO, the contour loss yields significant improvements in instance segmentation performance, as shown in Figure A, B, C (References [1], [2], and [3]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0204", "problem_id": "02040001", "content": "This study explores the implementation of Reinforcement Learning (RL) to fully control actual soccer robots in the IEEE Very Small Size Soccer (VSSS) league, a well-established competition in the Latin American Robotics Competition (LARC). The VSSS involves matches between two teams, each consisting of three small robots. A simulated environment is introduced to train both continuous and discrete control policies, along with a Sim-to-Real technique to transfer these policies for real-world robot control. The findings demonstrate that the learned policies exhibit diverse behaviors that are challenging to define manually. The proposed method, VSSS-RL, outperformed the human-designed strategy of the striker from the third-ranked team in the 2018 LARC during one-on-one matches.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0205", "problem_id": "02050001", "content": "We introduce an innovative technique for dissecting and visualizing the intricacies of conventional reinforcement learning (RL) benchmarks through score distributions. By randomly determining the parameters of numerous policy networks and assessing their performance on benchmark tasks, we can glean valuable insights into the complexity of these benchmarks from their combined outcomes. This method ensures an objective evaluation by completely avoiding the learning process: the parameters of the policy networks are produced using Random Weight Guessing (RWG), rendering our approach independent of (i) the traditional RL framework, (ii) any specific learning algorithm, and (iii) hyperparameter optimization. Our findings demonstrate that this technique effectively isolates environmental complexity, identifies specific challenge categories, and establishes a solid basis for the statistical examination of task difficulty. We validate our method on several classic control benchmarks from OpenAI Gym and demonstrate that small, untrained networks can serve as a reliable baseline across various tasks. Interestingly, these generated networks often exhibit commendable performance without any sequential learning, thereby inadvertently revealing the simplicity of some widely used benchmarks.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0206", "problem_id": "02060001", "content": "The incorporation of domain knowledge into reinforcement learning (RL) can be effectively achieved through reward shaping, a technique that has shown promise in enhancing RL performance. However, conventional methods, such as potential-based reward shaping, often rely heavily on a predefined shaping reward function, which may be imperfect due to limitations such as human cognitive bias in translating knowledge into numerical reward values. This imperfection can hinder the improvement of RL algorithms. To address this issue, this paper explores the adaptive utilization of a given shaping reward function by formulating it as a bi-level optimization problem. This involves optimizing the policy using shaping rewards at the lower level and optimizing a parameterized shaping weight function to maximize the true reward at the upper level. By deriving the gradient of the expected true reward with respect to the shaping weight function parameters, we propose three learning algorithms under different assumptions. Our experiments, conducted in sparse-reward cartpole and MuJoCo environments, demonstrate that these algorithms can effectively leverage beneficial shaping rewards while disregarding or transforming unbeneficial ones into beneficial rewards, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0207", "problem_id": "02070001", "content": "Meta-learning has garnered significant attention as a means to enhance the adaptability and data efficiency of reinforcement learning. However, the field has been hampered by a lack of suitable benchmark tasks. Existing benchmarks often lack either the complexity necessary for compelling research or the well-defined structure required for rigorous analysis. This paper introduces Alchemy, a novel benchmark designed to address these shortcomings by offering both structural complexity and transparency for meta-RL research. Alchemy, a 3D video game built in Unity, features a latent causal structure that is procedurally resampled across episodes, enabling structure learning, online inference, hypothesis testing, and action sequencing grounded in abstract domain knowledge. We assess the performance of two advanced RL agents on Alchemy and provide a detailed examination of one agent. The findings reveal a clear and specific limitation of meta-learning, thus confirming Alchemy's utility as a demanding benchmark for meta-RL. Alongside this publication, we are making Alchemy publicly available, accompanied by a collection of analysis tools and example agent trajectories.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0208", "problem_id": "02080001", "content": "Recent years have seen a surge in interest in short-term traffic forecasting utilizing deep learning techniques, particularly long short-term memory (LSTM) neural networks. Nonetheless, the full potential of deep learning in traffic forecasting—regarding the model architecture's complexity, the geographical scope of the predictions, and the effectiveness of spatial-temporal data—remains largely untapped. This paper introduces a deep stacked bidirectional and unidirectional LSTM (SBU-LSTM) neural network architecture that incorporates both forward and backward temporal dependencies to predict traffic speed across an entire network. A bidirectional LSTM (BDLSTM) layer is employed to encapsulate spatial characteristics and bidirectional temporal dependencies from historical data. To our knowledge, this represents the inaugural use of BDLSTMs as foundational elements in a deep architecture model aimed at assessing the backward dependency of traffic data for forecasting. The model also effectively addresses missing values in the input data through a masking mechanism. Furthermore, this scalable approach is capable of predicting traffic speed in both freeway and intricate urban traffic environments. Comparisons with various classical and advanced models demonstrate that the proposed SBU-LSTM neural network achieves outstanding prediction performance throughout the traffic network, excelling in both accuracy and robustness.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0209", "problem_id": "02090001", "content": "Person re-identification (ReID), a crucial task in computer vision with significant applications in video surveillance, involves identifying individuals across disparate camera views or different temporal segments within a single camera feed. The primary difficulty stems from matching the visual features of the same person despite substantial variations in appearance and pose, while simultaneously distinguishing them from other individuals. Recent approaches commonly employ deep learning architectures trained with triplet loss for person ReID. However, triplet loss primarily emphasizes correct ranking within the training data, which we demonstrate to be suboptimal for clustering tasks. In this paper, we propose a novel cluster loss designed to encourage greater inter-class separation and reduced intra-class variance in the model's output compared to triplet loss. Consequently, our model exhibits enhanced generalization capabilities, leading to improved accuracy on test datasets, particularly in clustering scenarios. Furthermore, we introduce a batch hard training strategy to further enhance performance and accelerate the training convergence.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0210", "problem_id": "02100001", "content": "Visible watermarks are commonly employed in images to safeguard copyright ownership. Examining watermark removal techniques aids in bolstering anti-attack methods in an adversarial context. Existing removal strategies typically utilize image-to-image translation techniques. However, the variations in size, shape, color, and transparency of the watermarks present significant challenges for these approaches. To address this issue, we introduce a two-stage generator, termed Watermark-Decomposition Network (WDNet), which integrates traditional watermarked image decomposition. The first stage generates a preliminary decomposition of the entire watermarked image, while the second stage focuses specifically on the watermarked region to enhance the removal outcomes. This decomposition approach allows WDNet to isolate watermarks from images, rather than merely eliminating them. Additionally, we demonstrate that the separated watermarks can be utilized as supplementary resources for expanding the training dataset and improving removal efficacy. Furthermore, we have created a large-scale dataset called CLWD, primarily consisting of colored watermarks, to address the gap in datasets for colored watermark removal. Extensive evaluations on the public grayscale dataset LVW and CLWD consistently reveal that the proposed WDNet surpasses existing state-of-the-art methods in both accuracy and efficiency. The code and CLWD dataset are publicly accessible at https://github.com/MRUIL/WDNet.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0211", "problem_id": "02110001", "content": "The application of deep neural networks to scene-based text detection has yielded promising outcomes, with recent cutting-edge approaches shifting focus from word bounding box regression to character bounding box and pixel-level prediction, thereby requiring the linkage of adjacent characters. To address this, we introduce a novel Graph Neural Network (GNN) architecture in this paper, which enables the learning of both node and edge features, unlike traditional GNNs that only learn node features. A key benefit of utilizing GNNs for link prediction is their capacity to connect characters that are spatially separated and oriented arbitrarily. Our approach is demonstrated on the SynthText dataset, where it achieves superior results compared to state-of-the-art methods, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0212", "problem_id": "02120001", "content": "Enhancing user trust in neural networks for high-stakes applications requires explainability, yet existing research primarily addresses image analysis, overlooking 3D data. We adapt saliency methods, proven effective for images, to process 3D point clouds and voxel spaces, revealing that edges and corners are prioritized as significant features, whereas planar surfaces are considered less critical. This model-agnostic approach offers insights into learned features. Given the inherent sparsity of 3D data, we demonstrate that features extracted by a voxel-based classification network are similarly sparse and amenable to pruning, improving network efficiency. Our findings indicate that the Voxception-ResNet model can retain near-original accuracy even when reduced to 5% of its parameters.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0213", "problem_id": "02130001", "content": "We introduce learning bounds that are contingent on data for the broader case of non-stationary non-mixing stochastic processes. Our guarantees for learning are framed in terms of a data-dependent metric of sequential complexity alongside a discrepancy measure that can be inferred from data with certain minimal assumptions. Additionally, we offer a fresh analysis of a stable time series forecasting algorithm utilizing the new concept of discrepancy we have developed. Utilizing our learning bounds, we create innovative algorithms for non-stationary time series forecasting and present initial experimental findings.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0214", "problem_id": "02140001", "content": "This study examines the potential for enhancing reinforcement learning algorithms, with a three-part investigation. Initially, the analysis reveals that the traditional asymptotic convergence rate of O(1/) can be improved to O((\\log(N)/N)^), where \\frac ≤ β ≤ 1 and N represents the number of iterations, indicating a more optimistic outlook. The research then proposes a dynamic optimal policy for selecting the learning rate (\\gamma_k) in stochastic approximation (SA), comprising two interconnected levels: the inner and outer levels. At the inner level, the \\nameref algorithm is introduced, which constructs a new sequence (\\gamma^i_k) with a faster error decrease based on a predefined sequence (\\gamma^o_k). The outer level involves an optimal methodology for selecting the predefined sequence (\\gamma^o_k). Finally, empirical results demonstrate that the proposed learning rate selection methodology significantly outperforms standard reinforcement learning (RL) algorithms in three applications: drift estimation, optimal limit order placement, and large-scale share execution, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0215", "problem_id": "02150001", "content": "Deep learning (DL) has experienced exponential growth since 2006, establishing new performance benchmarks across diverse fields like object recognition, image segmentation, speech recognition, and machine translation. In contemporary manufacturing, data-driven machine health monitoring is increasingly favored, facilitated by the extensive integration of affordable sensors and internet connectivity. Deep learning offers valuable methodologies for handling and interpreting the resulting large machinery datasets. This paper aims to examine and synthesize recent research on the application of deep learning to machine health monitoring. Following a concise overview of deep learning techniques, the applications of deep learning in machine health monitoring systems are reviewed, focusing on Auto-encoders (AE) and their derivatives, Restricted Boltzmann Machines and their extensions, including Deep Belief Networks (DBN) and Deep Boltzmann Machines (DBM), Convolutional Neural Networks (CNN), and Recurrent Neural Networks (RNN). The paper concludes by discussing emerging trends in DL-based machine health monitoring approaches.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0216", "problem_id": "02160001", "content": "This paper presents a novel generative framework that integrates 3D facial pose tracking and face model adaptation in real-time, accommodating unconstrained environments with significant occlusions and diverse facial expressions. A key component of this framework is a statistical 3D morphable model that dynamically characterizes the facial surface, incorporating an efficient online adaptation mechanism that progressively learns the subject's identity and swiftly generates a personalized face model when the subject changes. Notably, our approach deviates from traditional ICP-based pose estimation methods by introducing a ray visibility constraint, which enhances robustness to occlusions by considering the face model's visibility in relation to the input point cloud. Experimental evaluations on the Biwi and ICT-3DHP datasets, including ablation studies, validate the efficacy of the proposed framework, surpassing existing state-of-the-art depth-based methods, as shown in Figure A, B, C (References [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0217", "problem_id": "02170001", "content": "Deep latent variable models (LVMs), including variational autoencoders (VAEs), have become increasingly significant in text generation by leveraging continuous latent structures to direct the process. However, their representational capacity is constrained by two main factors: (1) the frequent reliance on Gaussian assumptions for variational posteriors, and (2) the persistent problem of \"posterior collapse.\" This work proposes using sample-based variational distributions for natural language, yielding implicit latent features that offer greater flexibility than Gaussian-based alternatives. Additionally, we introduce an LVM framework that aligns the aggregated posterior with the prior, effectively extending VAEs through mutual information maximization to counteract posterior collapse. Our approach proves effective across diverse text generation tasks, such as language modeling, unsupervised style transfer, and dialogue response generation. The implementation code is accessible on GitHub.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0218", "problem_id": "02180001", "content": "Video advertisement content structuring involves dividing a video advertisement into segments and categorizing each segment based on aspects such as presentation format, scene, and style. Unlike general videos, advertisements often include rich multi-modal elements like captions and speech, which offer valuable semantic information to improve structuring. This paper introduces a multi-modal encoder designed to learn representations by integrating video-audio and textual data. Using these representations, we employ a Boundary-Matching Network to produce temporal proposals, which are then refined through scene-guided alignment and re-ranking for greater precision. Additionally, we integrate proposal-specific embeddings into the multi-modal encoder to analyze temporal relationships between localized proposal features and the video’s global context for classification. Experiments demonstrate that our approach outperforms baseline methods and secures Rank 1 in the Multi-modal Ads Video Understanding task at ACM Multimedia 2021 Grand Challenge. Ablation studies confirm that utilizing multi-modal content, such as captions and speech, substantially enhances performance.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0219", "problem_id": "02190001", "content": "Machine learning algorithms typically require large datasets for effective training. Nevertheless, data is often distributed among various entities and subject to legal and logistical restrictions that hinder seamless integration. This paper presents federated transfer learning (FTL), a novel technique and framework designed to enhance statistical modeling within a federated data environment. This approach facilitates knowledge sharing while upholding user privacy and enables the transfer of complementary information across the network. Consequently, a party in a target domain can construct more versatile and robust models by utilizing extensive labeled data from a source domain. Furthermore, a secure transfer cross-validation method is introduced to ensure reliable FTL performance within the federation. The framework necessitates only minor adjustments to existing model architectures and achieves comparable accuracy to non-privacy-preserving methods. The proposed framework exhibits considerable adaptability and can be efficiently applied to diverse secure multi-party machine learning applications.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0220", "problem_id": "02200001", "content": "Object detection can be viewed as a process of clustering pixels, with boundaries defined by four extreme points (leftmost, top, rightmost, and bottom). While existing research often emphasizes center or corner points, these are derived from extreme points. This paper introduces EPP-Net, an object detector that predicts the relative displacement vectors between each pixel and the four extreme points. Additionally, we propose Extreme Intersection over Union (EIoU), a novel metric for evaluating extreme point similarity, and integrate it as a regression loss. A dedicated branch is also introduced to predict EIoU between ground-truth and predicted results, using it as a localization confidence score to eliminate low-quality detections. On the MS-COCO dataset, EPP-Net achieves 44.0% AP with ResNet-50 and 50.3% AP with ResNeXt-101-DCN, surpassing existing anchor-free detectors and offering a new approach to object detection.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0221", "problem_id": "02210001", "content": "The simultaneous prediction of human movement paths and pose variations is crucial for numerous applications, including robotics, self-driving vehicles, and surveillance. Accurately forecasting body dynamics necessitates understanding intricate details within human-human and human-object interactions in a scene. This paper introduces a novel graph attentional network-based method, TRiPOD (TRajectory and POse Dynamics), to model these interactions in both the input and predicted output spaces. To efficiently integrate these interaction levels, a message-passing interface is implemented across the graphs. Additionally, to address real-world complexities, the method learns to predict the visibility of each estimated body joint at each frame, accounting for factors like occlusion. A new benchmark dataset, constructed from PoseTrack and 3DPW, is introduced for this combined task, along with new evaluation metrics designed to assess prediction accuracy in global coordinates, even when joint visibility varies. Experimental results demonstrate that TRiPOD surpasses existing methods and state-of-the-art approaches developed for individual trajectory and pose forecasting tasks.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0222", "problem_id": "02220001", "content": "Single Image Super-Resolution (SISR), the process of inferring high-resolution images from their low-resolution counterparts, presents a significant challenge due to its inherent ambiguity. While Convolutional Neural Networks (CNNs) have demonstrated exceptional performance in SISR, they often fail to accurately reproduce intricate image details. Generative Adversarial Networks (GANs) offer a potential solution for recovering sharper details, but their training is complex and prone to generating artifacts in the resulting high-resolution images. To address these limitations, we introduce a novel method that leverages CNNs to align images across diverse spaces, extending beyond simple pixel space alignment. This alignment is achieved through a space designed using convex optimization, encouraging CNNs to learn both high- and low-frequency image components. Our results demonstrate that this approach effectively recovers fine image details while maintaining training stability.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0223", "problem_id": "02230001", "content": "While 2D object detection in clear images has been extensively researched, its susceptibility to adversarial attacks remains a significant concern. Current approaches enhance detector robustness through adversarial training, yet this often leads to substantial reductions in average precision (AP) for clean images. This study suggests that aligning features in intermediate layers can simultaneously boost both clean AP and detection robustness. Building upon adversarial training, we introduce two feature alignment components: the Knowledge-Distilled Feature Alignment (KDFA) module and the Self-Supervised Feature Alignment (SSFA) module, designed to help networks produce more discriminative features. Comprehensive testing on PASCAL VOC and MS-COCO datasets demonstrates the efficacy of our method. The experimental code is accessible at https://github.com/grispeut/Feature-Alignment.git.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0224", "problem_id": "02240001", "content": "By 2040, cervical cancer is projected to result in 460,000 annual deaths, with nearly 90% occurring among women in Sub-Saharan Africa. The rising incidence in Africa has prompted the World Health Organization (WHO) to prioritize screening, diagnosis, and treatment for this disease. Traditional cancer diagnosis depends heavily on histopathological evaluation, a method prone to errors, necessitating intelligent computer-aided systems as affordable safety measures. However, the scarcity of labeled data in digital pathology restricts their widespread use. In this research, a limited number of cervical tissue digital slides from the TCGA data portal were pre-processed to address challenges with whole-slide images and incorporated into our VGG16-CNN classification framework. The model achieved 98.26% accuracy and a 97.9% F1-score, demonstrating the effectiveness of transfer learning in this weakly-supervised task.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0225", "problem_id": "02250001", "content": "We introduce a novel permutation-invariant network designed for processing 3D point clouds. This architecture consists of a recurrent set encoder along with a convolutional feature aggregator. When presented with an unordered point set, the encoder initially divides the surrounding space into parallel beams. The points contained within each beam are treated as a sequence and are encoded into subregional geometric features using a shared recurrent neural network (RNN). The regular spatial arrangement of the beams enables their corresponding features to be input into an efficient 2D convolutional neural network (CNN) for hierarchical feature aggregation. Our network excels in spatial feature learning and performs competitively against state-of-the-art (SOTA) methods across various benchmarks, while also demonstrating significantly improved efficiency compared to these SOTAs.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0226", "problem_id": "02260001", "content": "Current deep embedding approaches for vision tasks can construct a compact Euclidean space from images, with Euclidean distances reflecting similarity measures. To enhance learning efficiency, hard sample mining is typically applied by identifying samples through Euclidean feature distance calculations. Yet, this global Euclidean metric fails to accurately represent true feature similarity in intricate visual feature spaces, where intraclass distances in high-density areas might exceed interclass distances in sparse regions. We propose a Position-Dependent Deep Metric (PDDM) unit that learns a similarity metric adaptable to local feature structures, enabling the selection of genuinely challenging samples within local neighborhoods to robustly guide online deep embedding learning. This plug-and-play layer integrates seamlessly into any convolutional network and supports end-to-end training. Our locally aware feature embedding not only achieves quicker convergence and improved performance on two challenging image retrieval datasets but also exhibits strong generalization in large-scale and open-set scenarios, such as transfer learning and zero-shot learning on the ImageNet 2010 and ImageNet-10K datasets, due to its large-margin properties.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0227", "problem_id": "02270001", "content": "We assess the capability of temporal difference learning to monitor the evolution of a policy's reward function over time. Our findings utilize a novel adiabatic theorem that establishes limits on the mixing time of time-inhomogeneous Markov chains. We formulate finite-time constraints for both tabular temporal difference learning and Q-learning in scenarios where the training policy varies over time. To accomplish this, we create bounds for stochastic approximation in the context of asynchronous adiabatic updates.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0228", "problem_id": "02280001", "content": "Prior Networks, a novel model category, provide understandable uncertainty quantification and have demonstrated superior performance compared to leading ensemble methods across various applications. These networks facilitate the distillation of model ensembles through Ensemble Distribution Distillation (EnD^2), preserving accuracy, calibration, and uncertainty estimates within a singular model. However, current Prior Networks are limited to classification problems. This research broadens the applicability of Prior Networks and EnD^2 to regression problems by employing the Normal-Wishart distribution. The characteristics of Regression Prior Networks are showcased using synthetic data, specific UCI datasets, and a monocular depth estimation task, achieving performance comparable to ensemble techniques.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0229", "problem_id": "02290001", "content": "This paper introduces a new approach to color constancy, termed Bag of Color Features (BoCF), which leverages Bag-of-Features pooling to significantly decrease the number of parameters required for estimating illumination. The BoCF method aligns with the fundamental assumption of color constancy that local cues, such as edges, are sufficient for illumination estimation, rendering global spatial information unnecessary. Moreover, BoCF is compatible with statistical approaches to color constancy and can be viewed as a learning-based extension of these methods. To enhance the accuracy of illumination estimation, a novel attention mechanism is proposed for the BoCF model, with two variants based on self-attention. The BoCF approach and its variants demonstrate competitive performance relative to state-of-the-art methods, while utilizing substantially fewer parameters, as evidenced by results on three benchmark datasets: ColorChecker RECommended, INTEL-TUT version 2, and NUS8.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0230", "problem_id": "02300001", "content": "Offline Reinforcement Learning (RL) seeks to transform extensive datasets into effective decision-making systems without requiring real-time environment interactions. This potential has driven significant research aimed at mirroring RL’s achievements in simulated scenarios. From a practical perspective, this study evaluates these endeavors by first examining dataset characteristics that may determine the most effective offline approaches. These hypotheses are tested through experiments using custom datasets from environments with both discrete and continuous action spaces. The results confirm that data diversity and high-reward samples are essential for offline RL success, while behavioral cloning remains competitive with newer methods. This work serves as a guide to enhance understanding of current offline RL techniques and their practical applications.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0231", "problem_id": "02310001", "content": "Recently, the Weisfeiler-Lehman (WL) graph isomorphism test was employed to evaluate the expressive capabilities of graph neural networks (GNNs). It was demonstrated that the widely used message passing GNNs are unable to differentiate between graphs that the 1-WL test also cannot distinguish (Morris et al. 2018; Xu et al. 2019). Unfortunately, numerous straightforward graph instances are indistinguishable under the 1-WL test. To pursue more expressive graph learning models, we expand on the recent k-order invariant and equivariant graph neural networks (Maron et al. 2019a,b) and present two key findings: First, we establish that these k-order networks can effectively distinguish non-isomorphic graphs as efficiently as k-WL tests, which are verifiably more powerful than the 1-WL test for k>2. This indicates that these models are definitively superior to message passing models. However, this increased expressiveness incurs a computational burden due to the need to process higher-order tensors. Second, aiming to construct a provably stronger, simpler, and scalable model, we demonstrate that a reduced 2-order network, which includes only a scaled identity operator supplemented with a single quadratic operation (matrix multiplication), possesses provable 3-WL expressive power. In other words, we propose a straightforward model that integrates standard Multilayer-Perceptron (MLP) applications on the feature dimension with matrix multiplication. We confirm the effectiveness of this model by achieving state-of-the-art results on well-known graph classification and regression tasks. To our knowledge, this is the first practical invariant/equivariant model that guarantees 3-WL expressiveness, which is strictly superior to message passing models.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0232", "problem_id": "02320001", "content": "Recent progress in domain adaptive object detection (DAOD) focuses on aligning feature-level distributions between the source and a single-target domain to enhance detector generalization. However, this approach may overlook the influence of domain-specific information present within the aligned features. To address this, we propose a novel disentanglement method based on vector decomposition, aiming to extract domain-invariant object representations, which is crucial for DAOD. Initially, an extractor is designed to isolate domain-invariant representations from the input, which are then utilized for object proposal extraction. Subsequently, domain-specific representations are derived as the differences between the input and the domain-invariant representations. This difference operation expands the disparity between domain-specific and domain-invariant representations, encouraging the domain-invariant representations to encapsulate more domain-irrelevant information. The proposed method is evaluated on both single- and compound-target scenarios. Experimental results across four domain-shift scenes in the single-target case demonstrate a significant performance improvement over baseline methods. Furthermore, in the compound-target case, our method achieves approximately 4% higher performance than baseline methods, highlighting its efficacy.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0233", "problem_id": "02330001", "content": "Understanding scenes from images presents a significant challenge in the realm of autonomous driving. While 2D techniques have progressed from generating basic bounding boxes to achieving more detailed outputs such as instance segmentations, the 3D domain remains largely focused on estimating 3D bounding boxes. In this study, we introduce a novel method aimed at simultaneously inferring the 3D rigid-body poses and shapes of vehicles from a stereo image pair, utilizing shape priors. In contrast to earlier approaches that geometrically align shapes with point clouds obtained from dense stereo reconstructions, our method operates directly on images by incorporating both a photometric and a silhouette alignment term within the energy function. We also present an adaptive sparse point selection strategy to effectively assess consistency with these terms. Our experimental results demonstrate that our technique outperforms previous geometric methods in terms of 3D pose and shape estimation and can serve as a refinement step to significantly enhance the performance of several leading deep learning-based 3D object detectors. All related materials and demonstration videos are accessible on the project page https://vision.in.tum.de/research/vslam/direct-shape.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0234", "problem_id": "02340001", "content": "In numerous practical reinforcement learning (RL) scenarios, an agent must not only optimize its primary objective but also ensure compliance with multiple constraints. Ensuring safety during both training and deployment is critical—for instance, a robot must avoid actions, whether exploratory or not, that could permanently damage its hardware. To integrate safety into RL, we develop algorithms based on constrained Markov decision problems (CMDPs), which extend traditional MDPs by incorporating constraints on expected cumulative costs. Our methodology introduces an innovative technique for defining and constructing Lyapunov functions, enabling the enforcement of global safety for behavior policies through localized, linear constraints. Building on this theoretical foundation, we demonstrate how the Lyapunov framework can systematically adapt dynamic programming (DP) and RL algorithms to ensure safety. We validate the efficacy of our approach through experiments on a safety benchmark, testing various CMDP planning and decision-making tasks. The results indicate that our method substantially surpasses existing baselines in achieving a balance between constraint adherence and performance.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0235", "problem_id": "02350001", "content": "Sparse coding is an unsupervised method that derives compact, high-level representations from unlabeled data by expressing each input as a sparse combination of basis functions. Initially developed to model the human visual system, it has proven effective for self-taught learning, where supervised classification tasks benefit from additional unlabeled data from unrelated classes. Shift-invariant sparse coding (SISC) expands this approach by reconstructing inputs—often time-series data—using all basis functions across every possible shift. This paper introduces an efficient algorithm for training SISC bases, involving iterative solutions to two large convex optimization problems. The first computes linear coefficients via an L1-regularized least squares problem with vast variable counts, where our method provides an exact solution instead of relying on heuristic variable selection. The second solves for bases through constrained least squares, with efficiency gains achieved by optimizing complex-valued variables in the Fourier domain to decouple dependencies. Our results demonstrate that SISC-derived representations of speech and music serve as effective classification features, sometimes surpassing leading spectral and cepstral features under specific conditions.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0236", "problem_id": "02360001", "content": "The interpretation of speech through lip movements, known as Visual Speech Recognition (VSR), is hindered by the limited availability of training data, resulting in classification systems with low accuracy when encountering unseen classes. To address this challenge, a novel zero-shot learning approach is proposed, leveraging Generative Adversarial Networks (GANs) to generate new classes, thereby significantly enhancing the accuracy of VSR systems by 27% and enabling the handling of speaker-independent out-of-vocabulary phrases. Furthermore, the developed models demonstrate language agnosticism, allowing for the generation of videos in a new language, such as Hindi, using English training data. This work presents the first empirical evidence of utilizing GANs to generate training samples for unseen classes in VSR, facilitating zero-shot learning, and makes the additional videos and code publicly available, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0237", "problem_id": "02370001", "content": "Attention mechanisms are widely used in deep neural networks, but their performance has not been thoroughly analyzed in terms of the factors influencing them and the methods for computing attention. To enhance the broader comprehension of these mechanisms, we conduct an empirical investigation that systematically evaluates different spatial attention components within a generalized framework, covering both Transformer attention and widely used modules like deformable and dynamic convolutions. Our experiments across multiple applications reveal important insights about spatial attention, some of which challenge existing assumptions. For instance, while the comparison of query and key content in Transformer attention proves unimportant for self-attention, it remains crucial for encoder-decoder attention. Additionally, combining deformable convolution with key content-based saliency leads to the optimal balance between accuracy and efficiency in self-attention. These findings highlight considerable potential for refining attention mechanism designs.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0238", "problem_id": "02380001", "content": "Security inspections frequently involve luggage where object overlap significantly hinders the accurate detection of prohibited items in X-ray images, a subject underexplored in existing research and datasets. This study introduces the Occluded Prohibited Items X-ray (OPIXray) image benchmark, the first high-quality object detection dataset specifically designed for security inspection. OPIXray concentrates on \"cutter,\" a common prohibited item, and features manual annotations by professional airport inspectors. The test set is categorized into three occlusion levels to facilitate a detailed analysis of detector performance. Additionally, the De-occlusion Attention Module (DOAM) is proposed to address occlusion challenges in X-ray image detection. DOAM, a plug-and-play module, can be seamlessly integrated into existing detectors to enhance their performance. Capitalizing on the preserved shape appearance and distinctive material-based colors and textures in X-ray imaging, DOAM employs diverse appearance information of the prohibited item to generate attention maps that refine feature maps for general detectors. Comprehensive evaluations on the OPIXray dataset demonstrate that DOAM consistently improves the performance of state-of-the-art detection methods like SSD and FCOS, outperforming other common attention mechanisms. The benefits of DOAM are particularly pronounced in scenarios with higher levels of occlusion, highlighting its potential for real-world applications. The OPIXray benchmark and model are available at https://github.com/OPIXray-author/OPIXray.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0239", "problem_id": "02390001", "content": "We introduce a scalable method for estimating the upper bound of the Lipschitz constant in generative models by connecting it to the maximum norm achievable across the set of attainable vector-Jacobian products. To approximate this set, we employ layerwise convex approximations with zonotopes. This approach enhances and extends previous techniques involving zonotope transformers, enabling Lipschitz constant estimation for neural networks with high-dimensional outputs. The method delivers precise and computationally efficient bounds for smaller networks and can be applied to larger generative models, including those based on VAE and DCGAN architectures.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0240", "problem_id": "02400001", "content": "Ensemble learning enhances generalization by integrating multiple individual models. Deep learning models, characterized by multilayer architectures, currently outperform shallow or traditional classification methods. Deep ensemble learning leverages the strengths of both deep learning and ensemble techniques, resulting in improved generalization capabilities. This paper presents a comprehensive review of state-of-the-art deep ensemble models, offering researchers a valuable summary of the field. The categorization of ensemble models includes approaches such as bagging, boosting, and stacking; negative correlation-based deep ensembles; explicit/implicit ensembles; homogeneous/heterogeneous ensembles; decision fusion strategies; unsupervised, semi-supervised, reinforcement learning, and online/incremental ensembles; and multilabel-based deep ensemble models. Furthermore, the application of deep ensemble models across various domains is briefly examined. The paper concludes with recommendations for future research directions in this area.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0241", "problem_id": "02410001", "content": "Conveying a visual scene through a personal and creative lens is facilitated by sketches, and the incorporation of color significantly amplifies their expressive capacity. This study presents two novel approaches for replicating the appearance of human-created colored sketches, leveraging the Contour Drawing Dataset as a foundation. The first method employs image processing techniques, in conjunction with k-means color clustering, to produce colored outline sketches. Alternatively, a generative adversarial network is utilized to develop a model capable of generating colored sketches from unseen images. The efficacy of these methods is evaluated through a combination of quantitative and qualitative assessments, as seen in Figure A, B, C (References: [citation]), and the results are discussed in the context of existing research (citations).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0242", "problem_id": "02420001", "content": "In recent years, numerous studies have explored methods to interpret complex intelligent systems, including various algorithms that produce saliency maps highlighting pixel contributions to an agent's decisions. While most evaluations of these saliency maps concentrate on image classification, no comprehensive comparison exists for Deep Reinforcement Learning agents. This study evaluates four perturbation-based saliency map techniques for agents trained on four Atari 2600 games, each method altering input regions to assess their impact on the agent's output. The approaches are analyzed using three computational criteria: parameter dependence (sanity checks), alignment with the agent's reasoning (input degradation), and computational efficiency. Notably, the sanity checks reveal limitations in two methods, and a corrective solution is proposed for one of them.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0243", "problem_id": "02430001", "content": "Environmental Sound Classification (ESC) has become a vibrant research field within audio analysis, witnessing significant advancements in recent years. Nevertheless, numerous current methods attain high accuracy at the expense of relying on domain-specific features and architectures, which complicates the adoption of innovations from other areas, such as image processing. Moreover, some prior achievements can be linked to inconsistencies in evaluation methods, specifically using unofficial divisions of the UrbanSound8K (US8K) dataset, which skews the overall development in this domain. This paper makes two key contributions. First, we introduce a model capable of handling both mono and stereo audio inputs effectively. This model employs straightforward log-power Short-Time Fourier Transform (STFT) spectrograms and integrates various well-established techniques from image processing, such as ResNet, Siamese-like networks, and attention mechanisms. We explore the effects of cross-domain pre-training and architectural modifications, and we assess our model against standard datasets. Our findings indicate that our model surpasses all previously established methods in a just comparison, achieving accuracies of 97.0 % (ESC-10), 91.5 % (ESC-50), and 84.2 % / 85.4 % (US8K mono / stereo). Second, we deliver a detailed review of the current status of the field by distinguishing between official and unofficial reported results on the US8K dataset. To enhance reproducibility, we provide access to our code, including all relevant re-implementations.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0244", "problem_id": "02440001", "content": "The Kohonen self-organisation map, a widely recognised classification technique, has been applied to a broad range of problems, yet its use in time series forecasting has been relatively limited. This study introduces a novel forecasting approach tailored to predicting multi-dimensional long-term trends, which leverages the Kohonen algorithm in a dual-application framework, and also explores the method's practical applications, as illustrated in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0245", "problem_id": "02450001", "content": "We introduce an innovative approach to large-scale image stitching that effectively handles repetitive patterns and areas devoid of features. In such scenarios, advanced image stitching techniques frequently generate alignment artifacts due to the potential for incorrect pairwise image registrations that conflict within the overall connectivity framework. Our approach enhances existing methods by gathering all potential pairwise image registration options, from which globally consistent candidates are selected. This process allows for accurate determination of pairwise registrations by leveraging comprehensive information from the entire image set, including clear registrations beyond the areas of repetition and lack of features. We represent this method as a weighted multigraph, where the nodes signify the individual transformations of images within the composite, and multiple edges between pairs of nodes capture all potential pixel coordinate transformations of the two images. The weights of these edges reflect the likelihood of the transformations. We solve for image transformations and edge weights through a non-linear minimization problem subject to linear constraints, utilizing a projection method. As a demonstration, we implement our technique in a large-scale scanning context, where the primary transformations involve translations with minimal rotation and scaling. Despite these simplifications, current state-of-the-art methods falter in such cases due to minimal image overlap, which can either lack features or exhibit repetition, leading to unacceptable misalignment artifacts and their concealment.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0246", "problem_id": "02460001", "content": "This paper introduces a parallel acceleration technique for machine learning training that incorporates matrix orthogonality and unitarity constraints, relevant in various machine learning subfields. By employing a graph coloring approach commonly used in scheduling round-robin tournaments, we reorganize an inherently sequential elementary rotation parameterization into commutative operation blocks. This decomposition facilitates an algorithm for computing a fully-parameterized orthogonal matrix from its rotation parameters in O(n) sequential steps and enables the gradient computation of a training loss with respect to these parameters in O(n\\log n) steps. We explore parametric restrictions pertinent to generative modeling and demonstrate encouraging performance results obtained from a prototype GPU implementation.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0247", "problem_id": "02470001", "content": "Neural networks are commonly employed for the task of image classification; however, they can be susceptible to misclassification due to adversarial images. Efforts to enhance the robustness of neural network image classification have included various preprocessing methods (such as cropping, adding noise, and blurring), adversarial training, and dropout randomization. In this study, we developed a model for adversarial detection that integrates two of these strategies: dropout randomization and preprocessing applied to images within a specified Bayesian uncertainty framework. We assessed our model using the MNIST dataset, targeting adversarial images generated through the Fast Gradient Sign Method (FGSM), Jacobian-based Saliency Map Attack (JSMA), and Basic Iterative Method (BIM) attacks. Our model achieved an average detection accuracy of 97% for adversarial images, while the classification accuracy of the remaining images was 99%. Moreover, our average detection accuracy surpassed that reported in recent studies utilizing similar approaches.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0248", "problem_id": "02480001", "content": "Determining the reliability of a classifier's predictions is crucial for various applications and essential for the safe deployment of AI systems. Although machine learning research has primarily focused on enhancing classifier performance, assessing when predictions should or should not be trusted has received comparatively less attention. Conventional methods rely on the classifier's discriminant or confidence score, but we demonstrate that an alternative approach can be more effective in numerous scenarios. We introduce a novel metric, termed the trust score, which evaluates the alignment between the classifier and an adjusted nearest-neighbor classifier for a given test instance. Empirical results reveal that high (low) trust scores exhibit remarkable precision in distinguishing correctly (incorrectly) classified examples, consistently surpassing the classifier's confidence score and other baseline methods. Additionally, under certain mild distributional conditions, we prove that high (low) trust scores indicate strong agreement (disagreement) between the classifier and the Bayes-optimal classifier. Our theoretical guarantees include non-asymptotic statistical consistency rates across diverse nonparametric settings, leveraging advancements in topological data analysis.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0249", "problem_id": "02490001", "content": "Recent advancements in neural architecture search (NAS) have enabled the automated creation of neural architectures for practical applications, including object detection and semantic segmentation. Nonetheless, the effectiveness of NAS hinges on the availability of substantial labeled datasets and computational resources, making its use problematic in few-shot learning settings where multiple related tasks must be handled, each with minimal data and limited processing time. Consequently, few-shot learning often relies on a predetermined neural architecture. To address this issue, we introduce MetaNAS, the inaugural approach that seamlessly combines NAS with gradient-based meta-learning. MetaNAS concurrently optimizes a meta-architecture and meta-weights during the meta-training phase. In the meta-testing phase, the architectures can be tailored to accommodate a new task with only a few iterations by the task optimizer, resulting in cost-effective task adaptation that demands minimal data. Additionally, MetaNAS is model-agnostic, allowing it to function with any model-agnostic meta-learning techniques and various gradient-based NAS strategies. Empirical findings on established few-shot classification benchmarks demonstrate that MetaNAS, utilizing a collaboration of DARTS and REPTILE, achieves state-of-the-art performance.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0250", "problem_id": "02500001", "content": "Existing single-level segmentation algorithms, with the exception of those based on Convolutional Neural Networks (CNNs), typically result in oversegmentation; CNN-based approaches also present inherent challenges. To mitigate oversegmentation, region merging processes often employ multiple criteria thresholds, yielding hierarchical segmentation outcomes. Nonetheless, significant oversegmentation persists at the lower levels of the hierarchy, while prominent, small objects are incorporated into their larger neighboring regions at the higher levels. This paper introduces a novel region-merging-based image segmentation technique termed \"Dam Burst.\" This single-level segmentation algorithm prevents excessive segmentation and preserves fine details simultaneously. The name reflects its mechanism, which simulates an underground flood destroying dams between water pools. Edge detection results located on dams are treated as structural reinforcements. To emulate an underground flood, regions are merged in ascending order based on the average gradient within each region.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0251", "problem_id": "02510001", "content": "Meta-reinforcement learning (RL) enables the development of policies that can rapidly adapt to novel tasks using significantly less data than conventional RL methods; however, the meta-training phase is computationally expensive and time-intensive. Leveraging offline data for meta-training, through a single, reward-annotated dataset across various tasks, offers a way to reuse static data and train policies adaptable to new challenges during meta-testing. While promising for real-world applications, offline meta-RL introduces complexities beyond those in online meta-RL or standard offline RL. Meta-RL algorithms learn an exploration strategy for efficient data collection and concurrently meta-train a policy for rapid adaptation to new task data. Given that this policy is meta-trained on a static, offline dataset, its behavior during adaptation to data from the learned exploration strategy may be unpredictable due to distributional shifts induced by the systematic differences between the offline data and the newly collected data. Rather than mitigating this distributional shift via conservative exploration, we introduce a hybrid offline meta-RL algorithm that meta-trains an adaptive policy using reward-labeled offline data and subsequently collects additional, unlabeled online data to bridge the distribution gap. Eliminating the need for reward labels during online data collection significantly reduces the cost of data acquisition. Comparing our approach against existing offline meta-RL techniques on simulated robot locomotion and manipulation tasks, we demonstrate that the incorporation of unsupervised online data collection substantially enhances the adaptive capabilities of meta-trained policies, achieving performance comparable to fully online meta-RL across diverse and challenging domains that demand generalization to new tasks.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0252", "problem_id": "02520001", "content": "The object detection process relies heavily on bounding box regression, a critical step that has traditionally utilized the \\ell_n-norm loss, despite its lack of alignment with the commonly used evaluation metric, Intersection over Union (IoU). Although IoU loss and generalized IoU (GIoU) loss have been introduced to better support the IoU metric, they are plagued by issues of slow convergence and inaccurate regression. To address these limitations, this paper introduces the Distance-IoU (DIoU) loss, which incorporates the normalized distance between the predicted and target boxes, resulting in significantly faster training convergence compared to IoU and GIoU losses. Additionally, three key geometric factors influencing bounding box regression are identified - overlap area, central point distance, and aspect ratio - and used to develop the Complete IoU (CIoU) loss, leading to enhanced convergence and performance. The integration of DIoU and CIoU losses into prominent object detection algorithms, such as YOLO v3, SSD, and Faster RCNN, yields substantial improvements in both IoU and GIoU metrics, and the incorporation of DIoU into non-maximum suppression (NMS) as a criterion further amplifies performance gains, with the source code and trained models available at https://github.com/Zzh-tju/DIoU.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0253", "problem_id": "02530001", "content": "We introduce a novel approach to video understanding, reframing the video recognition problem as a task akin to image recognition, thereby eliminating the need for temporal modeling. By leveraging a single image classifier, our method achieves video understanding through a straightforward and universally applicable process. This involves creating a composite \"super image\" from input frames, which is then used to train an image classifier for action recognition, mirroring the process of image classification. The efficacy of this concept is demonstrated through experiments on four publicly available datasets, including Kinetics400, Something-to-something (V2), MiT, and Jester, utilizing a state-of-the-art vision transformer, as well as ResNet image classifiers commonly used in computer vision. Notably, our results on Kinetics400 are comparable to those of top-performing CNN approaches that rely on spatio-temporal modeling, as shown in Figure A, B, C (see References [citation] for details). Our code and models are available at https://github.com/IBM/sifar-pytorch.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0254", "problem_id": "02540001", "content": "A central challenge in location-based modeling and prediction is determining appropriate spatial and temporal resolutions. Specifically, effective spatial partitioning can substantially improve the accuracy of location-based forecasting models. This study examines two common tessellation methods, Geohash and Voronoi, for dividing urban areas, within the context of real-time taxi demand prediction. Using two different taxi demand datasets, we compare these methods across various temporal scales. Classical time-series techniques are used to model the spatio-temporal demand for comparison purposes. The results indicate that the effectiveness of each tessellation method is strongly influenced by the city's geographical characteristics, the spatial arrangement of the data, and the time of day, with neither method consistently outperforming the other across the entire forecast period. Consequently, we introduce a hybrid tessellation algorithm that dynamically selects the optimal tessellation strategy at each moment, based on past performance. This hybrid approach is a non-stationary adaptation of the HEDGE algorithm, designed for selecting the best advice from multiple experts. Empirical results demonstrate that the hybrid tessellation strategy consistently outperforms both individual strategies across the datasets examined, at various temporal scales, and with different performance metrics. The hybrid strategy achieves an average accuracy exceeding 80% per km^2 for both datasets at 60 minute aggregation levels.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0255", "problem_id": "02550001", "content": "Recent studies have enhanced Generative Adversarial Networks (GANs) by incorporating a consistency cost in the discriminator. We advance this approach by addressing key limitations. First, we demonstrate that consistency regularization can introduce artifacts in GAN-generated samples and provide a solution to mitigate this problem. Next, we introduce several refinements to the consistency regularization process to enhance its effectiveness. Through comprehensive experiments, we quantify the advantages of our proposed improvements. In unconditional image generation tasks on CIFAR-10 and CelebA, our method achieves superior FID scores across multiple GAN architectures. For conditional image synthesis on CIFAR-10, we advance the state-of-the-art FID from 11.48 to 9.21. Additionally, when applied to the BigGAN model on ImageNet-2012, our technique reduces the FID from 6.66 to 5.38, setting a new benchmark for models of that scale.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0256", "problem_id": "02560001", "content": "Real-time robust point cloud registration is a critical requirement for numerous mapping and localization techniques. Conventional approaches such as ICP often struggle with poor initialization, limited overlap, or dynamic environments. While deep learning-based registration methods offer superior performance, they are computationally intensive. To address these limitations, we propose StickyPillars, an efficient, precise, and highly resilient deep middle-end 3D feature matching technique for point clouds. This method employs graph neural networks and leverages transformer-based multi-head self and cross-attention for context aggregation on sparse 3D key-points. The network's output serves as the cost function for an optimal transport problem, determining the final matching probabilities. Unlike existing approaches, our system eliminates the need for manually designed feature descriptors or heuristic matching strategies. On the KITTI dataset, our method achieves state-of-the-art registration accuracy while operating four times faster than leading deep learning alternatives. Additionally, incorporating our matching system into a LiDAR odometry framework produces the most precise results on the KITTI odometry dataset. We further validate the robustness of our approach on KITTI odometry, maintaining consistent accuracy even in scenarios where state-of-the-art methods fail due to frame drops or increased speeds.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0257", "problem_id": "02570001", "content": "Deep reinforcement learning has demonstrated exceptional performance across complex, high-dimensional tasks, including video games and robot movement, largely due to the application of deep neural networks in approximating policy and value functions. However, achieving optimal results often necessitates extensive weight adjustments. As an alternative, we explore randomized function approximation, which offers reduced computational cost and enhanced numerical performance compared to training fully connected networks. We introduce \\texttt, a generalized policy iteration algorithm designed for Markov Decision Processes (MDPs) with continuous state and action spaces, where both policy and value functions are modeled using randomized networks. Furthermore, we provide finite-time performance guarantees for the proposed algorithm. Finally, we evaluate its numerical performance in demanding environments and benchmark it against algorithms employing deep neural networks.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0258", "problem_id": "02580001", "content": "This paper presents ChainerRL, a Python-based open-source library for deep reinforcement learning (DRL) developed on the Chainer framework. The library incorporates a wide range of cutting-edge DRL algorithms and methodologies from contemporary research. To support reproducibility and educational use, ChainerRL includes scripts that faithfully reproduce experimental conditions from original studies and benchmark outcomes for multiple algorithms. Additionally, it features a visualization tool for qualitative analysis of trained agents. The source code is available on GitHub: https://github.com/chainer/chainerrl.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0259", "problem_id": "02590001", "content": "Humans excel at recognizing and locating objects by combining visual and auditory information. Although machines have made significant strides in processing images, there has been less focus on sounds. This research introduces a method for the dense semantic labeling of sound-producing objects, relying exclusively on binaural audio. We present an innovative sensor arrangement and compile a new audio-visual dataset of street environments using eight professional binaural microphones and a 360-degree camera. This research leverages the simultaneous presence of visual and auditory signals for supervision transfer. Specifically, we implement a cross-modal distillation framework incorporating a vision 'teacher' method and a sound 'student' method, where the student method is trained to replicate the outcomes of the teacher method. This approach enables the auditory system to be trained without the need for human annotations. Furthermore, we introduce two supplementary tasks: a) a unique task focused on Spatial Sound Super-resolution to enhance the spatial quality of sounds, and b) precise depth prediction of the scene. We integrate these three tasks into a single end-to-end trainable multi-task network aimed at improving overall performance. Results from experiments on the dataset reveal that 1) our approach delivers promising results in semantic prediction and the auxiliary tasks; 2) the interdependence of the three tasks leads to superior performance when trained concurrently; and 3) both the quantity and orientation of microphones are crucial. The data and code will be made available to promote further research in this emerging area.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0260", "problem_id": "02600001", "content": "To mitigate the spread of COVID-19 in public areas, the World Health Organisation (WHO) advocates for social distancing, with most governments and health authorities enforcing a 2-meter physical distance in various settings, including shopping centres, schools, and other enclosed spaces. This study proposes a novel approach, leveraging a hybrid Computer Vision and YOLOv4-based Deep Neural Network model, to automatically detect individuals in crowds within both indoor and outdoor environments using standard CCTV security cameras. By integrating the proposed DNN model with an adapted inverse perspective mapping (IPM) technique and SORT tracking algorithm, a robust system for detecting people and monitoring social distancing is achieved. The model was trained on the Microsoft Common Objects in Context (MS COCO) and Google Open Image datasets, and its performance was evaluated on the Oxford Town Centre dataset, outperforming three state-of-the-art methods with a mean average precision of 99.8% and a real-time speed of 24.1 fps, even in challenging conditions such as occlusion, partial visibility, and varying lighting. Furthermore, an online infection risk assessment scheme is provided through statistical analysis of spatio-temporal data from people's movement trajectories and social distancing violation rates. The developed model offers a versatile and accurate solution for people detection and tracking, with potential applications extending to fields like autonomous vehicles, human action recognition, anomaly detection, sports, crowd analysis, and other areas where human detection is a primary focus, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0261", "problem_id": "02610001", "content": "Recommending items that complement a fashion ensemble is essential for completing outfits. Current methodologies primarily emphasize predicting outfit compatibility rather than focusing on item retrieval. This paper introduces a novel framework designed for retrieving complementary items for outfits. The framework incorporates a category-based subspace attention network, offering a scalable solution for learning subspace attentions. Furthermore, an outfit ranking loss is introduced to more effectively capture the relationships between items within a complete outfit. The proposed method is evaluated through outfit compatibility, FITB, and new retrieval tasks. Empirical evaluations reveal that the presented approach surpasses existing state-of-the-art techniques in both compatibility prediction and complementary item retrieval.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0262", "problem_id": "02620001", "content": "The power grid faces novel challenges due to rapid urbanization and the growing incorporation of distributed renewable energy, energy storage solutions, and electric vehicles. In the United States, buildings account for approximately 70% of total electricity consumption, and demand response strategies could potentially decrease peak electricity demand by roughly 20%. Realizing this potential requires data-driven and model-free control systems operating on distributed systems, making reinforcement learning (RL) algorithms increasingly relevant. However, the application of RL to demand response has suffered from a lack of standardization, hindering progress compared to other areas of computer science. To address this, we developed CityLearn, an OpenAI Gym Environment designed to facilitate the implementation, sharing, replication, and comparison of RL-based demand response strategies. We present this environment and The CityLearn Challenge, an RL competition established to stimulate advancements in this domain.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0263", "problem_id": "02630001", "content": "In this study, we introduce a new approach for acquiring hierarchical representations of Markov decision processes. Our approach involves dividing the state space into subsets and establishing subtasks for managing transitions between these partitions. We frame the task of state space partitioning as an optimization problem that can be addressed through gradient descent, utilizing a collection of sampled trajectories, which renders our method applicable to high-dimensional scenarios with extensive state spaces. We empirically demonstrate the effectiveness of the method by illustrating its ability to learn a valuable hierarchical representation within a navigation context. Once attained, this hierarchical representation is applicable for tackling various tasks within the specified domain, thereby enabling the transfer of knowledge across different tasks.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0264", "problem_id": "02640001", "content": "Semantic segmentation utilizing high-resolution remotely sensed images is vital for numerous real-world applications, including urban development, environmental conservation, and the monitoring of both natural and human-made landscapes. Nevertheless, automating the processes of semantic categorization and segmentation remains a difficult challenge, especially for high-resolution images that present significant spatial and spectral complexities. Tackling this issue constitutes an intriguing area of research that facilitates scene-level analysis of landscape patterns and informs decision-making. In this study, we introduce a method for automatic land segmentation that employs the Feature Pyramid Network (FPN). As a well-established architecture, FPN generates a feature pyramid that encompasses high-level semantics; however, limitations in feature extraction and fusion prevent FPN from consolidating more discriminative features. To overcome this, we present an Attention Aggregation Module (AAM) aimed at improving multi-scale feature learning through attention-driven feature amalgamation. Utilizing FPN and AAM, we create a new framework called Attention Aggregation Feature Pyramid Network (A2-FPN) for the semantic segmentation of fine-resolution remotely sensed images. Comprehensive experiments across three datasets validate the efficacy of our A2-FPN in enhancing segmentation accuracy. Code is available at https://github.com/lironui/A2-FPN.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0265", "problem_id": "02650001", "content": "Object detection remains a critical yet demanding task within computer vision. While significant progress has been made in detecting objects in natural scenes over the last ten years, similar advancements in aerial imagery have been limited. This delay stems from both the considerable diversity in object scale, orientation, and shape across the Earth's surface and the lack of comprehensively labeled datasets for aerial scenes. To address this gap in Earth Vision—also referred to as Earth Observation and Remote Sensing—we present the Dataset for Object deTection in Aerial images (DOTA), a large-scale benchmark. The dataset comprises 2806 aerial images captured from various sensors and platforms, each approximately 4000x4000 pixels in size, featuring objects with diverse scales, orientations, and shapes. Expert annotators meticulously labeled these images across 15 common object categories, resulting in 188,282 instances, each demarcated by an 8-degree-of-freedom quadrilateral. To establish a benchmark for Earth Vision, we assess leading object detection methods on DOTA, revealing that the dataset effectively mirrors real-world challenges and applications in this domain.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0266", "problem_id": "02660001", "content": "Traditional image captioning models predominantly employ Convolutional Neural Network (CNN) image features in conjunction with recurrent models to generate captions. The recent incorporation of image scene graphs into captioning models has enabled the exploitation of structural semantics, including object entities, relationships, and attributes. However, previous studies have highlighted that directly utilizing scene graphs from a black-box generator can compromise image captioning performance, and that scene graph-based models often require the explicit use of image features to produce satisfactory captions. To overcome these limitations, we introduce a novel framework that relies solely on scene graph labels to achieve competitive image captioning results. By bridging the semantic gap between the scene graphs derived from the input image and its corresponding caption, our approach leverages the spatial location of objects and Human-Object-Interaction (HOI) labels through an additional HOI graph. Our model, SG2Caps, significantly outperforms existing scene graph-only captioning models, demonstrating the potential of scene graphs as a viable representation for image captioning. Moreover, by directly utilizing scene graph labels, our approach eliminates the need for costly graph convolutions over high-dimensional CNN features, resulting in a 49% reduction in trainable parameters, and our implementation is available at https://github.com/Kien085/SG2Caps.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0267", "problem_id": "02670001", "content": "Deep reinforcement learning (RL) has recently seen significant achievements; however, the time taken for experiments remains a major limitation in both research and practical applications. We explore ways to enhance the performance of current deep RL algorithms on modern computer architectures, particularly those utilizing a combination of CPUs and GPUs. Our findings indicate that both policy gradient and Q-value learning methods can be effectively modified to leverage multiple parallel instances of simulators. Additionally, we demonstrate that it is feasible to train with batch sizes substantially larger than the norm without adversely impacting sample complexity or final outcomes. These insights enable us to create a comprehensive framework for parallelization that significantly accelerates experiments across both algorithm types. All neural network computations are performed on GPUs, facilitating faster data collection and training processes. Our results showcase the efficiency of utilizing an entire DGX-1 to master effective strategies in Atari games within minutes, employing both synchronous and asynchronous approaches.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0268", "problem_id": "02680001", "content": "Knowledge distillation (KD) is a popular technique for compressing large pre-trained models. Standard KD approaches involve training a smaller student model to replicate the soft targets produced by a larger teacher model. However, this interaction is unidirectional, with the teacher model typically remaining fixed after training, thus providing static soft targets for distillation. This one-way flow prevents the teacher from adapting to the student's characteristics and learning trajectory. To mitigate this limitation, we introduce Interactive Knowledge Distillation (IKD), which enables the teacher to learn how to teach by incorporating feedback from the student. Specifically, IKD trains the teacher model to generate tailored soft targets for a given student at each training iteration. Joint optimization of both models is achieved through iterative course and exam steps: the course step optimizes the student using the teacher's soft targets, while the exam step optimizes the teacher based on the student's feedback. IKD is a versatile framework that can be integrated with many existing knowledge distillation techniques. Experimental results demonstrate that IKD achieves superior performance compared to conventional KD methods across a range of NLP tasks.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0269", "problem_id": "02690001", "content": "Recent advancements in estimating 3D geometry from single images using deep convolutional networks trained on unlabeled videos have shown notable progress. While existing state-of-the-art approaches rely on rigid structure-from-motion frameworks that model only camera ego motion, real-world videos often contain dynamic objects like moving vehicles. To address this, our method integrates per-pixel 3D object motion into the learning framework, enhancing holistic 3D scene flow understanding and improving single-image geometry estimation. Our approach employs a motion network to predict relative 3D camera poses and segment moving objects from rigid backgrounds across consecutive video frames. An optical flow network estimates dense 2D correspondences, while a depth network predicts depth maps for both frames. These components—2D flow, camera pose, segmentation masks, and depth maps—are combined in a differentiable holistic 3D motion parser (HMP) to recover per-pixel 3D motion for both rigid and dynamic regions. We introduce specialized loss functions for these motion types to refine depth and motion network training, reducing geometric estimation errors. Additionally, stereo images are incorporated during training to resolve ambiguities in monocular 3D motion estimation. Evaluations on the KITTI 2015 dataset confirm that our method produces consistent geometry, 3D motion, and moving object masks while outperforming existing state-of-the-art techniques, validating the effectiveness of our approach.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0270", "problem_id": "02700001", "content": "Anomaly detection has garnered significant research interest; however, the current databases for anomaly detection face two key challenges. First, they are restricted in scale. Second, the training datasets only provide video-level labels that indicate whether an abnormal event occurs throughout the entire video, without precise time duration annotations. To address these issues, we introduce a new Large-scale Anomaly Detection (LAD) database that serves as a benchmark for anomaly detection in video sequences, characterized by two main features. 1) It includes 2000 video sequences with both normal and abnormal clips across 14 anomaly categories such as crash, fire, and violence, showcasing a wide variety of scenes, thus becoming the largest anomaly analysis database to date. 2) It offers annotation data consisting of video-level labels (abnormal/normal video, anomaly type) and frame-level labels (abnormal/normal video frame), which enhance the anomaly detection process. Utilizing the advantages of the LAD database, we further approach anomaly detection as a fully-supervised learning task and propose a multi-task deep neural network for its resolution. We initially extract local spatiotemporal contextual features using an Inflated 3D convolutional (I3D) network, followed by constructing a recurrent convolutional neural network that processes these local features to derive the spatiotemporal contextual features. With the global spatiotemporal contextual features in hand, both the anomaly type and score are simultaneously determined by a multi-task neural network. Experimental results indicate that our proposed method surpasses existing state-of-the-art anomaly detection techniques on our database as well as other publicly available anomaly detection databases. Codes can be accessed at https://github.com/wanboyang/anomaly_detection_LAD2000.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0271", "problem_id": "02710001", "content": "The restricted Boltzmann machine (RBM) serves as a versatile mechanism for modeling intricate data; however, there are notable computational challenges associated with employing RBMs for high-dimensional multinomial data. In the realm of natural language processing, words can be effectively represented by K-ary discrete distributions, with K corresponding to the vocabulary size, which can reach into the hundreds of thousands. The traditional methodology for training RBMs on word data is constrained as it necessitates sampling the states of K-way softmax visible units during block Gibbs updates, a process that incurs a time cost proportionate to K. This study tackles this challenge by utilizing a broader class of Markov chain Monte Carlo operators on the visible units, resulting in updates with computational complexity that is not dependent on K. We showcase the effectiveness of our method by training RBMs on hundreds of millions of word n-grams with significantly larger vocabularies than previously achievable, utilizing the extracted features to enhance performance in chunking and sentiment classification tasks, ultimately achieving state-of-the-art results in the latter.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0272", "problem_id": "02720001", "content": "Real-time segmentation of surgical instruments is essential in robot-assisted surgery, yet deploying deep learning models for this purpose remains challenging due to their high computational requirements and slow inference speeds. To address this, we introduce the attention-guided lightweight network (LWANet), a novel architecture designed for real-time surgical instrument segmentation. LWANet utilizes an encoder-decoder framework, combining the lightweight MobileNetV2 encoder with a decoder that incorporates depthwise separable convolution, attention fusion blocks, and transposed convolution. By leveraging depthwise separable convolution as the fundamental building block in the decoder, the model's size and computational costs are significantly reduced. The attention fusion block effectively captures global context and encodes semantic channel dependencies, allowing for precise localization of surgical instruments. Furthermore, transposed convolution enables the upsampling of feature maps to achieve refined edge detection. The LWANet achieves real-time segmentation with minimal computational overhead, demonstrating an inference speed of 39 fps and requiring only 3.39 GFLOPs for 960*544 inputs, alongside a compact model size of 2.06 M parameters. Evaluations on two datasets show that LWANet attains state-of-the-art performance, with a mean IOU of 94.10% on Cata7 and a record-breaking increase of 4.10% in mean IOU on EndoVis 2017, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0273", "problem_id": "02730001", "content": "This study explores the dynamic relationship between visual attention and motor actions in object interactions, leveraging a first-person camera perspective to capture an individual's intimate visual and tactile engagements with objects. The concept of action-objects is introduced, referring to objects that engage a person's conscious visual or tactile attention, such as watching TV or grasping a cup, which often exhibit characteristic 3D spatial distances and orientations relative to the person due to shared task-dependent spatial configurations. A predictive model, EgoNet, is proposed, utilizing a joint two-stream network that combines visual appearance and 3D spatial layout cues to predict the likelihood of action-objects on a per-pixel basis, while also incorporating a first-person coordinate embedding to learn the spatial distribution of action-objects. The effectiveness of EgoNet is demonstrated through its consistent outperformance of baseline approaches and strong generalization ability to novel first-person datasets, showing its potential to enhance robots' understanding of human-object interactions, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0274", "problem_id": "02740001", "content": "We introduce DeepV2D, a comprehensive deep learning framework designed for depth prediction from video inputs. This architecture integrates the representational power of neural networks with the geometric rules that dictate image formation. By transforming a set of traditional geometric algorithms into trainable components, we create an end-to-end differentiable system. DeepV2D operates through two intertwined phases: motion estimation and depth estimation. In the inference process, these two estimations are alternated to achieve precise depth results. The code can be found at https://github.com/princeton-vl/DeepV2D.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0275", "problem_id": "02750001", "content": "The use of wearable sensor data for human activity recognition (HAR) has grown significantly in recent years, leading to numerous real-world applications, particularly in health care. While machine learning techniques like Decision Trees, Support Vector Machine, Naive Bayes, K-Nearest Neighbor, and Multilayer Perceptron have been effectively applied to HAR, their performance can be limited in certain contexts. To address these limitations, this paper introduces a new ensemble learning approach designed to enhance the performance of existing machine learning methods in HAR.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0276", "problem_id": "02760001", "content": "Humans gather information from their environment by simultaneously processing and integrating high-dimensional data from various sources, such as visual and auditory inputs. In contrast, machine perception systems are usually tailored to specific modalities and optimized for unimodal standards, resulting in the prevalent use of late-stage fusion of final representations or predictions from individual modalities (`late-fusion`) in multimodal video classification tasks. We present an innovative transformer-based architecture that incorporates `fusion bottlenecks` for melding modalities at various layers. Unlike conventional pairwise self-attention mechanisms, our design compels information from different modalities to traverse through a limited number of bottleneck latents, necessitating the model to aggregate and distill the most pertinent information from each modality while only sharing essential data. Our findings indicate that this approach enhances fusion efficacy while simultaneously lowering computational demands. We perform extensive ablation studies and achieve cutting-edge results on several audio-visual classification benchmarks, including Audioset, Epic-Kitchens, and VGGSound. All code and models will be made publicly available.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0277", "problem_id": "02770001", "content": "Images acquired in low-light environments frequently exhibit compromised visibility. In addition to inadequate lighting, various forms of degradation, including noise and color distortion resulting from camera limitations, are inherent in these dark images. Consequently, merely increasing the brightness in poorly lit areas will invariably exacerbate these underlying artifacts. This paper introduces a straightforward yet potent network designed for \\textbfdling the \\textbfarkness (KinD), which, drawing inspiration from Retinex theory, decomposes images into illumination and reflectance components. The illumination component manages light adjustment, while the reflectance component addresses degradation removal. This decomposition effectively separates the original space into two smaller subspaces, facilitating improved regularization and learning. Notably, our network is trained using paired images captured under varying exposure settings, without relying on ground-truth reflectance or illumination data. Comprehensive experiments validate the effectiveness of our approach and its advantages over existing state-of-the-art methods. KinD demonstrates resilience to significant visual impairments and allows for intuitive adjustment of light levels. Furthermore, our model processes a VGA resolution image in under 50ms on a 2080Ti GPU. These characteristics make KinD a promising solution for real-world applications.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0278", "problem_id": "02780001", "content": "Tumor morphology is a crucial determinant influencing tumor growth and metastasis. This study introduces a topological feature derived from persistent homology to describe tumor advancement based on digital pathology and radiology images, while also exploring its impact on time-to-event data. The proposed topological features maintain invariance under scale-preserving transformations and effectively encapsulate diverse tumor shape characteristics. These features are presented within a functional space and utilized as functional predictors in a functional Cox proportional hazards model. The model allows for interpretable insights regarding the relationship between topological shape features and survival risks. Two case studies involving 143 lung cancer patients and 77 brain tumor patients are performed. The findings from both analyses indicate that the topological features effectively predict survival outcomes upon adjusting for clinical variables, with high-risk groups demonstrating significantly (at the 0.001 level) poorer survival rates compared to low-risk groups. Furthermore, the topological shape features identified as positively correlated with survival hazards exhibit irregular and heterogeneous patterns, known to be associated with tumor progression.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0279", "problem_id": "02790001", "content": "We propose Probabilistic Object Detection, which involves identifying objects in images while precisely measuring both spatial and semantic uncertainties in the detections. Due to the absence of existing methods for evaluating such probabilistic detections, we introduce the Probability-based Detection Quality (PDQ) metric. Unlike AP-based metrics, PDQ eliminates arbitrary thresholds, rewards accurate spatial and label predictions, and evaluates foreground/background separation while penalizing false positives and false negatives. We compare PDQ with mAP and moLRP by testing leading detectors and a Bayesian detector using Monte Carlo Dropout. Results show that standard detectors often exhibit excessive spatial confidence, leading to subpar performance in probabilistic detection. This work seeks to advance the creation of object detection methods that deliver reliable spatial and label uncertainty estimates, crucial for real-world applications in robotics and embodied AI systems.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0280", "problem_id": "02800001", "content": "This paper investigates the problem of generating new perspectives of a human body from a single input image. While current deep learning techniques perform adequately with rigid objects, they struggle with highly articulated subjects such as human bodies. Existing methods typically rely on CNNs to learn a mapping from available views to novel views. However, the complex articulation of the human body poses a significant challenge for CNNs to effectively memorize and interpolate the data. To overcome this limitation, we introduce a novel deep learning framework that explicitly estimates and utilizes the underlying human body geometry. Our proposed pipeline consists of a shape estimation network and an image generation network, connected by a perspective transformation that generates a forward flow for pixel value transfer. This architecture effectively decomposes the data variation, simplifying the learning process at each stage. Experimental results demonstrate a substantial improvement in performance for objects with variable poses. Furthermore, our method can be applied to real-world data obtained from 3D sensors, and the generated flow can be utilized to produce high-quality results at higher resolutions.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0281", "problem_id": "02810001", "content": "Contrastive learning (CL) has demonstrated considerable efficacy in graph-based semi-supervised learning (SSL) as it effectively enhances the limited task information derived from the labeled nodes within a graph. Nevertheless, current research on graph CL (GCL) fails to consider the uneven distribution of task information influenced by the graph's topology and the choice of labeled nodes. This results in a uniform application of CL across the entire graph, leading to a mismatch between CL and graph learning. To remedy this, we propose a more adaptive implementation of CL in graph learning by factoring in the task information received by each node. Initially, we employ Group PageRank to evaluate the information gain for nodes within the graph and discover that CL is most beneficial for nodes that are distantly located from the labeled nodes. Subsequently, we introduce our Distance-wise Graph Contrastive Learning (DwGCL) approach from two perspectives: (1) from a global standpoint regarding the distribution of task information across the graph, we bolster the CL effect on nodes that are topologically removed from the labeled nodes; and (2) from an individual perspective of each node's received information, we assess the relative distance between nodes and subsequently adjust the GCL sampling strategy. Comprehensive experiments across five benchmark graph datasets reveal that DwGCL significantly enhances performance compared to prior GCL techniques. Furthermore, our evaluations across eight graph neural networks featuring diverse architectures and three distinct annotation configurations underscore the robustness and adaptability of DwGCL.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0282", "problem_id": "02820001", "content": "In the absence of a specialized method for classifying imbalanced data, artificial intelligence algorithms struggle to effectively recognize data from minority classes. Typically, the sole approach to address imbalanced data involves adjusting the existing algorithms under the assumption of imbalanced training data; however, this often yields subpar results for normal data handling. In this study, we introduce the class expert generative adversarial network (CE-GAN) as a solution for imbalanced data classification. CE-GAN represents a modification to deep learning algorithm architecture that does not presuppose that the training data is imbalanced. Furthermore, CE-GAN is designed to capture more detailed characteristics of each class prior to the classification phase. This research demonstrates that CE-GAN performs effectively for the classification of imbalanced data.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0283", "problem_id": "02830001", "content": "This paper introduces a novel curriculum learning approach based on clustering to facilitate the training of generative models, such as Generative Adversarial Networks (GAN), on noisy data. By prioritizing data points with high centrality within underlying clusters, the proposed algorithm constructs a curriculum that guides the training process. To ensure scalability, an active set method is employed, where each training round focuses on a subset of previously trained data and newly added data with lower centrality. The paper also provides a geometric analysis to explain the importance of cluster-based curriculum learning for generative models. Experimental results on cat and human-face data demonstrate the effectiveness of the algorithm in learning optimal generative models, such as ProGAN, according to specific quality metrics, even with noisy data. Notably, the findings suggest a strong connection between the optimal cluster curriculum and the critical point of the geometric percolation process, as formulated in the paper.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0284", "problem_id": "02840001", "content": "In various medical image analysis tasks, such as lesion sub-type identification and detailed anatomical structure segmentation, the need for local discriminative representation is paramount. Nevertheless, conventional supervised representation learning methods are hindered by their requirement for extensive annotated data, while unsupervised discriminative representation learning approaches focus on global feature learning, distinguishing between images, which is not ideal for localized medical image analysis. To overcome these limitations, this study integrates local discrimination into unsupervised representation learning. The proposed model comprises two branches: an embedding branch that learns to dispersely map dissimilar pixels onto a low-dimensional hypersphere, and a clustering branch that learns to group similar pixels into the same cluster. By training these branches simultaneously in a mutually reinforcing manner, the resulting local discriminative representations effectively capture the similarity between local image regions. These representations can be leveraged to enhance performance in various downstream tasks and can also be utilized to cluster anatomical structures from unlabeled medical images, guided by topological priors derived from simulations or structures with analogous topological characteristics, as demonstrated through applications in retinal and chest X-ray images, Figure A, Figure B, Figure C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0285", "problem_id": "02850001", "content": "End-to-end deep Reinforcement Learning (RL) methods typically necessitate the simultaneous training of perception, decision-making, and low-level control mechanisms using minimal reward signals and complex input data, while struggling to integrate pre-existing knowledge. Consequently, these methods demand extensive training periods, hindering their applicability to real-world robotic applications. While algorithms like object detection can derive task-specific representations from intricate inputs, their variable-length outputs pose challenges for integration with RL techniques that require fixed-length inputs for neural networks. To address these limitations, we introduce a framework that integrates deep sets encoding, which accommodates variable-length abstract representations, with modular RL. This integration facilitates the separation of high-level decision-making from low-level control. We validate our framework on a robot manipulation task involving object sorting, demonstrating its capacity to learn efficient policies within minutes using a simplified simulation. Furthermore, the acquired policies can be directly implemented on a physical robot without additional training and exhibit generalization to task variations not encountered during the learning phase.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0286", "problem_id": "02860001", "content": "In this study, we present SalsaNext, a system designed for real-time, uncertainty-aware semantic segmentation of complete 3D LiDAR point clouds. SalsaNext builds upon the foundation of SalsaNet [1] by employing an encoder-decoder structure, where the encoder is comprised of ResNet blocks and the decoder integrates upsampled features from these residual blocks. Unlike SalsaNet, we have developed a novel context module, substituted the ResNet encoder blocks with a new stack of residual dilated convolutions that progressively expand the receptive fields, and introduced a pixel-shuffle layer in the decoder. Moreover, we have transitioned from using stride convolution to average pooling and incorporated central dropout techniques. To directly enhance the Jaccard index, we have integrated a combination of weighted cross-entropy loss and Lovasz-Softmax loss [2]. We also implement a Bayesian approach to estimate both epistemic and aleatoric uncertainties for each point within the cloud. Our comprehensive quantitative assessment on the Semantic-KITTI dataset [3] illustrates that SalsaNext surpasses other leading semantic segmentation networks, achieving top rank on the Semantic-KITTI leaderboard. Additionally, we have made our source code available at https://github.com/TiagoCortinhal/SalsaNext.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0287", "problem_id": "02870001", "content": "Deep learning has become a prominent artificial intelligence technique for analyzing medical images across diverse applications. Yet, the scarcity of well-annotated medical imaging datasets, essential for training these models, restricts their effectiveness. Acquiring such data is difficult due to privacy concerns, limited expert availability for annotation, insufficient representation of rare conditions, and high costs. Previous approaches have employed synthetically generated data, but models trained on such data frequently struggle to generalize to real-world scenarios. Cinematic rendering enhances realism by simulating light interactions within tissue models derived from CT scans, producing highly realistic images. This study introduces an early application of cinematic rendering in deep learning, where networks initially trained on synthetic data are refined using cinematically rendered CT scans for monocular depth estimation in endoscopy. Experimental results show that: (a) CNNs pretrained on synthetic data and fine-tuned with photorealistic cinematic rendering adapt more effectively to real medical images and exhibit greater robustness than those without fine-tuning, (b) these refined networks achieve optimal performance with fewer training samples, and (c) fine-tuning across varied photorealistic rendering conditions of the same scene reduces patient-specific biases and enhances model generalizability. Empirical analysis reveals that networks fine-tuned with cinematically rendered data achieve 56.87% lower error in rendered endoscopy images and 27.49% lower error in real porcine colon endoscopy images for depth prediction.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0288", "problem_id": "02880001", "content": "This study presents a theoretical framework for addressing the numerical stability challenges in training Variational AutoEncoders (VAE). Recent advancements enabling VAEs to achieve state-of-the-art generative performance on intricate image datasets reveal that deep architectures and complex output distributions often lead to erratic high training gradients and NaN losses. While empirical solutions exist to mitigate these issues, they lack strong theoretical foundations and practical reliability. Our analysis identifies the root cause of instability at the interaction between neural networks and their output probability distributions. We demonstrate that an uncareful parameterization of the encoded Normal distribution's variance is a frequent instability trigger and extend this analysis to less apparent sources. By refining the parameterization of these Normal distributions, we show that VAEs can be trained with improved stability.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0289", "problem_id": "02890001", "content": "The assessment of image aesthetic quality has gained considerable attention over the past ten years. Recently, the concept of assessing comments (aesthetic captions) has emerged to articulate the overall aesthetic impression of an image through text. In this study, we introduce Aesthetic Attributes Assessment of Images, which focuses on aesthetic attributes captioning. This innovative approach for evaluating image aesthetics predicts captions for aesthetic attributes in conjunction with an aesthetic score for each attribute. We present a new dataset that includes comments reflecting up to five aesthetic attributes for each image, achieved through knowledge transfer from a comprehensively annotated smaller dataset. Additionally, we introduce the Aesthetic Multi-Attribute Network (AMAN), which is trained using a combination of the fully-annotated small-scale PCCD dataset and a weakly-annotated large-scale DPC-Captions dataset. Our AMAN leverages transfer learning and an attention model within a unified framework. Experimental results from the DPC-Captions and PCCD datasets demonstrate that our approach can accurately predict captions for five aesthetic attributes while also providing numerical assessments for each attribute. We employ the evaluation metrics commonly used in image captions to demonstrate that our specially designed AMAN model surpasses conventional CNN-LSTM models and contemporary SCA-CNN models in the image captioning domain.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0290", "problem_id": "02900001", "content": "Current methods for reconstructing densely sampled light fields (LFs) typically employ a depth-free approach to handle non-Lambertian effects. These methods often encounter a fundamental trade-off: reducing aliasing artifacts, which arise from the angular sparsity of the input LF, inevitably introduces blurring. To address this challenge, we present a novel learning-based framework that incorporates a carefully designed epipolar plane image (EPI) structure. We begin by demonstrating analytically that reducing the spatial scale of an EPI is more effective at mitigating aliasing than traditional pre-filtering techniques. Based on this, we introduce a Laplacian Pyramid EPI (LapEPI) structure, which combines low spatial scale EPIs (for aliasing reduction) with high-frequency residuals (to preserve sharpness), effectively resolving the trade-off. We then propose a new network architecture, LapEPI-net, tailored for the LapEPI structure. To maintain non-Lambertian performance, we utilize a transfer-learning strategy, initially pre-training the network with natural images before fine-tuning it with unstructured LFs. Comprehensive experiments validate the superior performance and robustness of our method in addressing the aliasing-or-blurring problem and achieving accurate non-Lambertian reconstruction.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0291", "problem_id": "02910001", "content": "Convolutional neural networks, especially deep neural networks, have emerged as powerful methods for image compression and addressing inverse problems such as denoising, inpainting, and reconstruction from limited or noisy data. Their effectiveness stems partly from their capacity to accurately represent and synthesize natural images. Unlike traditional approaches like wavelets, deep neural networks for image generation contain numerous parameters—often exceeding their output dimensions—and require extensive training datasets. This work introduces an untrained, streamlined image model termed the deep decoder, a neural network capable of producing natural images using minimal weight parameters. The architecture of the deep decoder is straightforward, lacking convolutions and featuring fewer parameters than the output size, enabling efficient image compression into a compact set of weights that rivals wavelet-based thresholding. Additionally, its underparameterization prevents overfitting, achieving leading performance in denoising tasks. The network’s simplicity arises from uniform layers composed solely of an upsampling unit, channel-wise linear combinations, ReLU activation, and normalization, facilitating theoretical analysis and offering insights into the mechanisms that make neural networks effective for signal representation.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0292", "problem_id": "02920001", "content": "The short-term prediction of passenger flow is essential for managing transit systems and controlling crowds. Various challenges arise in forecasting passenger flow for urban rail transit networks due to spatial dependencies, temporal correlations, inter-station relationships influenced by latent factors, and external elements. To address these complexities, we introduce an innovative deep learning framework named Multi-Graph Convolutional-Recurrent Neural Network (MGC-RNN) for forecasting passenger flow in urban rail systems. This approach utilizes multiple graphs to represent spatial and other heterogeneous inter-station relationships. Additionally, the temporal dynamics of these correlations are captured through the proposed network structure. By employing a sequence to sequence (seq2seq) architecture, we can predict the inflow and outflow of all stations collectively for multiple future time steps. The developed method is tested on short-term passenger flow forecasts for Shenzhen Metro in China. Experimental results indicate that MGC-RNN significantly exceeds the accuracy of benchmark algorithms. Furthermore, it highlights that inter-station factors such as network distance, structure, and recent flow patterns are crucial for forecasting passenger flow. The LSTM-encoder-decoder design effectively captures temporal dependencies as well. Overall, this proposed framework offers diverse insights into passenger flow dynamics for precise predictions and suggests potential for integrating various sources of heterogeneous data in spatiotemporal forecasting tasks.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0293", "problem_id": "02930001", "content": "The widespread effectiveness of deep features learned from ImageNet for transfer learning applications raises important questions about the dataset's characteristics that enable the learning of robust, generalizable features. This study empirically examines several aspects of this issue, including the impact of increasing pre-training data, the relationship between feature quality and the number of training samples per category, and the effect of adding more object categories on performance. Furthermore, the study explores how to optimally allocate a fixed data budget across classes and investigates the necessity of fine-grained recognition for acquiring useful features. To address these questions, CNN features were pre-trained on different subsets of ImageNet, and their transfer performance was assessed on PASCAL detection, PASCAL action classification, and SUN scene classification tasks. The results indicate that many variations in pre-training data choices, previously considered vital, do not substantially influence transfer performance.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0294", "problem_id": "02940001", "content": "Advancements in image captioning are increasingly intricate, driven by efforts to generalize models and establish connections between visual features and natural language processing. This study explores such relationships by introducing Tensor Product Representation (TPR), a method that generalizes language modeling and structures linguistic attributes—specifically grammar and parts of speech—to yield improved structure and grammatical accuracy in generated sentences. TPR facilitates enhanced and distinctive representation and organization of the feature space, leading to superior sentence construction. Various approaches to defining and refining TPR are examined, and their performance is evaluated against traditional procedures and feature representations in image captioning. The proposed models demonstrate significant improvements over existing architectures.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0295", "problem_id": "02950001", "content": "Time series analysis, a subfield of data science, focuses on analyzing ordered sequences of numerical values collected over time. These sequences are valuable for visualizing and interpreting the progression of processes, uncovering trends, relationships, and similarities within the data. Time series data are prevalent in diverse domains, including healthcare (e.g., electrocardiograms, blood sugar levels), activity recognition, remote sensing, finance (e.g., stock market prices), and industry (e.g., sensor data). Time series classification involves creating algorithms that automatically assign labels to time series data. Due to the inherent sequential nature of time series, specialized algorithms are needed to leverage the temporal dependencies, rendering conventional machine learning models designed for tabular data less effective. Deep learning has become a prominent and powerful technique for supervised classification tasks, particularly in computer vision, and has been applied to time series as well. This thesis aimed to investigate and develop deep neural networks tailored for time series classification. A comprehensive experimental study was conducted to compare existing deep learning methods and benchmark their performance against state-of-the-art non-deep learning approaches. Furthermore, contributions were made to the field, specifically in transfer learning, data augmentation, ensembling techniques, and the study of adversarial attacks. Finally, a new architecture based on the Inception network (Google) was introduced, demonstrating high efficiency compared to existing methods.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0296", "problem_id": "02960001", "content": "This study introduces a novel video super-resolution reconstruction (SRR) technique designed to enhance robustness against outliers. While the R-LMS algorithm offers superior reconstruction quality relative to its computational demands and inherently handles registration errors well, its effectiveness significantly diminishes when faced with innovation outliers. Analyzing the proximal point cost function representation of the R-LMS iterative equation provides deeper insights into its behavior across varying conditions. Leveraging the statistical characteristics of common innovation outliers, the authors formulate a new cost function and develop two algorithms that exhibit greater outlier resilience while retaining computational efficiency similar to R-LMS. Monte Carlo simulations demonstrate that the proposed approach surpasses both conventional and regularized LMS variants and rivals advanced SRR methods while requiring substantially lower computational resources.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0297", "problem_id": "02970001", "content": "Scene text images encompass both visual texture and semantic information. Prior scene text recognition techniques have advanced considerably; however, the use of semantic information to improve text recognition has received relatively little attention, with research primarily focusing on RNN-like architectures to implicitly model semantics. We contend that RNN-based approaches exhibit limitations, including time-dependent decoding and unidirectional serial transmission of semantic context, which impedes the effective use of semantic information and computational efficiency. To address these issues, we introduce a novel end-to-end trainable framework, the Semantic Reasoning Network (SRN), designed for precise scene text recognition. This network incorporates a Global Semantic Reasoning Module (GSRM) to capture global semantic context through multi-way parallel transmission. Performance evaluations on seven public benchmarks, encompassing regular text, irregular text, and non-Latin long text, demonstrate the effectiveness and robustness of the proposed SRN. Furthermore, SRN exhibits significantly faster processing speeds compared to RNN-based methods, highlighting its practical utility.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0298", "problem_id": "02980001", "content": "Recent advancements in pedestrian detection have been driven by deep learning, with most detectors adopting conventional object detection frameworks involving default boxes and two-stage processes. While anchor-free and one-stage detectors have emerged in this field, their performance remains limited. To combine the simplicity of anchor-free approaches with the precision of two-stage methods, we introduce modifications to the Center and Scale Prediction (CSP) detector. Our key contributions include: (1) enhancing CSP's robustness and training efficiency, (2) introducing a new width prediction technique called compressing width, (3) achieving competitive results on the CityPersons benchmark with 9.3% MR on the reasonable set, 8.7% MR on the partial set, and 5.6% MR on the bare set, demonstrating that anchor-free one-stage detectors can maintain high accuracy, and (4) investigating previously unexplored aspects of Switchable Normalization.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0299", "problem_id": "02990001", "content": "Recent advances in reference-based face restoration techniques have demonstrated impressive capabilities in recovering fine details from low-quality images, but their applicability is limited by the need for a high-quality reference image of the same individual. To overcome this limitation, this paper proposes a deep face dictionary network, termed DFDNet, which guides the restoration process of degraded images without requiring a reference image. The approach begins by generating deep dictionaries for key facial components, such as eyes, nose, and mouth, using K-means clustering on high-quality images. The degraded input is then matched with the most similar features from these dictionaries, and high-quality details are transferred to the input via a dictionary feature transfer (DFT) block, which utilizes component AdaIN to mitigate style differences and a confidence score to adaptively fuse the dictionary features. Furthermore, a progressive multi-scale dictionary approach enables coarse-to-fine restoration. Experimental results demonstrate the effectiveness of the proposed method, achieving strong quantitative and qualitative performance, and notably, generating realistic results on real degraded images without requiring an identity-matched reference, as shown in Figure A, B, C (see References [citation] for details), and the source code and models are available at \\url.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0300", "problem_id": "03000001", "content": "We introduce techniques for training convolutional neural networks (CNNs) utilizing both binarized weights and activations, resulting in quantized models that are particularly suitable for mobile devices with limited computational resources and power capacity. Earlier research on quantizing CNNs primarily aimed to approximate floating-point data through a set of discrete values, referred to as value approximation, usually relying on the same architecture as full-precision networks. In contrast, we adopt an innovative approach of \"structure approximation\" in quantization, suggesting that architectures specifically designed for low-bit networks may yield better performance. Our method involves a \"network decomposition\" strategy, called Group-Net, wherein the network is divided into groups, allowing each full-precision group to be effectively reconstructed by combining a collection of uniform binary branches. We also establish effective connections between groups to enhance representation capabilities. Furthermore, Group-Net demonstrates excellent adaptability to various tasks, such as improving semantic segmentation by incorporating rich contextual information into its binary structure. Additionally, we are pioneers in applying binary neural networks to object detection. Experimental results across classification, semantic segmentation, and object detection tasks indicate that our proposed methods surpass existing quantized networks in the literature, achieving superior accuracy and computational efficiency compared to the previous best binary neural networks.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0301", "problem_id": "03010001", "content": "The feature maps in convolutional networks are vulnerable to spatial bias, which arises from a combination of architectural design decisions, resulting in systematic elevation or weakening of activation at specific locations. A primary contributor to this bias is the padding mechanism, whose uneven application, influenced by various aspects of convolution arithmetic, can lead to asymmetrical learned weights. This bias can have detrimental effects on tasks such as detecting small objects, where activation suppression occurs when the stimulus is located in the affected area, resulting in blind spots and incorrect detections, as shown in Figure A (Reference [1]). To address this issue, we propose mitigation strategies and demonstrate their potential to enhance model accuracy, as evident in Figure B (Reference [2]), and further discussed in [3].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0302", "problem_id": "03020001", "content": "The significant advancements achieved by deep learning methods can be attributed to their capacity for automatic learning of task-specific feature representations. However, in domains where large-scale training data is limited, such as medical imaging, transfer learning has proven to be a highly effective approach. This paper presents a systematic examination of the transfer process, wherein a Convolutional Neural Network, initially trained on ImageNet images for image classification, is adapted for kidney detection in ultrasound images. The investigation focuses on the relationship between the detection performance and the degree of transfer, demonstrating that a fine-tuned transferred CNN can surpass a state-of-the-art feature-engineered pipeline, with a hybrid approach yielding a 20% performance improvement. Furthermore, the study analyzes the evolution of intermediate response images generated by the network and compares them to state-of-the-art image processing filters, providing valuable insights into the efficacy of transfer learning in handling diverse imaging regimes.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0303", "problem_id": "03030001", "content": "The critical role of parameter selection in supervised learning is widely acknowledged, yet the vast number of possible combinations often leads to inadequate or incomplete procedures. This oversight can result in misleading conclusions. In this opinion piece, we illustrate through a compelling example that the consequences are more severe than commonly understood. In the context of multi-label classification for medical code prediction, a prominent study performed thorough parameter tuning on a full dataset but reused the same parameters for a subset of frequent labels without additional optimization. This subset later became a standard benchmark, with subsequent research claiming advancements. However, our analysis reveals that many of these later results would not outperform the original study’s approach if proper parameter tuning had been applied. Consequently, the true extent of progress in these developments remains questionable. This case underscores that neglecting rigorous parameter selection can render research advancements in the field uncertain or even illusory.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0304", "problem_id": "03040001", "content": "Adaptive learning rate techniques have proven effective across various domains, particularly in deep neural network training. However, recent studies indicate that adaptive approaches employing exponentially increasing weights on historical squared gradients (e.g., ADAM, RMSPROP) might not converge to optimal solutions. While solutions like AMSGRAD and ADAMNC address convergence problems, matching or surpassing ADAGRAD's data-dependent regret bound remains difficult. This work introduces weighted adaptive algorithm (WADA), a new method that employs a gentler linear weighting strategy for past squared gradients instead of exponential growth. We present the weighted adaptive gradient method framework (WAGMF) and develop WADA within this structure. Theoretical analysis shows WADA achieves a weighted data-dependent regret bound that can outperform ADAGRAD's original bound when gradients decline quickly, potentially explaining ADAM's practical success. Experimental results confirm WADA and its variants outperform multiple ADAM versions in both convex optimization and deep network training scenarios.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0305", "problem_id": "03050001", "content": "Substantial progress has been made in unsupervised exemplar-based image-to-image translation, which translates images based on a given exemplar without paired data. Current methods typically employ normalization techniques, such as adaptive instance normalization, to transfer information from the exemplar to the input image by adjusting the channel-wise statistics, including mean and variance, of the input activation map. In contrast, style transfer approaches, which share similarities with image translation, have achieved superior performance by leveraging higher-order statistics, including covariance among channels, to represent styles. This is achieved through a process of whitening, which transforms the covariance matrix of a zero-mean input feature into an identity matrix, followed by coloring, which modifies the covariance matrix to match that of the style feature. However, applying this approach to image translation is computationally expensive and prone to errors due to its high time complexity and complex backpropagation. To address this, we propose an end-to-end approach for image translation that efficiently approximates this transformation using novel regularization methods, and further extend it to a group-wise form to improve memory and time efficiency, as well as image quality, as demonstrated through extensive qualitative and quantitative experiments, and our code is available at https://github.com/WonwoongCho/GDWCT.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0306", "problem_id": "03060001", "content": "With the proliferation of cameras in everyday life, document images have become increasingly common. Distinct from natural images that depict physical objects, document images are characterized by substantial textual content with critical semantic information and intricate layouts. This paper introduces a general unsupervised method for learning multimodal relationships between textual entities within a document image, taking into account their visual style, textual content, and geometric arrangement. These learned relationships are then used to automatically group the textual entities into semantically distinct clusters. At the heart of our approach is a deep optimization framework designed to analyze a user-provided image, identify, and exploit reliable pairwise connections within the multimodal representation of textual elements to accurately learn the relationships. The results show that our technique is effective across a diverse range of document images and demonstrate its utility for various editing tasks involving content, appearance, and geometry manipulation.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0307", "problem_id": "03070001", "content": "Fine-grained object recognition, which often demands specialized knowledge for accurate labeling, has led to increased interest in learning from readily available web images. Nevertheless, training robust fine-grained recognition models using web data is challenging due to label noise and the presence of difficult instances. To address these challenges, this paper introduces a new method designed to filter out irrelevant samples from real-world web images during the training process, while simultaneously leveraging informative hard examples to refine the network's learning. This strategy mitigates the adverse effects of noisy and challenging web images, ultimately improving performance. Empirical evaluations conducted on three standard fine-grained datasets demonstrate the significant superiority of our proposed approach compared to existing state-of-the-art web-supervised techniques.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0308", "problem_id": "03080001", "content": "Policy gradient serves as an effective method for policy optimization in reinforcement learning, though standard online implementations are limited to on-policy approaches and cannot utilize off-policy data. This work introduces a novel approach that integrates policy gradient with off-policy Q-learning by leveraging experience from a replay buffer, inspired by the relationship between the fixed points of regularized policy gradient and Q-values. This linkage enables Q-value estimation from policy action preferences, which are then refined using Q-learning updates, leading to the proposed method termed 'PGQL' (policy gradient and Q-learning). Additionally, we demonstrate the equivalence between action-value fitting methods and actor-critic algorithms, revealing that regularized policy gradient methods can be viewed as advantage function learning techniques. Empirical results highlight PGQL's enhanced data efficiency and stability, with evaluations on the complete Atari game suite showing superior performance compared to both asynchronous advantage actor-critic (A3C) and Q-learning.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0309", "problem_id": "03090001", "content": "Automating the colorization of line art is both costly and difficult; therefore, this paper introduces Tag2Pix, a GAN-based method for line art colorization. Tag2Pix takes grayscale line art and color tag data as input to generate a high-quality colored image. Initially, the Tag2Pix line art colorization dataset is introduced. A generator network is proposed that uses convolutional layers to transform the input line art, along with a pre-trained semantic extraction network and an encoder for the input color data. The discriminator uses an auxiliary classifier GAN to assess the authenticity and correctness of the tag data. Furthermore, a novel network structure named SECat is introduced to enhance the generator's ability to accurately colorize even small details, such as eyes. A novel two-step training strategy is also suggested, allowing the generator and discriminator to initially understand object and shape concepts, and then apply this knowledge to learn colorization techniques, including color placement strategies. The effectiveness of the proposed method is demonstrated through quantitative and qualitative evaluations.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0310", "problem_id": "03100001", "content": "Estimating stereo depth at night remains a difficult task because the assumptions valid under daylight conditions no longer apply. Nighttime scenes present challenges beyond low illumination and high noise, including issues like glow, glare, flares, and uneven light distribution. While supervised training on nighttime stereo images is a potential solution, acquiring accurate, dense disparity ground-truths unaffected by glare and covering extensive depth ranges is highly impractical. To overcome this, we propose a network that integrates day-to-night image translation with stereo depth estimation. Our approach eliminates the need for nighttime disparity labels or paired day-night images during training. Instead, we employ a translation network to generate realistic nighttime stereo images from daytime stereo inputs. The stereo network is then trained on these synthetic nighttime images using disparity supervision from the original daytime data, while simultaneously refining the translation network. To mitigate artifacts such as unrealistic depth predictions caused by unsupervised translation—particularly in regions affected by light effects (e.g., glow) or lacking visual information (e.g., low-light areas)—we incorporate structure-preservation and weighted-smoothness constraints. Experimental results demonstrate that our method surpasses baseline approaches in nighttime depth estimation.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0311", "problem_id": "03110001", "content": "Many scientific tasks involve inferring obscured system parameters from observed data. Typically, the transformation from parameters to measurements is a clearly defined function, but the reverse is not, with a single measurement potentially corresponding to various parameter sets. Consequently, the posterior parameter distribution, given a specific measurement, needs to be established. We propose that Invertible Neural Networks (INNs) are particularly effective for this purpose, despite their limited exploration in existing research. Unlike conventional neural networks that directly tackle the ambiguous inverse problem, INNs can learn the forward process in conjunction with the inverse problem by employing additional latent output variables to preserve lost information. With a given measurement and sampled latent variables, the INN's inverse pass generates a comprehensive distribution across the parameter space. Through experiments involving synthetic data and real-world applications in astrophysics and medicine, we demonstrate that INNs are a robust analytical tool for identifying multi-modalities, revealing parameter correlations, and pinpointing unrecoverable parameters.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0312", "problem_id": "03120001", "content": "The automatic creation of superior 3D shapes remains a complex research challenge. Although various data-driven techniques employing neural networks have been developed for 3D shape generation, their quality does not yet match that achieved by deep learning synthesis methods for images. This paper introduces a convolutional point cloud decoder/generator that leverages recent progress in image synthesis, specifically utilizing Adaptive Instance Normalization and providing a rationale for its training benefits. Moreover, we introduce enhancements to the conventional Chamfer distance minimization for auto-encoding point clouds. We also demonstrate the importance of strategic sampling in both the input geometry and point cloud generation to enhance outcomes. The method's performance is assessed through qualitative and quantitative analyses within an auto-encoding framework. The proposed decoder's efficacy is confirmed via a comprehensive ablation study, demonstrating its ability to surpass current state-of-the-art results across multiple experiments. We illustrate the applicability of our approach in point cloud upsampling, single view reconstruction, and shape synthesis.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0313", "problem_id": "03130001", "content": "The multi-head attention mechanism can learn diverse representations from sequential data by focusing on different subsequences, like word-pieces or syllables in spoken language. By extracting more detailed information compared to single-head attention, which condenses the entire sequence into a single context vector, it has advantages. Nevertheless, simply applying multi-head attention does not ensure this advantage due to potential positional and representational redundancies among attention heads. This paper introduces a regularization method for the multi-head attention mechanism within an end-to-end neural keyword spotting system. By adding regularization terms that penalize positional and contextual non-orthogonality among attention heads, we promote distinct representations from different subsequences. This approach allows for the utilization of structured information without relying on explicit sequence models like hidden Markov models. Additionally, regularization of intra-head contextual non-orthogonality fosters similar representations across keyword examples for each attention head, which aids in classification by minimizing feature variability. Experimental results reveal that the proposed regularization method significantly enhances keyword spotting performance for the keyword \"Hey Snapdragon.\"", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0314", "problem_id": "03140001", "content": "The growing interest among researchers, producers, and consumers in sheep identification has been fueled by a significant rise in population and the need to enhance productivity. With the global population projected to surpass 9.6 billion by 2050, there is an increasing recognition of the importance of efficient livestock production. Sheep are regarded as a primary food source. Current research focuses on creating real-time applications that improve sheep identification for breed management and the acquisition of pertinent information such as weight and age, which are essential metrics for evaluating production efficiency. Recently, visual analysis has demonstrated notable advantages over other methods. These visual analysis techniques require a sufficient number of images for effective testing and study completion, making the establishment of a sheep image database a critical endeavor to achieve this aim. In this paper, we present datasets designed for evaluating and comparing ongoing algorithm development. Our compiled dataset includes 416 color images showcasing different characteristics of sheep in various postures. Images were gathered from fifty-two sheep aged between three months and six years. For each sheep, we took two images from each side of the body, two images of each side of the face, one top view image, one image of the hip, and one image of the teeth. The images encompass various lighting conditions, quality levels, and angles of rotation. This dataset can be utilized to test algorithms for sheep identification, weight estimation, and age detection, which are vital for disease management, animal evaluation, and ownership tracking.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0315", "problem_id": "03150001", "content": "Significant advancements have been achieved in estimating the pose of rigid objects from single RGB images, yet overcoming the issue of partial occlusions remains a daunting task. Refining pose estimates through rendering has demonstrated potential, particularly in scenarios where data is limited, and this approach is the focus of our research. We present a novel pose refinement technique that builds upon a simplified learning process, wherein a convolutional neural network (CNN) is trained to predict the reprojection error between a real-world image and a rendered image. Our experiments involve training the model using both entirely synthetic data and a combination of synthetic and real-world data, resulting in superior performance to current state-of-the-art methods on two out of three metrics in the Occlusion LINEMOD benchmark, while matching the performance on the third metric, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0316", "problem_id": "03160001", "content": "The increasing complexity of IT systems, driven by their rapid growth and widespread distribution, poses significant challenges for operation and maintenance, which can be mitigated by leveraging monitoring solutions that continually collect and analyze key performance indicators (KPIs), such as CPU utilization and allocated memory, to provide insights into system performance. The accumulation of these metrics over time creates an opportunity for predicting future KPI trends based on historical data, yet forecasting IT system KPIs is particularly difficult due to the diversity of KPI types, such as CPU utilization and allocated memory, which are hard to capture with a single model, and the dynamic nature of system components, which undergo frequent changes due to updates and modernization, necessitating regular model retraining or fine-tuning. To address these challenges, we propose a lightweight KPI prediction solution based on historical observations, utilizing a weighted heterogeneous ensemble method that combines a neural network and a mean predictor, with weights determined by a heuristic approach, and evaluate its effectiveness using the FedCSIS 2020 challenge dataset, achieving R^2 scores of 0.10 and 0.15 on the preliminary 10% test data and complete test data, respectively, with our code available at https://github.com/citlab/fed_challenge.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0317", "problem_id": "03170001", "content": "Estimating the articulated 3D pose of hands and objects from a single RGB image presents a considerable challenge due to its inherent ambiguity and the need for extensive datasets that encompass a variety of hand and object poses as well as different camera perspectives. However, many existing real-world datasets fall short in this diversity. On the other hand, synthetic datasets can easily provide a broader range of variations, although learning from them tends to be inefficient, requiring substantial resources for training. To tackle these challenges, we introduce ArtiBoost, a streamlined online data augmentation technique designed to enhance articulated hand-object pose estimation from a data-centric viewpoint. ArtiBoost is utilized in conjunction with a real-world source dataset and performs alternating data exploration and synthesis during the training phase. It can effectively address various hand-object poses and camera angles by leveraging a Compositional hand-object Configuration and Viewpoint space (CCV-space) and can adaptively enhance the samples that are difficult to differentiate through a mining strategy. We implement ArtiBoost on a basic learning framework and demonstrate notable performance improvements across several hand-object benchmarks. For instance, with ArtiBoost, even a rudimentary baseline network surpasses the previous state-of-the-art model based on Transformer for the HO3D dataset. Our code is accessible at https://github.com/MVIG-SJTU/ArtiBoost.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0318", "problem_id": "03180001", "content": "Typically, 3D object detectors that utilize LiDAR require a substantial amount of point cloud data with detailed labels for training purposes, but obtaining these labels can be costly. This paper presents a novel approach to 3D detection that eliminates the need for manual labeling by leveraging the CARLA simulator to generate a large quantity of self-labeled training samples. A new Domain Adaptive VoxelNet (DA-VoxelNet) model is introduced, which can effectively bridge the gap between synthetic data and real-world scenarios. The self-labeled samples are generated using high-quality 3D models embedded in the CARLA simulator, combined with a LiDAR-guided sampling algorithm. The proposed DA-VoxelNet incorporates both sample-level and anchor-level domain adaptation modules, enabling the detector to adapt seamlessly to real-world data after being trained on synthetic data. The experimental results on the KITTI evaluation set demonstrate that the proposed unsupervised domain adaptation 3D detector achieves 76.66% and 56.64% mean average precision (mAP) in bird's eye view (BEV) mode and 3D mode, respectively, as shown in Figure A, B, C (References [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0319", "problem_id": "03190001", "content": "A local feature detector ideally exhibits scale-invariance, precise localization, and resilience to noise and distortions. Current detectors often identify numerous unstable feature points, leading to increased computational demands and a greater number of keypoints for matching. This paper demonstrates the existence of robust and accurate keypoints within a defined scale-space. We present a mathematical model of the superimposition problem and derive a closed-form solution for multiscale analysis. This model, based on difference-of-Gaussian (DoG) kernels in continuous scale-space, reveals that setting the scale-space pyramid's blurring ratio and smoothness to 2 and 0.627, respectively, enhances the detection of reliable keypoints. For discrete image application, the model is discretized using the undecimated wavelet transform and the cubic spline function. The proposed method's complexity is theoretically less than 5\\% of that of Scale Invariant Feature Transform (SIFT). Experimental evaluations demonstrate that the proposed feature detector outperforms existing hand-crafted and learning-based methods in terms of accuracy and computational efficiency. The code and supplementary materials can be found at~}.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0320", "problem_id": "03200001", "content": "Infants born extremely prematurely frequently need endotracheal intubation and mechanical ventilation in their initial days. Since extended invasive mechanical ventilation (IMV) can have harmful consequences, medical professionals strive to extubate these infants as early as deemed appropriate. However, current methods for assessing extubation readiness lack consistency among clinicians and hospitals, resulting in elevated reintubation rates. Our study introduces a method utilizing Random Forest classifiers to evaluate cardiorespiratory variability for predicting extubation readiness. To mitigate data imbalance, we apply random undersampling of the majority class instances prior to training each Decision Tree within an ensemble. By integrating clinical expertise, we show that our classifier could have detected 71% of infants who experienced extubation failure while achieving a 78% success detection rate.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0321", "problem_id": "03210001", "content": "Unsupervised representation learning has achieved remarkable success across various applications. It serves as a particularly effective method for learning representations of environments characterized by partial or noisy observations. In domains where observations are incomplete, it is crucial for the representation to encapsulate a belief state, which acts as a sufficient statistic reflecting the observations obtained thus far. This paper explores the feasibility of learning such belief representations utilizing contemporary neural architectures. We specifically examine one-step frame prediction and two variations of contrastive predictive coding (CPC) as the objectives for learning the representations. To assess the efficacy of these learned representations, we evaluate their capability to predict different aspects of the underlying environmental state, such as the agent's location within a 3D maze. Our findings indicate that all three approaches effectively learn belief representations of the environment, encapsulating not only the state information but also its associated uncertainty, which is a vital component of belief states. Furthermore, we discover that multi-step predictions and action-conditioning in CPC are essential for achieving precise belief representations in visually intricate environments. The capacity of neural representations to capture belief information holds the potential to facilitate new advancements in learning and planning within partially observable domains, where embracing uncertainty is crucial for optimal decision-making.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0322", "problem_id": "03220001", "content": "The interpretation of remote sensing images can be hindered when images from different sensors are unclear, necessitating a series of pre-processing and corrective steps to enhance visual perception, followed by main processing steps for more accurate analysis. Various processing approaches exist, depending on the type of remote sensing image, and this article focuses on image fusion, specifically using the natural colors of an optical image to add color to a grayscale satellite image, such as the HR image of the OLI sensor of Landsat-8, for improved observation. While the fusion technique has been previously explored, this study applies the concept of interpolation, noting that popular remote sensing image processing tools like ENVI and ERDAS primarily employ classical interpolation techniques, such as bi-linear (BL) and bi-cubic/cubic convolution (CC), and thus, research in image fusion often overlooks the potential of advanced interpolators. This approach concentrates on the impact of interpolation on fusion quality in Landsat-8 multispectral images, utilizing a statistical, adaptive, and edge-guided interpolation method to enhance color quality, and numerical simulations demonstrate that selecting suitable interpolation techniques in MRF-based images yields better quality than classical interpolators, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0323", "problem_id": "03230001", "content": "This paper introduces a new point set registration method utilizing a single-step adversarial learning framework. Drawing inspiration from the advancements in generative adversarial networks, the algorithm conceptualizes point clouds as three-dimensional probability distributions. A single-step adversarial optimization process is then employed, where a critic neural network is trained to differentiate between source and target point sets. Simultaneously, the transformation parameters are learned to mislead the critic, causing it to misidentify the point sets. Unlike many current point set registration algorithms, our approach does not require establishing any correspondences between the point clouds. The algorithm's effectiveness is demonstrated through evaluations on multiple demanding benchmarks, and its performance is compared against existing baselines.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0324", "problem_id": "03240001", "content": "Attention-based scene text recognition models, which utilize a condensed intermediate representation to acquire one- or two-dimensional attention through a recurrent neural network (RNN) encoder-decoder architecture, have achieved considerable success. Nevertheless, these approaches encounter an attention-drift issue because significant similarity among encoded features results in attention ambiguity under the RNN-based local attention mechanism. Furthermore, RNN-based techniques exhibit limited efficiency due to inadequate parallelization. To address these challenges, we introduce MASTER, a self-attention-based scene text recognizer that (1) encodes input-output attention and learns self-attention, capturing feature-feature and target-target relationships within the encoder and decoder; (2) develops a more potent and resilient intermediate representation to spatial distortion; and (3) achieves high training efficiency because of extensive training parallelization, along with rapid inference enabled by an efficient memory-cache mechanism. Comprehensive evaluations across diverse benchmarks confirm MASTER's superior performance on both regular and irregular scene text. Pytorch code can be found at https://github.com/wenwenyu/MASTER-pytorch, and Tensorflow code can be found at https://github.com/jiangxiluning/MASTER-TF.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0325", "problem_id": "03250001", "content": "Visual place recognition in dynamic environments involves identifying correspondences between query and reference observation sets despite significant visual variations. While CNN-based image descriptors have shown considerable success in recent studies, current evaluations often presume uniform environmental conditions within each set (e.g., reference: daylight, query: nighttime). Our research reveals that when conditions vary within a single set (e.g., reference: day, query: daytime-dusk-night-dawn sequence), locations under identical conditions may appear more alike than the same location under differing conditions, causing state-of-the-art CNN-based methods to underperform. This study examines the critical challenge of in-sequence environmental variations, categorizing them into three progressive scenarios: (1) no intra-sequence changes, (2) discrete intra-sequence variations, and (3) continuous intra-sequence fluctuations. We assess the impact of these variations on two leading CNN descriptors, highlighting both the value and constraints of descriptor standardization, particularly with continuous changes. To tackle this practical challenge, we explore unsupervised learning techniques, evaluating two existing PCA-based methods and introducing an enhanced clustering-based normalization approach for statistical adjustment.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0326", "problem_id": "03260001", "content": "Direct load management of a diverse group of residential demand flexibility sources presents a complex control challenge characterized by partial observability. This study introduces an innovative method that leverages a convolutional neural network to uncover latent state-time characteristics, effectively addressing the issues arising from partial observability. Specifically, the convolutional neural network serves as a function approximator for estimating the state-action value function, known as the Q-function, during the supervised learning phase of fitted Q-iteration. The proposed method is tested through a qualitative simulation involving a collection of thermostatically controlled devices, which share their air temperature while their envelope temperature remains concealed. The results of the simulation indicate that the proposed method successfully identifies the hidden features and significantly lowers the electricity costs for the cluster.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0327", "problem_id": "03270001", "content": "Different fonts evoke distinct impressions, which are typically conveyed through words. This study introduces Impressions2Font (Imp2Font), a system designed to create font images that embody specific impressions. Imp2Font is an enhanced version of conditional generative adversarial networks (GANs) and is capable of taking any number of impression words as input to produce font images. An impression embedding module, based on a word embedding technique, transforms these impression words into a soft-constraint vector. Both qualitative and quantitative assessments demonstrate that Imp2Font produces font images of superior quality compared to alternative approaches, utilizing multiple impression words as well as unfamiliar words.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0328", "problem_id": "03280001", "content": "Belief Propagation is commonly employed for marginal inference; however, it tends to perform slowly when faced with large-domain variables and high-order factors. Although prior research has offered beneficial approximations to enhance inference in such scenarios, these approaches often lack crucial anytime features, including: 1) delivering accurate and consistent marginals when terminated early, 2) enhancing the approximation with prolonged execution, and 3) achieving convergence to the fixed point of BP. To address these issues, we present a message passing algorithm designed for sparse (partially instantiated) domains, which converges to consistent marginals by utilizing dynamic message scheduling. This algorithm incrementally expands the sparse domains by choosing the next value to include based on prioritization schemes derived from the gradients of the marginal inference objective. Our experimental results reveal local anytime consistency and rapid convergence, demonstrating substantial speed improvements over BP for producing low-error marginals, achieving speedups of up to 25 times on grid models and up to 6 times on a real-world natural language processing task.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0329", "problem_id": "03290001", "content": "Recent advancements in deep generative inpainting techniques leverage attention layers, enabling the generator to explicitly utilize feature patches from known areas to fill in missing regions. However, the absence of supervisory signals to establish correspondence between the missing and known areas can result in inadequately identified reference features, often culminating in visual artifacts. Additionally, the approach computes pair-wise similarity across the entirety of the feature map during inference, leading to considerable computational burdens. To mitigate these challenges, we introduce a method to train an attention-free generator to adopt patch-borrowing characteristics through joint training of an auxiliary contextual reconstruction task. This task promotes the generation of plausible outputs even when they are reconstructed from surrounding regions. The auxiliary component acts as a learnable loss function, referred to as contextual reconstruction (CR) loss, which simultaneously optimizes query-reference feature similarity and a reference-based reconstructor alongside the inpainting generator. The CR loss is only utilized during the training phase, with the inpainting generator being the sole requirement during inference. Experimental findings indicate that our proposed inpainting model demonstrates superior performance relative to existing state-of-the-art methods, both quantitatively and visually.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0330", "problem_id": "03300001", "content": "The growing availability of Electronic Health Records (EHR) offers significant potential for data scientists to advance healthcare research through the application of sophisticated analytical techniques to clinical data. A crucial prerequisite is the extraction of valuable insights from clinical data characterized by high dimensionality, sparsity, and complexity. Common data science strategies tackle this issue by employing feature learning to construct more dependable and informative feature representations, subsequently using these representations for supervised learning. This paper introduces a predictive modeling technique that leverages deep learning-based feature representations and word embedding methodologies. Our method uses different deep architectures (stacked sparse autoencoders, deep belief network, adversarial autoencoders and variational autoencoders) to represent features at higher levels of abstraction, thereby deriving effective and robust features from EHRs, which are then used to develop prediction models. Our approach proves especially beneficial when there is a surplus of unlabeled data but a limited amount of labeled data. We assess the efficacy of representation learning via a supervised learning framework. The primary objective is to present a comparative analysis that evaluates the performance of various deep architectures within a supervised learning context, offering insights into the selection of appropriate deep feature representation techniques. Empirical results indicate that for smaller datasets, stacked sparse autoencoder exhibits superior generalization performance in prediction due to sparsity regularization, while variational autoencoders surpass other methods for larger datasets because of their ability to learn the representation distribution.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0331", "problem_id": "03310001", "content": "The increasing availability of depth sensors has led to a growing interest in utilizing dynamic human body skeletons as a robust means of recognizing actions, owing to their expressive potential. However, existing approaches that rely on RNN or CNN to model skeletons are limited in their capacity to handle irregular joint structures. To address this challenge, graph convolutional networks (GCN) have been proposed for handling irregular graph-structured data, but constructing the underlying graph remains a significant hurdle. This paper introduces a novel approach, representing skeletons on graphs and proposing a graph regression-based GCN (GR-GCN) for skeleton-based action recognition, with the goal of capturing spatio-temporal variations in the data. By statistically learning the underlying graph from multiple observations through graph regression, we establish a sparse and efficient graph representation that connects each joint to its neighboring joints within the same frame, as well as relevant joints in preceding and subsequent frames. The optimized graph is then utilized in conjunction with the GCN and skeleton sequence coordinates for feature learning, leveraging high-order and fast Chebyshev approximation of spectral graph convolution. Our analysis demonstrates the effectiveness of the Chebyshev approximation in characterizing variation, and experimental results on the NTU RGB+D, UT-Kinect, and SYSU 3D datasets validate the proposed graph regression and show that the GR-GCN achieves state-of-the-art performance, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0332", "problem_id": "03320001", "content": "This paper addresses the challenge of inferring undirected graphical models from data produced by Glauber dynamics, a Markov chain that iteratively updates individual nodes in the model and is commonly employed to sample from its stationary distribution. Unlike conventional approaches that rely on independent and identically distributed samples, our study explores learning under this dynamical framework, which arises naturally in diverse applications. While prior work has emphasized computationally efficient algorithms for graphical model learning, we demonstrate that reconstructing binary pairwise models from Glauber dynamics observations is computationally feasible. Specifically, we prove that a binary pairwise model with p nodes and maximum degree d can be reconstructed in time f(d)p^2\\log p, where f(d) is a function of d, while requiring nearly the minimal number of samples from an information-theoretic perspective.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0333", "problem_id": "03330001", "content": "This study presents an innovative method for predicting vehicle trajectories by leveraging graphical representations, where vehicles are modeled as Gaussian distributions within a Bird Eye View framework. A U-net model is utilized to generate sequence-to-sequence predictions, with the entire framework being trained on the HighD dataset, which comprises aerial imagery of vehicles detected on highways. By framing the problem as an image-to-image regression task, the deep learning-based approach enables the network to learn the inherent relationships between traffic participants, ultimately yielding an estimated future representation of the input scene. To obtain precise positional information, an additional step is performed to extract positions from the predicted representation at subpixel resolution. The methodology has been validated through various network configurations, demonstrating prediction errors of up to three seconds ahead that are comparable to the resolution of the representation. Furthermore, the model has been successfully tested in complex highway scenarios involving over 30 vehicles across two opposing traffic flows, producing favorable qualitative and quantitative results.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0334", "problem_id": "03340001", "content": "This paper introduces a semi-automatic pipeline for locally registering 2D images with 3D models, enabling the colorization of 3D scans using uncalibrated images. By leveraging the Structure from Motion (SfM) technique, a sparse 3D representation of the object is reconstructed and camera parameters are extracted from image feature matches. The reconstructed 3D model is then roughly aligned with the scanned model using the Scale Iterative Closest Point (SICP) algorithm, which determines global scale, rotation, and translation parameters with minimal user input. A subsequent local refinement stage optimizes the projection of aligned photos onto the 3D object, eliminating blurring and ghosting artifacts caused by minor registration inaccuracies. The proposed pipeline demonstrates versatility in handling diverse real-world scenarios, from objects with minimal geometric features to complex ones, as shown in Figure A, B, C (References [1], [2], [3]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0335", "problem_id": "03350001", "content": "Inverse reinforcement learning (IRL) allows an agent to acquire sophisticated behaviors by observing expert demonstrations from a (near-)optimal policy. While conventional approaches assume the learner aims to replicate the teacher's behavior, this work explores scenarios where the learner incorporates its own preferences. These preferences may reflect behavioral biases, differing perspectives, or physical limitations. We examine two instructional methods: learner-agnostic teaching, where the teacher provides optimal demonstrations without considering the learner's preferences, and learner-aware teaching, where the teacher adapts to these preferences. Our developed learner-aware algorithms demonstrate substantial performance gains compared to learner-agnostic approaches.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0336", "problem_id": "03360001", "content": "This paper presents a novel approach to detect text within natural images. The proposed method involves two key components: a rapid and adaptable system for creating synthetic images of text embedded in complex scenes by realistically superimposing artificial text onto background images, considering the local 3D scene geometry; and a Fully-Convolutional Regression Network (FCRN) trained using these synthetic images to efficiently identify text and predict bounding boxes at various locations and scales within an image. The FCRN is compared to the recently-introduced YOLO detector and other deep learning-based end-to-end object detection systems. The resulting detection network demonstrates superior performance compared to existing text detection methods in natural images, achieving an F-measure of 84.2% on the standard ICDAR 2013 benchmark, while processing 15 images per second on a GPU.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0337", "problem_id": "03370001", "content": "Facial muscle movements, represented by Action Units (AUs), induce specific appearance changes at distinct facial locations due to their geometric nature. This insight motivates the introduction of a novel problem in AU modelling, which involves the concurrent estimation of their localisation and intensity. To address this, a straightforward yet effective approach is proposed, leveraging Heatmap Regression to consolidate the two problems into a single task. The heatmap serves as an indicator of whether an AU is present at a particular spatial location, while variable-sized heatmaps are introduced to accommodate the modelling of AU intensity, with their amplitude and size adjusted according to the labelled intensity. By utilising Heatmap Regression, the advancements in facial landmark localisation can be leveraged. Furthermore, a transfer learning approach is devised, exploiting the knowledge gained from a network trained on large-scale facial landmark datasets, with alternatives including fine-tuning, adaptation layers, attention maps, and reparametrisation. This approach enables the inheritance of rich facial features from a robust face alignment network with minimal additional computational cost, and its effectiveness is empirically validated through the establishment of a new state-of-the-art on the BP4D, DISFA, and FERA2017 datasets.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0338", "problem_id": "03380001", "content": "Deep Bregman divergence, which leverages neural networks, provides a non-Euclidean measure for assessing data point divergence, enabling the capture of divergence across distributions. This paper introduces deep Bregman divergences to enhance visual representation learning within a contrastive learning framework. The goal is to improve contrastive loss in self-supervised learning by training supplementary networks grounded in functional Bregman divergence. Unlike traditional contrastive learning approaches that rely on single-point divergences, the proposed framework accounts for divergence between distributions, thus improving the learned representation's quality. Integrating the conventional contrastive loss with the proposed divergence loss allows the method to surpass baseline approaches and most prior self-supervised and semi-supervised learning techniques across various classification and object detection tasks and datasets. The source code and experimental details are available in the supplementary materials.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0339", "problem_id": "03390001", "content": "The intricacies of the visual world pose substantial obstacles to achieving a thorough comprehension of visual data, and despite notable advancements in visual recognition, current vision systems remain inadequate in handling complex visual queries that demand more in-depth reasoning. To address this limitation, we present a knowledge base framework designed to accommodate a wide range of visual queries without requiring the training of new classifiers for each novel task. However, constructing a large-scale multimodal knowledge base that integrates visual, textual, and structured data, along with their varied relationships, raises significant scalability concerns. By representing a large-scale Markov Random Field as a knowledge base, our approach incorporates diverse data types and relations, and we develop a scalable system for knowledge base construction that can efficiently build a knowledge base with hundreds of millions of variables and millions of parameters within a few hours, yielding competitive performance on standard recognition and retrieval tasks, as seen in Figure A, and demonstrating greater versatility in responding to more complex visual queries, as shown in Figure B, and referenced in [1].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0340", "problem_id": "03400001", "content": "In this study, we present a framework for employing Reinforcement Learning (RL) control within a radar system operating in a crowded spectral environment. We subsequently analyze the effectiveness of various RL algorithms through experiments conducted on Commercial off-the-shelf (COTS) hardware. Each RL method is assessed based on convergence, radar detection capability in crowded spectral conditions, and its capacity to share 100MHz of spectrum with an uncooperative communication system. Our analysis includes policy iteration, which addresses the environment modeled as a Markov Decision Process (MDP) by directly determining a stochastic mapping between environmental states and radar waveforms, alongside Deep RL methods that leverage a form of Q-Learning to approximate a parameterized function aiding the radar in choosing optimal actions. We demonstrate that RL methods outperforms a Sense-and-Avoid (SAA) strategy and discuss the scenarios in which each method proves to be most advantageous.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0341", "problem_id": "03410001", "content": "The representation of information in the brain, such as in the neocortex and hippocampus, is increasingly understood to be in the form of sparse distributed codes (SDCs), which are a type of cell assembly. Two fundamental questions arise: how are these codes formed from single trials, and how is similarity maintained during the learning process, such that more similar inputs are mapped to more similar SDCs. A novel solution, known as Modular Sparse Distributed Code (MSDC), is presented, offering straightforward and neurally plausible answers to these questions. The MSDC coding field consists of Q competitive modules, each comprising K binary units, and its modular structure enables a single-trial, unsupervised learning algorithm that preserves similarity in approximately fixed time, regardless of the growing number of stored items. This algorithm allows for fixed-time best-match retrieval and belief update, where the probabilities of all stored items are updated, once items are stored as MSDCs in superposition, reflecting input similarity. The core principle of this algorithm involves introducing noise into the code selection process, proportional to the novelty of the input, resulting in the expected intersection of codes for inputs X and Y being proportional to their similarity, as demonstrated in the appendix for spatial patterns, Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0342", "problem_id": "03420001", "content": "Identifying causal links between observed variables is a crucial aspect of data analysis. Linear acyclic causal models are frequently employed for continuous data to represent the data-generating mechanism, and inferring these models has been extensively researched. Nevertheless, current methodologies have notable shortcomings. Techniques relying on conditional independencies (Spirtes et al. 1993; Pearl 2000) are unable to differentiate between models that are equivalent in terms of independence, while methods solely based on Independent Component Analysis (Shimizu et al. 2006) cannot be applied to data that is partly Gaussian. This paper introduces a generalization and combination of these two strategies, resulting in a method capable of learning model structures in numerous scenarios where existing methods yield inaccurate or suboptimal results. We present precise graphical criteria for determining when two different models represent the same distribution family and validate the efficacy of our method through comprehensive simulations.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0343", "problem_id": "03430001", "content": "This study examines how the structure of a discrete graphical model relates to the support of the inverse of a generalized covariance matrix. We demonstrate that, for specific graph configurations, the inverse covariance matrix's support for vertex indicator variables captures the graph's conditional independence relationships. Our findings generalize prior results limited to multivariate Gaussian graphical models, resolving a key question about the role of inverse covariance matrices in non-Gaussian distributions. The analysis integrates concepts from exponential family geometry, junction tree theory, and convex analysis. These population-level insights have implications for existing and new graph selection techniques, including an innovative approach for estimating structure with incomplete or corrupted data. We establish nonasymptotic performance bounds for these methods and validate their precision through simulation studies.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0344", "problem_id": "03440001", "content": "Current models face significant challenges when answering questions that involve interpreting text within images, primarily due to the frequent occurrence of rare, polysemous, or ambiguous words, such as place names, product labels, or sports team identifiers. Relying solely on pre-trained word embeddings is insufficient for this task. Instead, an effective model must leverage multimodal image information—such as visual cues indicating that prominent text on a bottle likely represents a brand—to enhance text comprehension. To address this, we introduce the Multi-Modal Graph Neural Network (MM-GNN), a novel VQA approach that constructs an image graph with three sub-graphs representing visual, semantic, and numeric modalities. Three specialized aggregators facilitate cross-modal message passing, refining node features by integrating contextual information from different modalities. These enhanced features improve performance in downstream question answering. Experiments demonstrate that MM-GNN significantly enhances scene text representation and boosts accuracy on VQA tasks requiring text interpretation.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0345", "problem_id": "03450001", "content": "Drawing from the observation that human visual tracking skills develop during early and middle childhood, we introduce tracking as a surrogate task for training visual representations in a computer vision system. Mimicking the children's game of Catch, we create a Catch-the-Patch (CtP) game where a 3D-CNN model learns visual representations beneficial for video-based tasks. In our pretraining approach, an image patch extracted from a video is scaled and moved along a predefined path, with the model tasked to predict its position and size across subsequent frames using only the initial bounding box. Incorporating multiple patches enhances performance, while randomly hiding patches increases task complexity. Evaluations on standard benchmarks show CtP outperforming other video pretraining techniques. Moreover, CtP-learned features exhibit greater resilience to domain shifts compared to those from supervised action recognition training. Notably, when pretrained on Kinetics-400, CtP achieves significantly higher action classification accuracy on the Something-Something dataset than its fully supervised counterpart. Code is available at: github.com/microsoft/CtP.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0346", "problem_id": "03460001", "content": "This study investigates the challenge of task-oriented downsampling of 3D point clouds, with the goal of reducing the point cloud density while preserving the efficacy of downstream applications on the resulting sparse points. From a matrix optimization perspective, we introduce MOPS-Net, a novel, interpretable deep learning approach that diverges from existing methods due to its inherent interpretability. The optimization problem posed is complex, owing to its discrete and combinatorial characteristics, which we address by relaxing the binary variable constraints and formulating a constrained, differentiable matrix optimization problem. A deep neural network is then designed to emulate this optimization process, capturing both local and global input data structures. MOPS-Net allows for end-to-end training with a task network, is permutation-invariant, and can be extended to handle any downsampling ratio after a single training session. Experimental results, as seen in Figure A, B, C, demonstrate that MOPS-Net outperforms state-of-the-art deep learning methods in various tasks, including classification, reconstruction, and registration, and its robustness is validated on noisy data, as discussed in References [citation].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0347", "problem_id": "03470001", "content": "The importance of situational awareness in connected and autonomous vehicles (CAV) has sparked a surge of research in recent years, as the safety of drivers is intimately tied to the robustness, reliability, and scalability of these systems. To enhance situational awareness, cooperative mechanisms leveraging high-speed wireless vehicular networks have been developed, effectively addressing issues such as occlusion and sensor range limitations. Nevertheless, the capacity of the network remains a critical factor in determining the volume of information that can be shared among cooperative entities. Our previous work introduced the concept of feature sharing, which seeks to strike a balance between computational and communication loads. This study proposes a novel mechanism that introduces flexibility in adapting to communication channel capacity, as well as a decentralized method for aligning shared data, with the aim of further enhancing cooperative object detection performance. The efficacy of the proposed framework is validated through experiments conducted on the Volony dataset, with the results demonstrating that it surpasses our previously proposed cooperative object detection method (FS-COD) in terms of average precision.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0348", "problem_id": "03480001", "content": "Recent advancements in deep learning have demonstrated that progressively broadening the complexity of learning tasks can effectively tackle more intricate challenges. Our work explores incremental learning within the framework of sequence learning by implementing generative RNNs through multi-layer recurrent Mixture Density Networks. While the benefits of incremental or curriculum learning in enhancing educational outcomes are acknowledged, applying these principles without discretion does not consistently yield benefits; thus, it is crucial to identify which types of incremental or curriculum learning foster positive results. This study addresses this need by examining three variations of incremental or curriculum learning. We present Incremental Sequence Learning, an uncomplicated incremental method for sequence learning, which begins by utilizing only the initial few steps of each sequence for training. With each achievement of a performance target, the extent of sequence parts used for training is gradually expanded. Additionally, we introduce a novel sequence learning task and dataset focused on predicting and classifying MNIST pen stroke sequences. Our findings indicate that Incremental Sequence Learning significantly accelerates the sequence learning process, achieving the optimal test performance level of standard sequence learning 20 times quicker, reducing test error by 74%, and exhibiting overall enhanced robustness; it demonstrates lower variance and maintains progress after the other three comparison methods cease to improve. The other variants of curriculum learning do not show any significant enhancements. Furthermore, a trained sequence prediction model is applied to transfer learning for the classification task, resulting in better classification performance compared to methods starting from scratch.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0349", "problem_id": "03490001", "content": "Generative Adversarial Networks (GANs) have become increasingly prominent due to their capability to model image distributions. By learning to replicate the training data, they facilitate sampling from that domain and applying the acquired knowledge in practical scenarios. Various approaches have been introduced to improve GANs, such as incorporating feature matching into the loss regularization. Our goal is to extend GANs beyond the training data and investigate unexplored regions of the image manifold. We introduce a novel K-NN selective feature matching regularizer for GANs, termed K-GAN, which aligns high-level features with a target set Y during adversarial training on the base set X. We demonstrate that minimizing this additional term corresponds to reducing cross-entropy between the GAN’s distribution and set Y. Additionally, we propose a cascaded GAN framework, named Imaginative Adversarial Network (IAN), designed to synthesize a new distribution merging sets X and Y through sequential sampling and translation GANs. We evaluate various IAN configurations objectively and subjectively, highlighting applications such as manifold traversal and creative face generation for character design in films and video games.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0350", "problem_id": "03500001", "content": "Deep convolutional neural networks have made significant strides in various medical image computing applications. A prevalent challenge in deploying supervised deep learning techniques on medical images is the scarcity of labeled data, which is often costly and labor-intensive to gather. In this study, we introduce an innovative semi-supervised approach for medical image segmentation, where the network is fine-tuned using a weighted mix of a standard supervised loss applied to labeled data and a regularization loss applicable to both labeled and unlabeled datasets. To effectively leverage unlabeled data, our method promotes consistent predictions from the network-in-training for identical inputs subjected to different regularizations. Focusing on the semi-supervised segmentation issue, we strengthen the impact of regularization on pixel-level predictions by incorporating a transformation that includes a consistent scheme for rotation and flipping within our self-ensembling framework. We have thoroughly tested the proposed semi-supervised approach on three established and challenging medical image segmentation tasks: (i) skin lesion segmentation from dermoscopy images using the International Skin Imaging Collaboration (ISIC) 2017 dataset, (ii) optic disc segmentation from fundus images with the Retinal Fundus Glaucoma Challenge (REFUGE) dataset, and (iii) liver segmentation from volumetric CT scans in the Liver Tumor Segmentation Challenge (LiTS) dataset. In comparison to leading methods, our proposed technique demonstrates superior segmentation accuracy on demanding 2D and 3D medical images, highlighting the efficacy of our semi-supervised approach for medical image segmentation.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0351", "problem_id": "03510001", "content": "We introduce an attention-based approach that integrates local image features into a subject-level representation aimed at predicting disease severity. Unlike traditional deep learning methods that necessitate a fixed-dimensional input, our technique works with a collection of image patches, allowing it to handle variable-length input images without the need for resizing. The model develops a clinically interpretable subject-level representation that accurately reflects the severity of the disease. It is composed of three interdependent modules: (1) a discriminative network that extracts a fixed-length representation from local features and correlates them with disease severity; (2) an attention mechanism that enhances interpretability by emphasizing the anatomical regions most relevant to the prediction task; and (3) a generative network that promotes diversity within the local latent features. The generative component guarantees that the attention weights remain meaningful while ensuring the local areas are pertinent to the disease severity. We train our model in an end-to-end manner within the framework of a large-scale lung CT investigation of Chronic Obstructive Pulmonary Disease (COPD). Our model achieves state-of-the-art results in predicting clinical severity measures for COPD, and the distribution of attention illustrates the regional significance of lung tissue concerning the clinical assessments.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0352", "problem_id": "03520001", "content": "The widespread adoption of attention mechanisms has raised questions regarding the interpretability of their distributions. Despite offering insights into model operation, using attention as a direct explanation for model predictions remains questionable. The research community continues to seek more interpretable methods for pinpointing local active regions that significantly influence final decisions. To enhance the interpretability of current attention models, we introduce a novel Bilinear Representative Non-Parametric Attention (BR-NPA) approach designed to extract task-relevant, human-understandable information. Initially, the target model undergoes distillation to yield higher-resolution intermediate feature maps. Subsequently, representative features are grouped based on local pairwise feature similarity, generating more refined and precise attention maps that emphasize task-relevant input segments. These attention maps are then ranked according to the 'active level' of the composite feature, providing insights into the importance of highlighted regions. This model is readily adaptable to a range of contemporary deep learning models involving classification and demonstrates enhanced accuracy and speed, alongside a reduced memory footprint, compared to typical neural attention modules. Extensive experiments demonstrate more comprehensive visual explanations than state-of-the-art visualization models across various tasks, including few-shot classification, person re-identification, and fine-grained image classification. The proposed visualization model provides significant insights into the variations in neural network attention across different tasks.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0353", "problem_id": "03530001", "content": "Driven by the necessity for a versatile object proposal generation algorithm that effectively balances object detection recall, proposal localization accuracy, and computational speed, we introduce BING++. This novel algorithm builds upon the computational efficiency of BING while substantially enhancing its proposal localization quality. Our approach frames object proposal generation from a new probabilistic standpoint, enabling BING++ to refine localization accuracy through the sequential utilization of edges and segments for estimating object boundaries and updating proposals. To efficiently learn parameters, we employ a quantized parameter space to identify approximate solutions, thereby reducing complexity. We validate the generalizability of BING++ by using fixed parameters across diverse object classes and datasets. Empirical results demonstrate that BING++ achieves a considerable improvement in localization quality—18.5% on VOC2007 and 16.7% on Microsoft COCO—while operating at approximately half the speed of BING on a CPU. In comparison to other advanced methods, BING++ delivers competitive performance with significantly faster execution times.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0354", "problem_id": "03540001", "content": "Dense video captioning, which involves identifying and narrating key events within unedited videos, is typically addressed using methods that focus solely on visual information, disregarding the audio component. Although some earlier studies have incorporated both audio and visual data, they have either yielded unsatisfactory outcomes or demonstrated relevance only within narrowly defined datasets. This paper presents a Bi-modal Transformer, an adaptation of the Transformer architecture designed to process two modalities concurrently. The proposed model's efficacy is demonstrated through its application to dense video captioning, utilizing both audio and visual inputs, while maintaining the flexibility to handle any pair of modalities in sequence-to-sequence tasks. Furthermore, the pre-trained bi-modal encoder, a component of the Bi-modal Transformer, can function as a feature extractor for a basic proposal generation module. Empirical results on the demanding ActivityNet Captions dataset showcase the model's superior performance. The code is available: v-iashin.github.io/bmt", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0355", "problem_id": "03550001", "content": "Semi-supervised anomaly detection involves identifying anomalies by learning the distribution of normal data, with backpropagation neural networks (BP-NNs) gaining attention due to their strong generalization capabilities. Typically, BP-NN-based models are optimized iteratively on server machines using data from edge devices, but this approach has two major drawbacks: (1) the optimization process struggles to adapt to changes in the normal data distribution, known as concept drift, and (2) the transfer of data between edge devices and servers introduces additional latency and energy consumption. To mitigate these issues, this study proposes ONLAD and its IP core, ONLAD Core, which is designed for rapid sequential learning to track concept drift in under one millisecond. The ONLAD Core enables on-device learning on edge devices with low power consumption, allowing for standalone execution without requiring data transfers between edge devices and servers. Experimental results demonstrate ONLAD's effective anomaly detection capabilities in environments simulating concept drift, while evaluations of ONLAD Core show that it achieves 1.95x~6.58x faster training latency and 5.0x~25.4x lower runtime power consumption compared to other software implementations, as confirmed on the PYNQ-Z1 board, a small FPGA/CPU SoC platform.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0356", "problem_id": "03560001", "content": "Estimating 3D motion vectors for individual points in a dynamic 3D scene, known as scene flow, has proven valuable for tasks like action classification and collision avoidance. However, precise annotation of scene flow data acquired from LiDAR sensors and stereo cameras is computationally expensive and labor-intensive. To mitigate this annotation challenge, we introduce both a 3D scene flow benchmark and a novel self-supervised training method for flow models. The benchmark comprises datasets designed to progressively examine various aspects of flow estimation, ranging from a single moving object to real-world environments. Additionally, we present Adversarial Metric Learning for self-supervised flow estimation, where the flow model estimates flow from point cloud sequences. A secondary model learns a latent metric to differentiate between points moved by the estimated flow and the target point cloud. This latent metric is learned through a Multi-Scale Triplet loss that uses intermediate feature vectors for loss calculation. Utilizing our benchmark, we analyze the performance of baseline methods and various models trained with our approach. Our findings indicate that our setup effectively maintains motion coherence and preserves local geometries, aspects often missed by self-supervised baselines. Addressing occlusions, however, remains an ongoing challenge.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0357", "problem_id": "03570001", "content": "Recent advancements in teaching machines to answer visual questions have been substantial; however, existing techniques still fall short of human-level abilities, particularly in their capacity to integrate new visual categories and concepts modularly, provide answer justifications, and adapt to new domains without specific examples. We introduce a new method comprising two primary components: the generation of a question graph representation and an answering process that leverages the question graph's abstract structure to activate a flexible array of visual estimators. The language and visual components are trained independently; yet, unlike current methods, our approach does not require training on image-question-answer triplets. This methodology can manage new domains, including expanded question types and novel object classes, properties, and relations, provided that relevant visual estimators exist. Furthermore, it can elucidate its answers and propose alternatives when questions lack grounding in the image. We demonstrate that our approach yields both superior performance and domain extensibility without relying on question-answer training.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0358", "problem_id": "03580001", "content": "This paper tackles the challenge of generating accurate, dense depth maps from individual RGB images. Beginning with a standard encoder-decoder convolutional neural network, we investigate the impact of global information processing on enhancing depth estimation. To achieve this, we introduce a novel transformer-based architectural component, AdaBins, which partitions the depth range into bins with adaptively estimated centers for each image. Final depth values are then calculated as weighted sums of these bin centers. Experimental results demonstrate significant performance gains over existing state-of-the-art methods on multiple widely used depth datasets across various evaluation metrics. Furthermore, we confirm the efficacy of the AdaBins block through an ablation study and provide the code and pre-trained weights for our new state-of-the-art model.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0359", "problem_id": "03590001", "content": "Effective reinforcement learning relies on suitable representations. Early reinforcement learning applications were confined to limited domains. Deep reinforcement learning, conversely, offers scalability, avoids reliance on domain-specific knowledge, and has demonstrated success in diverse applications like Atari gameplay, 3D navigation from visual input, and controlling complex robots. However, deep reinforcement learning's performance is susceptible to hyper-parameter tuning and architectural decisions, leading to instability within single trials and across repeated experiments, even with optimized configurations. Achieving optimal performance often demands considerable expertise and experimentation. Catastrophic interference, where subsequent training diminishes prior learning, is a potential cause. Paradoxically, the potent generalization capabilities that make Neural Networks (NN) effective in supervised learning may contribute to difficulties in reinforcement learning. This paper investigates the interplay between online NN training and interference within reinforcement learning. We discover that mapping input observations to a high-dimensional space enhances learning speed and reduces parameter sensitivity. Our findings also indicate that this preprocessing mitigates interference in prediction tasks. Furthermore, we introduce a straightforward and computationally efficient NN training method. Through comprehensive experiments in classic control domains, we validate that our method enhances performance in both prediction and control.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0360", "problem_id": "03600001", "content": "Recent research has focused significantly on neural rendering techniques. Certain methods rely on 3D geometry reconstructed via Multi-View Stereo (MVS), yet they are susceptible to the inaccuracies inherent in this reconstruction. Conversely, other methods learn a volumetric neural representation directly, which leads to high computational costs during training and inference. We present a versatile technique that leverages MVS for initialization but subsequently refines scene attributes, such as depth and reprojected features, within the input view space to enhance novel-view synthesis. A core component of our method is a novel differentiable point-based rendering pipeline. This pipeline is built upon bi-directional Elliptical Weighted Average splatting, a probabilistic depth test, and optimized camera selection. Integrating these elements into our neural renderer allows it to surpass existing methodologies in both quality and speed across the majority of scenes evaluated. Furthermore, beyond novel-view synthesis, our pipeline is applicable to multi-view harmonization and stylization tasks.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0361", "problem_id": "03610001", "content": "The identification of visual relationships is crucial for comprehensive image analysis. Nevertheless, the localization and classification of (subject, predicate, object) triplets pose significant difficulties due to the vast number of potential relationships, their uneven distribution in real-world images, and the high cost of annotation. In this paper, we propose a new weakly-supervised approach for visual relationship detection, utilizing only basic image-level predicate labels. We train a graph neural network to categorize predicates in images using a graph-based representation of identified objects, which inherently encodes a bias towards pairwise relationships. Subsequently, we reframe relationship detection as the interpretation of this predicate classifier, where a full relation is derived by identifying the subject and object associated with a predicted predicate. Our results, evaluated on the challenging and diverse datasets HICO-DET, Visual Relationship Detection, and UnRel, are on par with state-of-the-art fully- and weakly-supervised methods, demonstrating resilience to incomplete annotations and effective few-shot generalization.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0362", "problem_id": "03620001", "content": "This paper introduces the Multiview Extended Video with Activities (MEVA) dataset, a novel, large-scale resource designed for human activity recognition. Unlike current security datasets that either emphasize activity counts using aggregated public videos lacking consistent background footage, or maintain persistence by monitoring public spaces without controlling activity content, our dataset offers over 9300 hours of continuous, unedited video. This video is specifically designed to encompass a variety of concurrent activities alongside unscripted background actions. Within this dataset, 144 hours have been annotated for 37 distinct activity types, with bounding boxes identifying actors and props. Data acquisition involved approximately 100 actors engaging in both scripted scenarios and spontaneous background behaviors over three weeks at a controlled-access location. The collection process incorporated multiple modalities from both overlapping and non-overlapping viewpoints, indoors and outdoors, and includes video from 38 RGB and thermal IR cameras, 42 hours of UAV footage, and actor GPS locations. Furthermore, 122 hours of annotated data are reserved for the NIST Activity in Extended Video (ActEV) challenge. The remaining 22 hours of annotated data, along with associated video, are publicly accessible on our website, accompanied by an additional 306 hours of ground camera data, 4.6 hours of UAV data, and 9.6 hours of GPS logs. Additional derived data, such as camera models for geo-registering outdoor cameras and a high-density 3D point cloud model of the outdoor environment, are also provided. Data collection adhered to IRB guidelines, and the dataset is released under a CC-BY-4.0 license.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0363", "problem_id": "03630001", "content": "A crucial component of Automated Machine Learning (AutoML) systems involves selecting the appropriate model types for various tasks. While tree ensemble models, such as XGBoost, are commonly recommended for classification and regression tasks involving tabular data, recent studies have introduced deep learning models for tabular data, asserting their superiority over XGBoost in certain scenarios. This paper conducts a comprehensive comparison of these novel deep models with XGBoost across multiple datasets to determine their suitability as a recommended option for tabular data. The comparison encompasses not only a systematic evaluation of their accuracy but also an examination of the tuning and computational requirements. The results indicate that XGBoost surpasses the deep models in performance across the datasets, including those utilized in the studies that initially proposed the deep models, and additionally requires substantially less tuning. Nevertheless, the study also reveals that combining the deep models with XGBoost in an ensemble yields better performance on these datasets than using XGBoost alone, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0364", "problem_id": "03640001", "content": "Recent developments in self-supervised learning (SSL) have significantly narrowed the performance disparity with supervised ImageNet pretraining. However, these techniques have predominantly been tested on unlabeled ImageNet images and yield only minimal improvements when applied to larger collections of uncurated images. We propose that existing SSL methods excel with iconic images but face challenges when dealing with intricate scene images featuring multiple objects. An examination of contrastive SSL approaches reveals inadequate visual grounding and weak supervisory signals when applied to scene images. To address these issues, we introduce Contrastive Attention-Supervised Tuning (CAST), which utilizes unsupervised saliency maps for intelligent crop sampling and facilitates grounding supervision through a Grad-CAM attention loss. Experimental results on COCO indicate that CAST significantly enhances the feature extraction capabilities of SSL methods in scene images, and additional experiments demonstrate that models trained with CAST exhibit greater resilience to variations in backgrounds.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0365", "problem_id": "03650001", "content": "Sparse-reward reinforcement learning poses a significant challenge due to insufficient guidance toward desired outcomes. However, leveraging prior knowledge can enhance the learning process in certain scenarios. Reward shaping offers a mechanism to integrate such knowledge into the original reward function, thereby accelerating learning. Building on prior research that explored expert knowledge for potential function generation, this study investigates the application of the A* search algorithm to automatically create a potential function for reward shaping within the context of Sokoban, a widely recognized planning problem. The findings reveal that learning with the shaped reward function outperforms learning without it. The results suggest that distance functions are well-suited for Sokoban. This research showcases the potential of reward shaping to solve multiple instances of the problem. Furthermore, the acquired knowledge can be distilled into a single policy, representing an initial step toward developing a generalized policy capable of addressing previously unseen instances.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0366", "problem_id": "03660001", "content": "The rise of blockchain technologies and cryptocurrencies like Bitcoin has led to a growing interest in studying the network dynamics underlying Blockchain graphs. Unlike traditional financial networks, blockchain-based cryptocurrencies provide unrestricted access to their entire transaction graph, enabling comprehensive analysis. This raises an important question: does the transaction graph's dynamics influence the underlying cryptocurrency's price? Our research reveals that conventional graph features, such as degree distribution, may be inadequate for capturing network dynamics and their potential impact on Bitcoin price fluctuations. In contrast, novel topological features derived from persistent homology demonstrate significant utility in predicting Bitcoin price dynamics, as they can capture higher-order interactions among nodes in Blockchain graphs, facilitating the development of more accurate price prediction models. By leveraging these persistent homology-based techniques, we propose a novel, efficient, and scalable approach to graph representation learning on Blockchain, offering a flexible and computationally lightweight framework for future applications.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0367", "problem_id": "03670001", "content": "This paper introduces a learning approach designed to identify object-part concepts from a pre-trained convolutional neural network (CNN). The goal is twofold: first, to investigate the explicit semantic information embedded within CNN units, and second, to progressively develop a semantically meaningful graphical model on the pre-trained CNN for hierarchical object understanding. Utilizing a limited number of object part annotations (e.g., 3-12), the proposed method identifies specific latent patterns within the pre-trained CNN and links them to various semantic parts. A four-layer And-Or graph is employed to structure these mined latent patterns, thereby elucidating their inherent semantic hierarchy. Guided by a minimal set of part annotations, the method demonstrates significantly improved performance (approximately 13%-107% enhancement) in part center prediction on the PASCAL VOC and ImageNet datasets.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0368", "problem_id": "03680001", "content": "Ensemble learning, a machine learning approach that integrates multiple algorithms, has demonstrated considerable potential across various applications. This study investigates unsupervised ensemble classification, where the ensemble combiner lacks access to the true labels used to train individual classifiers. Unlike existing unsupervised ensemble classification methods primarily designed for independent and identically distributed (i.i.d.) data, this research proposes an unsupervised framework for learning from classifier ensembles when data dependencies exist. Specifically, we address two forms of data dependencies: sequential data and networked data represented by a graph. We introduce moment matching and Expectation Maximization algorithms tailored to these scenarios, and we assess their effectiveness using both synthetic and real datasets.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0369", "problem_id": "03690001", "content": "Structured stochastic processes in continuous time are commonly used to model natural and engineering phenomena, though they often rely on the Markov property for computational feasibility. Among these memoryless models, Continuous Time Bayesian Networks (CTBNs) are particularly prominent. This study extends CTBNs beyond exponential survival times to accommodate arbitrary distributions, addressing limitations in existing approaches that rely on auxiliary states and compromise tractability. Instead, we propose a framework using node-wise clocks to create a system of interconnected semi-Markov chains. We develop efficient algorithms for parameter and structure inference by leveraging local dependencies and validate our approach using synthetic data and a gene regulatory network benchmark dataset. Our results highlight improvements over current CTBN extensions.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0370", "problem_id": "03700001", "content": "This review provides a thorough overview of monocular vision techniques for simultaneously detecting objects and estimating their pose. It details conventional methodologies employing descriptors or models—such as chordiograms, shape-aware deformable parts model, bag of boundaries, distance transform templates, natural 3D markers, and facet features—along with diverse estimation techniques like iterative clustering estimation, probabilistic networks, and iterative genetic matching. Hybrid strategies that combine handcrafted feature extraction with deep learning-based estimation are also presented. Pure deep learning approaches (single-stage and multi-stage) are examined and compared where feasible. A detailed explanation of various accuracy measures and metrics is included. To provide clarity, the properties of pertinent datasets are discussed, and the evolution of approaches to this problem is traced from its early stages to the present.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0371", "problem_id": "03710001", "content": "This study explores the application of the Sugeno integral in machine learning, focusing on binary classification. We introduce a technique where the Sugeno integral serves as an aggregation function, merging multiple local feature-based evaluations into a unified global score. Given the integral's properties, this method is particularly effective for ordinal data, a relatively underexplored area in machine learning. The primary challenge involves determining the capacity associated with the Sugeno integral, addressed here through a linear programming-based algorithm. The approach incorporates feature value transformation into local utility scores and threshold optimization for global evaluations. To enhance classifier adaptability and prevent overfitting, we extend the framework to k-maxitive capacities, with k acting as a tunable hyper-parameter. Experimental results demonstrate the performance of our method against alternative approaches on various benchmark datasets.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0372", "problem_id": "03720001", "content": "The ubiquity of temporal data in the digital age, driven by the proliferation of big data and the internet of things, has highlighted the need for effective time series comparison methods. A key challenge in this context is determining a suitable distance or dissimilarity measure, with Dynamic Time Warping (DTW) emerging as a popular choice due to its ability to accommodate temporal variations. However, the quadratic computational cost of DTW limits its applicability to large-scale applications. To address this issue, this study proposes a sparsification approach to reduce the computational cost of DTW-like measures without compromising their quality. The resulting measures, SP-DTW and SP-K rdtw, are evaluated on a range of public datasets using 1-Nearest Neighbor (1-NN) and Support Vector Machine (SVM) classification algorithms, with results showing a significant speed-up and comparable accuracy. Moreover, the proposed measures outperform the traditional Sakoe-Chiba approach, which reduces the DTW path search space using a symmetric corridor, with only a slight reduction in speed-up, demonstrating their potential for efficient and accurate time series comparison.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0373", "problem_id": "03730001", "content": "This research investigates adversarially robust self-supervised learning for graph-structured data. Within the contrastive learning paradigm, we propose a novel approach to enhance the robustness of learned representations by employing i) adversarial transformations and ii) edge manipulation techniques involving both deletion and insertion. Initial experimental evaluations demonstrate encouraging outcomes. The study represents a significant advancement in integrating robustness as a complementary objective in graph contrastive learning.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0374", "problem_id": "03740001", "content": "Incremental Learning (IL) presents a significant challenge in AI, particularly when operating under computational constraints. This issue becomes especially pronounced in deep learning-based IL approaches, where memory limitations lead to catastrophic forgetting and delays from retraining to accommodate new classes. We propose DeeSIL, a modified transfer learning framework that employs a fixed deep feature extractor alongside independent shallow classifiers to enhance recognition capabilities. This approach addresses both challenges by functioning efficiently with restricted memory and enabling rapid addition of new concepts. Additionally, DeeSIL eliminates the need for deep retraining during model updates, allowing the incorporation of larger initial datasets to improve feature transferability. Evaluated on ImageNet LSVRC 2012 against three leading methods, DeeSIL outperforms the top baseline by 23 and 33 points under equivalent and expanded initial data conditions, respectively.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0375", "problem_id": "03750001", "content": "LSTM units possess the capability to retain and utilise long-range dependencies between input sequences to produce predictions on time series data. By incorporating rotation matrices, parameterised by an additional set of trainable weights, into the modification of the cell state (memory) of LSTMs, we demonstrate a notable enhancement in performance on certain tasks from the bAbI dataset.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0376", "problem_id": "03760001", "content": "We investigate the challenge of developing mappings from one state to another that are appropriate for model-based reinforcement learning, which both generalize to unfamiliar states and effectively represent stochastic transitions. Our findings indicate that widely used generative adversarial networks face difficulties in learning these stochastic transition models; however, altering their loss functions leads to an effective learning algorithm for this category of issues.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0377", "problem_id": "03770001", "content": "This study introduces SAFE, a practical technique for identifying non-stationarity in time series forecasting by tracking spectral content changes using a distance metric. The approach integrates seamlessly with modern machine learning models, enabling real-time adjustments when non-stationarity is detected. Additionally, an algorithm is proposed to selectively incorporate historical data during adaptation to mitigate Catastrophic Forgetting. Extensive experiments on synthetic and real-world datasets demonstrate the method's efficacy, showing substantial computational savings in processor and GPU usage without compromising prediction accuracy.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0378", "problem_id": "03780001", "content": "The swift expansion of renewable energy has resulted in the emergence of numerous small photovoltaic (PV) prosumers. Given the unpredictability of solar power generation, it is essential for aggregated prosumers to forecast solar energy output and determine if it will exceed demand. This study introduces two interpretable neural networks to address this issue: a binary classification neural network and a regression neural network, both developed using TensorFlow. The analysis of global feature importance and local feature contributions is conducted through three gradient-based techniques: Integrated Gradients, Expected Gradients, and DeepLIFT. Additionally, we identify atypical instances where predictions may fail by assessing prediction uncertainty via Bayesian neural networks. The combination of neural networks interpreted through gradient-based methods and enhanced by uncertainty assessment offers reliable and transparent forecasting for decision-makers.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0379", "problem_id": "03790001", "content": "Visual object tracking plays a crucial role in various real-time video surveillance applications, including the localization and recognition of individuals in space and time. In practical scenarios, an object detector and tracker need to periodically communicate to identify new objects and initiate tracking processes. Such interactions also enable the tracker to validate or modify its object template with new bounding boxes. Nevertheless, the bounding boxes generated by advanced detectors tend to be noisy due to variations in appearance, background, and occlusions, which can lead to drifting of the tracker. Additionally, while CNN-based detectors offer high accuracy, they come with increased computational demands, necessitating minimization of interactions for real-time effectiveness. This paper presents a novel strategy for managing interactions between detectors and trackers, specifically for trackers belonging to the Siamese-FC family. By incorporating a change detection mechanism into a deep Siamese-FC tracker, the template can be adjusted in response to alterations in the target's appearance that cause tracking drifts. An abrupt change detection prompts an update of the tracker template using the bounding box from the detector, whereas, for gradual changes, the detector aids in updating a dynamic set of templates for more reliable matching. Experiments conducted with cutting-edge Siamese-FC trackers and the YOLOv3 detector on a subset of the OTB-100 dataset, designed to simulate video surveillance conditions, underscored the necessity of utilizing precise detectors for dependable visual object tracking. The findings suggest that our adaptive Siamese trackers are resilient against noisy object detections and can significantly enhance the performance of Siamese-FC tracking.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0380", "problem_id": "03800001", "content": "A model-free approach to forecasting nonlinear time series, delay embedding, involves reconstructing dynamical systems using delay coordinates and is commonly utilized. For multivariate time series, existing methods can combine multiple forecasts from different embeddings into a single prediction, but their performance is often subpar due to random embedding selection or brute force application, without considering embedding diversity. To address these limitations, we propose a novel forecasting framework that leverages \"suboptimal embeddings\" generated through combinatorial optimization to minimize in-sample error. Our framework outperforms existing methods on both sample toy datasets and a real-world flood dataset, demonstrating its applicability across various data lengths and dimensions, and has potential applications in diverse fields, including neuroscience, ecology, finance, fluid dynamics, weather, and disaster prevention, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0381", "problem_id": "03810001", "content": "We introduce an algorithm designed to adapt a semantic segmentation model, initially trained on a labeled source domain, to perform effectively on an unlabeled target domain. While this problem is well-explored in unsupervised domain adaptation (UDA), current UDA methods necessitate simultaneous access to labeled source and unlabeled target data to train a domain-agnostic semantic segmentation model. By removing this requirement, users can adapt pretrained models for generalization on a target domain without needing the original source data. Our approach involves learning a prototypical distribution in an intermediate embedding space that represents abstract knowledge acquired from the source domain. This distribution is then used to align the target domain distribution with that of the source domain within the embedding space. We present a theoretical analysis outlining the conditions for the algorithm's efficacy. Experimental results on standard adaptation tasks show that our method achieves comparable performance to joint UDA approaches.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0382", "problem_id": "03820001", "content": "In the context of high-mobility Vehicle-to-Everything (V2X) communications utilizing millimeter Wave (mmWave) and sub-THz frequencies, estimating Multiple-Input Multiple-Output (MIMO) channels poses a significant challenge. The spatial and temporal characteristics of MIMO channels at these frequencies are marked by a limited number of dominant paths, which can be leveraged to improve channel estimation. Traditional Algebraic Low-rank (LR) channel estimation methods capitalize on the space-time sparsity of MIMO channels by computing position-dependent eigenmodes, but this approach requires knowledge of vehicles' geographical positions and numerous training vehicle passages, resulting in substantial complexity and control signaling overhead. To address these limitations, this work proposes a Deep Learning (DL)-based LR channel estimation approach, which can infer MIMO channel eigenmodes in urban V2X settings using a single Least Squares (LS) channel estimate and without relying on vehicle position information, as shown in Figure A (Reference [1]). Numerical results demonstrate that the proposed method achieves comparable Mean Squared Error (MSE) performance to position-based LR, and furthermore, it can be effectively transferred to different urban scenarios with distinct space-time channel features without requiring an explicit transfer learning procedure, as illustrated in Figure B (Reference [2]), thereby facilitating deployment in diverse dense urban environments, see Figure C (Reference [3]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0383", "problem_id": "03830001", "content": "This study seeks to reconstruct the complete three-dimensional shape of a human from a single image, addressing the limitations of existing vertex-level and parameter regression methods that rely on predefined adjacency matrices to represent positive relationships between nodes, thereby neglecting the complex topological relationships on the surface of the human body. Furthermore, current approaches often struggle with significant performance degradation when confronted with occlusion cases in real-world scenarios due to domain gaps. To overcome these challenges, we introduce the Deep Mesh Relation Capturing Graph Convolution Network (DC-GNet), which incorporates a shape completion task to enhance 3D human shape reconstruction. Our approach involves two key innovations: firstly, an adaptive matrix is proposed to capture both positive and negative relationships between mesh vertices, and secondly, a shape completion task is designed to learn about various types of occlusions. By encoding mesh structures based on subtle relationships between nodes in more distant regions, our method improves performance, particularly in outdoor scenes where occlusions are common. The efficacy of our approach is demonstrated through extensive experiments on several benchmarks, surpassing existing methods for 3D human pose and shape estimation, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0384", "problem_id": "03840001", "content": "Recent advancements in smart technologies have led to an increased emphasis on leveraging artificial intelligence and machine learning in affective computing to improve user experiences through emotion recognition. Typically, machine learning models employed in this domain are trained on features that are manually extracted from biological signals. Such features often do not generalize effectively across extensive datasets and may inadequately represent the raw data information. One solution to this problem is the implementation of fully supervised deep learning techniques to derive latent representations of biosignals. However, this approach necessitates human intervention for data labeling, which can be challenging to obtain. In this study, we introduce an unsupervised framework to minimize dependence on human supervision. Our framework employs two stacked convolutional autoencoders to extract latent representations from wearable electrocardiogram (ECG) and electrodermal activity (EDA) signals, which are then applied within a random forest model for binary arousal classification. This methodology reduces the need for human oversight and facilitates the integration of datasets, thereby enhancing generalizability. We construct an aggregated dataset from the AMIGOS, ASCERTAIN, CLEAS, and MAHNOB-HCI datasets to validate our framework. The results obtained from our proposed method are contrasted with those generated by convolutional neural networks and techniques that utilize manual feature extraction. We also explore the methodology for integrating the two modalities. Ultimately, our findings demonstrate that our approach surpasses current state-of-the-art results in arousal detection using ECG and EDA biosignals within the same datasets, highlighting the broad applicability of stacked convolutional autoencoders in machine learning for affective computing.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0385", "problem_id": "03850001", "content": "This study introduces a deep reinforcement learning approach designed for real-time accompaniment generation, enabling interactive improvisation between humans and machines. Unlike offline music creation and harmonization techniques, online accompaniment necessitates immediate responses to human input while sequentially producing the machine's musical counterpart. We formulate this as a reinforcement learning task, where an agent learns a policy to produce musical notes (actions) based on prior context (state). A crucial component is the reward model, which is learned from both monophonic and polyphonic training data rather than relying on predefined composition rules. This model evaluates how well the machine-generated note aligns with both the machine's previous output and the human's input. Experimental results demonstrate the algorithm's ability to create melodic, harmonious, and varied accompaniments in response to human performance. Subjective assessments indicate that the proposed method produces higher-quality music compared to baseline approaches.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0386", "problem_id": "03860001", "content": "Current studies in action recognition regard activities as singular events within videos. Recently, insights gained from representing actions as combinations of fundamental actions have indicated potential for enhancing action comprehension, particularly with the advent of datasets equipped with such annotations that facilitate the learning of relevant representations. Nevertheless, research that explores action composition while utilizing diverse viewpoints and various data modalities for representation learning remains scarce. To address this gap, we present the Home Action Genome (HOMAGE): a multi-view action dataset featuring multiple modalities and perspectives, enriched with hierarchical activity and atomic action labels, alongside detailed scene composition labels. By harnessing this rich multi-modal, multi-view environment, we introduce Cooperative Compositional Action Understanding (CCAU), a collaborative learning framework focused on hierarchical action recognition that incorporates compositional action components. CCAU demonstrates consistent performance enhancements across all modalities. In addition, we illustrate the effectiveness of co-learning compositions in few-shot action recognition, achieving a mean Average Precision (mAP) of 28.6% with only one sample.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0387", "problem_id": "03870001", "content": "Cutting-edge adversarial attacks target neural network classifiers. Typically, neural networks minimize their loss function by employing gradient descent. Gradient-based adversarial attacks leverage the gradient of a classifier's loss function to create adversarially altered images. This study investigates whether an alternative optimization method could enhance the efficacy of neural network classifiers. We propose a new strategy utilizing minimax optimization to counteract gradient-based adversarial attacks. Our minimax classifier functions as the discriminator in a generative adversarial network (GAN) that engages in a minimax game with the GAN generator. Furthermore, our GAN generator projects all points onto a different manifold than the original, as the original manifold may be responsible for the adversarial attacks. To evaluate the effectiveness of our minimax defense, we test it against adversarial attacks - Carlini Wagner (CW), DeepFool, and Fast Gradient Sign Method (FGSM) - across three datasets: MNIST, CIFAR-10, and German Traffic Sign (TRAFFIC). The performance of our minimax defense against CW attacks is 98.07% (MNIST-default 98.93%), 73.90% (CIFAR-10-default 83.14%), and 94.54% (TRAFFIC-default 96.97%). For DeepFool attacks, the performance is 98.87% (MNIST), 76.61% (CIFAR-10), and 94.57% (TRAFFIC). When facing FGSM attacks, we report 97.01% (MNIST), 76.79% (CIFAR-10), and 81.41% (TRAFFIC). Our Minimax adversarial strategy signifies a substantial advancement in defensive approaches for neural network classifiers.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0388", "problem_id": "03880001", "content": "Low-light images often exhibit reduced visibility, diminished contrast, and muted colors. While deep convolutional neural networks (CNNs) outperform traditional methods in image enhancement, current data-driven models rely on limited fixed primitives to capture dependencies, failing to leverage multi-scale spatial contexts essential for tasks like contrast adjustment, brightness correction, and color restoration. To address this, we propose a context-aware deep network that incorporates a global context module to extract complementary spatial correlations across the entire image and a dense residual block to capture local context with an expanded receptive field. Evaluated on the MIT-Adobe FiveK, LoL, and SID datasets, our method surpasses state-of-the-art techniques in standard image quality metrics. Notably, it achieves a PSNR improvement from 23.04 dB to 24.45 dB over the leading approach on the MIT-Adobe FiveK dataset.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0389", "problem_id": "03890001", "content": "Video classification has proven valuable in numerous real-world applications, with deep learning significantly enhancing its precision. Current approaches typically treat all video frames uniformly, yet motion analysis reveals that frames naturally consist of salient and non-salient regions. These regions require distinct network architectures, as salient areas contain both appearance and motion cues, while non-salient regions primarily represent static backgrounds. This study proposes an unsupervised method to predict video saliency using optical flow. Subsequently, two separate 3D CNNs process raw frames and optical flow in salient regions, while a 2D CNN handles raw frames in non-salient areas. Since each stream contributes differently per class, class-specific adaptive weighting is applied. Experiments demonstrate that combining saliency-aware modeling with adaptive weighting yields mutual improvement, achieving state-of-the-art performance.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0390", "problem_id": "03900001", "content": "Time-series mining has emerged as a significant research challenge in recent years, particularly in monitoring applications that necessitate the analysis of extensive time-series datasets to discern typical patterns. Deviations from these learned patterns are subsequently identified as anomalous events. Furthermore, sophisticated applications may necessitate the temporal analysis of diverse, heterogeneous parameters. This paper introduces a method for mining heterogeneous multivariate time-series to extract significant patterns. The proposed method accommodates mixed time-series data, incorporating both pattern and non-pattern information, to account for imprecise matches, outliers, stretching, and global translation of pattern instances over time. We present preliminary results of our method in the context of in-home health monitoring, with the aim of constructing a behavioral profile of an individual through analysis of temporal variations in quantitative and qualitative parameters acquired via sensors installed in the home.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0391", "problem_id": "03910001", "content": "This study addresses the challenge of spectrum scarcity in unmanned aerial vehicle (UAV) networks during critical operations, including wildfire surveillance, search and rescue, and disaster assessment. These missions necessitate a significant demand for high-throughput data communications such as streaming real-time video, images, and voice, where the spectrum allocated to the UAV network may not suffice to achieve the required Quality of Service (QoS). In these instances, the aerial network can acquire additional spectrum from existing terrestrial networks in exchange for providing relaying services. We introduce a spectrum-sharing framework where UAVs are categorized into two groups: relaying UAVs that assist the spectrum owners, and sensing UAVs that carry out disaster response activities with the acquired spectrum. The UAV network's functioning is overseen by a hierarchical system, wherein a central controller allocates tasks based on UAV resources and designates their operational regions according to the urgency of affected locations. Subsequently, the UAVs autonomously adjust their positions through a model-free reinforcement learning approach to optimize individual throughput and extend their operational lifespan. We evaluate the performance and convergence of the proposed strategy both analytically and through extensive simulations across various scenarios.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0392", "problem_id": "03920001", "content": "The computation of Nash equilibrium (NE) in multi-player games has experienced a resurgence of interest, driven by recent breakthroughs in generative adversarial networks, yet efficient computation of equilibrium remains a significant challenge. To address this, we propose the Gradient-based Nikaido-Isoda (GNI) function, which fulfills two key roles: (i) it acts as a merit function that vanishes at the first-order stationary points of each player's optimization problem, and (ii) it provides error bounds to a stationary Nash point. We demonstrate that gradient descent converges sublinearly to a first-order stationary point of the GNI function. Notably, in the specific cases of bilinear min-max games and multi-player quadratic games, the GNI function exhibits convexity, enabling gradient descent to achieve linear convergence to an NE, provided one exists, as shown in Figure A (see References [1] and [2] for details). Our numerical experiments, illustrated in Figure B and Figure C, consistently show that the GNI formulation converges to the first-order stationary point of each player's optimization problem, as discussed in citations [3] and [4].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0393", "problem_id": "03930001", "content": "At its core, this tutorial addresses the challenge of modeling network users' path selection behavior, a longstanding issue in transportation science commonly referred to as the route choice problem. The prevailing approach in this field involves utilizing discrete choice models to forecast individuals' path preferences. This tutorial focuses on a specific subset of discrete choice models, namely recursive models, and offers three key contributions: firstly, it provides an exhaustive background on the route choice problem, establishing connections to inverse optimization and inverse reinforcement learning to facilitate future research; secondly, it presents a formal introduction to the problem, the recursive modeling concept, and an overview of existing models, including their characteristics and applications; thirdly, it conducts an in-depth analysis of illustrative examples from multiple perspectives, enabling novice readers to develop an understanding of the problem and appreciate the benefits of recursive models over path-based alternatives.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0394", "problem_id": "03940001", "content": "Image segmentation plays a crucial role in numerous medical applications. While convolutional neural networks achieve high accuracy in this area, they generally necessitate extensive labeled datasets for supervised training. The annotation of medical images is a labor-intensive process demanding specialized knowledge, and conventional data augmentation techniques often struggle to represent the intricate variations present in these images. In this work, we introduce an automated data augmentation technique designed to generate synthetic labeled medical images. We evaluate our method on segmenting magnetic resonance imaging (MRI) of the brain, utilizing a semi-supervised approach that requires only one segmented scan alongside other unlabeled scans. By learning a transformation model from the images, we synthesize additional labeled examples using this model and the initial labeled scan. Each transformation incorporates both a spatial deformation field and an intensity change, facilitating the creation of complex effects that mimic anatomical variations and diverse image acquisition protocols. Results indicate that training a supervised segmentation model with these synthetically generated examples leads to substantial improvements over existing one-shot biomedical image segmentation techniques. Our code is available at https://github.com/xamyzhao/brainstorm.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0395", "problem_id": "03950001", "content": "This study introduces a deep Inverse Reinforcement Learning (IRL) framework capable of acquiring a priori unknown nonlinear reward functions from demonstrations provided by unlabeled experts. To achieve this, we utilize tools from Dirichlet processes and propose a flexible method that addresses both the complexity and the unknown quantity of reward functions concurrently. By applying the conditional maximum entropy principle, we represent the multi-intention behaviors of experts as a mixture of latent intention distributions and develop two algorithms for estimating the parameters of the deep reward network along with the number of intentions displayed by the experts from unlabeled demonstrations. The effectiveness of the proposed algorithms is tested against three benchmark scenarios, two of which have been uniquely adapted in this work for multi-intention IRL, and they are compared to established baselines. Our experimental results illustrate the superiority of our algorithms compared to existing methods and highlight the advantages of online inference for determining the number of expert intentions, instead of predefining it.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0396", "problem_id": "03960001", "content": "A fundamental aspect of decision-making under uncertainty involves receiving feedback on the quality of actions taken. However, in numerous applications, such feedback is limited—for instance, repeatedly soliciting user evaluations in recommendation systems may lead to frustration. This study formalizes decision-making problems with a querying budget, imposing a strict (and potentially time-varying) constraint on the number of reward queries permitted. We examine multi-armed bandits, linear bandits, and reinforcement learning scenarios. Initially, we evaluate the effectiveness of 'greedy' algorithms that query rewards whenever possible, demonstrating their strong performance in purely stochastic environments but their susceptibility to linear regret under adverse conditions. To address this limitation, we introduce the Confidence-Budget Matching (CBM) principle, which triggers reward queries when confidence intervals exceed the inverse square root of the remaining budget. Our analysis reveals that CBM-based algorithms exhibit robust performance despite adversities in contexts, initial states, and budget constraints.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0397", "problem_id": "03970001", "content": "Image captioning produces textual descriptions of visual content in input images, traditionally designed for high-quality images captured under clear conditions. In adverse weather scenarios like heavy rain, snow, or fog, visibility is significantly reduced due to rain streaks, accumulated precipitation, or snowflakes, leading to degraded image quality. This impairment obstructs the extraction of meaningful visual features and diminishes captioning performance. To tackle these challenges, this research proposes a novel encoder tailored for images affected by heavy rain. The core approach involves converting features from such degraded images into semantically meaningful visual representations aligned with word and sentence context. First, a target encoder is pretrained within an encoder-decoder framework to link visual features with semantic words. Next, an initial reconstruction subnetwork (IRS), grounded in a heavy rain model, enhances object visibility in rain-affected images. The IRS is integrated with a semantic visual feature matching subnetwork (SVFMS) to align its output features with the pretrained target encoder's semantic representations. The proposed encoder jointly learns the IRS and SVFMS in an end-to-end fashion before being connected to a pretrained decoder for caption generation. Experiments confirm that this encoder successfully derives semantic visual features from rain-degraded images, improving caption accuracy.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0398", "problem_id": "03980001", "content": "Fairness considerations are increasingly significant in machine learning. This study investigates the integration of group fairness constraints into kernel regression techniques, which include Gaussian processes, support vector machines, neural network regression, and decision tree regression. The primary focus is on analyzing the impact of these constraints on decision tree regression, with relevance to widely used inference methods such as random forests and boosted trees. The research demonstrates that the memory and computational complexity remain consistent for these models and establishes tight bounds on the anticipated model perturbations relative to the number of tree leaves. Notably, this method is applicable to already trained models, facilitating straightforward implementation on existing models, and necessitates group labels only within the training dataset.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0399", "problem_id": "03990001", "content": "In recent years, deep generative models, including generative adversarial networks \\autocite, variational autoencoders \\autocite, and their variants, have gained widespread acceptance for modelling complex data distributions, despite their limitation of not providing explicit access to the induced probability density functions. This limitation has hindered their application in tasks requiring the scoring of new data instances with the learned distributions. Normalizing flows have addressed this issue by utilizing the change-of-variables formula and transformations with tractable Jacobians, although they previously lacked the ability to incorporate discrete structure, such as mixtures, in an unsupervised setting until recently \\autocites. This study resolves this limitation by integrating normalizing flows into a mixture model and developing an end-to-end training procedure based on variational inference, which employs a neural network-parameterized variational posterior. The proposed model is well-suited for applications such as multimodal density estimation, semi-supervised learning, and clustering, and is demonstrated on synthetic and real-world datasets. Keywords: Deep generative models, normalizing flows, variational inference, probabilistic modelling, mixture models.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0400", "problem_id": "04000001", "content": "This research investigates the exploration challenge in episodic Markov Decision Processes (MDPs) where complex observations stem from a limited set of latent states. By leveraging specific identifiability conditions, we propose an approach to iteratively estimate a mapping from observations to latent states via regression and clustering procedures—using previously inferred latent states as labels for subsequent regression tasks—and employ this mapping to develop effective exploration strategies. We establish finite-sample bounds for the accuracy of the learned state decoding function and exploration policies, supported by experimental validation on a set of difficult exploration scenarios. The proposed approach achieves exponential gains over Q-learning with basic exploration, even when Q-learning is granted privileged access to latent states.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0401", "problem_id": "04010001", "content": "Recent advancements in representation learning have focused on creating methods to separate the underlying generative factors of datasets and establishing measures to assess the extent of this disentanglement. However, existing approaches and evaluation metrics typically presume that both learned representations and true factors are unstructured, continuous, and independent, despite many real-world data-generation processes exhibiting complex hierarchical relationships, mixed discrete and continuous interdependent variables, and dynamic intrinsic dimensionality. This study introduces frameworks, techniques, and evaluation criteria for acquiring hierarchical representations that account for these complexities.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0402", "problem_id": "04020001", "content": "In recent years, Graph Neural Networks (GNN) have gained widespread acceptance across various applications, prompting researchers to explore neural architecture search (NAS) methods to determine optimal, data-specific GNN architectures, a approach that has yielded notable successes in the realm of convolutional neural networks. Although initial studies, such as GraphNAS and Auto-GNN, have pioneered the application of NAS to GNN, their designed search spaces have been found to be limited in terms of expressive capability and search efficiency, thereby hindering their potential. To address these limitations, this work introduces the SNAG framework (Simplified Neural Architecture search for Graph neural networks), which comprises a novel search space and a reinforcement learning-based search algorithm. The efficacy of the SNAG framework is validated through comprehensive experiments on real-world datasets, surpassing the performance of both human-designed GNNs and existing NAS methods, including GraphNAS and Auto-GNN.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0403", "problem_id": "04030001", "content": "This research explores adaptive traffic planning and control through Reinforcement Learning (RL), moving beyond conventional approaches to incorporate advanced deep RL techniques. The study integrates recent algorithmic enhancements that refine the original Deep Q-Networks (DQN) framework for discrete control, alongside traffic-specific insights. A new DQN-based algorithm, TC-DQN+, is introduced to enable rapid and dependable traffic decision-making. Additionally, a novel reward function is proposed, with its implications analyzed through comparative examples against traditional traffic control methods.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0404", "problem_id": "04040001", "content": "This study introduces a novel algorithm for unsupervised learning of latent variable models utilizing unlabeled data. Our approach relies on spectral decomposition, demonstrating strong robustness both theoretically and empirically. Furthermore, we detail the application of this algorithm to estimate parameters for the single topic model and Latent Dirichlet Allocation (LDA), two established text mining models, offering an efficient method for parameter retrieval. We benchmark the performance of our algorithm against existing methods using synthetic datasets and showcase its applicability to real-world text corpora for both the single topic model and LDA, yielding insightful outcomes.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0405", "problem_id": "04050001", "content": "This paper addresses the limitations of the conventional reinforcement learning framework in accurately representing lifelong learning systems, which are characterized by continuous, trial-and-error-based learning throughout their existence. Arguments are presented to demonstrate this inadequacy, followed by insights into the nature of lifelong reinforcement learning. Finally, a basic prototype of a lifelong reinforcement learning system is introduced to illustrate these concepts.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0406", "problem_id": "04060001", "content": "Evaluating the robustness of machine learning models is essential prior to their deployment. For deep neural networks applied to image analysis, variations in object position, orientation, and scale can significantly influence predictions in complex ways. This study conducts a detailed examination of robustness concerning these variables by leveraging SI-Score, a synthetic dataset. Specifically, we analyze ResNets, Vision Transformers, and CLIP, uncovering notable qualitative distinctions among these architectures.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0407", "problem_id": "04070001", "content": "Snake robots, which consist of interconnected joint actuators, have recently attracted increasing interest in industrial applications, such as life detection in confined spaces. These robots are capable of maneuvering through complex environments by coordinating multiple motors along their backbone. However, controlling these robots in unfamiliar surroundings presents significant challenges, and traditional control methods can be energy-inefficient or may not successfully guide the robot to its destination. In this study, we develop a snake locomotion gait policy using deep reinforcement learning (DRL) aimed at achieving energy-efficient control. We utilize proximal policy optimization (PPO) for each joint motor, parameterized by angular velocity, allowing the DRL agent to learn the standard serpenoid curve at each time step. The robot simulation and task environment are constructed using PyBullet. In comparison to traditional control methods, the snake robots operated by the trained PPO agent exhibit quicker movement and a more energy-efficient locomotion gait. This research illustrates that DRL offers an energy-efficient approach to robot control.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0408", "problem_id": "04080001", "content": "Breast cancer screening plays a crucial role in modern women's healthcare, yet current AI-integrated systems often fall short of the accuracy desired by clinicians. To enhance system reliability, this study implements three key approaches: 1) Ultrasound image super-resolution using SRGAN to address image clarity issues, thereby boosting detection model performance; 2) Modifications to the YOLOv4 and CenterNet models tailored for medical imaging needs; and 3) A multi-AI framework leveraging dual-model cross-validation, where only concordant results are accepted. Experimental outcomes demonstrate significant improvements: the super-resolution approach elevated mAP scores by 9.6% (YOLOv4) and 13.8% (CenterNet), while the classification-adapted multi-AI system achieved 95.91% sensitivity and 96.02% specificity, though rejecting 23.55% of uncertain cases. This research advances medical AI by optimizing detection models for breast ultrasound analysis and establishing a transferable framework for adapting target detection models to medical screening applications.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0409", "problem_id": "04090001", "content": "Multivariate time-series modeling and forecasting is a crucial task applicable across various domains. While established methods like Vector Auto-Regressive (VAR) models and contemporary Recurrent Neural Networks (RNNs) are essential for time-series data analysis, many multivariate scenarios exhibit both substantial linear dependencies, suited for VAR models, and nonlinear characteristics, better addressed by RNNs. Relying solely on either VAR or RNN models in such cases can result in suboptimal predictions or necessitate complex models and extended training periods. To address this, we introduce a hybrid model, Residual RNN (R2N2), which initially employs a linear model, such as VAR, to capture linear patterns, and subsequently uses RNNs to model the remaining residual errors. R2N2 leverages existing VAR and RNN training algorithms. Empirical evaluations on two real-world datasets (aviation and climate domains) demonstrate that R2N2 achieves competitive or superior performance compared to standalone VAR or RNN models. Furthermore, R2N2 exhibits faster training times and requires fewer hidden units than RNNs.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0410", "problem_id": "04100001", "content": "We introduce an automated variational inference technique designed for Bayesian structured prediction tasks that employ Gaussian process (GP) priors and linear-chain likelihoods. This method operates without requiring explicit knowledge of the structured likelihood model and efficiently handles large datasets. Additionally, we demonstrate that the expected likelihood term and its gradients in the variational objective (ELBO) can be approximated effectively using expectations derived from low-dimensional Gaussian distributions. The optimization of the ELBO is fully parallelizable across sequences and compatible with stochastic optimization, enhanced by control variates and advanced incremental optimization techniques for practical applicability. Experimental evaluations on natural language processing tasks reveal that our approach performs comparably to—and occasionally surpasses—specialized methods such as SVM-struct and CRFs, while addressing the scalability issues of prior sampling-based inference algorithms. This advancement represents a significant contribution toward automating inference in Bayesian structured prediction.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0411", "problem_id": "04110001", "content": "Generative adversarial networks (GANs) typically require extensive datasets that comprehensively cover the data space to achieve high learning accuracy. Yet, in many cases, datasets may be restricted and dispersed among multiple agents, each aiming to independently learn the underlying data distribution. These local datasets are often private, and agents may be unwilling to share them. This paper introduces a novel brainstorming GAN (BGAN) framework to tackle this multi-agent GAN challenge, enabling agents to produce realistic data samples in a fully decentralized manner while maintaining data privacy. BGAN facilitates knowledge exchange among agents without sharing actual datasets, instead relying on collaborative \"brainstorming\" through generated samples. Unlike existing distributed GAN approaches, BGAN operates without a central controller and supports scalability while remaining agnostic to the hyperparameters of agents' deep neural networks (DNNs), allowing diverse DNN architectures. The interactions among BGAN agents are theoretically modeled as a game with a unique Nash equilibrium. Empirical results demonstrate that BGAN outperforms other distributed GAN frameworks by producing higher-quality samples with reduced Jensen-Shannon divergence (JSD) and Fr\\'echet Inception distance (FID).", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0412", "problem_id": "04120001", "content": "Deep learning, encompassing a wide array of methodologies, employs multiple representational layers to autonomously acquire pertinent features directly from structured data. These methods have recently achieved unprecedented results across various challenging machine learning applications in computer vision, speech recognition, and natural language processing. Although deep learning has seen substantial success, the underlying theoretical reasons for its efficacy in feature learning and compression remain largely unexplored. This work demonstrates a strong connection between deep learning and the renormalization group (RG), a prominent and effective technique in theoretical physics. The RG is an iterative coarse-graining process that facilitates the extraction of relevant features (i.e., operators) by examining a physical system at varying length scales. We establish a precise mapping between the variational renormalization group, initially proposed by Kadanoff, and deep learning architectures grounded in Restricted Boltzmann Machines (RBMs). We demonstrate these concepts using the nearest-neighbor Ising Model in one and two-dimensions. Our findings imply that deep learning algorithms might be utilizing a generalized RG-like framework to discern relevant features from data.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0413", "problem_id": "04130001", "content": "Graph embedding techniques, or node representation learning, have gained significant attention in recent years due to their effectiveness in downstream applications such as node classification, link prediction, and recommendation systems. These algorithms seek to maintain both local and global network structures by defining node neighborhoods. Yet, many current methods produce embeddings that inadequately preserve network topology or yield unstable representations due to stochastic processes like random walks, limiting their applicability to multi-graph scenarios. This paper introduces RECS, a stable and innovative graph embedding framework that leverages connection subgraphs by drawing parallels between graphs and electrical circuits. RECS maintains local and global connectivity patterns, handles high-degree nodes effectively, and incorporates weak ties and meta-data often overlooked by existing approaches. Experimental results demonstrate that RECS surpasses state-of-the-art methods by up to 36.85% in multi-label classification tasks. Additionally, unlike baseline methods, RECS is deterministic, ensuring complete stability.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0414", "problem_id": "04140001", "content": "We introduce a neural transducer model incorporating visual attention, designed to convert images of mathematical formulas into corresponding LaTeX markup. Leveraging sequence modeling and transduction approaches that have proven effective across various domains including natural language, images, handwriting, and audio, our image-to-markup system generates syntactically and semantically accurate LaTeX code exceeding 150 words in length, achieving a BLEU score of 89%—surpassing prior benchmarks for the Im2Latex task. Additionally, through heat-map visualizations, we illustrate how attention mechanisms enhance model interpretability by precisely identifying and localizing symbols in the input images, even without training on bounding box annotations.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0415", "problem_id": "04150001", "content": "The significance of metric learning in instance-level image retrieval has been widely acknowledged, with numerous studies contributing to its development as a crucial component of high-performing methods. However, the potential of pre-processing and post-processing techniques to substantially enhance performance has received relatively less attention. Notably, previous research has often relied on small-scale datasets, overlooking the importance of examining feature representation behavior in deep learning models within large-scale environments, where both domain and data play a critical role. This paper provides an in-depth analysis of the impact of established pre-processing and post-processing techniques, as well as their combined effects, on large-scale image retrieval. Our findings demonstrate that the judicious application of these techniques can yield significant improvements in model performance without requiring complex architectural modifications or incurring additional loss, as evidenced by our competitive results in the Google Landmark Retrieval Challenge 2019.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0416", "problem_id": "04160001", "content": "We introduce a new method for estimating mutual information between ordinal vectors x and y. This inductive approach relies on nonparametric properties that expose data associations rather than requiring complete knowledge of the underlying joint distributions P_. The method involves (i) recognizing that I(y; x) equals I(u_y; u_x), where u_y and u_x are copula-uniform representations of y and x, and (ii) approximating the copula entropies h(u_y), h(u_x), and h(u_y, u_x) by solving a maximum-entropy problem constrained by α_m = E[ϕ_m(u_y, u_x)]. We demonstrate that this problem has a unique exponential family solution, obtainable through convex optimization, provided the constraint is feasible. The derived estimator, MIND, is marginal-invariant, non-negative, consistent, and achieves an MSE rate of O(1/n), outperforming alternative methods in data efficiency. Additionally, we show its application in reducing mode collapse in GANs by enhancing the entropy of fake sample copulas, an approach termed Copula Entropy Regularized GAN (CER-GAN).", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0417", "problem_id": "04170001", "content": "Adversarial representation learning presents a promising framework for deriving data representations that are insensitive to specific sensitive attributes while preserving the crucial information needed to forecast target attributes. Current methodologies address this challenge using iterative adversarial minimax optimization, but they do not offer theoretical assurances. In this study, we initially examine the \"linear\" version of the problem, where all participants are linear functions. Our findings reveal that the associated optimization problem is non-convex and non-differentiable. We derive an exact closed-form expression for the global optima utilizing spectral learning and offer performance guarantees through analytical bounds on the achievable utility and invariance. Subsequently, we expand this solution and analysis to incorporate non-linear functions via kernel representation. Numerical tests conducted on the UCI, Extended Yale B, and CIFAR-100 datasets demonstrate that (a) our solution effectively provides provable invariance to any biased pre-trained data representation, and (b) the empirical balance between utility and invariance achieved by our approach is on par with the iterative minimax optimization found in established deep neural network methods. The code is accessible at https://github.com/human-analysis/Kernel-ARL.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0418", "problem_id": "04180001", "content": "This work introduces a framework to enhance the resilience of continuous control Reinforcement Learning (RL) algorithms against disturbances in transition dynamics, termed model misspecification. The focus is on integrating robustness into Maximum a-posteriori Policy Optimization (MPO), a contemporary continuous control RL method. This is accomplished by training a policy that optimizes for the lowest expected return in a worst-case scenario, and a robust entropy-regularized Bellman contraction operator is derived. Furthermore, a less restrictive, soft-robust, entropy-regularized objective, along with its associated Bellman operator, is presented. Empirical evaluations across nine Mujoco domains, subject to environmental perturbations, demonstrate that both robust and soft-robust policies surpass their non-robust equivalents. Enhanced robust performance is also shown on a complex, simulated, dexterous robotic hand. Several investigative experiments offer further understanding of the robustness framework, including its adaptation to a different continuous control RL algorithm and the learning of the uncertainty set using offline data. Performance videos are available at https://sites.google.com/view/robust-rl.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0419", "problem_id": "04190001", "content": "This study introduces an innovative approach for detecting microaneurysms (MAs) in color fundus images to facilitate early diabetic retinopathy screening. As MAs often represent the initial signs of diabetic retinopathy, precise identification is crucial for timely intervention. A deep neural network employing dropout training and maxout activation classifies each image pixel as MA or non-MA, eliminating the need for preprocessing or manual feature extraction. The proposed technique significantly outperforms conventional MA detection methods, which rely on sequential steps of preprocessing, feature extraction, classification, and post-processing. Evaluations on the Retinopathy Online Challenge (ROC) and Diaretdb1v2 datasets demonstrate state-of-the-art performance.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0420", "problem_id": "04200001", "content": "Conventional neural networks allocate equal computational resources to each example, disregarding the input's inherent complexity. Existing methods that adjust computation based on the example typically determine a fixed computational graph at inference time, without considering external computational constraints or variable inference time limits. This study enhances the efficiency and computational awareness of neural sequence models, specifically Transformers, during inference by leveraging conditional computation. By modifying the Transformer architecture to enable conditional execution of operations based on the output of a learned control network, and training the model in a multi-task setting with varying computational budgets, a single model is developed that can operate at different points on the computation-quality trade-off curve. The proposed approach is evaluated on WMT English-French Translation and Unsupervised representation learning (BERT), demonstrating that the Conditional Computation Transformer (CCT) is comparable to standard Transformers when utilizing its full computational capacity, and outperforms equivalent baselines when operating under reduced computational budgets.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0421", "problem_id": "04210001", "content": "We introduce a technique for identifying part boundaries in 3D point cloud representations of shapes. This approach utilizes a graph convolutional network to estimate the likelihood of a point being situated at the junction between multiple parts. The boundary detection framework is versatile, capable of training for both semantic part divisions and geometric primitives frequently employed in 3D modeling. Experimental results indicate that our method produces boundaries with higher precision and greater alignment with ground-truth data than competing approaches. Additionally, we showcase the network's utility in detailed semantic shape segmentation, where it enhances part-labeling accuracy.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0422", "problem_id": "04220001", "content": "This paper introduces a reinforcement learning method that uses an online tree-based Bayesian framework. The inference process utilizes a generalised context tree model, establishing a distribution over multivariate Gaussian piecewise-linear models that allows for closed-form updates. The cover tree method is used to construct the tree structure, ensuring efficiency even with high-dimensional data. By integrating this model with Thompson sampling and approximate dynamic programming, effective exploration strategies are achieved in unfamiliar environments. The model's adaptability and computational efficiency make it well-suited for addressing various reinforcement learning challenges in continuous state spaces, as demonstrated through experimental comparisons with least squares policy iteration.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0423", "problem_id": "04230001", "content": "The increasing popularity of Online Social Networks (OSNs) has attracted significant attention from both advertisers and researchers who are interested in leveraging their capabilities. Researchers are working to create methods for understanding how information spreads among users within an OSN, using diffusion or influence models. This work examines influence models for the IM-RO problem, a new formulation of the Influence Maximization (IM) problem that uses Stochastic Dynamic Programming (SDP). Unlike current methods that rely on influence spread and submodular functions, the SDP approach emphasizes maximizing clicks and, consequently, revenue for advertisers in OSNs. While influence maximization has been extensively studied in recent years and applied to various fields, our method offers a more practical alternative to the original IM problem. This paper analyzes the influence models of the IM-RO problem through experiments on both synthetic and real-world datasets. We introduce a Bayesian and Machine Learning approach for estimating the parameters of these influence models for the (Influence Maximization- Revenue Optimization) IM-RO problem. Specifically, we present a Bayesian hierarchical model and apply the Naive Bayes classifier (NBC), Decision Trees classifier (DTC), and Random Forest classifier (RFC) to three real-world datasets. Our approach to estimating influence model parameters offers a significant advantage over existing methods by being readily implementable in standard software packages like WinBUGS/OpenBUGS/JAGS and Apache Spark. The effectiveness and applicability of our methods are demonstrated in the context of OSNs, focusing on information dissemination and revenue generation for advertisers.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0424", "problem_id": "04240001", "content": "Scene flow refers to the three-dimensional motion occurring within a three-dimensional environment. It can be approached either as a singular task or inferred from the supporting tasks of stereo depth estimation and optical flow analysis. Although the latter method can attain real-time performance through the application of real-time auxiliary techniques, it generally results in sparse data. In this discourse on a fundamental combined approach for estimating scene flow, we will address the issue of sparsity using interpolation methods.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0425", "problem_id": "04250001", "content": "This study revisits the NOTEARS framework for continuous optimization in Bayesian network learning. Initially, it extends prior algebraic descriptions of acyclicity to a broader category of matrix polynomials. In the context of a one-parameter-per-edge setup, the analysis reveals that the Karush-Kuhn-Tucker (KKT) optimality conditions for NOTEARS are only met in trivial scenarios, clarifying algorithmic behavior. The work then derives KKT conditions for an alternative formulation, demonstrating their necessity and linking them to explicit constraints on edge exclusion. Despite the constraint's non-convexity, these conditions become sufficient for local minima when the score function is convex. Leveraging these insights, a post-processing local search method is introduced, significantly enhancing structural Hamming distance across all tested algorithms—often doubling performance. Certain hybrid approaches integrating local search outperform the original NOTEARS in both accuracy and computational efficiency.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0426", "problem_id": "04260001", "content": "The increasing prevalence of autonomous vehicles has made LiDAR data indispensable, as its 360-degree horizontal field of view point clouds enhance situational awareness for self-driving systems. Although synthetic LiDAR data generation pipelines offer a valuable means to further machine learning research on LiDAR, they are limited by substantial rendering times. Physically accurate LiDAR simulators, such as Blensor (Gschwandtner et al., 2011), demand significant computational resources, requiring 14-60 seconds per frame for urban environments, a problem often mitigated by using simplified 3D models as seen in CARLA (Dosovitskiy et al., 2017), which results in less realistic LiDAR point clouds. Here, we introduce a new approach to simulate LiDAR point clouds with improved rendering speeds of 1 second per frame, leveraging spherical UV unwrapping of Equirectangular Z-Buffer images. Comparing our method against the baseline simulator Blensor (Gschwandtner et al., 2011), we observed a reported error of 4.28cm for complex urban terrains within a 2-120 meter range, using Velodyne HDL64-E2 parameters. Our method achieved a total processing time of 3.2 +/- 0.31 seconds per frame, while the BlenSor baseline method required 16.2 +/- 1.82 seconds.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0427", "problem_id": "04270001", "content": "Developing machine learning models utilizing EEG data collected outside of controlled laboratory environments necessitates techniques that are resilient to noisy information and randomly absent channels. This demand is especially critical when dealing with sparse EEG montages (1-6 channels), which are common in consumer-grade or mobile EEG devices. Traditional machine learning techniques and deep neural networks that are trained end-to-end on EEG data are generally not configured or evaluated for resilience against corruption, particularly regarding randomly missing channels. Although certain studies have suggested methods for handling missing channel data, these solutions are often impractical when employed with sparse montages and limited computational resources, such as in wearables and mobile devices. To address this issue, we introduce dynamic spatial filtering (DSF), a multi-head attention mechanism that can be integrated before the first layer of a neural network, enabling it to manage missing EEG channels by learning to prioritize reliable channels while disregarding unreliable ones. We evaluated DSF on public EEG datasets consisting of approximately 4,000 recordings with simulated channel corruption, as well as on a private collection of around 100 at-home recordings of mobile EEG data with natural corruption. Our proposed method matches the performance of baseline models in the absence of noise but surpasses them by as much as 29.4% accuracy in scenarios with substantial channel corruption. Additionally, the outputs generated by DSF are interpretable, allowing for real-time assessment of channel relevance. This method has the potential to facilitate EEG analysis in complex environments where channel corruption obstructs the interpretation of brain signals.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0428", "problem_id": "04280001", "content": "Establishing correspondences between data samples is a crucial task, with applications spanning representation learning, image translation, generative modeling, and spatial deformation estimation, where feature vectors or spaces are mapped to each other. For these mappings to be well-behaved, they should exhibit regularity, which can be either explicitly enforced or inherently derived from the data. This study investigates the factors that contribute to regularity in spatial transformations, such as image registration, and examines whether it is feasible to achieve spatial regularity using only an inverse consistency loss. Unlike traditional optimization-based models that rely on explicit regularizers for well-posedness, and recent deep learning approaches that leverage the sample population to avoid regularizers, we demonstrate that deep networks combined with an inverse consistency loss and randomized off-grid interpolation can produce well-behaved, approximately diffeomorphic spatial transformations. Our experimental results on both synthetic and real data provide compelling evidence that regular maps can be obtained without relying on carefully tuned explicit regularizers, while achieving competitive registration performance, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0429", "problem_id": "04290001", "content": "Curiosity-driven exploration has proven beneficial in environments with limited external rewards, offering intrinsic motivation for agents to explore and learn. However, current curiosity methods often lack nuanced understanding of environmental dynamics due to their generalized perspective on state transitions. To address this, we propose a curiosity framework grounded in question answering, where agents are motivated to ask and seek answers about their surroundings, becoming curious when these answers evolve. We demonstrate that using natural language questions allows the agent to acquire targeted knowledge regarding their environment, including object properties and spatial relationships. This focused information acquisition yields more effective curiosity rewards, leading to improved performance in solving sparse-reward tasks.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0430", "problem_id": "04300001", "content": "We investigate a simplified dynamic assortment planning problem during a selling season of limited duration T. At each time interval, the seller presents an incoming customer with a selection of interchangeable products, from which the customer chooses a product based on a discrete choice model. The seller's objective is to maximize expected revenue, or equivalently, to reduce the worst-case expected regret. A significant challenge arises from the fact that the seller does not initially know the utilities of the products and must learn them. Although dynamic assortment planning has garnered increased interest in revenue management, much of the existing literature relies on multinomial logit choice models (MNL). In this paper, we explore the dynamic assortment planning issue under a broader choice framework—the nested logit model, known for capturing hierarchical choice behavior and being \"the most widely used member of the GEV (generalized extreme value) family.\" By utilizing the revenue-ordered structure of the optimal assortment within each nest, we introduce an innovative upper confidence bound (UCB) policy featuring an aggregated estimation approach. Our policy effectively learns customer choice behavior while making dynamic assortment decisions grounded in current knowledge. It achieves an accumulated regret in the order of \\tilde(), where M denotes the number of nests and N represents the number of products in each nest. We also establish a lower bound result of \\Omega(), indicating the near-optimal nature of the upper bound when T is significantly larger than M and N. Additionally, when the number of items per nest N is substantial, we present a discretization heuristic to enhance our algorithm's performance. Numerical results are provided to illustrate the empirical effectiveness of the algorithms we propose.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0431", "problem_id": "04310001", "content": "A camera mounted on a person's head captures objects that are significant to the wearer, but traditional methods for detecting these important objects rely on manual labeling of first-person data, which is a costly and limited process due to the need for the camera wearer to provide importance labels based on their internal state, such as intentions and attention. This work proposes an alternative approach that eliminates the need for supervision by the camera wearer or third-party labelers, instead formulating the important object detection problem as an interaction between two agents: a segmentation agent that generates potential important object masks and a recognition agent that predicts important objects using visual semantics and spatial features. This interaction is implemented through an alternating cross-pathway supervision scheme within the proposed Visual-Spatial Network (VSN), which comprises spatial and visual pathways that learn common visual semantics and spatial location cues, respectively, with one pathway providing predictions to the segmentation agent, which in turn generates a candidate important object mask used as a supervisory signal for the other pathway, as demonstrated by the method's success on two important object datasets, achieving comparable or superior results to supervised methods, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0432", "problem_id": "04320001", "content": "This review explores the challenges of creating interpretable and explainable machine learning models, which are essential in fields like medicine, economics, law, and the natural sciences. While no universally accepted definitions for interpretability and explainability exist, numerous techniques have emerged over the past three decades, with recent attention turning to deep learning approaches. The paper highlights the distinction between interpretability and explainability, presenting state-of-the-art examples to clarify these research paths. Aimed at a broad machine learning audience, the review encourages engagement with interpretation and explanation beyond conventional methods like logistic regression or random forest variable importance. Rather than providing a comprehensive survey, it serves as an introductory guide, selectively focusing on research directions the authors deem insightful or noteworthy.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0433", "problem_id": "04330001", "content": "How can we formalize the problem of credit assignment in reinforcement learning? Conventional wisdom often points to the scarcity of rewards as a significant factor complicating credit assignment, with traditional strategies favoring the principle of temporal recency as a solution, as exemplified by the well-known eligibility trace. We assert that the challenge in credit assignment does not stem from the infrequency of the reward itself, but rather from another underlying issue. To address this, we utilize information theory to define this issue, allowing us to elucidate the conditions under which credit assignment becomes a barrier to effective learning. From this angle, we present various information-theoretic methods for assessing credit within a fixed behavior policy, emphasizing the promise of information theory as a vital instrument for establishing provably efficient credit assignment.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0434", "problem_id": "04340001", "content": "We introduce Aggregated Wasserstein, a framework designed to measure the dissimilarity between two Hidden Markov Models (HMMs) where state-conditional distributions are Gaussian. In these Gaussian mixture model-HMMs (GMM-HMMs), the marginal distribution at any time point forms a Gaussian mixture, allowing for soft state alignment. This alignment leverages the connection between optimal transport and the Wasserstein metric, where components of the marginal GMMs are matched by solving an optimal transport problem with the Wasserstein distance as the cost for Gaussian components. The resulting solution provides an efficient approximation of the Wasserstein metric between GMMs. The Aggregated Wasserstein distance is a semi-metric that avoids Monte Carlo sampling and remains invariant to state permutations. It also accommodates HMMs with differing dimensionalities, which may arise from missing variables. This distance captures dissimilarity by evaluating both marginal GMM differences and transition matrix disparities. Evaluated on retrieval, classification, and t-SNE visualization tasks, the proposed distance outperforms existing Kullback-Leibler divergence-based methods in accuracy and efficiency across synthetic and real-world datasets.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0435", "problem_id": "04350001", "content": "The process of style transfer involves transforming an image's content to mimic the style of another image, with the latter serving as a representation of the desired style. However, according to Liu et al. (2017), conventional style transfer approaches, such as those proposed by Gatys et al. (2016) and Johnson et al. (2016), often fail to capture the depth of the content image, a crucial aspect of human perception. To address this limitation, Liu et al. (2017) suggest incorporating an additional regularizer into the optimized loss function to preserve the depth map, although these traditional methods are either computationally inefficient or require training a separate neural network for each style. In contrast, the AdaIN method developed by Huang et al. (2017) enables efficient style transfer without requiring a separate model for each style, but it does not preserve the depth map of the content image. This study proposes an extension of the AdaIN method, which allows for the preservation of depth maps by applying variable stylization strength, and both qualitative analysis and user evaluation study results demonstrate that the proposed method yields superior stylizations compared to the original AdaIN style transfer method, as shown in Figure A, B, C (Huang et al., 2017; Liu et al., 2017).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0436", "problem_id": "04360001", "content": "Knowledge graphs in practical applications commonly exhibit a skewed distribution of relation frequencies, with many relations appearing infrequently. This characteristic has spurred research into low-shot learning techniques that can effectively generalize from limited data. However, current methodologies are primarily designed for static knowledge graphs and struggle to adapt to temporal knowledge graphs, where data scarcity is exacerbated, particularly with the introduction of novel relations. To overcome this limitation, we introduce a one-shot learning framework designed for temporal knowledge graph link prediction. Our framework utilizes a self-attention mechanism to model temporal entity interactions and a network to assess the similarity between a query and a single example. Experimental results on established benchmarks demonstrate that our method surpasses existing state-of-the-art approaches, particularly for relations with limited occurrences.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0437", "problem_id": "04370001", "content": "This study focuses on modeling and forecasting temporal events within intelligent transportation systems. To utilize shared information across various events, we introduce a multi-task learning approach. A support vector regression model is designed for jointly analyzing interdependent time series, extending existing regularization-based multi-task learning from classification to time series analysis. We examine the relationships among observed time series, initially employing dynamic time warping to group similar sequences. Subsequently, we incorporate both temporal and scale variations, proposing an alignment method for multiple time series by estimating their underlying latent structure. The models are evaluated using travel demand prediction data from Nancy's public transport network (France), demonstrating the advantages of multi-task learning.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0438", "problem_id": "04380001", "content": "Partial orders and directed acyclic graphs are frequently encountered data structures that naturally emerge in various fields and applications, serving to depict ordered relationships among entities in these areas. Illustrative cases include dependencies between tasks in project planning, the sequence of transactions in distributed ledgers, and the order of task execution in computer programs, among others. This study focuses on the challenge of performing order-preserving hierarchical clustering on such ordered data. Specifically, if the relationship a < b exists in the original dataset, denoted by their clusters as [a] and [b], it should follow that [a] < [b] in the resulting clustering. The clustering is based on similarity and utilizes standard linkage functions, including single and complete linkage, representing an extension of conventional hierarchical clustering. To facilitate this, we define the results from executing traditional hierarchical clustering on strictly ordered data as partial dendrograms, which are sub-trees of classical dendrograms containing multiple connected components. Subsequently, we construct an embedding of partial dendrograms over a set into the collection of ultrametrics pertaining to the same set. An optimal hierarchical clustering is characterized as the partial dendrogram that aligns with the ultrametric most closely approximating the original dissimilarity measure, determined via the p-norm. Therefore, this approach merges traditional hierarchical clustering with ultrametric fitting. A reference implementation is utilized for conducting experiments on both synthetic random data and genuine datasets sourced from a machine parts database. The experimental results indicate that our method outperforms existing approaches in terms of cluster quality and preservation of order.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0439", "problem_id": "04390001", "content": "This paper introduces non-local operations as a versatile class of building blocks designed to capture long-range dependencies, complementing traditional convolutional and recurrent operations that process local neighborhoods. Drawing inspiration from the non-local means method in computer vision, our proposed non-local operation generates a response at a given position by computing a weighted sum of features from all positions. This modular building block can be seamlessly integrated into various computer vision architectures, yielding competitive or superior performance to existing state-of-the-art models on video classification tasks, as evidenced by results on the Kinetics and Charades datasets. Furthermore, our non-local models demonstrate improved performance in static image recognition tasks, including object detection, segmentation, and pose estimation on the COCO suite of tasks, with code available at https://github.com/facebookresearch/video-nonlocal-net.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0440", "problem_id": "04400001", "content": "Generative Adversarial Networks (GANs) have proven to be a highly effective method for training models to generate data that appears authentic. This paper introduces the Recurrent GAN (RGAN) and Recurrent Conditional GAN (RCGAN), designed to produce realistic, real-valued, multi-dimensional time series data, focusing on applications within the medical field. RGANs employ recurrent neural networks in both the generator and discriminator components. RCGANs extend this by conditioning both RNNs on supplementary information. The models are evaluated on several artificial datasets, with visual and quantitative assessments (using sample likelihood and maximum mean discrepancy) confirming their ability to generate realistic time-series data. Furthermore, the paper presents innovative evaluation techniques for GANs, involving the creation of a synthetic labelled training dataset and evaluating a model's performance on a real-world test set after training on the synthetic data, and vice-versa. Using these metrics, it is shown that RCGANs can generate time-series data suitable for supervised training, exhibiting only a slight decline in performance on actual test data. This is exemplified through digit classification using 'serialised' MNIST and by developing an early warning system based on a medical dataset comprising 17,000 intensive care unit patients. Finally, the study explores and analyses potential privacy issues that may emerge from the use of RCGANs in generating realistic synthetic medical time series data.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0441", "problem_id": "04410001", "content": "Polarization cameras offer significant promise for 3D reconstruction because the angle of polarization (AoP) of reflected light correlates with the surface normal of an object. This paper introduces a new 3D reconstruction technique, Polarimetric Multi-View Inverse Rendering (Polarimetric MVIR), which leverages geometric, photometric, and polarimetric information derived from multi-view color polarization images. The method begins by estimating camera poses and generating an initial 3D model through geometric reconstruction using a standard structure-from-motion and multi-view stereo pipeline. Subsequently, the initial model is refined by minimizing photometric and polarimetric rendering errors, utilizing multi-view RGB and AoP images. A key component of this refinement is a novel polarimetric rendering cost function designed to effectively constrain the estimated normal of each surface vertex, accounting for the four potential azimuth angle ambiguities inherent in AoP measurements. Experiments conducted with both synthetic and real-world data reveal that Polarimetric MVIR can reconstruct detailed 3D shapes without requiring assumptions about specific polarized reflection properties of the material.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0442", "problem_id": "04420001", "content": "In this study, award-winning deep neural networks utilizing pretrained weights are employed for gender recognition and age estimation based on images. The research investigates transfer learning through the application of both VGG19 and VGGFace pretrained models, evaluating the impacts of various design modifications and training parameters to enhance prediction accuracy. Different training methods, including input standardization, data augmentation, and label distribution for age encoding, are analyzed. Ultimately, a structured approach using deep CNNs is implemented, which first categorizes individuals by gender and subsequently applies distinct age models for males and females to estimate age. The results indicate a gender recognition accuracy of 98.7% alongside a mean absolute error of 4.1 years. This paper demonstrates that with appropriate training methodologies, satisfactory outcomes can be achieved by repurposing existing convolutional filters for a different application.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0443", "problem_id": "04430001", "content": "This study tackles the challenge of identifying anomalous processes from a predefined set through sequential selection and observation. A decision-maker observes each process individually, receiving a noisy binary signal indicating whether the process is anomalous or not. To address this, an anomaly detection algorithm is developed, which determines the process to observe at any given time, decides when to cease observations, and identifies anomalous processes. The algorithm's primary goal is to achieve a desired level of accuracy while minimizing decision-making delays. By utilizing a Markov decision process based on the conditional marginal probability of each process being normal or anomalous, given the observations, the detection algorithm is formulated. The implementation of this algorithm leverages the deep actor-critic reinforcement learning framework, yielding computational and memory requirements that scale polynomially with the number of processes, in contrast to existing methods which exhibit exponential complexity. The effectiveness of the proposed algorithm is validated through numerical experiments, where it is compared to state-of-the-art methods, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0444", "problem_id": "04440001", "content": "Molecule generation remains a significant challenge in cheminformatics, with current deep generative methods falling into two main categories based on molecular representation. The first method translates molecular graphs into text strings and trains a character-based language model, while the second, more flexible approach works directly with the graph structure. This study tackles two key drawbacks of the text-based method: the production of invalid and repetitive molecules. To enhance validity, we introduce a language model focused on small molecular fragments, drawing inspiration from Fragment-Based Drug Design principles—generating molecules fragment-wise rather than atom by atom. To increase uniqueness, we propose a masking strategy based on fragment frequency, encouraging the generation of molecules with rare fragments. Experimental results demonstrate that our model significantly surpasses other language model-based techniques, achieving performance levels comparable to graph-based methods. Additionally, the generated molecules exhibit properties consistent with the training data, even without task-specific optimization.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0445", "problem_id": "04450001", "content": "This paper presents GIMP-ML v1.1, a collection of Python plugins designed for the popular GNU Image Manipulation Program (GIMP). It facilitates the integration of recent developments in computer vision into the traditional image editing workflow. The implementation includes applications from deep learning such as monocular depth estimation, semantic segmentation, mask generative adversarial networks, image super-resolution, de-noising, de-hazing, matting, as well as enhancement and coloring methods, all accessible via Python plugins. Furthermore, image processing techniques like k-means color clustering have been incorporated. GIMP-ML utilizes standard Python libraries including numpy, pytorch, open-cv, and scipy. In addition, various image manipulation methods using these plugins have been compiled and showcased on the YouTube channel (https://youtube.com/user/kritiksoman) to illustrate machine learning applications in image modification. Moreover, GIMP-ML aims to integrate the advantages of deep learning networks utilized in computer vision tasks into everyday image processing practices. The code and installation guidelines for setting up these plugins can be found at https://github.com/kritiksoman/GIMP-ML.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0446", "problem_id": "04460001", "content": "In web-based multimedia applications, textual and visual elements often exhibit pairwise relationships that suggest shared semantic meanings. This research explores cross-modal learning by leveraging these pairwise constraints to uncover latent common structures across diverse data types. We introduce a compound regularization framework designed to handle pairwise constraints, serving as a versatile foundation for cross-modal algorithm development. For unsupervised scenarios, we present a subspace clustering approach that identifies shared structures among modalities. In supervised settings, we develop a matching technique using compound ?21 regularization with an iterative reweighting scheme to minimize semantic discrepancies and outlier effects while achieving global optimization. Comprehensive experimental results highlight the advantages of jointly modeling text and image data through semantic pairwise constraints, confirming that our cross-modal approaches effectively bridge semantic gaps and enhance clustering/retrieval performance.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0447", "problem_id": "04470001", "content": "The effectiveness of deep learning, particularly in image classification applications such as automated melanoma detection, is significantly influenced by knowledge transfer. Deep learning models typically require extensive training data, which can be scarce in medical applications, making transfer learning—a technique that repurposes knowledge from models trained on other tasks—a valuable solution. While transfer learning is commonly used in state-of-the-art melanoma screening systems, a thorough assessment of its implementation has been lacking. This study examines the role of transfer learning, identifies optimal source tasks, and evaluates the benefits of fine-tuning (post-transfer model retraining). Additionally, we explore whether deeper, more computationally intensive models yield better performance. Our findings indicate that deeper models, pre-trained on ImageNet and fine-tuned, achieve superior results, with AUC scores of 80.7% and 84.5% on the two skin-lesion datasets tested.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0448", "problem_id": "04480001", "content": "Generative Adversarial Networks (GANs) are capable of producing high-quality samples in generative modelling tasks, but are often plagued by the mode collapse issue, whereas Variational Autoencoders (VAE) explicitly optimize a reconstruction-based data log-likelihood, ensuring coverage of all modes, albeit at the cost of inferior sample quality. Although recent studies have attempted to combine the strengths of both models through hybrid VAE-GAN frameworks, which incorporate a GAN-based synthetic likelihood into the VAE objective, these efforts have been met with limited success due to the inherent trade-off between data log-likelihood and divergence from the latent prior in the VAE objective, as well as instability in the synthetic likelihood ratio term during training. To address these challenges, we introduce a novel objective that utilizes a \"Best-of-Many-Samples\" reconstruction cost and a stable direct estimate of the synthetic likelihood, enabling our hybrid VAE-GAN framework to simultaneously achieve high data log-likelihood and low divergence to the latent prior, resulting in significant improvements over existing hybrid VAE-GANs and traditional GANs in terms of mode coverage and sample quality.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0449", "problem_id": "04490001", "content": "In recent years, both assistant driving and self-driving technologies have garnered significant attention. Nonetheless, most research predominantly emphasizes safe driving, with limited studies addressing in-vehicle climate control or assistant driving tailored to travelers' individual habits and preferences. This paper introduces an innovative approach for climate control, driver behavior recognition, and driving recommendations aimed at better aligning with drivers' preferences during their daily journeys. The proposed algorithm comprises three key components: (1) an in-vehicle sensing and context feature enhancement module utilizing an Internet of Things (IoT) platform to gather pertinent environmental, vehicle operation, and traffic data that influence driver behavior; (2) a non-intrusive intelligent detection module for recognizing driver behavior and vehicle status, capable of automatically identifying the vehicle's condition (such as open windows or air conditioning activation) through advanced feature extraction and machine learning techniques; and (3) a personalized component for learning driver habits and recommending preferences to enhance comfort and health. We have developed a prototype based on a client-server architecture, which includes an iOS application and an air-quality monitoring sensor, to collect heterogeneous data and evaluate our algorithms. Comprehensive real-world tests, utilizing driving data from 11,370 km (320 hours) across various drivers in multiple cities globally, have been conducted, demonstrating the effectiveness and accuracy of our approach.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0450", "problem_id": "04500001", "content": "Several reinforcement learning approaches face impracticality in real-world applications due to their high sample complexity. Transfer learning techniques like Q-function reuse can mitigate this issue by lowering sample requirements, thereby enhancing the applicability of current algorithms. Previous studies have demonstrated the empirical success of Q-function reuse across diverse environments in model-free contexts. However, no theoretical analysis has yet examined the regret bounds of Q-function reuse in tabular, model-free scenarios. This work seeks to connect theoretical and empirical research on Q-function reuse by offering theoretical insights into its efficacy when integrated with the Q-learning and UCB-Hoeffding algorithm. Our key result establishes that, under certain conditions, Q-function reuse achieves regret independent of the state or action space when applied to this algorithm. Additionally, we present experimental evidence validating our theoretical conclusions.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0451", "problem_id": "04510001", "content": "The fact that deep learning models rely heavily on data can be both advantageous and limiting. While they can learn optimal models for specific problems with sufficient training data, this is often not feasible, leading to models being learned from limited data or pre-trained on different problems and fine-tuned, which can result in suboptimal performance and limited generalizability. Motivated by this challenge, we explore approaches to enhance deep learning models for geospatial image analysis by incorporating domain knowledge, particularly in scenarios with limited training data or when applying models to new contexts. We leverage the inherent spatial patterns and relationships on the Earth's surface, which remain relatively consistent across locations, to develop a novel feature pooling method for convolutional neural networks, utilizing Getis-Ord Gi* analysis from geostatistics. Our experiments demonstrate that the proposed pooling function significantly outperforms traditional data-driven methods in terms of generalization performance for overhead image segmentation, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0452", "problem_id": "04520001", "content": "High-dimensional tensors, also known as multi-way data, are increasingly common in fields like biomedical imaging, chemometrics, networking, and bibliometrics. Conventional methods for reducing the dimensionality of tensor data involve either flattening the data and applying matrix factorizations like principal components analysis (PCA) or using tensor decompositions such as the CANDECOMP / PARAFAC (CP) and Tucker decompositions. Flattening can result in the loss of crucial data structure, while Higher-Order PCA (HOPCA) methods can encounter difficulties in high-dimensional spaces with numerous irrelevant features. We present frameworks for sparse tensor factorizations, or Sparse HOPCA, utilizing both heuristic algorithms and by addressing penalized optimization problems connected to the CP decomposition. Expanding on these methods, we develop approaches for general regularized tensor factorizations, multi-way Functional HOPCA, and generalizations of HOPCA for structured data. The effectiveness of our methods is demonstrated through dimension reduction, feature selection, and signal recovery tasks using simulated data, multi-dimensional microarrays, and functional MRIs.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0453", "problem_id": "04530001", "content": "Graph neural networks represent an extension of deep learning advancements for graph-structured data, with numerous variants primarily employing neighborhood aggregation to iteratively update node features by combining those of adjacent nodes. Despite growing research interest, GNNs still lag behind CNNs in computer vision and RNNs in natural language processing in terms of performance. Addressing this limitation, we examine information propagation within GNNs and introduce heterogeneous aggregation to improve inter-layer information flow. We contend that enhanced information transfer from shallow to deeper layers can strengthen the discriminative power of GNN-derived features. In this initial exploration, we present a generalized GNN layer framework and introduce HAG-Net, a novel GNN variant based on this approach. Experimental results on multiple graph classification benchmarks demonstrate HAG-Net's efficacy, while we also detail its design considerations and evaluation criteria.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0454", "problem_id": "04540001", "content": "Generative Adversarial Networks (GANs) have shown promise in enhancing single image super-resolution (SISR) by reconstructing realistic details. To elevate the visual quality of super-resolved outputs, the PIRM2018-SR Challenge utilized perceptual measures like PI, NIQE, and Ma, which correlate strongly with human evaluations. Current approaches, however, struggle to directly optimize these non-differentiable perceptual metrics. To overcome this limitation, we introduce RankSRGAN, a Super-Resolution GAN incorporating a Ranker to guide the generator toward improved perceptual performance. Our method involves training a Ranker to emulate perceptual metric behavior and introducing a novel rank-content loss to enhance perceptual quality. Notably, this approach leverages the strengths of various SR techniques to produce superior results. Comprehensive experiments demonstrate that RankSRGAN delivers visually appealing outputs and achieves leading performance in perceptual metrics. Project page: https://wenlongzhang0724.github.io/Projects/RankSRGAN", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0455", "problem_id": "04550001", "content": "Decision trees serve as a key technique in induction research and data mining, primarily employed for classification and predictive modeling. Among decision tree algorithms, ID3 remains the most prevalent, though it has limitations, such as favoring attributes with numerous values. This paper addresses this issue by introducing an enhanced version of ID3, where attributes are grouped and evaluated using selection measure 5. If the information gain is insufficient, attribute values are further subdivided iteratively until an optimal classification-to-misclassification ratio is achieved. The proposed algorithm demonstrates improved accuracy and efficiency in classifying datasets.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0456", "problem_id": "04560001", "content": "Federated learning systems often face limitations due to communication expenses and variability in training outcomes. While strategies like model compression, partial device involvement, and intermittent aggregation are commonly used to mitigate communication overhead, they can amplify training variance. Unlike conventional distributed learning, federated learning contends with data heterogeneity, leading to increased inter-device variance during training. Although variance-reduction methods have been developed to address this issue, they typically demand extra communication resources for control information transmission. Furthermore, data privacy is paramount in federated learning, prompting the integration of Differential Privacy to balance utility and privacy. This study explores the trade-off between communication costs and training variance within a resource-limited federated system, both theoretically and empirically, while also examining the interactions of communication reduction techniques in a differentially private context. The findings offer significant insights for creating practical, privacy-conscious federated learning systems.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0457", "problem_id": "04570001", "content": "Industrial bin picking systems commonly determine a workpiece's position by aligning a CAD model with a 3D sensor-acquired point cloud. However, differentiating flat workpieces from the bin's bottom within the point cloud presents localization challenges, often resulting in incorrect or phantom detections. To address this, we introduce a framework that automatically segments workpiece regions from non-workpiece regions in point cloud data. This is achieved in real time using a fully convolutional neural network, trained on both simulated and real data, where the real data is labeled using our novel technique for automatic ground truth label generation. Our framework not only enables real-time workpiece segmentation but also improves the detection rate and pose estimation accuracy. Furthermore, it reduces computation time by approximately 1s by decreasing the search space during object pose estimation.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0458", "problem_id": "04580001", "content": "Current Generative Adversarial Network (GAN)-based image-to-image translation models utilize a cycle-consistency loss to learn the mapping between source and target domains, operating on the principle that translating an image to another domain and back should yield the original image. However, these methods typically employ symmetric network architectures for both forward and backward translations. Due to varying task complexities and input characteristics between domains, significant disparities arise in bidirectional cycle translations, resulting in unequal information transfer. This paper addresses the limitations of symmetric GANs in asymmetric translation scenarios by introducing AsymmetricGAN, which employs translation and reconstruction generators of varying sizes and parameter-sharing strategies. This design adapts to the inherent asymmetry present in unsupervised and supervised image-to-image translation tasks. Furthermore, to mitigate model collapse, a common issue during training that reduces the quality of generated images, we explore alternative optimization losses to enhance AsymmetricGAN's training, leading to improved consistency and stability in image translation. Comprehensive experiments across several publicly available datasets demonstrate that AsymmetricGAN exhibits superior model capacity and generation performance compared to existing GAN models in both supervised and unsupervised generative tasks. We believe this is the first study to explore the asymmetric GAN framework for both unsupervised and supervised image-to-image translation tasks. The source code, data and trained models are available at https://github.com/Ha0Tang/AsymmetricGAN.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0459", "problem_id": "04590001", "content": "This study introduces an online initialization approach for optimization-based monocular visual-inertial odometry (VIO), enabling real-time calibration of spatial transformations and temporal offsets between the camera and IMU while estimating initial metric scale, velocity, gravity, and sensor biases. The method incorporates short-term motion interpolation techniques for pose estimation to mitigate time offset effects and employs a three-stage refinement process. Initially, extrinsic rotation, gyroscope bias, and time offset are derived by minimizing rotational discrepancies between the camera and IMU. Next, approximate values for metric scale, gravity, and extrinsic translation are computed using corrected camera poses, temporarily disregarding accelerometer bias. Finally, these estimates are refined by incorporating accelerometer bias and gravitational constraints. A nonlinear optimization algorithm, accounting for time offsets, is applied for global and local state refinement. Evaluations on public datasets demonstrate the method’s accuracy in estimating initial states, extrinsic parameters, and sensor poses.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0460", "problem_id": "04600001", "content": "This paper presents an economical approach for generating image descriptions involving unfamiliar objects. Typically, developing models capable of interpreting images with novel objects is expensive due to two factors: (1) the need for extensive category-specific data collection, and (2) the requirement to retrain the entire system. Inspired by human ability to infer properties of new objects by relating their appearance to familiar ones after minimal exposure, we develop a technique that explains images containing novel objects without retraining. This method leverages word embeddings derived from limited image features of the objects and can be incorporated into standard image-captioning frameworks. Experimental evaluations demonstrate the efficacy of our proposed solution.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0461", "problem_id": "04610001", "content": "We introduce our three-branch approach developed for the International Challenge on Activity Recognition at CVPR2019. This model aims to integrate comprehensive information from global video clips, brief human attention, and extended human activities into a cohesive framework. We participated in two distinct challenges: Task A, the Kinetics challenge, and Task B, the spatio-temporal action localization challenge. For the Kinetics challenge, we attained an error rate of 21.59%. In the AVA challenge, our final model achieved a mean Average Precision (mAP) of 32.49% on the test sets, surpassing all entries to the AVA challenge at CVPR 2018 by over 10% mAP. Looking ahead, we plan to incorporate human activity knowledge, a new dataset that includes key aspects of human activity.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0462", "problem_id": "04620001", "content": "The increasing prevalence of superpixel segmentation in computer vision has highlighted the need for a hierarchical approach, as objects can be effectively represented at various levels of detail, ranging from fine-grained segments to broader regions. However, existing methods struggle to generate accurate superpixels across multiple scales in real-time. To address this limitation, this paper introduces the Super Hierarchy (SH) algorithm, a straightforward yet efficient solution that achieves state-of-the-art accuracy while being 1-2 orders of magnitude faster. By integrating SH with advanced edge detectors, such as structured forest edges, significant improvements in segmentation accuracy can be achieved, outperforming current state-of-the-art methods. The effectiveness of the proposed method is demonstrated through quantitative and qualitative evaluations across various computer vision applications, establishing it as the top-performing approach, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0463", "problem_id": "04630001", "content": "Monitoring individuals in densely populated video sequences is a crucial aspect of visual scene analysis. As crowd density rises, human visibility diminishes, restricting the effectiveness of current pedestrian tracking methods in high-density scenarios. To address this, we introduce the Crowd of Heads Dataset (CroHD), containing 9 sequences with 11,463 frames, featuring more than 2,276,838 annotated heads and 5,230 tracks across varied environments. We also present IDEucl, a novel evaluation metric that assesses a tracking algorithm's ability to maintain consistent identities over extended periods in image coordinates, linking crowd dynamics to tracking performance. Additionally, we develop HeadHunter, a specialized detector for identifying small heads in crowded settings, enhanced with a Particle Filter and a color histogram-based re-identification module for improved tracking. By benchmarking our approach against leading pedestrian trackers on CroHD, we show superior performance, particularly in identity retention metrics. Our lightweight head detector and identity-aware tracker offer significant advancements for pedestrian tracking in dense crowds.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0464", "problem_id": "04640001", "content": "To minimize expenses related to storing, processing, and visualizing extensive point clouds, we explore a randomized resampling approach. This approach aims to choose a characteristic subset of points that retains features crucial for specific applications. Our strategy leverages graphs, which effectively represent underlying surfaces and facilitate efficient computation. We utilize a versatile feature-extraction operator to capture application-specific features and introduce a comprehensive reconstruction error metric to assess resampling quality. By minimizing this reconstruction error, we derive a general optimal resampling distribution. This distribution inherently maintains invariance to shift, rotation, and scale transformations in 3D space. Subsequently, we define the feature-extraction operator as a graph filter and investigate distinct resampling strategies based on all-pass, low-pass, and high-pass graph filtering, as well as graph filter banks. We then apply our methods to large-scale visualization, accurate registration, and robust shape modeling. Empirical results confirm the effectiveness and efficiency of the proposed resampling techniques.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0465", "problem_id": "04650001", "content": "Recent image colorization techniques demonstrate promise in generating realistic colors from grayscale images, but often lack a deeper understanding of image semantics. To overcome this limitation, we introduce a novel approach that leverages pixelated object semantics to enhance colorization, based on the principle that humans associate colors with object categories. Utilizing an autoregressive model, we generate distributions of image colors and sample diverse colorizations. Object semantics are integrated into the colorization process through two mechanisms: a pixelated semantic embedding and a pixelated semantic generator. Our proposed convolutional neural network architecture consists of two branches: one dedicated to object recognition and the other to color prediction. The network is trained end-to-end by simultaneously optimizing a color embedding loss, a semantic segmentation loss, and a color generation loss. Evaluations on PASCAL VOC2012 and COCO-stuff datasets demonstrate that our method, trained with semantic segmentation labels, achieves more realistic and detailed colorization results compared to existing state-of-the-art techniques.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0466", "problem_id": "04660001", "content": "The challenge of autonomous unpowered flight for control and guidance systems lies in the need to directly extract all operational energy from the atmosphere. This study focuses on developing an algorithm aimed at optimizing the closed-loop control of a glider's bank and sideslip angles while operating within the lower convective layer of the atmosphere to enhance mission endurance. By employing a Reinforcement Learning framework, we highlight the potential for real-time adjustments to the glider’s performance in response to the variable and unpredictable conditions typical of thermal soaring flight. Our method is characterized as online, data-driven, and model-free, which allows us to circumvent the limitations of aerological and aircraft modeling, enabling us to address uncertainties and dynamic environments effectively. Furthermore, we prioritize maintaining low computational demands to ensure the feasibility of on-board execution. This paper outlines the stochastic, time-dependent aerological model utilized for simulation alongside a conventional aircraft model. Subsequently, we present a modified Q-learning algorithm and illustrate its effectiveness in controlling the aircraft and boosting endurance by leveraging updrafts in variable scenarios.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0467", "problem_id": "04670001", "content": "This paper investigates the challenge of identifying crucial structures in graphs using an embedding model, a task that existing models struggle with due to their inability to accurately detect task-specific structures at a global level. To address this limitation, we introduce the Ego-CNNs model, a novel graph embedding approach that leverages ego-convolutions at each layer and employs an ego-centric layer stacking method to efficiently detect precise critical structures. The Ego-CNN model can be trained jointly with a task-specific model, enabling it to provide insights into and discover knowledge relevant to the task. Our comprehensive experimental evaluation demonstrates that Ego-CNNs achieve comparable performance to state-of-the-art graph embedding models, facilitate effective visualization of detected structures using CNN visualization techniques, and offer efficient training that can be further enhanced by incorporating scale-free priors commonly found in social network datasets, as shown in Figure A, B, C (References [1], [2], and [3]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0468", "problem_id": "04680001", "content": "We present a probabilistic framework for interpreting attention mechanisms, demonstrating that conventional dot-product attention in transformers represents a specific instance of Maximum A Posteriori (MAP) inference. Our method advocates employing Expectation Maximization algorithms to dynamically adjust key and value parameters during inference. This technique proves particularly beneficial when external sources, such as annotators, supply real-time information about certain tokens—for instance, the semantic labels of specific pixels—requiring systematic propagation of such updates across related tokens. We validate the approach through an interactive semantic segmentation application, where annotators and models jointly enhance labeling efficiency. Empirical results on established benchmarks indicate that key adaptation enhances model accuracy (∼10% mIoU) with limited feedback, while value propagation improves responsiveness under extensive feedback conditions. A PyTorch implementation of our probabilistic attention layer will be accessible at: https://github.com/apple/ml-probabilistic-attention.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0469", "problem_id": "04690001", "content": "Automated segmentation of skin lesions in dermoscopic images is a crucial aspect of computer-assisted melanoma diagnosis. Nonetheless, this process is complicated by the considerable diversity in lesion appearances among various patients. The difficulty increases notably when managing large volumes of image data. This study builds upon our earlier research by introducing a deeper network architecture featuring smaller kernels aimed at improving its discriminative ability. Moreover, we incorporated color information from different color spaces to support network training, thereby enhancing segmentation performance. Our approach was rigorously tested during the ISBI 2017 skin lesion segmentation challenge. By utilizing the 2000 training images from the challenge, our method attained an average Jaccard Index (JA) of 0.765 on the 600 testing images, securing the top position in the challenge.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0470", "problem_id": "04700001", "content": "Machine learning models, particularly deep neural networks (DNNs), have proven effective across diverse applications like computer vision, natural language processing, and speech recognition; however, their complexity often renders them as uninterpretable black-box systems. This paper introduces a novel adversarial machine learning algorithm designed to elucidate the predictions of DNNs. Our method assesses the significance of input features relative to predictions by analyzing the effects of adversarial attacks on the DNN. The proposed algorithm is characterized by its speed, consistency, and ease of implementation and interpretation. Through detailed analysis, we demonstrate the consistent behavior of adversarial attacks across various input test data points for a given DNN and task, thereby affirming the generality of our approach, enabling consistent and efficient explanations. The efficacy of our approach is showcased through experiments involving various DNNs, tasks, and datasets. A comparative analysis against established techniques in the literature is also provided.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0471", "problem_id": "04710001", "content": "A significant obstacle in scaling self-driving technology is the ability to produce accurate, low-cost high-definition maps (HD maps). Existing efforts to automate this mapping process generally concentrate on straightforward scenarios, estimate separate maps for each frame, or lack the precision needed for contemporary self-driving vehicles. In this study, we concentrate on delineating lane boundaries on complex multi-lane highways featuring topological alterations due to forks and merges. To achieve this, we model the problem as inference in a directed acyclic graphical model (DAG), where the graph's nodes represent the geometric and topological characteristics of the local areas around the lane boundaries. Since the lane topology is not predetermined, we also infer the DAG topology (i.e., nodes and edges) for each area. We validate the effectiveness of our method on two major North American highways in different states, achieving high precision and recall along with an accuracy of 89% in topology correctness.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0472", "problem_id": "04720001", "content": "In many natural and physical systems, particularly biological ones, multiple measurement types are often collected from the same underlying system. To integrate such diverse data and enhance system understanding, it is essential to align the manifolds derived from each measurement. We address this challenge using generative adversarial networks (GANs), which have recently been employed to identify correspondences between sample sets, though they lack explicit design for manifold alignment. We introduce the Manifold-Aligning GAN (MAGAN), specifically developed to align two manifolds by matching related points across measurement spaces. We apply MAGAN to single-cell biology, integrating genomic (single-cell RNA-sequencing) and proteomic (mass cytometry) data from the same tissue. Our results show that MAGAN effectively aligns these measurements, enhancing known marker correlations compared to other recent models.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0473", "problem_id": "04730001", "content": "This paper explores the application of reinforcement learning to create learning agents capable of recognizing formal languages. The agents are structured as basic multi-head automata—a novel type of finite automaton employing multiple heads—and six distinct languages are framed as reinforcement learning challenges. Two optimization algorithms are implemented: Q-learning, which utilizes gated recurrent units to acquire optimal policies, and a genetic algorithm, which seeks optimal solutions through evolution-based operations. The findings indicate that the genetic algorithm generally outperforms Q-learning; however, Q-learning discovers solutions more rapidly for regular languages.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0474", "problem_id": "04740001", "content": "As camera networks have increasingly proliferated over the last ten years, scholarly attention in video management has transitioned towards analytics within multi-camera networks. This encompasses executing functions such as object detection, identifying attributes, and tracking vehicles and individuals across various cameras without overlapping views. Existing management frameworks cater to multi-camera environments within closed datasets, characterized by limited camera variability and well-understood surveillance conditions. Additionally, these frameworks are tailored for offline analytics, relying on human oversight for forensic purposes. This paper introduces a collaborative classifier framework designed for video analytics in diverse many-camera networks facing challenging conditions, such as multi-scale and multi-resolution cameras that capture the scene with inconsistent occlusion, blur, and angles. We outline an implementation focused on vehicle tracking and re-identification (re-id), featuring a zero-shot learning (ZSL) system that automates vehicle tracking continuously. Our assessments on VeRi-776 and Cars196 demonstrate that the teamed classifier framework is resilient to adverse conditions, adaptable to evolving video attributes like new vehicle types and brands as well as new cameras, and provides real-time performance in contrast to existing offline video analytics methods.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0475", "problem_id": "04750001", "content": "Many Siamese network-based trackers operate without updating their models, limiting their ability to adapt to target-specific changes. Additionally, these trackers typically predict object states using axis-aligned bounding boxes, which often include irrelevant background noise and fail to precisely capture rotation and scale transformations, leading to degraded tracking accuracy. To overcome these limitations, we introduce the Rotation-Scale Invariant Network (RSINet), which incorporates a target-distractor discrimination branch and a rotation-scale estimation branch. These components enable explicit learning of rotation and scale variations through multi-task learning in an end-to-end framework. Furthermore, the tracking model is dynamically optimized and updated via spatio-temporal energy control, ensuring stability, reliability, and efficient performance. Extensive evaluations on OTB-100, VOT2018, and LaSOT benchmarks show that RSINet achieves superior state-of-the-art results while maintaining real-time processing at approximately 45 FPS.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0476", "problem_id": "04760001", "content": "Creating convolutional neural networks (CNNs) for mobile platforms presents difficulties due to the necessity for models to be compact, rapid, and precise. Despite considerable research dedicated to refining mobile CNNs across various aspects, manually optimizing the balance between these competing factors remains complex given the vast number of potential architectures. This paper introduces a mobile neural architecture search (MNAS) method that integrates model latency directly into the primary optimization goal, facilitating the identification of models that effectively balance accuracy and speed. Diverging from prior studies that rely on indirect and often imprecise latency proxies (e.g., FLOPS), our method assesses actual inference latency by running the model on mobile devices. To optimize flexibility against search space size, we introduce a novel factorized hierarchical search space promoting diverse layer configurations throughout the network. Empirical evaluations demonstrate that our method consistently surpasses current state-of-the-art mobile CNN models across different vision tasks. On the ImageNet classification task, our MnasNet attains 75.2% top-1 accuracy with a latency of 78ms on a Pixel phone, which is 1.8x faster than MobileNetV2 [29] with 0.5% increased accuracy and 2.3x faster than NASNet [36] with a 1.2% accuracy improvement. Furthermore, our MnasNet achieves enhanced mAP compared to MobileNets in COCO object detection. Code is at https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0477", "problem_id": "04770001", "content": "This paper introduces novel algorithms designed to estimate heterogeneous treatment effects, integrating recent advancements in neural network transfer learning with established principles from causal inference. By leveraging transfer learning techniques, the proposed approach facilitates the efficient utilization of diverse datasets linked to shared causal processes. The algorithms' performance is evaluated against existing methods through comprehensive simulations grounded in large-scale voter persuasion experiments and the MNIST database. Results indicate that the presented methods achieve an order of magnitude improvement over current benchmarks, while requiring significantly less data.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0478", "problem_id": "04780001", "content": "Recently, multi-view clustering has garnered significant interest due to its utilization of information from multiple perspectives. Nevertheless, current multi-view clustering techniques often suffer from elevated computational and spatial demands or exhibit limited representational power. To mitigate these limitations, this paper introduces a deep embedded multi-view clustering framework with collaborative training (DEMVC). Initially, deep autoencoders are used to independently learn embedded representations for each view. Subsequently, a novel collaborative training strategy is introduced to leverage both the consensus and complementary aspects of the multiple views. Specifically, feature representations and cluster assignments for all views are learned in a collaborative manner. Furthermore, a novel consistency strategy is developed for initializing cluster centers, thereby enhancing multi-view clustering performance with collaborative training. Empirical evaluations conducted on several widely used multi-view datasets demonstrate that DEMVC outperforms existing state-of-the-art methods by a significant margin.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0479", "problem_id": "04790001", "content": "Current approaches to contrastive representation learning typically involve estimating the mutual information (MI) between multiple perspectives of a shared context, which can be achieved by applying data augmentation to an image or dividing a sequence into past and future components. Although contrastive lower bounds on MI are relatively easy to optimize, they are prone to significant underestimation when dealing with large amounts of MI. To address this limitation, we introduce a novel framework that decomposes the overall MI estimation into a series of smaller sub-problems by progressively subdividing one of the views into more informed sub-views and applying the chain rule to the MI between these decomposed views. This decomposition yields a sum of unconditional and conditional MI terms, each capturing a manageable portion of the total MI, thereby facilitating approximation through contrastive bounds. By formulating a contrastive lower bound on the conditional MI, we can efficiently maximize this sum. Our proposed method, termed Decomposed Estimation of Mutual Information (DEMI), demonstrates its capability to capture a greater amount of MI than conventional non-decomposed contrastive bounds in a synthetic setting, and learns more effective representations in both vision and dialogue generation domains.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0480", "problem_id": "04800001", "content": "In recent years, characterizing the brain as a functional network, where the links between brain regions are depicted through correlation values derived from time series, has gained significant attention. While this portrayal has enhanced our comprehension of brain functionality, it simplifies the intricate dynamic spatio-temporal characteristics of brain connectivity. This simplification may limit the benefits of utilizing sophisticated non-linear feature extraction algorithms. To address this issue, we introduce a dynamic adaptive spatio-temporal graph convolution (DAST-GCN) model aimed at overcoming the limitations of fixed, correlation-based graph structures. Our method facilitates end-to-end inference of dynamic interconnections between brain regions through a layer-wise graph structure learning module, simultaneously linking brain connectivity to a phenotype within a supervised learning context. This approach harnesses the computational resources of the model, data, and targets to effectively portray brain connectivity, potentially leading to the discovery of biomarkers related to the supervised target. We assess our approach using the UKBiobank dataset for age and gender classification tasks based on resting-state functional scans, demonstrating superior performance compared to currently utilized linear and non-linear methods in neuroimaging. Moreover, we evaluate the generalizability of the derived graph structure by applying the pre-trained graph to an independent dataset for the same tasks, revealing the robustness of the graph across varying scanning parameters and demographic groups.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0481", "problem_id": "04810001", "content": "Spectral normalization (SN) is a commonly employed method to enhance the training stability and output quality of Generative Adversarial Networks (GANs), yet its underlying effectiveness remains poorly understood. This study demonstrates that SN mitigates two critical GAN training failures: gradient explosion and vanishing gradients. Our analysis reveals an implicit link between SN and the well-known LeCun initialization, clarifying why the standard SN implementation for GANs operates effectively without hyperparameter adjustments, while more rigid SN variants perform poorly by default. Unlike LeCun initialization, which only prevents gradient vanishing at training onset, SN maintains this stability throughout the learning process. Leveraging these insights, we introduce Bidirectional Scaled Spectral Normalization (BSSN), a novel technique that integrates advancements from subsequent initialization methods like Xavier and Kaiming initialization. Theoretically, BSSN provides superior gradient regulation compared to SN, and empirical results confirm its improved performance in sample quality and training stability across multiple benchmark datasets.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0482", "problem_id": "04820001", "content": "Policy distillation in deep reinforcement learning serves as an effective method for transferring control policies from a larger, more complex network to a smaller, untrained network while maintaining performance levels. Nonetheless, policy distillation remains relatively underexplored within deep reinforcement learning, with current methods being computationally demanding, which leads to prolonged distillation durations. Furthermore, the success of the distillation process is often constrained by the model's capacity. We introduce a novel distillation approach, termed real-time policy distillation, whereby the teacher model's training and the simultaneous policy distillation to the student model occur concurrently. This allows the most current policy of the teacher to be conveyed to the student model in real-time, effectively halving the distillation time or even reducing it further. Additionally, this method enables very small student models to acquire expert-level skills. We tested the proposed algorithm within the Atari 2600 environment, and the findings indicate that our method can accomplish full distillation in the majority of games, even with compression ratios reaching up to 1.7%.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0483", "problem_id": "04830001", "content": "Heterogeneous Domain Adaptation (HDA) is a transfer learning approach that tackles the challenge of adapting data from source and target domains with disparate modalities, such as text and images, or differing feature dimensions, which is particularly useful in multi-modal data analysis. Conventional domain adaptation algorithms are limited by their assumption that source and target samples share the same feature space, rendering them ineffective in addressing HDA problems. Recent state-of-the-art HDA methods often rely on complex optimization objectives, which can be computationally expensive and lack generalizability. To overcome these limitations, this study proposes a novel algorithm, Cross-Domain Structure Preserving Projection (CDSPP), which extends the traditional LPP to heterogeneous domains. CDSPP learns domain-specific projections that map source and target domain features into a shared subspace, preserving class consistency and aligning data distributions, with the advantage of being simple and having deterministic solutions obtained through a generalized eigenvalue problem. The proposed method is inherently suitable for supervised HDA and can be extended to semi-supervised HDA, where unlabeled target domain samples are available. Comprehensive experiments were conducted on benchmark datasets, including Office-Caltech, Multilingual Reuters Collection, NUS-WIDE-ImageNet, and the newly introduced Office-Home dataset, which features a significantly larger number of classes than existing datasets. The results of both supervised and semi-supervised HDA experiments demonstrate the superiority of the proposed CDSPP method over contemporary state-of-the-art approaches, as shown in Figure A, B, C (References [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0484", "problem_id": "04840001", "content": "A simpler approach to image reconstruction and manipulation has yielded surprisingly strong results, leveraging texture loss alone to generate high-quality images that are perceptually impressive. By exploring the mechanism of texture constraints, we have developed a novel method that utilizes semantic guidance to further enhance texture constraining, resulting in state-of-the-art outcomes when evaluated using the LPIPS metric, which relies on \"deep features\". Notably, our findings indicate that texture representations of these deep features provide a more accurate capture of an image's perceptual quality than the original deep features. Furthermore, we demonstrate that off-the-shelf deep classification networks, without requiring training, can perform equally well as the best-performing LPIPS metrics when utilizing texture information, and the code for this method is publicly available.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0485", "problem_id": "04850001", "content": "Inferring the network structure from data is a crucial task in machine learning. This paper proposes a new prior distribution for investigating the inference of scale-free networks, commonly employed to represent social and biological networks. This prior encourages a favorable global node degree distribution while also accounting for the relative strengths of potential edges connected to a single node and the estimated degree of each node. To achieve this, ranking is integrated into the prior, increasing the complexity of the optimization. We address this challenge by using an ADMM (alternating direction method of multipliers) framework to solve the Gaussian Graphical model regularized by this prior. Empirical evaluations on synthetic and real-world datasets demonstrate that our prior not only generates a scale-free network but also achieves a significantly higher number of accurately predicted edges compared to alternative methods, including the scale-free inducing prior, the hub-inducing prior, and the l_1 norm.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0486", "problem_id": "04860001", "content": "As smart grid technology becomes increasingly widespread, short-term load forecasting (STLF) plays a critical role in power system management. While numerous STLF methods exist, choosing the most appropriate one under diverse conditions remains difficult. This study introduces a new dynamic model selection (DMS) approach for STLF using reinforcement learning. First, a forecasting model pool is created, incorporating ten advanced machine learning-based models. A Q-learning agent then determines the optimal policy for selecting the best forecasting model for the next time step, based on performance metrics. The optimal DMS policy is implemented with a moving window to choose the most suitable model at each step. Simulations using two years of load and weather data demonstrate rapid convergence of the Q-learning algorithm, leading to efficient DMS. The proposed Q-learning-based DMS STLF model achieves around 50% higher accuracy than existing machine learning-based STLF methods.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0487", "problem_id": "04870001", "content": "Time series anomaly detection is crucial for ensuring the reliability of systems. Nevertheless, real-world applications often lack a clear distinction between normal and abnormal patterns, necessitating the use of varied anomaly detection algorithms and procedures tailored to specific time series contexts. While this approach enhances detection accuracy, configuring numerous algorithms for extensive datasets presents a significant time burden for practitioners, escalating the costs associated with developing and maintaining anomaly detection systems. To address this, we introduce CRATOS, a self-adaptive algorithm that extracts time series features and clusters series with comparable characteristics. An evolutionary algorithm is then employed to identify optimal anomaly detection methods and processes for each cluster. Our method substantially lowers the costs of anomaly detection development and maintenance. Experimental results demonstrate that our clustering technique achieves state-of-the-art performance, with the anomaly detection algorithms reaching an accuracy of 85.1%.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0488", "problem_id": "04880001", "content": "Chromosome karyotype analysis plays a crucial role in diagnosing and treating various diseases, particularly genetic disorders. Given the significant time and effort required for manual analysis, computer-assisted automated chromosome karyotype analysis utilizing images is commonly employed to enhance both efficiency and accuracy. The elongated shape of chromosomes can lead to overlaps when imaged, which negatively impacts the subsequent accuracy of the analysis. Traditional methods for segmenting overlapping chromosomes often rely on manually identified features, making them susceptible to variations in image quality, such as resolution and brightness. To tackle this issue, this paper introduces an adversarial multiscale feature learning framework aimed at enhancing the accuracy and adaptability of overlapping chromosome segmentation. We utilize a nested U-shaped network with dense skip connections as the generator to uncover the optimal representation of chromosome images by leveraging multiscale features. Furthermore, a conditional generative adversarial network (cGAN) is employed to produce images similar to the originals, with improved training stability achieved through the application of the least-square GAN objective. Lastly, we implement Lovasz-Softmax to facilitate model convergence within a continuous optimization framework. Our framework demonstrates superior performance compared to established algorithms, validated across eight evaluation metrics using public datasets, highlighting its significant potential in overlapping chromosome segmentation.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0489", "problem_id": "04890001", "content": "The optimization phase-ordering challenge in contemporary compilers, despite considerable research, largely persists. Current optimization sequences available to users are typically handcrafted by compiler engineers who must select the optimization passes, their corresponding parameters, and their order within a sequence. These sequences often fail to achieve optimal runtime for specific source code and can sometimes even diminish performance relative to unoptimized code. In this work, we apply deep reinforcement learning to address the phase-ordering problem. Trained on sub-sequences derived from LLVM's O3 sequence, our agent surpasses the O3 sequence on training source codes and demonstrates competitive performance on a validation set, achieving speed improvements of up to 1.32x on previously unseen programs. Unlike autotuning methods, our approach doesn't rely on program test runs for optimization decisions; instead, it depends solely on statically derived intermediate representations of the source code, without requiring any dynamic features. We propose that models trained with our method can be integrated into modern compilers as neural optimization agents, initially to augment and ultimately to supersede manually created optimization sequences.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0490", "problem_id": "04900001", "content": "Estimating causal effects necessitates fundamental assumptions concerning the data-generating process, including the direction of the effect, the existence of instrumental variables or mediators, and the observation of all pertinent confounders. Any breach of these assumptions results in considerable inaccuracies in the effect estimate. Unlike the approach of cross-validation in predictive modeling, there is no overarching validation method for causal estimates. Therefore, it is essential to formally articulate various causal assumptions and, where feasible, validate them in any analysis. We introduce DoWhy, a framework that facilitates the explicit declaration of assumptions via a causal graph and offers several validation tests to assess a subset of these assumptions. Our experience with DoWhy raises several research inquiries for the future, including the creation of new methods beyond causal graphs for expressing assumptions, exploring the role of causal discovery in identifying relevant aspects of the graph, and developing validation tests that more effectively reveal errors for both average and conditional treatment effects. DoWhy can be accessed at https://github.com/microsoft/dowhy.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0491", "problem_id": "04910001", "content": "This paper introduces a new transformation technique for event camera data that ensures equivariance to optical flow within 3-D spatiotemporal convolutions. Since event cameras capture changes in image intensity primarily caused by motion, the resulting events are inherently motion-dependent. To address this issue in static-scene learning tasks like classification, traditional approaches either require the learning model to disentangle objects from motion or necessitate extensive data augmentation to account for all possible motions. Our proposed transformation normalizes the x and y positions of each event using its timestamp, producing an event representation equivariant to constant optical flow. This enables deep neural networks to learn classification tasks effectively without substantial data augmentation. The method's performance was evaluated on the N-MNIST dataset and a newly introduced N-MOVING-MNIST dataset, which exhibits greater motion diversity than the standard N-MNIST. Results demonstrate that the transformed network achieves comparable or superior performance to networks using standard volumetric event input and significantly outperforms them when tested on datasets with motion patterns not encountered during training.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0492", "problem_id": "04920001", "content": "Recent advancements in deep learning for single-view depth estimation have demonstrated significant potential, yet these approaches often overlook motion—a critical element in human depth perception. We introduce a learning-based technique for multi-view dense depth mapping and odometry estimation, employing Recurrent Neural Networks (RNNs) and leveraging multi-view image reprojection along with forward-backward flow-consistency losses during training. The model supports both supervised and unsupervised training and is tailored for depth and visual odometry estimation from temporally linked video frames, while also extending to single-view scenarios. Evaluated on the KITTI driving dataset, our approach outperforms current state-of-the-art methods in both single-view and multi-view depth estimation.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0493", "problem_id": "04930001", "content": "High-dimensional time series data are prevalent across various fields, yet their complexity poses challenges for human understanding, suggesting a need for easily interpretable, low-dimensional representations. Current representation learning algorithms often lack interpretability due to obscure mappings between data characteristics and representation features, along with temporal instability. To mitigate these issues, we introduce a novel representation learning framework that combines interpretable discrete dimensionality reduction with deep generative modeling. This framework enables the learning of discrete time series representations, yielding smooth, interpretable embeddings and enhanced clustering results. We present a novel approach to address the non-differentiability inherent in discrete representation learning and offer a gradient-based self-organizing map algorithm, outperforming the traditional method. By incorporating a Markov model into the representation space, we enable a probabilistic interpretation, which reveals temporal transition structures, enhances clustering accuracy, provides further explanatory insights, and naturally represents uncertainty. We assess our model's clustering performance and interpretability using static (Fashion-)MNIST data, linearly interpolated (Fashion-)MNIST image time series, a chaotic Lorenz attractor system, and a complex real-world medical time series application from the eICU data set. The resulting representations demonstrate competitive performance compared to other methods and facilitate downstream tasks using real-world data.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0494", "problem_id": "04940001", "content": "Existing NAS approaches for GNNs primarily concentrate on identifying various layer aggregation components through shallow and simplistic architectures, which are constrained by the 'over-smooth' issue. To better leverage structural diversity and deeper GNN architectures, we introduce a GNN generation framework featuring a novel two-stage search space designed to automatically produce high-performance and transferable deep GNN models in a block-wise fashion. Additionally, to mitigate the 'over-smooth' problem, we integrate multiple adaptable residual connections within the search space and employ identity mapping in fundamental GNN layers. Our search algorithm utilizes deep Q-learning with an epsilon-greedy exploration strategy and reward reshaping. Comprehensive evaluations on real-world datasets demonstrate that our generated GNN models surpass both manually designed and existing NAS-based counterparts.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0495", "problem_id": "04950001", "content": "We introduce a nonparametric approach to modeling time series data with missing values, leveraging low-rank matrix factorization to represent each time series instance as a linear combination of a limited set of shared basis functions. By imposing nonnegativity constraints on both the functions and their corresponding coefficients, we obtain a low-dimensional and interpretable representation of the data. The incorporation of a time-smoothing regularization term enables the model to capture significant trends in the data, mitigating the risk of overfitting to short-term fluctuations. This low-dimensional representation facilitates the detection of outliers, clustering of time series based on extracted features, and forecasting using kernel regression. Our methodology is applied to a large, real-world dataset of infant sleep patterns collected via a mobile phone app, yielding daily sleep patterns that align with existing literature and enabling the computation of sleep development trends for the cohort, including the emergence of circadian sleep patterns and napping habits. Furthermore, our approach allows for the identification of anomalous individuals, clustering of the cohort into distinct sleeping tendency groups, and improved prediction of future sleep behavior, as demonstrated in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0496", "problem_id": "04960001", "content": "This study aims to develop an effective method for creating new stable multi-element chemical compounds with practical applications. Addressing this challenge as a combinatorial problem typically requires extensive human effort for data construction and evaluation. While unsupervised techniques like Generative Adversarial Networks (GANs) show promise in data generation, and cross-domain GANs have demonstrated success in image processing, materials science demands the synthesis of data with greater complexity than existing samples—a capability lacking in current cross-domain GAN implementations. To bridge this gap, we present CrystalGAN, a novel GAN designed to produce chemically stable crystallographic structures with enhanced domain complexity. Our approach includes a unique architecture, specialized loss functions, and validation showing the model's ability to generate plausible data. We demonstrate the method's effectiveness through its application to discovering new hydrides, which holds potential for advancing hydrogen storage materials.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0497", "problem_id": "04970001", "content": "This paper introduces a dynamic filtering strategy for Convolutional Neural Networks (ConvNets) named Large Sampling-field Dynamic Filtering Network (LS-DFN). This approach enables position-specific kernels to learn from both the corresponding location and numerous sampled neighboring areas. To facilitate training during sampling, residual learning is incorporated, and an attention mechanism is used to integrate features from various samples. These multiple samples effectively expand the receptive fields of the kernels without increasing the number of parameters. LS-DFN retains the benefits of DFN, specifically mitigating feature map blurring through position-wise kernels while preserving translation invariance, and it also effectively reduces overfitting, a common problem due to the higher parameter count compared to standard CNNs. The proposed model is efficient, trainable end-to-end using standard back-propagation, and its effectiveness is demonstrated on both sparse and dense prediction tasks, including object detection, semantic segmentation, and flow estimation. Experimental results on the VOC benchmark for object detection and semantic segmentation, as well as the FlyingChairs dataset for flow estimation, indicate that LS-DFN exhibits enhanced recognition capabilities and generates more refined responses compared to strong baseline models.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0498", "problem_id": "04980001", "content": "Neural network-based process monitoring is gaining increasing prominence. In comparison to traditional neural networks, higher-order neural networks offer inherent benefits when handling heteroscedastic data. Nevertheless, these networks may be susceptible to overfitting, potentially learning noise or anomalies in addition to pertinent information from the original data. Orthogonal constraints can effectively mitigate correlations among extracted features, thereby lowering the risk of overfitting. This paper introduces a new fault detection approach termed second-order component analysis (SCA). SCA addresses the heteroscedasticity of process data by optimizing a second-order autoencoder using orthogonal constraints. To tackle this constrained optimization challenge, a geometric conjugate gradient algorithm is employed, performing geometric optimization on the combination of Stiefel and Euclidean manifolds. Comprehensive experiments conducted on the Tennessee-Eastman benchmark process demonstrate that SCA exhibits superior performance in terms of missed detection rate (MDR) and false alarm rate (FAR) compared to PCA, KPCA, and autoencoder.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0499", "problem_id": "04990001", "content": "Deep learning approaches for graph-based data have demonstrated promising performance across multiple applications. Yet, the resilience of these models has received limited scrutiny compared to extensive adversarial research in image and text domains. This study examines attacks that manipulate the underlying data structure to deceive models. We introduce a reinforcement learning framework for generating transferable adversarial perturbations using only target classifier labels. Additionally, we develop genetic algorithm and gradient-based variants when prediction probabilities or gradients are accessible. Experiments on synthetic and real datasets reveal vulnerabilities in various Graph Neural Network architectures for both graph-level and node-level classification. Furthermore, we demonstrate how these adversarial strategies can serve as diagnostic tools for evaluating classifier performance.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0500", "problem_id": "05000001", "content": "The word2vec model introduced by Mikolov (2013) is a widely adopted technique for word embeddings in natural language processing. Although it has achieved significant success and widespread application, its theoretical foundations remain insufficiently explored. This paper presents a formal analysis of word2vec's highly nonlinear functional, revealing that its behavior may largely align with an underlying spectral approach. Such understanding could facilitate the derivation of theoretical guarantees for word2vec. Numerical experiments corroborate these observations. An intriguing unresolved issue is whether the nonlinear aspects of word2vec not explained by the spectral method provide advantages and, if they do, through what specific mechanisms.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0501", "problem_id": "05010001", "content": "A lesser-studied category of vision problems involves identifying objects within imagery generated from physics-based experiments, where these objects can exist in four dimensions (x, y, z, t) and manifest as disturbances against a largely uniform background distribution. These disturbances, often termed \"events,\" can be conceptualized as high-energy concentrations within the image, characterized by a limited set of features and significant variability in shape, size, and quantity, posing a challenge to the application of pre-trained models derived from supervised learning approaches. This paper presents an unsupervised methodology utilizing iterative clustering-based segmentation (ICS) for real-time detection of target objects, or \"events.\" The ICS methodology involves analyzing a test image through multiple cycles, with each cycle comprising four key steps: (1) image segmentation via a modified k-means clustering technique, (2) statistical analysis-driven elimination of empty segments, (3) merging of overlapping segments corresponding to the same event, and (4) selection of the most prominent event. This iterative process continues until all events have been identified, with the approach's hyper-parameters determined through statistical analysis of a test image dataset. The efficacy and applicability of the ICS method are demonstrated through its successful application to various 2D and 3D test cases, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0502", "problem_id": "05020001", "content": "Policy gradient (PG) approaches are well-regarded methods in reinforcement learning (RL), where a baseline is frequently utilized to lessen the variance in gradient estimates. In the context of multi-agent reinforcement learning (MARL), while the PG theorem can be easily adapted, the performance of multi-agent PG (MAPG) methods diminishes rapidly as the number of agents increases, leading to heightened variance in gradient estimates. This paper presents a thorough examination of MAPG methods by first assessing how the number of agents and their exploratory behaviors contribute to the variance of MAPG estimators. From this investigation, we formulate the optimal baseline (OB) designed to minimize variance. We also evaluate the additional variance present in current MARL algorithms, such as standard MAPG and COMA, in relation to the OB. Additionally, we introduce a surrogate version of the OB compatible with existing PG methods in MARL when deep neural networks are employed. Our OB strategy significantly stabilizes the training process and enhances the performance of multi-agent PPO and COMA algorithms, as demonstrated in benchmarks involving Multi-Agent MuJoCo and StarCraft challenges.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0503", "problem_id": "05030001", "content": "We introduce Kernel Point Convolution (KPConv), a novel point convolution method that directly processes point clouds without intermediate representations. KPConv positions its convolution weights in Euclidean space using kernel points, applying them to nearby input points. Unlike fixed grid convolutions, KPConv offers greater flexibility by supporting an arbitrary number of kernel points. These kernel point locations are spatially continuous and can be optimized during training, enabling the extension to deformable convolutions that adjust to local geometric structures. With an efficient subsampling approach, KPConv remains robust to density variations. Our networks, whether employing deformable KPConv for complex tasks or rigid KPConv for simpler ones, achieve superior performance in classification and segmentation across multiple datasets compared to existing methods. Additionally, we provide ablation studies and visualizations to analyze KPConv's learned features and demonstrate the effectiveness of deformable KPConv.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0504", "problem_id": "05040001", "content": "Recently, non-stationary spectral kernels have gained considerable interest due to their robust ability to represent features that uncover long-range correlations and input-specific properties. Nonetheless, these kernels remain shallow models, which limits their capacity to capture both hierarchical features and local interdependencies. This paper presents an interpretable convolutional spectral kernel network (\\texttt) constructed on the basis of the inverse Fourier transform, integrating deep architectures and convolutional filters into the representations of non-stationary spectral kernels to acquire hierarchical and local knowledge. Additionally, we establish generalization error bounds using Rademacher complexity and propose two regularizers to enhance performance. By combining these regularizers with recent progress in random initialization, we finalize the learning framework of \\texttt. Comprehensive experiments conducted on real-world datasets support the efficacy of the learning framework and align with our theoretical insights.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0505", "problem_id": "05050001", "content": "This study introduces a deep neural network-based method for efficiently predicting multiphase flow in complex heterogeneous environments. The model effectively manages permeability variations in high-dimensional systems and captures the interactions between viscous, gravitational, and capillary forces using limited training data. Focusing on CO2 storage applications, we show that the network accurately predicts CO2 saturation distributions based on permeability fields, injection parameters, and well locations. While demonstrating strong interpolation capabilities, the model also exhibits some capacity for extrapolation beyond its training scope. To enhance extrapolation accuracy, we present a transfer learning technique that enables rapid adaptation to new scenarios without extensive data acquisition or retraining. Additionally, we develop an online web tool for CO2-water flow simulations using this trained network. The proposed deep learning framework offers a computationally efficient alternative to conventional multiphase flow simulations, making it suitable for applications such as history matching and uncertainty analysis.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0506", "problem_id": "05060001", "content": "The annotation of histopathological images demands substantial time and effort, necessitating expert pathologists to meticulously analyze extensive whole-slide images at cellular and tissue levels. While transfer learning has shown promise in image understanding with limited labeled data, its application to histology image analysis often suffers from performance decline due to domain discrepancies, such as variations in tissue types, staining, and imaging equipment. To address this, we introduce a new unsupervised domain adaptation method for histopathology, leveraging a backbone network to embed images into a feature space, and a graph neural layer to propagate labeled data supervision. The graph neural network connects each image to its nearest neighbors in the embedded feature space and synthesizes new feature representations. During training, target samples receive dynamic pseudo-labels based on confident inferences. A cross-entropy loss function is then used to constrain predictions for both source samples with manual labels and target samples with pseudo-labels. Additionally, maximum mean diversity is employed to promote domain-invariant feature extraction, and contrastive learning is used to improve category discrimination. Experimental results demonstrate that our method achieves state-of-the-art performance on four public datasets for unsupervised domain adaptation in histopathological image classification.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0507", "problem_id": "05070001", "content": "Recent progress in image super-resolution (SR) has greatly benefitted from advancements in deep neural networks. Building on these insights, we observe that numerous cutting-edge deep SR models can be reinterpreted as a single-state recurrent neural network (RNN) with limited unfoldings. In this paper, we propose novel SR architectures that stem from this streamlined RNN perspective, resulting in a dual-state framework we term the Dual-State Recurrent Network (DSRN). Unlike single-state models that operate at a consistent spatial resolution, the DSRN simultaneously harnesses low-resolution (LR) and high-resolution (HR) signals. It facilitates the exchange of recurrent signals bidirectionally (from LR to HR and HR to LR) through delayed feedback mechanisms. Comprehensive quantitative and qualitative assessments on benchmark datasets, as well as a recent challenge, reveal that DSRN outperforms state-of-the-art algorithms in terms of memory efficiency and predictive precision.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0508", "problem_id": "05080001", "content": "Model compression strategies for Deep Neural Networks (DNN) have been widely recognized as effective means for acceleration across various platforms, with DNN weight pruning serving as a simple yet powerful approach. Currently, there are two primary categories of pruning methods that represent contrasting levels of regularity: non-structured, fine-grained pruning achieves significant sparsity and accuracy but lacks hardware compatibility; in contrast, structured, coarse-grained pruning utilizes hardware-efficient designs but can experience accuracy degradation at higher pruning rates. This paper introduces PCONV, incorporating a novel sparsity dimension—fine-grained pruning patterns within coarse-grained structures. PCONV consists of two types of sparsity: Sparse Convolution Patterns (SCP), arising from intra-convolution kernel pruning, and connectivity sparsity from inter-convolution kernel pruning. SCP enhances accuracy thanks to its unique visual characteristics, while connectivity sparsity boosts the pruning rate while ensuring a balanced workload in filter computation. To implement PCONV, we have developed an innovative compiler-assisted DNN inference framework that allows for real-time execution of PCONV models without accuracy loss, which was unattainable in previous studies. Our experimental findings demonstrate that PCONV surpasses three leading end-to-end DNN frameworks—TensorFlow-Lite, TVM, and Alibaba Mobile Neural Network—with speedups of up to 39.2x, 11.4x, and 6.3x, respectively, all without compromising accuracy. As a result, mobile devices can achieve real-time inference on large-scale DNNs.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0509", "problem_id": "05090001", "content": "Generative adversarial networks (GANs) have recently demonstrated significant promise across various computer vision applications. Given the rapid advancements in the field, GANs are expected to soon produce images and videos of such high quality that they will be nearly impossible to differentiate from authentic media. However, this capability introduces substantial security risks, primarily the potential proliferation of counterfeit multimedia content, necessitating the development of robust forensic techniques. This paper demonstrates that each GAN imprints a unique fingerprint on its generated images, analogous to the way cameras leave traces of their photo-response non-uniformity. Experiments involving source identification using several prevalent GANs reveal that these fingerprints offer valuable data for forensic investigations.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0510", "problem_id": "05100001", "content": "In recent years, computer vision has garnered substantial attention as a crucial component enabling robots to acquire knowledge about their surroundings. The performance of visual trackers is intimately tied to the practical application of robots, as they furnish essential physical and environmental parameters for mobile robots. This research presents an exhaustive survey of visual trackers, commencing with an introductory overview, followed by an in-depth examination of their fundamental framework and challenges. The study then delves into the architecture of generative and discriminative methods, cataloging the feature descriptors, modeling techniques, and learning methods employed in tracking algorithms. A comprehensive review and evaluation of cutting-edge discriminative trackers are undertaken from three distinct perspectives: correlation filters, deep learning, and convolutional features, as seen in Figure A, B, C (References [1], [2], [3]). Ultimately, the research investigates the trajectory of visual tracker development in mobile robots and delineates future trends in this domain, providing a foundation for further exploration, as discussed in citations [4] and [5].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0511", "problem_id": "05110001", "content": "We introduce a generative adversarial network architecture employing several discriminators, each trained to differentiate a specific subset of the real data. This specialization enhances the generator's ability to align with the true data distribution, thereby alleviating the persistent issue of mode collapse. Drawing inspiration from multiple choice learning, we direct each discriminator to specialize in a distinct portion of the data, enabling the generator to autonomously establish appropriate mappings between the latent space and the real data space, without requiring supervision regarding training instances or the quantity of discriminators. Although multiple discriminators are used, the shared backbone network design minimizes the added computational overhead during training. The efficacy of our method is validated on benchmark datasets using various evaluation metrics.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0512", "problem_id": "05120001", "content": "This paper introduces a new no-reference image quality assessment (NR-IQA) approach designed to estimate the perceptual quality score of an image without relying on a reference image. The method integrates three key components: (i) a visual attention mechanism, which plays a crucial role in visual perception and image quality evaluation but has been largely ignored in NR-IQA research, positing that fixation regions hold essential cues for quality assessment; (ii) a robust averaging strategy, grounded in psychological studies, to combine incremental evidence for perceptual judgment; and (iii) multi-task learning, an effective technique for enhancing representation learning and improving model generalization. By treating NR-IQA as a dynamic perceptual process, the model sequentially samples salient regions and consolidates information to jointly predict quality scores and distortion types. A reinforcement learning framework, implemented via a deep network using policy gradients and trained with back-propagation, optimizes the sampling policy by leveraging task-specific rewards to ensure accurate and efficient predictions with minimal sampling steps. Evaluated on the TID2008 dataset, the model surpasses existing state-of-the-art methods while demonstrating high efficiency through the use of few fixations.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0513", "problem_id": "05130001", "content": "Seismic inverse modeling is a prevalent technique used for reservoir prediction and is crucial in the oil and gas exploration and development process. Traditional seismic inversion methods often struggle to integrate complex geological models and assessing their uncertainty proves challenging. This paper introduces an inversion modeling technique that utilizes GAN, aligning it with geological data, well logs, and seismic information. GAN stands out as a leading generative model algorithm capable of capturing the spatial structure and abstract characteristics of training images. The trained GAN is capable of generating models that adhere to specific characteristics, and our tests demonstrated the generation of 1000 models in just 1 second. By employing the trained GAN and conducting an evaluation, we can determine the optimal model outcomes through a Bayesian inversion framework. Results indicate that the inversion models align well with observational data and exhibit low uncertainty, all while enabling rapid generation. This seismic inverse modeling approach enhances both the efficiency and quality of inversion iterations, making it a valuable method for integrating seismic data and geological insights.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0514", "problem_id": "05140001", "content": "We tackle the challenge of estimating conditional average treatment effects (CATEs) in scenarios where treatments exhibit graph-structured properties, such as molecular graphs of drugs. Under a mild condition regarding the effect, we introduce a plug-in estimation methodology that breaks down the complex task of CATE estimation into more manageable, separate optimization problems. This approach offers two key advantages: (a) it effectively isolates the causal estimands, thereby minimizing regularization bias, and (b) it enables the seamless integration of arbitrary models for learning purposes. Through empirical evaluations involving small-world and molecular graphs, our method demonstrates superior performance compared to existing approaches and exhibits robustness against varying degrees of selection biases, with the implementation being made available online.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0515", "problem_id": "05150001", "content": "The ability to incrementally expand the range of detectable object classes is a crucial requirement for real-world object detection systems. This is particularly beneficial when the detector can learn from only a small number of labeled examples, enhancing its adaptability in domains like autonomous driving and robotics. However, this sequential learning paradigm often leads to catastrophic forgetting and overfitting, especially with few-shot training. To tackle these challenges, this paper introduces a new Incremental Few-Shot Object Detection (iFSOD) method designed to enable effective continual learning from limited data. The proposed approach employs a Double-Branch Framework (DBF) to separate the feature representations of base and novel classes, facilitating both the preservation of existing knowledge and the adaptation to new classes. Additionally, a progressive model updating rule is implemented to maintain long-term memory of previously learned classes during sequential adaptation to new classes. Furthermore, an inter-task class separation loss is introduced to broaden the decision boundaries of new classes, leading to improved feature discrimination. Experimental results on Pascal VOC and MS-COCO demonstrate the effectiveness of our method in addressing incremental few-shot detection and significantly enhancing detection accuracy for both base and novel classes.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0516", "problem_id": "05160001", "content": "This workshop investigates the convergence of cognitive neuroscience and contemporary AI advancements focused on replicating human capabilities, notably in natural language processing and computer vision, with a specific emphasis on deep learning methodologies. Cognitive neuroscientists employ a system identification paradigm, presenting varied stimuli to participants and modeling the resultant activity in different brain regions. This approach seeks to elucidate brain function by determining the mathematical relationship between stimulus attributes and brain area activity. As experimental stimuli become increasingly complex, reflecting interest in real-world phenomena like natural image and sentence processing, there is a growing demand for comprehensive stimulus representations achievable through machine learning. Simultaneously, novel machine learning techniques, particularly within deep learning, draw inspiration from human behavior or biological mechanisms. While neural networks were initially modeled after biological neurons and recent developments incorporate attention mechanisms derived from human behavior, many of these methods operate independently of neuroscientific findings. Consequently, the utility of directly emulating brain function to achieve comparable machine learning outcomes remains an open question.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0517", "problem_id": "05170001", "content": "This work presents a novel Collaborative Causal Discovery framework, which addresses a scenario where multiple autonomous entities, each possessing its own causal graph, are considered, with the objective of concurrently learning all these graphs. The investigation of this problem is undertaken without relying on the causal sufficiency assumption, utilizing Maximal Ancestral Graphs (MAG) to represent the causal graphs, and assuming the capability to perform independent single vertex interventions on the entities. Under the condition that the M underlying, unknown causal graphs of the entities exhibit a clustering property, algorithms are developed that exploit this characteristic, enabling the recovery of all causal graphs using approximately logarithmic in M atomic interventions per entity, substantially reducing the requirement of n atomic interventions per entity needed to learn each graph separately, where n denotes the number of observable nodes in the causal graph, as shown in Figure A, and discussed in References [citation].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0518", "problem_id": "05180001", "content": "Efficient modeling of spatiotemporal data presents a significant yet complex challenge in video action recognition. Current leading approaches utilize motion cues to facilitate short-term temporal modeling by analyzing the temporal differences between successive frames. However, this method is prone to introducing minor noise from camera movement. Additionally, the movements associated with different actions can vary significantly. In this study, we introduce a Temporal Saliency Integration (TSI) block, comprising a Salient Motion Excitation (SME) module and a Cross-scale Temporal Integration (CTI) module. The SME is designed to accentuate motion-sensitive regions through local-global motion modeling, where saliency alignment and pyramidal feature differences are methodically applied between adjacent frames to effectively capture motion dynamics while minimizing noise due to background misalignment. The CTI module focuses on multi-scale temporal modeling using distinct groups of 1D convolutions. Furthermore, it integrates temporal interactions across various scales through an attention mechanism. Together, these modules enable efficient encoding of long and short-term temporal relationships with a minimal increase in parameters. Comprehensive experiments conducted on several widely-used benchmarks (i.e., Something-Something V1 & V2, Kinetics-400, UCF-101, and HMDB-51) validate the effectiveness and superiority of our proposed approach.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0519", "problem_id": "05190001", "content": "We have evaluated the maximum achievable recognition accuracy across various word image datasets utilizing manual segmentation along with an existing commercial OCR. A Matlab program featuring a graphical user interface has been crafted for semi-automated pixel-level segmentation of word images. The benefits of pixel-level annotation are examined. Our study encompasses five databases comprising over 3600 word images, which were sourced from camera-captured scenes, born-digital images, and street view photographs. Recognition of the segmented word images is performed using the trial edition of Nuance Omnipage OCR. We also address how distortions incurred during the image acquisition process or inaccuracies that arise during the creation of word images impact the recognition of the words contained within. Additionally, the discussion includes word images exhibiting various types of degradation, as well as methods for correcting slant and curvature in the text. The word recognition rates achieved on the ICDAR 2003, Sign evaluation, Street view, Born-digital, and ICDAR 2011 datasets are 83.9%, 89.3%, 79.6%, 88.5%, and 86.7%, respectively.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0520", "problem_id": "05200001", "content": "Highlight detection in sports videos is commercially valuable and widely appealing, necessitating improved methods for identifying highlight scenes that align with human interest and offer precise temporal accuracy. Given that blink suppression occurs during engaging moments and blinks increase at attention break points, the real-time blink rate serves as a reliable temporal indicator of human interest. This paper introduces a new automated highlight detection technique grounded in blink rate analysis. The approach employs a 1D-CNN to evaluate blink rates at each video frame using spatio-temporal pose data from figure skating videos. Experimental results demonstrate accurate blink rate estimation in 94% of the video clips and precise prediction of blink rate changes surrounding jump events. Furthermore, key frame detection encompasses both athletic actions and artistic expressions in figure skating. These findings suggest that a blink-rate-based supervised learning paradigm facilitates highly accurate highlight detection that closely mirrors human perception.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0521", "problem_id": "05210001", "content": "Airline Crew Pairing Optimization (CPO) seeks to identify the most cost-effective set of feasible flight sequences (crew pairings) that satisfy an airline's flight schedule. Typically, this is achieved via Column Generation (CG), a mathematical programming method employed for directed search-space exploration. CG leverages the relationships between successive iterations to create new variables (pairings) during the optimization process. Given the increasing scale and intricacy of modern flight networks, it is now essential to identify and apply higher-order dependencies within flight-connection graphs to improve CPO effectiveness. This paper introduces a novel adaptation of the Variational Graph Auto-Encoder to learn probable combinatorial patterns from flight-connection data acquired during search-space exploration by AirCROP (an Airline Crew Pairing Optimizer developed by the authors and validated by GE Aviation). This approach, a significant advancement over existing techniques, combines the resulting flight-connection predictions dynamically using a new heuristic to generate fresh pairings for the optimizer. The effectiveness of the proposed methodology is demonstrated using extensive (over 4200 flights), real-world, intricate flight networks from US-based airlines, featuring multiple hub-and-spoke subnetworks and numerous crew bases.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0522", "problem_id": "05220001", "content": "Image-to-image (i2i) translation involves the dense regression task of converting an input image into an output using aligned image pairs. Advances in this field have been driven by Deep Convolutional Neural Networks (DCNNs) and the application of Generative Adversarial Networks (GANs). When paired images are unavailable, i2i translation is addressed through single or multiple domain transformations, such as CycleGAN and StarGAN. This paper explores a novel challenge in i2i translation, focusing on continuous parameters that represent a physical process model. Specifically, we introduce SliderGAN, which modifies an input facial image based on the continuous values of a statistical blendshape model for facial movement. Our approach enables facial image editing guided by expression and speech blendshapes, using adjustable sliders to manipulate the model's continuous parameters. This offers enhanced flexibility for tasks like face editing, expression transfer, and face neutralization, surpassing methods reliant on discrete expressions or action units.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0523", "problem_id": "05230001", "content": "This work presents a novel family of multilayer graph kernels, bridging the gap between graph convolutional neural networks and kernel methods by extending convolutional kernel networks to graph-structured data. By representing graphs as a sequence of kernel feature maps, where each node encapsulates information about local substructures, our approach provides a flexible framework for graph representation. The kernel perspective offers a data representation that is not only expressive and easy to regularize but also unsupervised, making it particularly useful in scenarios with limited sample availability. Furthermore, our model can be trained end-to-end on large datasets, giving rise to innovative graph convolutional neural networks. As demonstrated through competitive performance on various graph classification benchmarks, our method offers a favorable balance between accuracy and model interpretability, with the accompanying code made publicly available at https://github.com/claying/GCKN.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0524", "problem_id": "05240001", "content": "This study investigates network anomaly detection through the analysis of NetFlow records. While prior research has predominantly employed supervised statistical models and machine learning methods, these approaches face challenges such as demanding extensive labeled training data and struggling to identify previously unseen zero-day attacks. Current anomaly detection systems also lack straightforward mechanisms for interpreting or pinpointing specific attacks within anomalous traffic. To overcome these issues, we introduce GEE, a novel framework designed for both detecting and explaining network anomalies. GEE integrates two key elements: (i) a Variational Autoencoder (VAE) that leverages unsupervised deep learning for anomaly detection, and (ii) a gradient-based fingerprinting method for anomaly interpretation. Testing GEE on the UGR dataset shows its effectiveness in detecting diverse anomalies and generating representative fingerprints for various attack types.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0525", "problem_id": "05250001", "content": "In this study, we tackle the challenge of eliminating haze from a single image captured at night. The removal of haze in nighttime images presents significant difficulties, primarily due to the presence of various light sources that emit different colors and exhibit uneven illumination. These light sources, which come in different shapes, create a pronounced glow in nighttime visuals. To mitigate these issues, we propose a deep learning-based iterative architecture called DeGlow-DeHaze, which considers the effects of varying color illumination and glows. Initially, our convolutional neural network (CNN)-based DeGlow model effectively reduces the glow effect, and subsequently, an additional DeHaze network is employed to eliminate the haze. For training our recurrent network, we generate hazy images and associated transmission maps from the NYU depth datasets, leading to the restoration of high-quality haze-free images. Experimental outcomes reveal that our hybrid CNN model surpasses other leading techniques in terms of computational efficiency and image quality. Additionally, we demonstrate the effectiveness of our model on several real images and compare our findings with existing heuristic models for nighttime haze removal.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0526", "problem_id": "05260001", "content": "The challenge of acquiring hierarchical policies in Reinforcement Learning that can identify options, where an option represents a sub-policy comprising a subset of fundamental actions, is examined. Over the past decade, various models have been put forth, often relying on a predetermined collection of options. This work focuses on the specific issue of automatically uncovering options within decision processes. A novel learning framework, termed the Budgeted Option Neural Network (BONN), is introduced, which enables option discovery through a budget-constrained learning objective. The efficacy of the BONN model is assessed across a range of traditional RL problems, yielding noteworthy results both quantitatively and qualitatively.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0527", "problem_id": "05270001", "content": "Ensemble classifiers have been a subject of research by various experts in the fields of artificial intelligence and machine learning. Two prevalent combination techniques within ensemble learning are majority voting and weighted majority voting. Nonetheless, the current understanding of these methods remains limited, with certain characteristics even being misconstrued. In this study, we formally present a set of properties for these two techniques within a geometric framework at the dataset level. We assess two critical aspects: the performance of each base classifier and the dissimilarity between each pair of component classifiers, using the Euclidean distance as a common metric. This transforms the ensembling process into a deterministic problem, allowing for the performance of an ensemble to be computed directly through a specific formula. We establish several noteworthy theorems and clarify their relevance to ensemble methodologies. Notably, we analyze the impact of the number of component classifiers on both ensemble schemes. Additionally, an empirical examination is carried out to validate our theoretical findings using other metrics like accuracy. We contend that the insights derived from this paper significantly enhance our comprehension of the core properties of these combination techniques and the overarching principles of ensemble classifiers. The findings also contribute to exploring critical issues in ensemble classifiers, including performance prediction and the selection of a limited number of base classifiers to create efficient and effective ensembles.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0528", "problem_id": "05280001", "content": "Clustering visual features offers an economical method for object segmentation in videos, yet current algorithms face limitations when handling scenarios involving an unspecified quantity of both stationary and moving objects amid significant camera motion. This study tackles the issue by proposing a clustering technique that utilizes superpixels and short-term Histogram of Oriented Optical Flow (HOOF), employing Salient Dither Pattern Feature (SDPF) for flow tracking and Simple Linear Iterative Clustering (SLIC) for superpixel extraction. The method merges superpixels by analyzing short-term local HOOF and color cues to generate high-level semantic segments. When evaluated against a recent K-Means-based clustering method in an eight-dimensional space, the proposed approach demonstrated superior performance in consistency, completeness, and spatial accuracy. Additionally, it effectively resolved the challenge of determining the number of objects in a scene.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0529", "problem_id": "05290001", "content": "Deep semi-supervised learning (SSL), which leverages both labeled and unlabeled data for neural network training, has gained traction due to the challenges in acquiring extensive labeled datasets. Most SSL methods rely on the low-density separation or consistency assumption, positing that decision boundaries should reside in low-density areas. However, these methods typically enforce this assumption through localized adjustments to the decision boundary at individual data points, neglecting the data's overall structure. In this study, we introduce an alternative strategy that utilizes global information from clustered data to refine decision boundaries. We present CycleCluster, a novel framework for deep semi-supervised classification, which is optimized using a new clustering-based regularization, graph-based pseudo-labels, and a shared deep network. This demonstrates that directly implementing the cluster assumption offers a feasible alternative to the prevalent consistency-based regularization. The effectiveness of our method is validated through comprehensive numerical experiments.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0530", "problem_id": "05300001", "content": "Pose-guided person image generation typically relies on paired source-target images for training supervision, which demands substantial data preparation and restricts model applicability. To address this limitation, we introduce an innovative multi-level statistics transfer framework that disentangles and transfers hierarchical appearance features from person images, combining them with pose features to reconstruct the original images. This enables self-supervised training by using the source images as supervision. Our model employs an appearance encoder to extract multi-level features, optimizing appearance representation through attention mechanisms and attribute statistics before transferring them to a pose-guided generator for appearance-pose fusion. This design facilitates flexible manipulation of both appearance and pose attributes, supporting pose transfer and clothing style adaptation tasks. Evaluations on the DeepFashion dataset show our method outperforms existing supervised and unsupervised approaches, while also demonstrating robust performance in real-world scenarios.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0531", "problem_id": "05310001", "content": "As the dependence on black-box model outputs grows in crucial applications, post-hoc explainability tools that do not require insight into model internals are frequently employed to help users comprehend and trust these models. Our focus is on a specific group of methods that can illustrate the effects of input features on predicted outcomes. Although widely used, current methods face various challenges, including computational burdens, significant uncertainties, and critically, an incapacity to address real-world domain shifts. In this paper, we introduce PRoFILE, a new method for estimating feature importance that overcomes these issues. By utilizing a loss estimator that is trained concurrently with the predictive model and follows a causal objective, PRoFILE can precisely compute feature importance scores even in the presence of complex distribution shifts, without necessitating any additional re-training. Additionally, we develop training strategies for the loss estimator, specifically contrastive and dropout calibration, which demonstrates effectiveness in identifying distribution shifts. Through empirical analyses on multiple benchmark datasets, both image and non-image, we present considerable enhancements over leading contemporary methods regarding fidelity and robustness.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0532", "problem_id": "05320001", "content": "Data cleaning is an essential component of any data analysis task. However, existing R packages lack efficient and reliable techniques for the cleaning and preparation of time series data. The open-source package tsrobprep offers advanced methods for addressing missing values and outliers using model-based strategies. A probabilistic replacement model is suggested for data imputation, which may include autoregressive elements and external variables. For outlier detection, a clustering algorithm founded on finite mixture modeling is introduced, incorporating conventional time series characteristics as factors. By calculating the probability of each observation being an outlier, the extent of its outlyingness can be assessed. The methods are robust and fully configurable. Additionally, the functionality auto_data_cleaning enables data preprocessing to be performed in a single step, eliminating the need for manual adjustments while yielding satisfactory outcomes. Although the primary focus of the package is on preprocessing energy system data, it is also applicable to other moderate to large time series datasets. Applications are demonstrated for electricity load, wind, and solar energy data.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0533", "problem_id": "05330001", "content": "Neural network classifiers can inadvertently capture visual similarities between categories when trained on annotated data, even without explicit instruction. This phenomenon is investigated beyond traditional supervised learning, with a focus on learning feature representations that discern similarities among individual instances rather than classes, by simply requiring the feature to distinguish between instances. This concept is formalized as a non-parametric instance-level classification problem, addressed using noise-contrastive estimation to overcome computational hurdles stemming from a large number of instance classes. Experimental results show that, in unsupervised learning settings, the proposed method significantly outperforms state-of-the-art results on ImageNet classification, and consistently enhances test performance with increased training data and improved network architectures. Furthermore, fine-tuning the learned feature yields competitive results in semi-supervised learning and object detection tasks, as seen in Figure A, B, C (References: [citation]). Notably, the non-parametric model is highly compact, requiring only 600MB of storage for one million images with 128 features per image, thereby facilitating rapid nearest neighbour retrieval at runtime.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0534", "problem_id": "05340001", "content": "The growing use of artificial intelligence has led to the exposure of inherent security and privacy weaknesses in machine learning systems, including the potential for model inversion attacks, which enable an adversary to extract private information about the training data used to develop a targeted model. Traditionally, these attacks involve sequentially utilizing classification scores to obtain high-confidence representations of various classes, but this approach often yields unrecognizable and useless representations, particularly for deep neural networks. This paper proposes a more realistic and refined definition of model inversion, where the adversary has knowledge of the model's general purpose, such as optical character recognition or facial recognition, and aims to discover realistic class representations within a lower-dimensional manifold relevant to the model's purpose, such as symbols or faces. By exploiting the properties of generative adversarial networks, we construct a connected lower-dimensional manifold and demonstrate the effectiveness of our model inversion attack within this manifold, as shown in Figure A (Reference [1], Citation [2]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0535", "problem_id": "05350001", "content": "Recent years have witnessed a surge in the adoption of Convolutional Neural Networks (CNNs) alongside the development of specialized accelerators to enable efficient CNN inference. While architectural research has predominantly focused on accelerating image recognition tasks, video recognition—despite its higher computational demands and anticipated dominance in internet traffic—has received comparatively less attention as an accelerator target. This work bridges the gap between algorithmic progress and hardware innovation for video recognition by presenting a comprehensive design space exploration and a flexible architecture tailored for 3D Convolutional Neural Networks (3D CNNs), which form the foundation of contemporary video analysis. Unlike their 2D counterparts used in image processing, 3D CNNs present unique engineering hurdles due to their substantial and dynamically varying memory requirements, as well as increased dimensionality. To overcome these challenges, we introduce Morph, an adaptive accelerator capable of dynamically adjusting spatial and temporal tiling strategies based on layer-specific requirements in target 3D CNNs. A co-designed software framework complements the Morph hardware to optimize control parameters. When tested on cutting-edge 3D CNNs, Morph demonstrates energy savings of up to 3.4x (2.5x on average) and performance-per-watt improvements of up to 5.1x (4x average) relative to a baseline 3D CNN accelerator, with only a 5% area overhead. Additionally, Morph achieves an average 15.9x energy reduction compared to Eyeriss for 3D CNN workloads.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0536", "problem_id": "05360001", "content": "Convolutional neural networks (CNN) have the ability to learn robust representations through various regularization methods and activations, owing to the spatial correlation inherent in convolutional layers. This property has led to the development of numerous regional dropout strategies, including Cutout, DropBlock, and CutMix, which aim to improve network generalization by selectively occluding distinctive object regions. Nevertheless, these methods randomly perform occlusion without identifying the most critical regions within objects. This paper introduces Attentive CutMix, an enhanced augmentation strategy that builds upon CutMix, where the most descriptive regions are selected based on intermediate attention maps from a feature extractor during each training iteration, allowing for the identification of the most discriminative image parts. The proposed approach is straightforward, effective, and easy to implement, yielding significant improvements to the baseline. Comprehensive experiments conducted on CIFAR-10/100 and ImageNet datasets using various CNN architectures in a unified setting demonstrate the superiority of the proposed method, consistently outperforming baseline CutMix and other methods by a substantial margin, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0537", "problem_id": "05370001", "content": "Semantic scene understanding is crucial for numerous applications, especially for self-driving vehicles that require detailed knowledge of nearby surfaces and objects. Light detection and ranging (LiDAR) technology, which offers precise geometric environmental data, is therefore integrated into almost all self-driving car sensor systems. However, despite the significance of semantic scene understanding for this application, a substantial dataset based on automotive LiDAR for this purpose is currently lacking. This paper introduces a large dataset designed to advance research in laser-based semantic segmentation. We have annotated all sequences from the KITTI Vision Odometry Benchmark, offering dense, point-wise annotations across the complete 360^ field-of-view of the automotive LiDAR used. Utilizing this dataset, we propose three benchmark tasks: (i) semantic segmentation of point clouds using a single scan, (ii) semantic segmentation using multiple past scans, and (iii) semantic scene completion, which involves predicting the future semantic scene. We present baseline experiments that highlight the need for more advanced models to effectively address these challenges. Our dataset facilitates the creation of more sophisticated methodologies and provides ample data for exploring novel research avenues.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0538", "problem_id": "05380001", "content": "This research investigates the problem of discovering compressed state representations from Markov state trajectories in high-dimensional spaces, focusing on scenarios where the transition kernel possesses a low intrinsic rank. Drawing inspiration from diffusion maps, we introduce a computationally efficient approach to learn a low-dimensional state embedding that effectively captures the dynamics of the process. This methodology further enables a kernel reshaping technique, enhancing the precision of nonparametric transition function estimation. The derived state embedding facilitates the clustering of states into metastable sets, enabling the identification of slow dynamics. We establish rigorous statistical error bounds and misclassification rates. Experimental results on a simulated dynamical system demonstrate the efficacy of the state clustering method in revealing metastable structures. Furthermore, experiments using time series data generated by layers of a Deep-Q-Network playing an Atari game show that the embedding method can identify similarities between game states based on shared future events, even when the raw data exhibits significant differences.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0539", "problem_id": "05390001", "content": "This paper introduces a one-dimensional convolutional neural network developed for plant leaf classification. The evaluation of this network-based classifier is conducted from two perspectives. First, the proposed network functions both as a standalone classifier and as an automatic feature extractor. When employed as a classifier, it utilizes a basic centroid contour distance curve as its sole input feature, achieving competitive results compared to advanced techniques that typically need multiple extracted features. When used as a feature extractor, it generates nearly linearly separable features, enhancing performance when combined with other classifiers like support vector machines. The proposed network uses straightforward one-dimensional input, making it adaptable to other applications, such as end-to-end classification of one-dimensional time series data without modification. Experiments conducted on standard benchmark datasets demonstrate that this architecture yields classification accuracies that meet or exceed those of established methods. Second, techniques such as gradient-weighted class activation mapping and maximum activation map analysis of neurons in the classification layer are applied to understand and confirm that latent features influencing the trained classifier's decisions are human-interpretable. The code is available at https://github.com/dykuang/Leaf_Project.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0540", "problem_id": "05400001", "content": "Deep learning models experience catastrophic forgetting in incremental learning scenarios. This study introduces a new method to tackle task incremental learning, where models are trained on sequentially arriving tasks. The challenge intensifies when test sets include unseen classes, forming a task incremental generalized zero-shot learning problem. Our solution applies to both zero-shot and non-zero-shot task incremental learning settings. The proposed technique employs weight rectifications and affine transformations to adjust the model for sequential tasks, modifying previous weights with minimal parameters. Additionally, affine transformations refine network outputs for new tasks. Experiments across multiple datasets demonstrate state-of-the-art performance, surpassing existing non-zero-shot task incremental learning methods by over 5% on CIFAR-100. For generalized zero-shot learning, our approach exceeds current benchmarks by 6.91% and 6.33% on AWA1 and CUB datasets, respectively. Ablation studies further confirm the method's effectiveness.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0541", "problem_id": "05410001", "content": "In the context of reinforcement learning (RL), we address the opacity of deep neural networks, where agents learn to maximize rewards without control, posing a risk in complex environments with vast state spaces, as it becomes impractical to anticipate and penalize all undesirable outcomes. In contrast to prior works that analyze learned neural features retrospectively, our approach integrates a disentangled representation learning method into an RL policy network, enabling the learning of interpretable latent features. This allows the RL agent to develop self-awareness by differentiating its own effects from uncontrollable environmental factors, akin to human scene understanding. Our experiments demonstrate that the learned latent factors are not only interpretable but also facilitate modeling the distribution of the entire visited state space under specific action conditions, and this property of our proposed structure can be leveraged to establish ex post facto control over the desired behaviors of RL agents, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0542", "problem_id": "05420001", "content": "The application of random alterations to ground truth data, including random translations or scaling of bounding boxes, serves as a widely used heuristic for data augmentation, effectively mitigating overfitting and enhancing generalization. However, since design decisions for data augmentation are primarily influenced by established best practices, assessing the optimality of these choices remains challenging. To offer a more systematic viewpoint, we propose a game-theoretic framework for understanding data augmentation in object detection. Our objective is to identify optimal adversarial perturbations of the ground truth data (specifically, the most challenging perturbations) that compel the object bounding box predictor to learn from the most difficult distribution of altered examples, thereby enhancing performance during testing. We demonstrate that the game theoretic outcome, specifically the Nash equilibrium, yields both an optimal predictor and an optimal distribution for data augmentation. Our adversarial training approach significantly enhances test performance in object detection tasks, achieving more than a 16% improvement on the ImageNet object detection benchmark compared to the leading data augmentation technique.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0543", "problem_id": "05430001", "content": "We introduce a novel methodology for learning from graph-structured data, specifically tailored to the task of graph classification. A key component of our approach is the development of a new readout operation that effectively aggregates node features to form a representation at the graph level. This is achieved by utilizing persistent homology, which is computed using a learnable, real-valued filter function. By establishing a theoretical framework for differentiating through persistent homology computations, we demonstrate the efficacy of our readout operation, showing that it outperforms existing techniques, particularly in scenarios where the graph's connectivity structure provides valuable information for the learning task.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0544", "problem_id": "05440001", "content": "Camera traps are widely employed for wildlife observation globally. Although Deep Learning (DL) models are becoming more accessible, their practical application in wildlife monitoring remains constrained by technical complexity and substantial computational demands. This study introduces the lightweight, cutting-edge YOLOv5 framework for automated labeling of mammal images captured by camera traps in Poland's Bialowieza Forest (BF). The data were structured and standardized using TRAPPER, an open-source tool designed for large-scale wildlife monitoring projects. The developed image recognition system attained an 85% F1-score average accuracy in detecting 12 prevalent medium and large mammal species in BF, utilizing a modest dataset of 2659 annotated animal images. Initial findings suggest that YOLOv5, enhanced through transfer learning, offers a viable lightweight DL approach. It can be seamlessly integrated into web-based camera trap data platforms like TRAPPER via API. Given TRAPPER's widespread use among European researchers for manual dataset classification, AI-driven species identification could greatly accelerate data processing, enhancing wildlife conservation efforts. Additionally, YOLOv5's optimized performance on edge devices holds potential for real-time animal population monitoring directly from camera traps.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0545", "problem_id": "05450001", "content": "As technology advances rapidly, vehicles have become indispensable in daily life, making Traffic Sign Recognition (TSR) systems a critical area of research. This study presents a real-time method for detecting and identifying traffic signs while addressing challenges posed by varying weather, lighting, and visibility conditions using transfer learning. The approach employs advanced multi-object detection frameworks, including Faster Recurrent Convolutional Neural Networks (F-RCNN) and Single Shot Multi-Box Detector (SSD), alongside feature extractors like MobileNet v1, Inception v2, and Tiny-YOLOv2, with particular emphasis on F-RCNN Inception v2 and Tiny-YOLOv2 due to their superior performance. These models were optimized using the German Traffic Signs Detection Benchmark (GTSDB) dataset and evaluated on a host PC, Raspberry Pi 3 Model B+, and TASS PreScan simulation. The findings for each model are analyzed in the conclusion.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0546", "problem_id": "05460001", "content": "We introduce an efficient method for training deep learning agents that can negotiate various clauses in a contract through a straightforward communication protocol. Utilizing Multi-Agent Reinforcement Learning, both agents are simultaneously trained as they interact in the negotiation setting. Additionally, we incorporate varying levels of selfish and prosocial behaviors within these agents. Empirical findings indicate consistent behavioral patterns among the agents. Moreover, we develop a meta agent that integrates a mix of behaviors by learning an ensemble of diverse models through reinforcement learning. To evaluate the practical application of the negotiating agents, we conducted experiments where these trained agents faced off against human players. The outcomes reveal that the agents perform competitively against humans, often securing victories in negotiations. Our experiments further illustrate that the meta agent can effectively mimic human behavior.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0547", "problem_id": "05470001", "content": "We introduce McAssoc, a deep learning method aimed at associating detection bounding boxes across different perspectives in a multi-camera setup. Although most academic efforts have focused on single-camera computer vision algorithms, there has been limited exploration into their integration within multi-camera systems. In this study, we developed a three-branch architecture that utilizes direct association and additional cross-localization information. Furthermore, we created a novel metric called image-pair association accuracy (IPAA) to specifically evaluate the performance of cross-camera detection association. Our experimental results demonstrate that localization information is vital for effective cross-camera association, particularly in scenarios with similar-looking objects. This work serves as a precursor to MessyTable, a comprehensive benchmark for instance association across multiple cameras.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0548", "problem_id": "05480001", "content": "The prevalent assumption in numerous machine learning applications is that high-dimensional datasets possess an underlying low-dimensional structure, aligning with the manifold hypothesis. Consequently, a fundamental problem is to approximate the intrinsic dimension of a population distribution using a limited number of samples. We present a novel estimator for this intrinsic dimension, offering finite-sample, non-asymptotic performance guarantees. Furthermore, we leverage our methods to derive new sample complexity limits for Generative Adversarial Networks (GANs) that are contingent solely on the data's intrinsic dimensionality.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0549", "problem_id": "05490001", "content": "Supervised learning has shown promise in medical image analysis, but its effectiveness is limited by its reliance on small labeled datasets, neglecting the abundance of unlabeled data often present in medical imaging. Moreover, supervised models struggle with domain shifts when labeled data inadequately represents diverse protocols or ethnicities. To address these challenges, we propose a novel approach that maximizes the use of unlabeled data from similar or different domains within a teacher-student semi-supervised framework. This approach leverages extreme consistency, where the student network receives a highly transformed version of an image and is then trained to align its prediction with the teacher network's prediction for the original image. Unlike related methods that enforce only mild prediction consistency and achieve suboptimal results, the extreme nature of our consistency loss is a key differentiator. Our method is auto-didactic, requiring no additional expert annotations; versatile, addressing both domain shift and limited annotation issues; generic, easily adaptable to classification, segmentation, and detection tasks; and simple to implement, avoiding adversarial training. We assess our method on lesion and retinal vessel segmentation tasks using skin and fundus images. Experimental results reveal a significant improvement in performance compared to state-of-the-art supervised networks and recent semi-supervised models. This enhanced performance can be attributed to the strong regularization imposed by extreme consistency, enabling the student network to effectively manage extreme variations in both labeled and unlabeled images, thereby improving its robustness to inherent data variability within and across domains during inference.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0550", "problem_id": "05500001", "content": "Vertical federated learning (VFL) has recently garnered considerable interest as it allows multiple organizations with distinct feature sets to enhance their machine learning models without revealing sensitive data or model parameters. However, like other machine learning techniques, VFL is susceptible to fairness concerns, potentially leading to discriminatory outcomes based on sensitive attributes. To address this issue, we introduce a fair VFL framework. Initially, we provide a structured formulation of the challenge of training fair models within VFL, representing the learning task as a constrained optimization problem. To facilitate a federated solution, we utilize the equivalent dual form and create an asynchronous gradient coordinate-descent ascent algorithm. This algorithm allows each data party to execute several parallelized local updates per communication round, significantly decreasing the number of communication rounds required. Under reasonable conditions, we demonstrate that the algorithm identifies a \\delta-stationary point of the dual objective within (\\delta^) communication rounds. Empirical results from comprehensive experiments on three benchmark datasets confirm that our method excels in training fair models.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0551", "problem_id": "05510001", "content": "A novel recurrent neural network (RNN) called the Legendre Memory Unit (LMU) was recently introduced and demonstrated to deliver top-tier results across multiple benchmark datasets. By utilizing the linear time-invariant (LTI) memory mechanism of the LMU, we develop a streamlined version that enables parallelized training while retaining RNN execution during inference, addressing a common GPU training bottleneck for RNNs. This modification, applicable to any deep network with linear recurrent elements, accelerates training by up to 200 times. Additionally, we assess its effectiveness by benchmarking it against the original LMU, various published LSTM architectures, and transformer models across seven tasks, including psMNIST, sentiment analysis, and machine translation. Our models consistently outperform alternatives, frequently with fewer parameters—for example, achieving a new state-of-the-art on psMNIST and surpassing DistilBERT and LSTMs in IMDB sentiment analysis while using half the parameters.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0552", "problem_id": "05520001", "content": "This study investigates local stochastic approximation across a network of agents, driven by its relevance to reinforcement learning and federated learning. The objective is to identify the root of an operator that combines local operators from each agent. The emphasis is on defining the finite-time performance of this approach, considering that each agent's data originates from Markov processes, thus introducing dependencies. Specifically, convergence rates for local stochastic approximation are established for both fixed and changing step sizes. These rates are shown to be within a logarithmic factor of those achieved with independent data. The applicability of these findings is then demonstrated through various relevant challenges in multi-task reinforcement learning and federated learning.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0553", "problem_id": "05530001", "content": "This paper introduces a gesture recognition technique that emphasizes critical movements to differentiate between similar gestures. The approach constructs a partial action sequence using optical flow images, represents this sequence within the eigenspace, and employs a weighted graph-based optimal path-searching method to highlight key actions within the feature vector sequence. The effectiveness of this method is demonstrated through an experiment involving the recognition of similar sign language words.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0554", "problem_id": "05540001", "content": "Neural Architecture Search (NAS) demonstrates significant promise in autonomously generating scalable network designs for dense image prediction tasks. Current NAS methods often limit their search spaces and rely on proxy tasks to manage computational constraints. To address these limitations and minimize discrepancies between target and proxy datasets, we introduce the Densely Connected NAS (DCNAS) framework, which directly explores optimal architectures for multi-scale visual representations on large-scale target datasets. Our approach employs a densely connected search space, where cells are linked via learnable weights, encompassing diverse mainstream network designs. Additionally, we integrate path-level and channel-level sampling strategies in a fusion module to optimize memory usage within this expansive search space. Experiments confirm that DCNAS-derived architectures achieve top-tier results on semantic segmentation benchmarks, such as 84.3% on Cityscapes and 86.9% on PASCAL VOC 2012, while maintaining competitive performance on more complex datasets like ADE20K and Pascal Context.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0555", "problem_id": "05550001", "content": "Self-supervised depth estimation in indoor settings presents more difficulties compared to outdoor scenarios for at least two reasons: (i) the depth variation in indoor sequences can be significant between frames, complicating the depth network's ability to generate consistent depth cues, while outdoor scenes generally maintain a consistent maximum depth, as the camera often captures the sky; (ii) indoor sequences exhibit a greater degree of rotational movements, which pose challenges for the pose network, whereas outdoor sequences tend to feature primarily translational movements, particularly in driving datasets like KITTI. This paper addresses these challenges and compiles a set of effective strategies to enhance the performance of self-supervised monocular depth estimation in indoor environments. The proposed approach incorporates two innovative modules, namely a depth factorization module and a residual pose estimation module, each crafted to address the identified challenges. The efficacy of these modules is validated through a comprehensive ablation study as well as by demonstrating state-of-the-art results on three indoor datasets, namely EuRoC, NYUv2, and 7-scenes.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0556", "problem_id": "05560001", "content": "We present a novel instant question answering system designed for ecommerce product pages, which retrieves relevant community question answer pairs for each user query. However, the disparate language characteristics of user queries and community question answer pairs pose a challenge for learning relevance. To address this, our transformer-based model learns a robust relevance function by jointly learning unified syntactic and semantic representations, eliminating the need for human-annotated data. This is accomplished through distant supervision, where our model is trained on the predictions of a syntactic matching system for user queries, in conjunction with community question answer pairs. By leveraging community question answer pairs, our model learns semantic question answering relevance, while distant supervision enables the acquisition of syntactic features and an understanding of user querying language nuances. Furthermore, our model's ability to independently encode queries and candidate responses facilitates offline candidate embedding generation, reducing the requirement for real-time transformer model execution and enabling scalability for large ecommerce question answering traffic. Our framework demonstrates significant performance improvements over both syntactic and semantic baselines in extensive evaluations on user queries, as well as in large-scale online A/B testing on a popular ecommerce service.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0557", "problem_id": "05570001", "content": "We introduce a novel approach for predicting systems composed of multiple interconnected time series. This method simultaneously learns forecasting models and identifies leading indicators within the system that enhance predictive accuracy while also structuring the predictive tasks into clusters. Grounded in the traditional linear vector autoregressive model (VAR), the approach connects the identification of leading indicators with the inference of sparse Granger causality graphs. We propose a new constrained optimization problem designed to encourage the desired sparse structures within the models and facilitate information sharing among learning tasks in a multi-task framework. An algorithm is presented to tackle this problem, and we provide comprehensive synthetic and real-data experimental results demonstrating the benefits of our method compared to baseline VAR models and contemporary sparse VAR learning techniques.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0558", "problem_id": "05580001", "content": "The reinforcement learning framework or optimal control offers a robust mathematical structure for intelligent decision-making that is widely applicable. Although the general structure of the reinforcement learning problem facilitates effective reasoning regarding uncertainty, the link between reinforcement learning and probabilistic model inference is not immediately clear. Nevertheless, establishing this connection is significantly beneficial for algorithm development: defining a problem as probabilistic inference enables the utilization of a variety of approximate inference techniques, allows for flexible and powerful model extensions, and facilitates reasoning about compositionality and partial observability. In this article, we will explore how a broader interpretation of the reinforcement learning or optimal control issue, often referred to as maximum entropy reinforcement learning, correlates to precise probabilistic inference under deterministic dynamics and to variational inference in the presence of stochastic dynamics. We will provide a comprehensive derivation of this framework, summarize previous research that has leveraged these concepts to innovate new reinforcement learning and control algorithms, and discuss viewpoints on future research directions.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0559", "problem_id": "05590001", "content": "The growing reliance on deep neural networks in various aspects of daily life has led to increased scrutiny of their vulnerability to adversarial examples, which is crucial for enhancing their robustness and understanding their input sensitivity. While the success of white-box attacks often hinges on the magnitude of distortion in the perturbations, the speed of these attacks is also a critical factor, particularly in the context of adversarial training, where rapid attacks are essential. As iterative methods can yield better solutions with more time, this work explores the trade-off between speed and distortion, introducing a novel attack method called boundary projection (BP) that significantly outperforms existing approaches. By conceptualizing the classification boundary as a manifold within the image space, our approach rapidly converges to this boundary and subsequently optimizes distortion on the manifold, thereby achieving a substantial improvement over current methods.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0560", "problem_id": "05600001", "content": "Static graph representation learning has proven valuable in numerous real-world applications. Nevertheless, temporal networks, where edges frequently change over time, have received comparatively less focus. Effective embeddings for these networks should capture both the graph's structure and its temporal evolution. Current methods for learning representations of temporally evolving networks often do not adequately address temporal interdependencies. This paper introduces Toffee, a new tensor decomposition-based approach for learning temporal network representations. Toffee leverages the tensor-tensor product operator to encode cross-time information, enabling the capture of periodic changes within the evolving networks. Empirical evaluations demonstrate that Toffee generates more effective embeddings for link prediction tasks on several real-world temporal networks compared to existing techniques.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0561", "problem_id": "05610001", "content": "To mitigate pollutant emissions, bicycles are experiencing a resurgence in popularity, particularly in urban environments, yet the number of cyclist fatalities remains a concern, failing to follow the declining trend observed in other traffic groups. As a result, collecting and analyzing cyclist data has become crucial for enhancing urban cyclist safety, enabling urban planners to design more secure routes. This study presents a comprehensive image-based framework for evaluating route risk from the cyclist's perspective, utilizing sequences of smartphone images to automatically detect events based on various risk criteria, including cyclist motion and object detection. The image-based approach provides contextual insights into the situation, eliminating the need for cyclist expertise, and is built upon an existing platform with enhancements to the mobile app for collecting smartphone sensor data, including video. By leveraging inertial sensor data, route segments traveled by bicycle are automatically identified, and behavior analysis techniques are applied. The proposed method yields promising results when tested on real data, demonstrating high accuracy in risk classification according to two distinct criteria and behavior analysis, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0562", "problem_id": "05620001", "content": "Efficient exploration in online reinforcement learning (RL) is especially difficult in high-dimensional settings with sparse rewards. While count-based upper confidence bound (UCB) methods achieve near-optimal performance in low-dimensional, tabular environments, their extension to non-linear function approximation in practical RL tasks remains unresolved. To tackle this, we introduce a novel exploration strategy based on measuring the divergence between the next policy's state occupancy and previously explored regions. This measure is incorporated as an adaptive regularizer into the standard RL objective to optimize the exploration-exploitation trade-off. We develop a provably convergent algorithm that generates an intrinsic reward, which modifies existing exploration bonuses. This reward is straightforward to integrate with various RL algorithms. Empirical validation on tabular environments demonstrates its superiority over count-only exploration methods. Additionally, experiments on MiniGrid and DeepMind Control Suite benchmarks for navigation and locomotion tasks show substantial gains in sample efficiency compared to leading approaches. Our implementation is accessible at https://github.com/tianjunz/MADE.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0563", "problem_id": "05630001", "content": "We investigate the video super-resolution (SR) challenge with the aim of enhancing video analytics tasks, such as action recognition, rather than focusing solely on visual quality. The commonly used action recognition techniques that leverage convolutional networks, like two-stream networks, are not suitable for videos with low spatial resolution. To address this, we advocate for applying video SR before the recognition step, which drives us to enhance the SR process to boost recognition performance. Specifically designed for two-stream action recognition networks, we introduce two distinct video SR approaches targeting the spatial and temporal streams individually. Firstly, we notice that regions where actions occur are critical for recognition; thus, we propose an optical-flow guided weighted mean-squared-error loss for our spatial-oriented SR (SoSR) network, prioritizing the reconstruction of dynamic objects. Secondly, we identify that existing video SR techniques lead to temporal inconsistencies between frames, negatively impacting recognition accuracy. Consequently, we present a siamese network for our temporal-oriented SR (ToSR), which focuses on maintaining temporal continuity among successive frames. We conduct experiments with two advanced action recognition networks across two prominent datasets—UCF101 and HMDB51. The results validate the efficacy of our SoSR and ToSR methods in enhancing recognition performance.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0564", "problem_id": "05640001", "content": "Sparse and irregularly sampled multivariate time series are prevalent in various fields, including clinical, climate, and financial domains. Recent methodologies primarily concentrate on classification, regression, or forecasting tasks with these types of data. In the context of forecasting, it is essential to predict not just the correct values but also the timing of their occurrence within the irregular time series. In this study, we introduce a method that forecasts both the values and their anticipated occurrence times.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0565", "problem_id": "05650001", "content": "We examine the performance of d-dimensional linear stochastic approximation algorithms (LSAs) that utilize a constant step-size and Polyak-Ruppert (PR) averaging of iterates, which are commonly employed in machine learning and reinforcement learning (RL) to compute an optimal \\theta_ \\in \\mathbb^d using noisy data with O(d) updates per iteration. This study is motivated by the problem of policy evaluation from experience replay in RL using the TD class of learning algorithms, which are a type of LSA. For LSAs with a constant step-size and PR averaging, we derive bounds for the mean squared error (MSE) after t iterations, assuming independent and identically distributed (iid) data with finite variance (underlying distribution P) and Hurwitz expected dynamics. Our results show that for a given LSA with PR averaging and a data distribution P satisfying these assumptions, there exists a range of constant step-sizes for which the MSE decays at a rate of O(\\frac). We investigate the conditions under which a uniform constant step-size can be chosen for a class of data distributions and demonstrate that not all data distributions allow for such a uniform choice. Additionally, we propose a heuristic step-size tuning algorithm for selecting a constant step-size for a given LSA and data distribution P, and compare our findings with related work, discussing their implications for TD algorithms that are LSAs.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0566", "problem_id": "05660001", "content": "We introduce a novel approach to multi-instance pose estimation, leveraging the combination of a convolutional neural network and a transformer to enable end-to-end training. By framing multi-instance pose estimation as a direct set prediction problem, we draw inspiration from recent advancements in object detection using transformers, and employ a transformer encoder-decoder architecture paired with a bipartite matching scheme to directly regress the poses of all individuals within an image. Our proposed model, termed POse Estimation Transformer (POET), utilizes a bespoke set-based global loss function, comprising keypoint loss, keypoint visibility loss, center loss, and class loss, to facilitate training. POET captures the relationships between detected humans and the broader image context, allowing for parallel pose prediction. As demonstrated through experiments, POET achieves high accuracy on the COCO keypoint detection task, and, to our knowledge, represents the first end-to-end trainable method for multi-instance human pose estimation.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0567", "problem_id": "05670001", "content": "This study seeks to elucidate the significance of multi-scale information in monocular image-based depth estimation, with a particular focus on examining the efficacy of four distinct deep CNN architectures that leverage multi-scale features throughout the network, in comparison to a cutting-edge single-scale approach, as seen in Figure A, B, C (citations). The findings demonstrate that incorporating multi-scale features into depth estimation yields not only enhanced accuracy but also superior qualitative depth maps, as evidenced by the results on the NYU Depth dataset (References), where the proposed methodology attains state-of-the-art performance, thereby underscoring the importance of multi-scale information in this context.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0568", "problem_id": "05680001", "content": "A Greedy Restrictive Boltzmann Machine, when implemented with a substantial number of hidden layers containing numerous neurons and accelerated by a graphics card, achieves a relatively low error rate of 0.72% on the MNIST handwritten digit dataset.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0569", "problem_id": "05690001", "content": "For intelligent robots to effectively plan and execute manipulation tasks, they must comprehend the context of such scenarios. Crucially, this requires the semantic interpretation of manipulation knowledge through structured descriptions of entities, relationships, and attributes. This paper introduces a framework for constructing high-level conceptual dynamic knowledge graphs derived from video clips. The framework leverages a Vision-Language model integrated with an ontology system to represent robot manipulation knowledge using Entity-Relation-Entity (E-R-E) and Entity-Attribute-Value (E-A-V) tuples, thereby aligning visual perception with contextual semantics. The proposed method demonstrates both flexibility and versatility. We present a case study utilizing the framework, wherein a robot performs manipulation actions within a kitchen environment, effectively bridging visual perception with contextual semantics via the generated dynamic knowledge graphs.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0570", "problem_id": "05700001", "content": "This paper examines spectral community detection within sparse networks characterized by varied degree distributions, introducing an algorithm designed for efficient community retrieval. The study reveals that a specifically parameterized, regularized Laplacian matrix facilitates spectral clustering in such networks, effectively mitigating issues arising from degree heterogeneity. Furthermore, it establishes significant relationships between this matrix and other well-known matrices, including the non-backtracking matrix, the Bethe-Hessian matrix, and the standard Laplacian matrix. Notably, unlike alternative approaches, the proposed parametrization implicitly addresses the inherent difficulty of the classification task. These results are synthesized into an algorithm that accurately estimates the number of communities and achieves significant community reconstruction.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0571", "problem_id": "05710001", "content": "The application of non-linear kernel methods to large-scale problems can be facilitated by approximating them with fast linear methods using explicit feature maps. This study examines the composition of convolution kernels for structured data from base kernels and the construction of corresponding feature maps, leading to the proposal of exact and approximate feature maps for widely used graph kernels, including those based on the kernel trick. The feasibility and efficiency of computation via explicit feature maps are analyzed for various kernels and graph properties, with a particular focus on deriving approximate, explicit feature maps for state-of-the-art kernels that support real-valued attributes, such as the GraphHopper and graph invariant kernels, as shown in Figure A, B, C (References [1], [2]). Experimental results demonstrate that the proposed approaches can achieve classification accuracy comparable to exact methods based on the kernel trick, but with significantly reduced running time, and further analysis of algorithms for computing random walk, shortest-path, and subgraph matching kernels using explicit and implicit feature maps reveals a phase transition in running time with respect to label diversity, walk lengths, and subgraph size, respectively, as discussed in citations [3] and [4].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0572", "problem_id": "05720001", "content": "We present DatasetGAN, an automated approach for creating extensive collections of high-quality, semantically segmented images with minimal human involvement. Modern deep learning models demand vast amounts of training data, yet manual annotation of large-scale datasets remains labor-intensive. Leveraging the capabilities of state-of-the-art GANs to produce realistic imagery, we demonstrate that a GAN's latent representation can be interpreted to yield detailed semantic segmentations. By training a decoder with only a handful of labeled samples, it generalizes across the entire latent space, enabling the generation of unlimited annotated data. These synthetic datasets function identically to real-world datasets for training computer vision systems. Since manual segmentation is limited to a small sample set, meticulous annotation becomes feasible, producing datasets with intricate object and component labels. To demonstrate our method's effectiveness, we created segmentation datasets for 7 distinct tasks, encompassing 34 facial components and 32 automotive parts at the pixel level. Our technique substantially surpasses semi-supervised benchmarks and matches fully supervised approaches, even when the latter utilize up to 100 times more labeled data than our solution.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0573", "problem_id": "05730001", "content": "The recent advancements in technology facilitated by the Internet and social media have led to the emergence of faster and more effective communication platforms. These platforms encompass visual, textual, and auditory formats and have given rise to a distinct social phenomenon known as Internet memes. Internet memes typically consist of images paired with clever, engaging, or ironic textual descriptions. In this study, we introduce a multi-modal sentiment analysis system that employs deep neural networks by integrating Computer Vision and Natural Language Processing. Unlike traditional sentiment analysis that usually focuses on determining whether a text conveys positive or negative sentiment, our goal is to classify Internet memes as positive, negative, or neutral, ascertain the humor type presented, and measure the degree of effect conveyed. Our system, developed using CNN and LSTM, has surpassed the baseline performance.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0574", "problem_id": "05740001", "content": "Gradient Boosted Decision Trees (GBDT) is a highly effective ensemble learning technique that is extensively utilized across different application domains. Recently, various adaptations of GBDT training methods and implementations have been developed and significantly refined within popular open-source frameworks such as XGBoost, LightGBM, and CatBoost. In this paper, we demonstrate that the accuracy and efficiency of GBDT can be further improved by employing more sophisticated base learners. Specifically, we enhance gradient boosting by utilizing piecewise linear regression trees (PL Trees) instead of piecewise constant regression trees as base learners. Our findings illustrate that PL Trees can expedite the convergence of GBDT and increase accuracy. Additionally, we introduce several optimization techniques to considerably decrease the training duration of PL Trees while maintaining accuracy. We also suggest multiple implementation strategies to accelerate our algorithm on contemporary computer architectures that support robust Single Instruction Multiple Data (SIMD) parallelism. The experimental results indicate that GBDT with PL Trees achieves highly competitive testing accuracy with equal or reduced training time.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0575", "problem_id": "05750001", "content": "Self-supervised representation learning has gained prominence due to its effectiveness in leveraging unlabeled data. Among prevalent self-supervised approaches, augmentation-based contrastive learning has excelled in computer vision tasks where manual annotations are scarce. However, existing techniques often face constraints such as high memory or storage overhead, and their performance remains suboptimal. This paper introduces AAG, a novel self-supervised learning framework incorporating an auxiliary augmentation technique and GNT-Xent loss. The auxiliary augmentation enhances contrastive learning by enriching image diversity, while the GNT-Xent loss ensures stable and efficient training while delivering high accuracy. Evaluations on CIFAR10, CIFAR100, and SVHN confirm AAG's superiority over prior state-of-the-art methods. Notably, AAG achieves 94.5% top-1 accuracy on CIFAR10 with a batch size of 64, outperforming SimCLR's best result (batch size 1024) by 0.5%.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0576", "problem_id": "05760001", "content": "The challenge of transferring learned models to new tasks is particularly pronounced when only a limited number of labeled examples are available. While the few-shot learning paradigm has garnered significant attention lately, most existing methods primarily concentrate on distinguishing new classes. In contrast, we explore the broader framework of generalized few-shot learning (GFSL), where the model must classify within a combined label space that includes both known and unknown classes. We introduce a graph-based approach that explicitly represents the relationships among all known and novel classes within this joint label space. Our model, Graph-convolutional Global Prototypical Networks (GcGPN), utilizes graph convolution to incorporate these inter-class relationships, allowing for the representation of new class features within the established space of known classes cohesively. This methodology facilitates rapid adaptation and comprehensive discrimination, which are critical challenges in GFSL. We illustrate the advantages of our model using two demanding benchmark datasets.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0577", "problem_id": "05770001", "content": "The reconstruction of seismic data with missing traces has been a persistent challenge in seismic data processing. Recently, rank reduction techniques have gained popularity for addressing this issue, yet they necessitate prior knowledge of the seismic data's rank, which is often unknown in field situations. Adjusting the rank manually can be time-consuming and typically yields only an approximate value. Deep learning approaches require extensive training datasets, which are often hard to obtain due to practical physical or financial limitations. In this study, we present a novel method that employs unsupervised learning by leveraging the intrinsic characteristics of a convolutional neural network called U-net, eliminating the need for training datasets. Our approach relies solely on a single undersampled seismic dataset, allowing the network to utilize its deep seismic prior for efficient reconstruction. This method is capable of processing both irregular and regular seismic data. We evaluated the performance of the proposed DSPRecon algorithm using synthetic and field data, comparing its effectiveness against singular spectrum analysis (SSA) for irregular data reconstruction and the de-aliased Cadzow method for regular data reconstruction. Experimental findings demonstrated that our method outperformed both the SSA and Cadzow techniques, achieving signal-to-noise ratios (SNRs) of 32.68 dB and 19.11 dB for the DSPRecon and SSA algorithms, respectively, and 35.91 dB and 15.32 dB for the DSPRecon and Cadzow methods, respectively.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0578", "problem_id": "05780001", "content": "The analysis of discrete data time-series for applications such as statistical inference, threat detection, and brain activity prediction has gained significant attention recently. However, this field is faced with challenges including large data sizes, missing data that hinder the construction of complex time-varying networks, and the impact of unaccounted sources. This study addresses these challenges by developing a statistical neuron system model that incorporates multiple covariates and unknown inputs. Unlike previous research that primarily focused on the effects of spiking history and neuronal interactions, this work introduces the concept of \"unknown unknowns\" to account for the influence of unseen stimuli, undetected neuronal activities, and other hidden error sources. The model utilizes maximum likelihood estimation with a fixed-point iteration method, which converges rapidly and can be efficiently parallelized, offering a computational advantage, particularly for long-term input spiking trains. The framework provides insight into the importance of extra degrees-of-freedom in the data, supporting the need for unknowns. The algorithm is validated using simulated spike trains and real-world experimental data from mouse somatosensory, mouse retina, and cat retina, demonstrating a significant increase in system likelihood and convergence with iterations, and suggesting that the neural connection model with unknown unknowns can effectively estimate statistical properties by enhancing network likelihood.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0579", "problem_id": "05790001", "content": "Object detection and localization in real 3D space is a fundamental aspect of scene understanding, yet it poses significant challenges when relying on a single RGB image, as the imagery projection process results in a loss of geometric information. To address this, we introduce MonoGRNet, a unified network architecture that facilitates amodal 3D object detection from monocular RGB images by leveraging geometric reasoning in both the observed 2D projection and the unobserved depth dimension. Our MonoGRNet framework consists of four specialized subnetworks, each responsible for a distinct task: 2D object detection, instance depth estimation, 3D localization, and local corner regression. Notably, our instance depth estimation approach deviates from traditional pixel-level depth estimation methods, which require dense annotations, by instead predicting the depth of the target 3D bounding box's center using sparse supervision. The 3D localization process is further refined by estimating the object's position in both the horizontal and vertical dimensions. Ultimately, MonoGRNet is trained through a joint optimization process that focuses on accurately determining the locations and poses of 3D bounding boxes within a global context, yielding state-of-the-art results on demanding datasets, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0580", "problem_id": "05800001", "content": "Knowledge graph embeddings (KGEs) have garnered considerable interest, leading to the creation of numerous software libraries for their training and assessment. Addressing specific requirements, we present a community-driven redesign and reimplementation of PyKEEN, a pioneering KGE library. PyKEEN 1.0 allows users to construct knowledge graph embedding models (KGEMs) utilizing diverse interaction models, training methodologies, loss functions, and the direct modeling of inverse relationships. Furthermore, automatic memory optimization has been implemented to maximize hardware utilization, and the integration of Optuna facilitates comprehensive hyper-parameter optimization (HPO).", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0581", "problem_id": "05810001", "content": "The automatic extraction of road curbs from irregular, disorganized, noisy, and extensive 3D point clouds presents significant challenges. Current approaches frequently involve projecting 3D point clouds onto 2D surfaces to obtain curb outlines. Nevertheless, this projection leads to a loss of 3D data, which hampers detection performance. This paper introduces a reliable, precise, and efficient technique for extracting road curbs from 3D mobile LiDAR point clouds. Our approach comprises two phases: 1) identifying candidate curb points utilizing a newly proposed energy function, and 2) refining these candidate points through a least cost path model we introduced. We assessed our method with large datasets from a residential area (16.7GB, 300 million points) and an urban area (1.07GB, 20 million points) of mobile LiDAR point clouds. The results demonstrate that our proposed method outperforms current state-of-the-art techniques in robustness, accuracy, and efficiency. The curb extraction technique achieved a completeness score of 78.62% and a correctness score of 83.29%. These findings illustrate that our method is a promising approach for extracting road curbs from mobile LiDAR point clouds.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0582", "problem_id": "05820001", "content": "This study presents an optimal adversarial attack strategy targeting autoregressive time series forecasting based on the Linear Quadratic Regulator (LQR). Within this threat scenario, the environment follows a dynamical system, where an autoregressive model predicts future states based on current observations, and an adversary manipulates these states to influence subsequent forecasts. The attacker aims to steer predictions toward a desired trajectory while minimizing intervention costs. For white-box scenarios with full knowledge of the environment and forecasting models, we derive optimal attacks using LQR for linear systems and Model Predictive Control (MPC) for nonlinear ones. In black-box settings, we integrate system identification with MPC. Experimental results validate the efficacy of our proposed attacks.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0583", "problem_id": "05830001", "content": "Restricting linear layers in neural networks to adhere to symmetry transformations from a group G is a widely used approach for designing invariant networks, with numerous applications in machine learning. This study addresses a fundamental yet underexplored question: Can such networks approximate all continuous invariant functions? We examine the general scenario where G ≤ Sₙ (an arbitrary subgroup of the symmetric group) operates on ℝⁿ by permuting coordinates, encompassing several recent invariant network architectures. Our key findings include: (1) G-invariant networks achieve universality when higher-order tensors are permitted, and (2) certain groups G necessitate higher-order tensors for universality. Networks using only first-order tensors hold particular practical importance, and we establish a necessary condition for their universality.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0584", "problem_id": "05840001", "content": "Feature map attention mechanisms and multi-path representations are established as significant components in visual recognition tasks. This paper introduces a modular architecture that employs channel-wise attention across various network branches, capitalizing on their effectiveness in modeling cross-feature relationships and acquiring varied feature representations. The proposed design yields a streamlined and cohesive computational unit, parameterized by a minimal set of variables. The resulting model, termed ResNeSt, demonstrates enhanced accuracy and latency balance compared to EfficientNet in image classification tasks. Furthermore, ResNeSt exhibits excellent transfer learning performance as a backbone on multiple public benchmarks and has been utilized by top-performing solutions in the COCO-LVIS challenge. The complete system's source code and pre-trained models are accessible to the public.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0585", "problem_id": "05850001", "content": "This study focuses on accelerating semantic segmentation for video applications, where real-time or faster processing is often required. While existing approaches reduce computational load by propagating features from keyframes, recent improvements in rapid image segmentation diminish their effectiveness. To enhance video segmentation using these advancements, we present a straightforward yet effective propagation method. Our approach involves estimating optical flow in downscaled images for efficient temporal warping in segmentation space. Additionally, we employ a guided spatially-varying convolution to combine segmentations from consecutive frames, reducing propagation errors and allowing lightweight feature extraction for non-keyframes. Evaluations on Cityscapes and CamVid demonstrate that our method achieves the best balance between accuracy and processing speed for video segmentation.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0586", "problem_id": "05860001", "content": "Deep reinforcement learning typically emphasizes behavioral learning, often neglecting the significant influence of an agent's morphology on its capabilities. Consequently, the question arises of how to identify an appropriate morphology for task resolution within a specific environment. Existing methods that simultaneously adapt morphology and behavior typically utilize task-specific rewards to guide morphology optimization. However, this approach frequently necessitates computationally intensive policy optimization and produces task-dependent morphologies lacking generalization capabilities. To address these limitations, we introduce Task-Agnostic Morphology Evolution (TAME), a novel approach designed to mitigate both concerns. TAME evolves morphologies independent of any task or reward specification by applying randomly sampled action primitives to a population of agents. This is achieved through an information-theoretic objective function that effectively evaluates agents based on their capacity to achieve diverse states within the environment and the causality of their actions. Empirically, we demonstrate that TAME can evolve morphologies in 2D, 3D, and manipulation environments that achieve comparable multi-task performance to those learned with task-supervised algorithms. Our code and videos can be found at https://sites.google.com/view/task-agnostic-evolution.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0587", "problem_id": "05870001", "content": "The prevalence of LiDAR in modern robotics systems can be attributed to its ability to provide geometrically rich data, with rolling shutter LiDARs being particularly widespread, utilizing a rotating base to scan the scene with an array of lasers, emitting points as a stream of packets that cover a 360-degree sector. However, conventional perception algorithms introduce additional latency by waiting for a complete sweep of data to be compiled before processing, resulting in a 100ms delay for typical 10Hz LiDARs, which can lead to outdated outputs that no longer accurately reflect the current state of the world, posing a significant challenge for robotics applications that require rapid reaction times to respond to safety-critical situations. To address this issue, this paper presents StrObe, a novel approach that reduces latency by processing LiDAR packets in a streaming fashion, without waiting for a full sweep to be completed, and achieves accurate low-latency perception by reusing computations from previous packets and iteratively updating a latent spatial representation of the scene, effectively acting as a memory that incorporates new evidence as it becomes available, as demonstrated through experiments on a large-scale real-world dataset, which show that StrObe outperforms state-of-the-art methods when considering latency and matches their performance in traditional settings, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0588", "problem_id": "05880001", "content": "This study presents a surrogate agent-environment interface (SAEI) for reinforcement learning, demonstrating that probability-based SAEI learning yields optimal policies for task agent-environment interfaces. We propose the concept of surrogate probability action and formulate the probability surrogate action deterministic policy gradient (PSADPG) algorithm, which facilitates continuous control over discrete actions. Experimental results indicate that PSADPG matches DQN's performance in specific tasks while exhibiting stochastic optimal policy characteristics during early training phases.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0589", "problem_id": "05890001", "content": "The task of graph-based semi-supervised learning involves assigning labels to a large set of unlabelled data points based on a limited number of labelled instances. This study examines the consistency of optimization-based methods for graph-based semi-supervised learning, focusing on scenarios where label noise is minimal and the underlying unlabelled data exhibits strong clustering. Specifically, it investigates the application of graph-based probit to binary classification and its extension to multi-class classification using one-hot encoding. The optimization objective consists of two components: a quadratic form defined by a rational function of the graph Laplacian, which only depends on the unlabelled data, and a fidelity term that only involves the labelled data. By analyzing the consistency of this approach, insights are gained into the selection of the rational function that underlies the optimization process.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0590", "problem_id": "05900001", "content": "Graph Attention Networks (GATs), a widely used and state-of-the-art Graph Neural Network (GNN) architecture for graph representation learning, employ an attention mechanism where each node attends to its neighbors based on its own representation acting as the query. This paper demonstrates that GATs are limited to a specific form of attention, termed static attention, in which the ranking of attended nodes does not depend on the query node. We differentiate this static attention from a more expressive dynamic attention and show that GAT's static attention mechanism limits its ability to express certain graph problems; specifically, GAT fails to fit training data in a controlled experiment. To address this limitation, we propose GATv2, a dynamic graph attention variant with a modified order of operations, which is strictly more expressive than GAT. Through comprehensive evaluation across 11 OGB and other benchmarks, GATv2 is shown to outperform GAT while maintaining comparable parametric costs. Our code is available at https://github.com/tech-srl/how_attentive_are_gats .", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0591", "problem_id": "05910001", "content": "We investigate the challenge of recognizing fingerspelling in American Sign Language within real-world contexts, utilizing videos sourced from online platforms. We present the most extensive dataset to date for fingerspelling recognition, marking the first use of naturally occurring video content. Our work represents the initial endeavor to identify fingerspelling sequences in such a complex environment. Contrary to previous research, our video dataset poses significant difficulties due to low frame rates and high visual variability. To address these visual challenges, we develop a specialized hand detector trained on a limited portion of our dataset. Following the output from the hand detector, we employ a sequence model to decode the predicted sequence of fingerspelled letters. In exploring the sequence model, we investigate both attention-based recurrent encoder-decoders and CTC-based methods. As this is the first foray into fingerspelling recognition in uncontrolled settings, our research aims to establish a foundation for future sign language recognition studies under realistic conditions. We observe that, as anticipated, the letter error rates are considerably higher than those reported in earlier studies involving more controlled datasets, and we examine the sources of errors along with the impact of different model variations.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0592", "problem_id": "05920001", "content": "This study proposes a novel method for predicting the future presence and location of human hands and objects in a scene. The objective is to forecast the objects that will be present in a subsequent frame, such as one 5 seconds in the future, and their corresponding locations, even if they are not currently visible. The approach relies on two main concepts: first, that a convolutional object recognition model can abstract scene information into an intermediate representation, and second, that this representation can be used to predict future scene representations through regression. A new two-stream convolutional neural network (CNN) architecture is designed for video analysis, building upon a state-of-the-art convolutional object detection network, and a fully convolutional regression network is introduced to predict future scene representations. Experimental results demonstrate that combining the predicted future representation with the detection network enables accurate estimation of future hands and objects in videos, outperforming the current state-of-the-art method for forecasting future object presence on a public dataset.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0593", "problem_id": "05930001", "content": "This work introduces a novel approach to projecting input images into the domain of a class-conditional generative neural network, with a focus on mitigating model biases inherent in these networks. To address the object-center bias and color bias commonly found in Generative Adversarial Networks, our method optimizes for image transformations, including translation, scale, and global color adjustments, during the projection process. However, this optimization problem is challenging, and traditional gradient-based methods often fail to yield satisfactory results. To overcome this limitation, we propose a hybrid optimization strategy that combines transformation estimation with class parameter estimation, enabling the discovery of effective projections. The efficacy of our approach is demonstrated through experiments on real images, and we further illustrate how the resulting projections enhance the editability of these images, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0594", "problem_id": "05940001", "content": "Traditional nonlinear subspace learning methods, such as manifold learning, typically present issues regarding explainability (explicit mapping), cost-effectiveness (linearization), generalization ability (out-of-sample), and representability (spatial-spectral discrimination). To address these limitations, a new linearized subspace analysis approach with spatial-spectral manifold alignment has been introduced for semi-supervised hyperspectral dimensionality reduction (HDR), referred to as joint and progressive subspace analysis (JPSA). The JPSA develops a high-level, semantically relevant joint spatial-spectral feature representation from hyperspectral data by 1) collectively learning latent subspaces alongside a linear classifier to identify an efficient projection direction that aids classification; 2) progressively exploring various intermediate subspace states to approximate an optimal mapping from the original space to a potentially more discriminative subspace; 3) spatially and spectrally aligning the manifold structure within each learned latent subspace to maintain identical or similar topological features between the compressed and original data. A straightforward yet effective classifier, namely nearest neighbor (NN), is investigated as a potential application to assess the performance of the algorithm across different HDR methods. Comprehensive experiments are conducted to showcase the effectiveness and superiority of the proposed JPSA on two extensively utilized hyperspectral datasets: Indian Pines (92.98%) and the University of Houston (86.09%), compared to prior state-of-the-art HDR techniques. A demonstration of this foundational work (i.e., ECCV2018) is publicly accessible at https://github.com/danfenghong/ECCV2018_J-Play.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0595", "problem_id": "05950001", "content": "This paper introduces a novel approach that extends subspace learning techniques founded on eigenvalue and generalized eigenvalue problems. Termed Roweis Discriminant Analysis (RDA), in recognition of Sam Roweis' significant contributions to subspace learning, this method encompasses an infinite set of algorithms, with Principal Component Analysis (PCA), Supervised PCA (SPCA), and Fisher Discriminant Analysis (FDA) as specific instances. A unique extreme case, designated Double Supervised Discriminant Analysis (DSDA), leverages labels in a dual manner and represents an entirely new contribution. For certain specific scenarios, a dual formulation of RDA is presented. Furthermore, kernel RDA is developed, which generalizes kernel PCA, kernel SPCA, and kernel FDA, through the application of both dual RDA and representation theory. Theoretical analysis elucidates established properties, including the suitability of regression for SPCA but not FDA, the existence of duals for PCA and SPCA but not FDA, the applicability of the kernel trick to kernel PCA and kernel SPCA but not kernel FDA, and the optimality of PCA for linear reconstruction. Generalizations of eigenfaces, Fisherfaces, supervised eigenfaces, and their kernel counterparts are also introduced as Roweisfaces and kernel Roweisfaces. Empirical evaluations on standard benchmark datasets demonstrate the efficacy of both RDA and kernel RDA.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0596", "problem_id": "05960001", "content": "This study introduces a two-stage approach to enhance pedestrian detection in thermal imagery. Initially, a generative data augmentation technique is employed, followed by a domain adaptation process that leverages synthesized data to modify an existing RGB pedestrian detector. Utilizing a Least-Squares Generative Adversarial Network, our model generates realistic thermal representations from RGB inputs, thereby expanding the limited labeled thermal dataset for training. By integrating this generative augmentation strategy, we adapt a pretrained YOLOv3 detector for thermal-only detection. Evaluations on the KAIST Multispectral Pedestrian Detection Benchmark confirm the efficacy of our method: with under 50% of real thermal training data and supplemented by synthetic images during domain adaptation, our detector attains state-of-the-art performance. Furthermore, incorporating GAN-generated images alongside additional real thermal data further boosts detection accuracy, highlighting their value as an augmentation tool. To our knowledge, our detector achieves the highest single-modality performance on the KAIST benchmark compared to existing methods.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0597", "problem_id": "05970001", "content": "Memory-based neural networks process temporal data by retaining information over extended durations. However, their capacity for intricate relational reasoning with stored information remains uncertain. We initially validate the hypothesis that conventional memory architectures may underperform in tasks requiring deep comprehension of entity relationships—specifically those involving relational reasoning. To address these limitations, we introduce a novel memory module—the Relational Memory Core (RMC)—that facilitates memory interactions through multi-head dot product attention. We evaluate the RMC on various tasks benefiting from enhanced relational reasoning in sequential data, demonstrating significant improvements in reinforcement learning (e.g., Mini PacMan), program evaluation, and language modeling. The model achieves state-of-the-art performance on the WikiText-103, Project Gutenberg, and GigaWord datasets.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0598", "problem_id": "05980001", "content": "We present a method for dividing an image into sections that align with surfaces in the environment that are partially enclosed by the medium. This approach combines appearance and motion statistics into a cost functional, initialized with occluded areas and optimized efficiently through a linear programming problem. In cases where a brief observation period fails to establish whether an object can be detached, the outcomes of the initial minimization can serve as a foundation for a more complex optimization utilizing a longer video sequence. Consequently, this leads to a fully unsupervised framework for identifying and segmenting an arbitrary and unknown number of objects. We evaluate our method to demonstrate both its potential and the constraints of our approach.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0599", "problem_id": "05990001", "content": "Recent approaches have concentrated on developing a cohesive semantic-aligned visual representation for transferring knowledge between two domains, neglecting the role of semantic-free visual representations in mitigating biased recognition issues. In this study, we introduce a novel Domain-aware Visual Bias Eliminating (DVBE) network that generates two complementary visual representations—semantic-free and semantic-aligned—to handle seen and unseen domains independently. We specifically investigate cross-attentive second-order visual statistics to refine the semantic-free representation and implement an adaptive margin Softmax to maximize inter-class discrepancies. Consequently, the semantic-free representation becomes sufficiently discriminative to accurately predict known classes while also filtering out unseen images through domain detection based on predicted class entropy. For unseen images, we automatically identify an optimal architecture for semantic-visual alignment rather than relying on manual configurations for predicting unseen classes. With precise domain detection, the issue of biased recognition towards the seen domain is considerably alleviated. Experiments conducted on five benchmark datasets for classification and segmentation reveal that DVBE surpasses existing methods by an average improvement of 5.7%.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0600", "problem_id": "06000001", "content": "Determining the underlying structure of graphs based on node observations represents a significant challenge in network science. Moving beyond traditional single-graph inference and inspired by social and biological networks, this work addresses the joint estimation of multiple graphs from node signals that exhibit stationarity within these graphs. Mathematically, graph stationarity establishes a polynomial relationship between signal covariance and the sparse matrix representing the graph structure, as exemplified by Markov random fields where the precision matrix reveals the sparse graph. From a modeling standpoint, such stationary signals can represent linear processes operating across potentially unknown networks. By exploiting the commutative property of matrix polynomials, we develop a convex optimization approach with theoretical guarantees for exact graph recovery under ideal covariance conditions. We also establish probabilistic bounds on estimation error relative to sample size and other critical parameters. Experimental results with both synthetic and real datasets validate the method's accuracy with perfect covariance information and demonstrate its resilience to noise.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0601", "problem_id": "06010001", "content": "The impressive cognitive capabilities of Convolutional Neural Networks (CNNs) come at the cost of substantial computational requirements. To mitigate this, numerous optimization techniques have been devised to reduce model redundancy by identifying and eliminating non-essential components, such as weight sparsity and filter pruning, based on their static significance assessed using internal parameter information. Nevertheless, these approaches overlook the dynamic interplay between model components and external inputs, which can cause the significance of model components to fluctuate with per-input feature activation, resulting in sub-optimal outcomes. To address this limitation, this work presents a dynamic CNN optimization framework that leverages the neural network attention mechanism to optimize model performance. The proposed framework comprises two key components: (1) pruning of channel and column feature maps during the testing phase, and (2) targeted dropout for optimization during the training phase. This dynamic framework offers several advantages, including the ability to precisely identify and remove redundant features on a per-input basis, while considering model-input interactions, as well as flexibly eliminating feature map redundancy across multiple dimensions. Furthermore, the co-optimization of training and testing phases facilitates dynamic pruning while maintaining model accuracy, even at high feature pruning ratios, as demonstrated by extensive experiments showing FLOPs reductions of 37.4% to 54.5% with negligible accuracy loss on various test networks, as shown in Figure A, B, C (References [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0602", "problem_id": "06020001", "content": "The Hamiltonian formalism is fundamental in both classical and quantum physics, serving as a primary method for modeling the continuous-time evolution of systems with conserved quantities. Hamiltonians possess advantageous characteristics, including time reversibility and smooth temporal interpolation, which are valuable for various machine learning tasks like sequence prediction, reinforcement learning, and density modeling. However, standard recurrent neural networks often lack these inherent properties. This paper introduces the Hamiltonian Generative Network (HGN), a novel approach for consistently learning Hamiltonian dynamics from complex, high-dimensional data (e.g., images) without strong assumptions about the domain. After training, HGN can generate new trajectories, perform forward and backward rollouts in time, and modulate the learned dynamics' speed. Furthermore, a simple architectural modification transforms HGN into a Neural Hamiltonian Flow (NHF), a potent normalizing flow model leveraging Hamiltonian dynamics for expressive density modeling. This research aims to showcase the practical benefits of applying the Hamiltonian formalism within deep learning.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0603", "problem_id": "06030001", "content": "Numerous practical applications involve structured data, where both input and output exhibit complex interdependencies. Conventional classification and regression approaches frequently fail to capture high-order interactions within both input and output simultaneously. This study introduces a deep learning framework designed to establish an effective nonlinear mapping from structured input to structured output. The proposed method combines high-order hidden units, supervised discriminative pretraining, and high-order auto-encoders to achieve this objective. Experimental results on three datasets demonstrate superior performance compared to existing techniques. While the current research primarily addresses structured output regression—an underexplored domain—the framework can also be adapted for structured label classification tasks.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0604", "problem_id": "06040001", "content": "We introduce a computationally efficient technique designed to enhance the explainability of classification networks that perform localized predictions. This method integrates (Grad)CAM maps into the training procedure by adjusting the loss function, avoiding the need for supplementary architectural components. Empirical evidence suggests that this approach improves (Grad)CAM interpretability, as quantified by multiple metrics. Intended for use in both embedded systems and conventional deep architectures, the technique leverages second-order derivatives during training, thereby eliminating the necessity for additional model layers.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0605", "problem_id": "06050001", "content": "The efficiency of learning new tasks that change over time can be enhanced through meta-transfer learning, which leverages both meta-learning and transfer learning. However, the traditional attention mechanism may not be effective in this context due to the dynamic nature of tasks and limited context. This study utilizes the Sequential Neural Processes (SNP) model to empirically demonstrate its susceptibility to underfitting, similar to Neural Processes, and further shows that standard attention mechanisms are inadequate in the meta-transfer setting. To address this, a novel attention mechanism called Recurrent Memory Reconstruction (RMR) is proposed, which generates an imaginary context that is updated and reconstructed through interaction, proving essential for effective attention in meta-transfer learning. By integrating RMR into SNP, the Attentive Sequential Neural Processes-RMR (ASNP-RMR) model is developed, and its superior performance over baseline models is demonstrated across various tasks, as shown in Figure A, B, C (citations remain unchanged).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0606", "problem_id": "06060001", "content": "Generative Adversarial Networks (GANs) are presently the leading approach for creating visual content, with specific architectures and training techniques achieving remarkable results in producing lifelike synthetic images, especially of human faces. Nevertheless, GANs have not yet matched this success when applied to 3D objects, largely because existing implementations rely on 3D convolutional architectures working with discrete volumetric representations. This paper introduces MeshGAN, the first intrinsic GAN framework designed to operate directly on 3D meshes. Both quantitative and qualitative evaluations demonstrate that MeshGAN can synthesize high-quality 3D facial models with diverse identities and expressions.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0607", "problem_id": "06070001", "content": "Safely navigating an unfamiliar dynamical system is essential for the implementation of reinforcement learning (RL) in physical contexts where failures could lead to severe consequences. In situations where knowledge of the system's dynamics is limited, a diverse set of transition data that encompasses pertinent areas of the state-action space is required for both model-based and model-free RL approaches. Inspired by the cooling processes within Google's data centers, we investigate methods for reliably determining the parameters of a system model with specified accuracy and confidence. Our emphasis is on learning an unknown linear system impacted by Gaussian noise with the initial assumption that only a nominal safe action is recognized. We characterize safety by adhering to particular linear constraints on the state space (such as requirements for process variables) that must be maintained throughout the entirety of a trajectory. Moreover, leveraging a Probably Approximately Correct (PAC) style estimation error bound for model parameters, we demonstrate a method for establishing safe regions within the action space by incrementally expanding a ball around the nominal safe action. This framework allows for the application of any exploration strategy that selects actions from these identified safe regions. Experiments conducted on a simplified model of data center cooling dynamics indicate that accurately determining safe regions can enhance the sample efficiency of safe exploration.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0608", "problem_id": "06080001", "content": "Utilizing synthetically generated data presents significant opportunities to enhance monocular depth estimation and similar geometric tasks, yet bridging the gap between synthetic and real-world domains remains a complex challenge. Recent efforts have largely explored unsupervised domain adaptation, but we examine a more practical scenario where a limited set of real images with ground-truth labels complements a large synthetic dataset. In this context, current domain translation methods prove difficult to train and provide minimal benefits compared to basic approaches that combine real and synthetic data. A major limitation arises from real-world images containing unfamiliar objects and clutter absent in synthetic training, a high-level domain shift that existing translation models fail to address. To overcome this, we introduce an attention mechanism designed to detect and eliminate challenging out-of-domain regions in real images, thereby enhancing depth prediction for models primarily trained on synthetic data. Our proposed attend-remove-complete (ARC) method is rigorously evaluated and demonstrates superior performance over leading domain adaptation techniques for depth estimation. The visualization of removed regions offers interpretable insights into the synthetic-real domain disparity.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0609", "problem_id": "06090001", "content": "Predicting pedestrian trajectories is a crucial aspect of autonomous driving, yet it remains highly challenging due to intricate interactions among pedestrians. Existing approaches relying on dense, undirected interactions often model unnecessary connections and overlook movement trends, leading to significant deviations from real-world behavior. To address these limitations, we introduce a Sparse Graph Convolution Network (SGCN) for trajectory prediction. The SGCN employs a sparse directed spatial graph to selectively model relevant interactions among pedestrians. Additionally, a sparse directed temporal graph captures motion tendencies, enhancing predictions by leveraging observed directional patterns. These two sparse graphs are combined to estimate parameters for a bi-Gaussian distribution, generating trajectory forecasts. Evaluations on the ETH and UCY datasets demonstrate that our method achieves superior performance, reducing Average Displacement Error (ADE) by 9% and Final Displacement Error (FDE) by 13% compared to state-of-the-art techniques. Visualization results further confirm the model's ability to identify meaningful interactions and movement trends among pedestrians.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0610", "problem_id": "06100001", "content": "While discussions on Green AI often overlook explainability, this study investigates efficient methods to approximate resource-intensive explainers. We introduce Empirical Explainers, which employ feature attribution modeling by learning to predict costly explainers' attribution maps from data. Evaluated in the language domain, Empirical Explainers demonstrate strong performance in replicating their expensive counterparts while drastically reducing computational overhead. These models offer a viable solution to alleviate the computational demands of neural explanations in scenarios where minor approximation errors are acceptable.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0611", "problem_id": "06110001", "content": "Multi-scale point cloud analysis relies heavily on sampling, grouping, and aggregation. This paper introduces a new data-driven sampler learning approach tailored for point-wise analysis. Departing from conventional techniques like Farthest Point Sampling (FPS), our method learns sampling strategies in conjunction with downstream applications. We posit that uniform sampling methods such as FPS may not be universally optimal; for instance, increased sampling density near boundaries can enhance point-wise classification for segmentation. To this end, we propose a sampler learning strategy that learns sampling point displacement, guided by task-specific ground truth, and integrates seamlessly with the target tasks during training. We showcase the effectiveness of our approach across diverse point-wise analysis architectures, including semantic part segmentation, point cloud completion, and keypoint detection. Experimental results demonstrate significant performance gains over existing baseline methods through the joint learning of the sampler and task.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0612", "problem_id": "06120001", "content": "Creating animations from textual descriptions has diverse applications, including visualizing movie scripts, animating virtual characters, and planning robot movements. These descriptions may specify various actions, their velocities, orientations, and target locations. The primary challenge in converting language to motion sequences lies in translating linguistic elements into corresponding animations. To tackle this multimodal task, we present a neural framework named Joint Language to Pose (JL2P), which learns a unified representation of language and motion. This shared embedding space is trained end-to-end using a curriculum strategy that prioritizes simpler, shorter sequences before progressing to more complex ones. Our model is tested on a publicly accessible dataset containing 3D motion sequences paired with human-written descriptions. Quantitative assessments and subjective evaluations demonstrate that our method produces more precise animations, which human evaluators perceive as more realistic compared to alternative data-driven techniques.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0613", "problem_id": "06130001", "content": "Static gesture recognition serves as an efficient non-verbal communication medium between users and their devices; nonetheless, many contemporary techniques are affected by the relative positioning of the user's hands concerning the capturing device, leading to potential occlusion of gesture components. In this study, we introduce two approaches for gesture recognition utilizing synchronized recordings from two depth cameras to mitigate the occlusion issue. The first approach employs a traditional method involving iterative closest point registration for precise fusion of point clouds combined with a single PointNet architecture for classification, while the second utilizes a dual Point-Net architecture for classification without the need for registration. Through evaluation on a manually assembled dataset of 20,100 point clouds, we demonstrate a 39.2% decrease in misclassification for the fused point cloud technique and a 53.4% reduction for the dual PointNet, in comparison to a conventional single camera method.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0614", "problem_id": "06140001", "content": "The ability to interpret text within visual environments is critical for various reasoning applications, as textual elements often convey vital information. For instance, a \"deep water\" label on a warning sign alerts individuals to potential hazards. While the TextVQA task, which necessitates reading and comprehending image-based text to answer questions, has gained attention, current TextVQA methodologies primarily rely on specialized pairwise fusion techniques between two modalities. These methods are often limited to a single prediction step by framing TextVQA as a classification problem. To address these limitations, we introduce a new TextVQA model that leverages a multimodal transformer architecture combined with a comprehensive representation of text in images. Our model facilitates a natural and uniform fusion of modalities by embedding them into a shared semantic space, where self-attention mechanisms are used to model context within and between modalities. Additionally, it incorporates iterative answer decoding through a dynamic pointer network, enabling the model to construct answers via multi-step prediction rather than single-step classification. Empirical evaluations demonstrate that our model significantly surpasses existing methods on three established benchmark datasets for the TextVQA task.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0615", "problem_id": "06150001", "content": "We tackle a question answering challenge involving real-world images, designed as a Visual Turing Test. By integrating recent developments in image representation and natural language processing, we introduce Neural-Image-QA, a comprehensive approach to this issue where all components are trained in unison. Unlike prior work, we confront a multi-modal challenge where the language response (the answer) depends on both visual and linguistic inputs (the image and the question). Our Neural-Image-QA approach significantly enhances performance, achieving double the efficacy of the previous top method in this area. Furthermore, we offer deeper insights into the issue by examining the extent of information contained solely within the language component, for which we create a new human baseline. To evaluate human agreement, which is connected to the ambiguities present in this demanding task, we introduce two innovative metrics and gather additional responses, thereby expanding the original DAQUAR dataset into DAQUAR-Consensus.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0616", "problem_id": "06160001", "content": "This research investigates the adversarial multi-armed bandit problem with partial observability, incorporating switching costs for changing actions alongside action-specific losses. Existing approaches yield regret bounds tied to the feedback graph's independence number, but we propose a novel algorithm whose performance depends solely on the graph's domination number. We complement this finding with a lower bound analysis. Additionally, we present an enhanced algorithm that achieves better policy regret guarantees when partial counterfactual feedback is accessible.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0617", "problem_id": "06170001", "content": "This paper introduces a reinforcement learning agent designed to tackle challenging exploration games by acquiring a diverse set of directed exploratory policies. To train these policies, an episodic memory-based intrinsic reward is developed, utilizing k-nearest neighbors applied to the agent's recent experiences, which motivates the agent to consistently re-explore its environment. A self-supervised inverse dynamics model trains the embeddings for nearest neighbor searches, orienting the novelty signal towards controllable aspects of the environment. The Universal Value Function Approximators (UVFA) framework is used to concurrently learn multiple directed exploration policies with a shared neural network, allowing for varying exploration/exploitation trade-offs. This shared network facilitates transfer learning, where primarily exploratory policies enhance the effectiveness of exploitative policies. The presented approach is compatible with modern distributed RL agents that gather extensive experience from numerous actors operating in parallel across different environment instances. Results show that the proposed method doubles the baseline agent's performance in difficult exploration scenarios within the Atari-57 suite, while also maintaining high scores across other games, achieving a median human-normalized score of 1344.0%. Significantly, this method is the first to achieve non-zero rewards (mean score of 8,400) in Pitfall! without relying on demonstrations or hand-engineered features.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0618", "problem_id": "06180001", "content": "Transfer learning stands out as a major achievement in deep learning. The discovery that pre-training a network on a comprehensive source dataset (e.g., ImageNet) enhances performance after fine-tuning on a typically smaller target dataset has significantly benefited numerous language and vision applications. However, its efficacy in 3D point cloud understanding remains largely unexplored, presenting a valuable opportunity given the intensive effort required for 3D data annotation. This study seeks to promote research in 3D representation learning, distinguishing itself from prior work by focusing on high-level scene understanding tasks. To this aim, we evaluate the impact of unsupervised pre-training on a large source set of 3D scenes using a selection of diverse datasets and tasks. The results are highly promising: by employing a consistent architecture, source dataset, and contrastive loss triplet for pre-training, we surpass recent state-of-the-art results in segmentation and detection across six benchmarks encompassing indoor and outdoor, real and synthetic datasets. This demonstrates the learned representation's ability to generalize across domains. Moreover, the degree of improvement resembled that of supervised pre-training, implying that future research should prioritize scaling data collection over detailed annotation. We anticipate that these outcomes will stimulate further investigation into unsupervised pretext task design for 3D deep learning.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0619", "problem_id": "06190001", "content": "This study addresses transfer learning, which involves leveraging knowledge acquired from addressing one problem to facilitate the solution of a distinct, yet related, problem. We introduce a novel method and an efficient algorithm for prioritizing and choosing representations derived from a Restricted Boltzmann Machine trained on a source domain, for subsequent transfer to a target domain. Empirical evaluations using the MNIST, ICDAR, and TiCC image datasets demonstrate that the presented adaptive feature ranking and transfer learning approach yields statistically significant enhancements in the training of RBMs. The proposed method's generality lies in its capacity to select knowledge independent of specific target domain relationships, functioning effectively with unsupervised learning and knowledge-based transfer.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0620", "problem_id": "06200001", "content": "Addressing multi-goal reinforcement learning problems with sparse rewards poses significant challenges. While existing methods have employed goal relabeling techniques to mitigate the issues associated with sparse rewards, they remain limited in terms of efficiency and fail to fully utilize the collected experiences. This paper introduces Model-based Hindsight Experience Replay (MHER), a novel approach that enhances the efficiency of experience utilization by harnessing environmental dynamics to generate virtual achieved goals. By substituting original goals with virtually generated goals, derived from interactions with a trained dynamics model, MHER devises a new relabeling method. This method, in conjunction with reinforcement learning and supervised learning, facilitates efficient policy improvement. Theoretical analysis demonstrates that the supervised component of MHER, which involves goal-conditioned supervised learning using the proposed relabeling data, optimizes a lower bound on the multi-goal reinforcement learning objective. Experimental evaluations across various point-based tasks and simulated robotics environments reveal that MHER yields substantially higher sample efficiency compared to state-of-the-art methods, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0621", "problem_id": "06210001", "content": "The utilization of Deep Neural Networks (DNNs) in automotive Cyber-Physical Systems (CPSs) for autonomous tasks is widespread, but their propensity to make incorrect predictions when faced with anomalous inputs, whether due to Out-of-Distribution (OOD) data or adversarial attacks, is a significant concern. Typically, a separate DNN, known as an assurance monitor, is trained and deployed alongside the primary controller DNN to detect such anomalies, thereby increasing the latency and resource burden. To mitigate this, we propose the development of a single network capable of performing both control predictions and anomaly detection, thereby reducing resource requirements. Deep-Radial Basis Function (RBF) networks, which provide a rejection class in addition to class predictions, offer a potential solution for runtime anomaly detection. Although RBF activation functions have limited the applicability of these networks to classification tasks, we demonstrate their effectiveness in detecting anomalies in CPS regression tasks, such as continuous steering predictions. By designing deep-RBF networks based on popular DNNs like NVIDIA DAVE-II and ResNet20, and leveraging the rejection class to detect adversarial attacks, including physical and data poison attacks, we evaluate the performance of these networks using the DeepNNCar hardware CPS testbed and the German Traffic Sign Benchmark (GTSB) dataset, showing that deep-RBF networks can effectively detect these attacks without incurring additional resource requirements.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0622", "problem_id": "06220001", "content": "Zero-shot learning (ZSL) endeavors to classify images from classes not encountered during training by leveraging semantic relationships between these unseen classes and the seen classes. Recent studies have highlighted the significance of localized features and fine-tuning the feature extractor to acquire features that are both discriminative and transferable. Nevertheless, these approaches typically necessitate intricate attention mechanisms or part detection modules to achieve explicit localization within the visual domain. Conversely, this paper introduces a method for localizing representations within the semantic/attribute space, employing a straightforward yet efficient framework that achieves implicit localization. By concentrating on attribute representations, our proposed method attains state-of-the-art results on the CUB and SUN datasets, while also demonstrating competitive performance on the AWA2 dataset, surpassing more complex methodologies that rely on explicit visual localization. The ease of implementation of our method renders it suitable as a new baseline for zero-shot learning. Furthermore, the generated localized representations offer high interpretability as attribute-specific heatmaps.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0623", "problem_id": "06230001", "content": "The increasing availability of unlabeled data has made Positive Unlabeled (PU) learning a significant challenge. Recent GAN-based PU methods have shown potential in addressing this task, as Generative Adversarial Networks (GANs) avoid deterministic bias and dimensionality constraints. However, current GAN-based PU approaches face limitations, including sensitivity to prior knowledge, complex architectures, and initial overfitting. To address these challenges, we integrate a biased PU risk into the standard GAN discriminator loss function, guiding the discriminator to push the generator toward the unlabeled data distribution while diverging from the positive samples. Our proposed model, D-GAN, learns the negative examples distribution without requiring prior knowledge. Experimental results show that D-GAN surpasses existing PU methods without prior knowledge, effectively resolving their shortcomings.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0624", "problem_id": "06240001", "content": "Reinforcement learning addresses a range of complex issues. Many relevant applications feature extremely large state spaces, which necessitate function approximation to facilitate feasible computations. Furthermore, the learner typically possesses only a solitary stream of experience to assess numerous potential actions, requiring algorithms capable of off-policy learning. However, the integration of off-policy learning with function approximation can result in the divergence of temporal difference methods. While recent advancements in gradient-based temporal difference methods have shown promise for achieving stability, they often involve costly hyperparameter tuning. Concurrently, advancements in online learning have introduced parameter-free approaches that provide minimax optimal guarantees, albeit up to logarithmic terms; however, their use in reinforcement learning remains largely unexamined. In this study, we merge these two approaches, developing parameter-free, gradient-based temporal difference algorithms. Our algorithms operate in linear time and secure high-probability convergence guarantees aligned with those of GTD2, adjusted for logarithmic factors. Our experimental results indicate that our methods maintain strong predictive performance when compared to fully-tuned baselines, without requiring any tuning at all.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0625", "problem_id": "06250001", "content": "This paper centers on a forecasting technique for groups of time series utilizing cluster analysis algorithms. The K-means algorithm is proposed as a fundamental approach for clustering. The coordinates of cluster centers are aligned with summary data of time series, represented by the clusters' centroids. A representation of the time series and the centroids is achieved through forecasting models grounded in strict binary trees and an adapted clonal selection algorithm. These forecasting models reveal the potential to establish analytical relationships. It is proposed to employ a unified forecasting model, developed for the centroid time series of a cluster, to forecast individual time series within that cluster. The effective application of the proposed method for forecasting grouped time series is illustrated.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0626", "problem_id": "06260001", "content": "In the realm of 2D convolutional networks for point clouds, point-based methods directly process fixed-size point sets. Through an examination of PointNet, a foundational work in applying deep learning to point sets, we demonstrate that existing point-based techniques primarily function as spatial relationship processors. This paper proposes an alternative framework called PE-Net, which learns high-dimensional representations of point clouds and transforms unordered input points into feature vectors compatible with standard 2D CNNs. Unlike current approaches, our network accommodates varying input point counts. Experimental results on classification and part segmentation tasks demonstrate that PE-Net achieves state-of-the-art performance across challenging datasets, including ModelNet and ShapeNetPart.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0627", "problem_id": "06270001", "content": "Unsupervised anomaly detection (UAD) constructs one-class classifiers using only normal images to identify abnormal samples that deviate from established normal patterns. UAD offers two key advantages over fully supervised methods: it can directly utilize extensive datasets from health screening programs, which primarily contain normal images, thereby avoiding the expensive manual annotation of abnormal samples and the challenges associated with training on highly imbalanced datasets. Moreover, UAD methods have the potential to identify and localize various types of lesions that differ from normal patterns. A major hurdle for UAD methods is learning effective low-dimensional image representations for detecting and localizing subtle abnormalities, often characterized by small lesions. To overcome this, we introduce a new self-supervised representation learning technique, termed Constrained Contrastive Distribution learning for anomaly detection (CCD), which acquires detailed feature representations by concurrently predicting the distribution of augmented data and image contexts, employing contrastive learning with pretext constraints. These learned representations can then be used to train more sensitive anomaly detection models. Comprehensive experimental results demonstrate that our method surpasses existing state-of-the-art UAD approaches across three distinct colonoscopy and fundus screening datasets. Our code is available at https://github.com/tianyu0207/CCD.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0628", "problem_id": "06280001", "content": "This study addresses the exploration-exploitation trade-off in finite-horizon reinforcement learning within a metric state-action space. We propose Kernel-UCBVI, a model-based optimistic algorithm that employs a non-parametric kernel estimator to approximate rewards and transitions, utilizing the MDP's smoothness to effectively manage exploration and exploitation. Unlike previous methods with regret guarantees, our approach avoids partitioning the state-action space. For problems involving K episodes and horizon H, we derive a regret bound of O\\left( H^3 K^, \\frac\\right)}\\right), where d represents the covering dimension of the joint state-action space. Experimental results demonstrate the effectiveness of Kernel-UCBVI on both discrete and continuous MDPs.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0629", "problem_id": "06290001", "content": "This study identifies a limitation in standard GANs, where the discriminator fails to capture high-frequency information due to the downsampling layers in its architecture, leading to a lack of incentive for the generator to learn high-frequency data content and resulting in a notable difference in spectral characteristics between generated and real images. Given the bijective nature of the Fourier transform, reducing this spectral discrepancy is expected to improve GAN performance. To address this issue, we propose SSD-GAN, an enhanced GAN framework that incorporates a frequency-aware classifier into the discriminator to assess input realness in both spatial and spectral domains, thereby encouraging the generator to learn and reproduce high-frequency details of real data. The SSD-GAN method is versatile and can be readily integrated into most existing GAN frameworks without incurring significant additional costs. Our approach is evaluated on various network architectures, objective functions, and datasets, demonstrating its effectiveness, and the code is available at https://github.com/cyq373/SSD-GAN.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0630", "problem_id": "06300001", "content": "In (\\cite), we approached machine learning as a problem of coding and dimensionality reduction, subsequently introducing a straightforward unsupervised dimensionality reduction technique called deep distributed random samplings (DDRS). This paper expands the method to accommodate supervised learning in an incremental manner. The central concept is to integrate label information into the coding process by redefining each center in DDRS to have multiple output units corresponding to its class. While our supervised learning method shares similarities with random forests (\\cite), we highlight several distinctions: (i) each layer of our approach examines the relationships among a subset of training data points with all training points, whereas random forests construct each decision tree using only a portion of the training data independently; (ii) our technique creates a progressively narrowed network by sampling fewer data points, while random forests achieve a similar effect by merging subclasses; (iii) our approach trains linearly from the bottom to the top layer, whereas random forests develop each tree from the top to the bottom layer through splitting; (iv) our method implicitly encodes output targets in sparse codes, in contrast to random forests, which store output targets by retaining the class attributes of activated nodes. Consequently, our method presents a simpler, more direct, and potentially superior alternative, despite both methods relying on two fundamental components—randomization and nearest neighbor optimization—as their core elements. This preprint serves to safeguard the incremental idea presented in (\\cite), with a comprehensive empirical evaluation to be disclosed at a later date.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0631", "problem_id": "06310001", "content": "Face performance capture and reenactment methods traditionally rely on multiple cameras and sensors, either distanced from the subject or integrated into bulky wearable devices, which restricts their usability in mobile and outdoor contexts. We introduce EgoFace, an innovative and lightweight method for capturing facial performance and enabling front-view videorealistic reenactment using a single egocentric RGB camera. This streamlined setup is designed for use in uncontrolled environments and is suitable for telepresence applications, such as video conferencing in dynamic settings. The input images are encoded into a lower-dimensional latent space corresponding to facial expression parameters. By employing careful adversarial training for the synthetic rendering in the parameter space, we generate a videorealistic animation. Our challenge lies in the fact that the human visual system is highly attuned to minor facial irregularities that may appear in the final output, with this sensitivity being particularly pronounced in video formats. Our approach is refined during a pre-processing phase using supervised techniques that do not require manual annotations. EgoFace is capable of capturing a broad range of facial expressions, including mouth and asymmetrical movements, and performs effectively under varying lighting conditions and backdrops, accommodating individuals from diverse ethnic backgrounds while functioning in real time.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0632", "problem_id": "06320001", "content": "The task of person image synthesis, such as transferring poses, poses significant challenges due to the substantial variability and occlusion present in images. Current approaches struggle to accurately predict obscured areas and disentangle the shape and style of clothing, thereby limiting their utility in person image editing applications. This paper introduces PISE, a groundbreaking two-stage generative model designed for person image synthesis and editing, capable of producing realistic images with specified poses, textures, or semantic layouts. Our approach involves initially generating a human parsing map that aligns with the target pose using a parsing generator, which represents the clothing shape, and subsequently producing the final image via an image generator for pose transfer. To effectively separate the shape and style of clothing, we employ a joint global and local per-region encoding and normalization strategy, enabling the prediction of reasonable clothing styles for invisible regions, and propose spatial-aware normalization to preserve spatial context relationships from the source image. The outcomes of both qualitative and quantitative experiments underscore the superiority of our model in human pose transfer, and additional results from texture transfer and region editing demonstrate its applicability to person image editing, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0633", "problem_id": "06330001", "content": "This paper introduces Recursive Reasoning-Based Bayesian Optimization (R2-B2), a recursive reasoning framework within Bayesian optimization (BO) designed to simulate interactions in repeated games. This framework models the reasoning of self-interested agents with limited rationality, each possessing intricate, costly-to-evaluate payoff functions that are initially unknown. The R2-B2 algorithm's versatility lies in its ability to accommodate diverse game types—including constant-sum, general-sum, and common-payoff games—by not imposing constraints on the relationships between agents' payoff functions. We demonstrate that an R2-B2 agent, by employing a reasoning level of at least 2 and exceeding that of other agents by one level, achieves a more rapid asymptotic convergence to a state of no regret compared to strategies lacking recursive reasoning. Additionally, we introduce R2-B2-Lite, a computationally efficient adaptation of R2-B2, which offers a less stringent convergence guarantee. Empirical evaluations using synthetic games, adversarial machine learning, and multi-agent reinforcement learning validate the effectiveness and broad applicability of our R2-B2 algorithm.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0634", "problem_id": "06340001", "content": "Specifying a task or environment distribution is crucial for various reinforcement learning (RL) challenges, such as ensuring robustness, facilitating transfer learning, enabling unsupervised RL, and fostering emergent complexity. However, the manual creation of effective environment distributions is a difficult and time-consuming process. We introduce Unsupervised Environment Design (UED) as a novel approach where developers supply environments with undefined parameters, which are then automatically configured to generate a distribution of feasible and solvable environments. Current automated environment generation techniques often fail because domain randomization lacks the capacity to create structure or adjust environmental difficulty based on the agent's learning, while minimax adversarial training results in intractable, worst-case scenarios. To address these limitations and produce structured, solvable environments, we introduce a second, antagonist agent that collaborates with the environment-generating adversary. This adversary aims to generate environments that maximize regret, which we define as the disparity between the protagonist's and antagonist's achieved returns. We term this methodology Protagonist Antagonist Induced Regret Environment Design (PAIRED). Our experiments show that PAIRED generates a progressive curriculum of increasingly challenging environments, and agents trained with PAIRED exhibit improved zero-shot transfer performance when evaluated in new environments.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0635", "problem_id": "06350001", "content": "Estimating age from a single facial image is a critical task in human-computer interaction and computer vision, with broad real-world applications. Current methods often yield low accuracy for unconstrained face images, primarily due to their reliance on global features while overlooking fine-grained details in age-sensitive regions. To address this, we introduce an attention-based long short-term memory (AL) network for fine-grained age estimation, drawing inspiration from fine-grained categorization and visual attention mechanisms. Our approach integrates residual networks (ResNets) or residual networks of residual networks (RoR) with LSTM units to form AL-ResNets or AL-RoR networks, enhancing local feature extraction from age-sensitive areas and boosting estimation accuracy. Initially, a ResNets or RoR model pretrained on ImageNet is fine-tuned on the IMDB-WIKI-101 dataset for age prediction. Subsequently, the model is further fine-tuned on target age datasets to capture global facial features. LSTM units are employed to automatically locate age-sensitive regions and extract their local features. Age classification is performed on the Adience dataset, while regression experiments use the Deep EXpectation (DEX) algorithm on MORPH Album 2, FG-NET, and 15/16LAP datasets. By fusing global and local features, our method achieves superior prediction results. Experiments confirm the robustness and effectiveness of AL-ResNets and AL-RoR, demonstrating state-of-the-art performance compared to other convolutional neural networks.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0636", "problem_id": "06360001", "content": "Contemporary supervised sketch-based image retrieval (SBIR) techniques demonstrate outstanding results. Nonetheless, the challenges associated with data acquisition and annotation present a significant obstacle to the practical implementation of real-world applications. This paper introduces the first endeavor in unsupervised SBIR, aiming to eliminate the need for labeling costs (such as category labels and sketch-photo alignments) traditionally required for training. Existing single-domain unsupervised representation learning methods show subpar effectiveness in this context, owing to the distinctive cross-domain (sketch and photo) characteristics of the issue. Consequently, we propose a novel framework that conducts both unsupervised representation learning and sketch-photo domain alignment concurrently. This is fundamentally supported by using joint distribution optimal transport (JDOT) for aligning data across different domains during the learning process, which we enhance with adaptable cluster prototypes and feature memory banks to boost scalability and effectiveness. Extensive evaluations indicate that our framework delivers exceptional performance in the newly established unsupervised context, matching or exceeding the performance of state-of-the-art approaches in zero-shot scenarios.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0637", "problem_id": "06370001", "content": "This study introduces a new transfer learning methodology, termed corresponding projections, designed to facilitate orphan screening by predicting the binding affinities of compounds to a protein lacking training data. The ability to identify high-affinity compounds is crucial in medical research, as it has significant implications for drug discovery and development. By leveraging a set of prediction models for proteins with available labelled training data, as well as a defined similarity metric between these proteins, corresponding projections generates a tailored model for the orphan protein, ensuring that the similarity between the models mirrors the similarity between the proteins. Based on this assumption, an efficient algorithm is derived for kernel methods, and empirical results demonstrate that this approach surpasses current state-of-the-art methods in orphan screening.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0638", "problem_id": "06380001", "content": "RF-sensing, which involves examining and interpreting patterns in electromagnetic signals caused by movement or environmental changes, has been a focus of research for over ten years. Given the widespread availability of electromagnetic signals through cellular networks, RF-sensing could serve as a versatile sensing method with uses in smart homes, retail, localization, gesture recognition, and intrusion detection. Notably, current cellular infrastructure could simultaneously support both communication and sensing, a concept expected to be integral in future networks. This work explores leveraging NR-sidelink device-to-device communication to enable flexible, device-driven sensing in next-generation cellular systems. A key challenge addressed here is the angle and rotation sensitivity of sidelink-based RF-sensing. The study examines techniques for achieving rotational invariance in mmWave point-cloud data and distributed processing across devices with varying angles and distances. A graph-based encoder is introduced to extract spatio-temporal features, alongside four multi-angle learning methods, evaluated on a publicly available dataset featuring 15 participants performing 21 gestures captured from 8 different angles.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0639", "problem_id": "06390001", "content": "The field of reinforcement learning (RL) remains highly dynamic, with numerous advancements, particularly in the growing domain of deep RL (DRL). Despite this progress, several scientific and technical hurdles persist, including action abstraction and environmental exploration challenges, which intrinsic motivation (IM) can help mitigate. This paper presents a comprehensive review of IM's role in DRL, classifying various types of intrinsic motivations and analyzing their respective strengths and weaknesses in addressing these obstacles. Furthermore, we examine critical unresolved research questions in DRL, focusing on task achievement strategies. We propose that addressing these challenges could enable the development of a more comprehensive architecture capable of handling diverse tasks. This envisioned architecture is structured around key components, integrating an RL algorithm with an IM module designed for information compression.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0640", "problem_id": "06400001", "content": "In numerous reinforcement learning applications, defining both a reward function and explicit constraints offers a more practical approach compared to solely shaping behavior through reward function design, particularly when safety constraints are paramount for systems interacting with humans. While recent progress in policy search algorithms (Mnih et al., 2016, Schulman et al., 2015, Lillicrap et al., 2016, Levine et al., 2016) has facilitated advancements in high-dimensional control, these methods typically do not address constrained environments. To address this, we introduce Constrained Policy Optimization (CPO), a novel, versatile policy search algorithm designed for constrained reinforcement learning, ensuring near-constraint adherence at every iteration. This method facilitates the training of neural network policies for high-dimensional control while guaranteeing policy behavior throughout the training process. These guarantees stem from a novel theoretical finding, which is valuable in its own right: we establish a relationship between the expected returns of two policies and their average divergence. We validate the efficacy of our method through experiments on simulated robot locomotion tasks, where agents are required to meet safety-related constraints.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0641", "problem_id": "06410001", "content": "Medical image analysis relies heavily on deformable image registration and regression, but these tasks are hindered by their high computational costs, particularly when dealing with large datasets comprising thousands of images. As a result, researchers often resort to cluster computing, which makes these approaches reliant on substantial computational infrastructure, and the demand for resources increases with the growth of study sizes. This limitation hinders the adoption of deformable image registration and regression in clinical settings and as components of other image analysis algorithms. To address this, we introduce a rapid predictive approach for image registration, leveraging fast registration predictions to approximate a simplified geodesic regression model that captures longitudinal changes in the brain. Our method achieves significantly faster performance than traditional optimization-based regression models, enabling large-scale analysis on a single graphics processing unit (GPU). We validate our approach using 3D brain magnetic resonance images (MRI) from the ADNI datasets.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0642", "problem_id": "06420001", "content": "The widespread adoption of deep learning has led to the extensive use of face recognition systems in safety-critical domains, but the presence of adversarial examples poses significant security threats to these systems. To assess and enhance their vulnerability, this paper introduces Meaningful Adversarial Stickers, a novel attack method that leverages everyday stickers and manipulates their placement parameters on the face, offering a physically viable and straightforward approach. Unlike existing methods that involve designing and printing perturbation patterns, our approach operates in a black-box setting with limited information, making it more challenging and realistic. To optimize sticker placement, rotation, and other parameters, we developed the Region-based Heuristic Differential Algorithm, which employs an inbreeding strategy based on regional aggregation of effective solutions and adaptive evaluation criteria adjustment. Our extensive experiments on the LFW and CelebA datasets, using FaceNet, SphereFace, and CosFace models, yielded attack success rates of 81.78%, 72.93%, and 79.26%, respectively, with only a few hundred queries. Furthermore, physical world experiments confirmed the efficacy of our method under complex conditions, achieving success rates of up to 98.46%, 91.30%, and 86.96% when testers' face postures were continuously changed, as shown in Figure A, B, C (References [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0643", "problem_id": "06430001", "content": "Understanding agent interactions and making informed decisions heavily relies on modeling the behaviors of other agents. Traditional approaches to agent modeling typically operate under the assumption that knowledge of the local observations and actions of the modeled agents is available during execution. To address this assumption, we utilize encoder-decoder architectures to derive representations from the local information of the controlled agent. By using the observations and actions of the modeled agents during the training phase, our models acquire the ability to extract representations about these agents, relying solely on the local observations of the controlled agent. These representations enhance the decision-making policy of the controlled agent, which is trained through deep reinforcement learning; consequently, during execution, the policy does not need access to the information of other agents. We conduct extensive evaluations and ablation studies in various cooperative, competitive, and mixed multi-agent scenarios, demonstrating that our approach significantly outperforms baseline methods that do not leverage the learned representations.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0644", "problem_id": "06440001", "content": "This research investigates reinforcement learning in scenarios with model misspecification, where the true environment is unavailable and only a sufficiently accurate approximation exists. We tackle this challenge by adapting the robust MDP framework to model-free reinforcement learning, which involves sampling states without direct access to model parameters. Robust variants of Q-learning, SARSA, and TD-learning are introduced, with proofs demonstrating their convergence to near-optimal robust policies and value functions. To handle large MDPs, we employ function approximation and establish convergence in two distinct scenarios. Additionally, we prove the convergence of robust approximate policy iteration and value iteration for linear architectures under reasonable conditions. A robust loss function, the mean squared robust projected Bellman error, is formulated, along with stochastic gradient descent algorithms that reliably converge to local minima.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0645", "problem_id": "06450001", "content": "Previous research indicates that face anti-spoofing relies on detecting subtle image artifacts known as \"spoof traces,\" such as color irregularities, 3D mask edges, or Moiré patterns. Developing a universal model to identify these traces can enhance both the generalization of spoof detection and the interpretability of model decisions. However, this task is complicated by the variety of spoofing methods and the absence of ground-truth spoof trace annotations. To address this, we propose an adversarial learning framework that decomposes spoofed faces into spoof traces and their genuine counterparts. Inspired by physical principles, spoof generation is modeled as a combination of additive and inpainting processes—the former introduces additional patterns (e.g., Moiré effects) that can be removed to reveal the live face, while the latter requires reconstructing obscured regions. Our method employs three additive components and one inpainting component to capture traces across different frequency bands. These disentangled traces enable the synthesis of realistic spoofed images after geometric adjustments, which can then enhance training and detection robustness. Our framework achieves state-of-the-art performance in detecting known, unknown, and open-set spoofing attacks while providing visually plausible trace estimations. Source code and pre-trained models will be released upon publication.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0646", "problem_id": "06460001", "content": "The economic viability of peanut production is heavily influenced by seed maturity, which affects not only seed weight and yield but also seed vigor and overall quality. As peanut seeds mature, the inner mesocarp layer of the pericarp undergoes a color transition from white to black, a process that is currently assessed through manual removal of the exocarp and visual evaluation of the mesocarp color, categorized into various classes ranging from immature (white, yellow, orange) to mature (brown, black). However, this method is time-consuming and prone to variability due to human error in color assessment. To address these limitations, this study explored the application of hyperspectral imaging (HSI) to determine peanut pod maturity without the need for exocarp removal, utilizing spectral differences between mature and immature pods within a classification algorithm. The results demonstrated high accuracy and consistency in classification across different years and cultivars, and the proposed method also enabled the estimation of a continuous-valued, pixel-level maturity index for individual peanut pods, providing a valuable tool for seed quality research and overcoming the issues of labor intensity and subjective error associated with traditional methods, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0647", "problem_id": "06470001", "content": "Deep learning techniques aimed at identifying individuals through electroencephalographic (EEG) brain activity face challenges in leveraging the temporally correlated patterns and session-specific variability inherent in EEG recordings. Additionally, most contemporary methodologies have primarily focused on training and evaluating models using data from a single recording session. To tackle this issue, we approach it through the lens of invariant representation learning. We introduce an adversarial inference strategy to adapt deep learning models for the acquisition of session-invariant, person-discriminative representations that enhance robustness for longitudinal applications. By employing adversarial learning within a deep convolutional network framework, we provide empirical evidence demonstrating the effectiveness of our method using longitudinal EEG data for person identification from half-second EEG epochs.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0648", "problem_id": "06480001", "content": "Decision trees offer versatile solutions for numerous statistical regression challenges. Within a Bayesian approach to regression trees, Markov Chain Monte Carlo (MCMC) methods are employed to sample tree models based on their posterior probabilities. A key challenge in these MCMC algorithms lies in designing effective Metropolis-Hastings steps to modify the tree structure, as they often suffer from slow convergence and poor mixing due to local mode trapping. Previous approaches have relied on discrete-time birth-death mechanisms for Bayesian regression tree models, which perform well only when acceptance rates are sufficiently high—a condition not always met. To address this limitation, we introduce a novel search algorithm based on a continuous-time birth-death Markov process. This method navigates the model space by transitioning between parameter spaces associated with varying tree configurations, ensuring that all proposed moves are accepted. This enhancement significantly improves the convergence and mixing efficiency of the MCMC algorithm. We present theoretical validation for this approach in Bayesian regression tree models and evaluate its empirical performance.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0649", "problem_id": "06490001", "content": "In the field of Computer Vision, Domain Adaptation remains a significant area of investigation. This paper introduces a novel technique that utilizes unlabeled data to minimize the discrepancy between source and target data distributions within a jointly learned feature representation. This is achieved through the creation of a mutually beneficial interaction between the learned embedding and a generative adversarial network, distinguishing it from existing methodologies that employ adversarial networks for generating realistic data and subsequently retraining deep models. The efficacy and versatility of our method are validated through experimentation on three distinct tasks, encompassing varying degrees of complexity: (1) Digit classification (MNIST, SVHN and USPS datasets), (2) Object recognition using OFFICE dataset, and (3) Domain adaptation from synthetic to real data. The results demonstrate that our approach attains state-of-the-art performance in the majority of experimental configurations and represents the only GAN-based method to exhibit robust performance across diverse datasets, including OFFICE and DIGITS.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0650", "problem_id": "06500001", "content": "The rapid advancement of image super-resolution (SR) techniques can be attributed to the development of deep networks and their subsequent innovations. Nevertheless, the opacity of deep learning and deep neural networks is a well-recognized challenge, and SR networks are no exception, with limited research attempting to elucidate their underlying mechanisms. This paper addresses this knowledge gap by conducting an attribution analysis of SR networks, focusing on identifying the input pixels that exert a significant influence on the SR outcomes. To this end, a novel attribution approach, termed local attribution map (LAM), is proposed, which builds upon the integral gradient method with two distinctive features: utilizing the blurred image as the baseline input and employing the progressive blurring function as the path function. The findings based on LAM reveal that SR networks benefiting from a broader range of input pixels tend to perform better, attention and non-local networks extract features from a wider range of input pixels, the receptive field of most deep networks is sufficiently large, and SR networks are more adept at recognizing textures with regular patterns rather than complex semantics, as shown in Figure A, B, C (References [1], [2], [3]). These insights pave the way for new approaches to designing SR networks and interpreting deep models for low-level vision tasks, as discussed in [4] and [5].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0651", "problem_id": "06510001", "content": "In this study, we aim to create a model that characterizes the dynamics of similar yet distinct sets of interacting objects. These sets adhere to shared physical principles while presenting unique features that are represented through a vector description. We propose a model enabling conditional generation from any of these groups based solely on its vectorial description. In contrast to existing approaches to learning dynamical systems, which typically rely on partially provided trajectory dynamics for trajectory completion during generation, our method generates outputs using only the conditioning vector, without any trajectory data at the generation stage. We assess our model's performance in the context of modeling human gait, specifically focusing on pathological gait patterns.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0652", "problem_id": "06520001", "content": "In this study, we introduce an end-to-end feature fusion attention network (FFA-Net) aimed at restoring images free from haze directly. The architecture of FFA-Net is composed of three primary elements: 1) A novel Feature Attention (FA) module that merges Channel Attention with Pixel Attention mechanisms, acknowledging that distinct channel-wise features provide varying degrees of weighted information and that haze distribution varies across different image pixels. FA treats features and pixels with differing importance, thereby enhancing the ability of CNNs to handle different types of data. 2) A basic block design incorporating Local Residual Learning and Feature Attention, where Local Residual Learning enables the less significant data, such as areas with thin haze or low-frequency components, to be bypassed through several local residual connections, allowing the main network to concentrate on more relevant information. 3) An Attention-based Feature Fusion (FFA) structure that adaptively learns feature weights from the Feature Attention (FA) module, assigning greater weight to significant features while also preserving information from shallow layers to be transmitted to deeper layers. Experimental findings indicate that our FFA-Net significantly outperforms existing state-of-the-art single image dehazing techniques both in quantitative and qualitative terms, elevating the highest published PSNR metric from 30.23 dB to 36.39 dB on the SOTS indoor test dataset. The code is available on GitHub.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0653", "problem_id": "06530001", "content": "The field of affective computing and facial expression modeling has witnessed a surge in research on facial expression synthesis and editing, yet existing methods are often hindered by limitations such as reliance on paired training data, low-resolution outputs, and loss of identity information. To overcome these constraints, this paper proposes a novel approach, termed Local Attentive Conditional Generative Adversarial Network (LAC-GAN), which operates at the Action Unit (AU) level and leverages face action units annotations. By incorporating local AU regional rules and an attentive mechanism, LAC-GAN can generate photo-realistic facial expressions or arbitrary expressions based on desired AU labels, effectively combining multiple AUs into a cohesive output. Furthermore, the proposed method utilizes unpaired training data to learn a mapping between facial expression manifolds, enabling the manipulation module to be trained with corresponding AU labels. The efficacy of the proposed AU synthesis method is validated through extensive qualitative and quantitative evaluations on the BP4D dataset, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0654", "problem_id": "06540001", "content": "Differentiable Neural Architecture Search, a widely used Neural Architecture Search (NAS) technique, achieves search efficiency and simplicity by simultaneously optimizing model weights and architecture parameters within a weight-sharing supernet using gradient-based methods. The conventional approach involves selecting operations with the highest architecture parameters to construct the final architecture, based on the assumption that these parameters accurately represent the operational strength. However, despite extensive research on supernet optimization, the architecture selection process has been largely overlooked. We present both empirical and theoretical evidence demonstrating that the magnitude of architecture parameters does not reliably correlate with an operation's contribution to supernet performance. To address this, we introduce a perturbation-based architecture selection method that directly quantifies each operation's impact on the supernet. By re-evaluating several differentiable NAS methods using our proposed architecture selection, we consistently extract architectures with significant improvements from the existing supernets. Moreover, we observe that our selection method substantially mitigates several failure modes of DARTS, suggesting that the suboptimal generalization often seen in DARTS is primarily due to the limitations of magnitude-based architecture selection, rather than solely the supernet's optimization process.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0655", "problem_id": "06550001", "content": "In this study, we introduce a model designed to adjust individual visual characteristics of objects in actual scenes by leveraging examples that illustrate how these attribute modifications influence simulation outcomes. For instance, we train our model to alter the expression of a human face utilizing nonphotorealistic 3D representations of faces with different expressions. The model successfully maintains all other visual features of a real face, such as head position, despite these and other attributes not being labeled in either the real or synthetic contexts. By learning to modify a particular property in isolation through \"synthetic demonstrations\" without explicitly defined labels, our model is also applicable to other challenging properties like shape, texture, and lighting. We evaluate how effectively our model retains other attributes of a real image when one specific characteristic is altered. Additionally, we utilize digit datasets to investigate how variations in attribute distributions impact our model's performance, and we showcase findings in a more complex scenario: manipulating real human faces using nonphotorealistic 3D renders.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0656", "problem_id": "06560001", "content": "We introduce a new factor analysis approach designed to identify shared common factors in multivariate time series trajectories, where these factors adhere to a precedence-ordering constraint—some factors are only engaged after the activation of others. This constraint emerges in scenarios where variables follow an unknown sequential activation pattern. Our method employs a linear model incorporating inherent delays and relative ordering among factors. An unsupervised learning algorithm, leveraging convex and non-convex optimization techniques, is developed to fit the model while ensuring sparse factor scores and maintaining consistent precedence in factor loadings. The effectiveness of the Order-Preserving Factor Analysis (OPFA) method is demonstrated through its application in extracting precedence-ordered factors from longitudinal gene expression data.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0657", "problem_id": "06570001", "content": "The capacity to calculate similarity scores between graphs using metrics such as Graph Edit Distance (GED) is crucial for various practical applications, including 3D action recognition and biological molecular identification. Determining exact GED values is generally an NP-hard challenge, and conventional algorithms often struggle to strike a satisfactory balance between accuracy and efficiency. Recently, Graph Neural Networks (GNNs) have emerged as a data-driven approach to this problem, offering enhanced efficiency while preserving predictive accuracy in the computation of similarities for small graphs (approximately 10 nodes each). Current GNN-based approaches, which either independently embed two graphs (lacking low-level cross-graph interactions) or engage in cross-graph interactions for entire graph pairs (which can be redundant and time-consuming), still fall short of delivering competitive performance as the number of nodes in the graphs increases. In this study, we concentrate on similarity computation for large graphs and introduce the \"embedding-coarsening-matching\" framework, which initially embeds and coarsens large graphs into coarsened versions with denser local topology, followed by the application of fine-grained interactions on these coarsened graphs to obtain the final similarity scores.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0658", "problem_id": "06580001", "content": "Analyzing motor behavior is crucial in biomedical research and clinical diagnostics as it offers a non-invasive method for detecting motor impairments and changes resulting from interventions. Current advanced movement analysis techniques are both time-consuming and costly because they necessitate the use of physical or virtual markers. In addition to the labor involved in marking keypoints or creating annotations for training or fine-tuning a detector, users must have prior knowledge of the pertinent behaviors to provide relevant keypoints. We present unsupervised behavior analysis and magnification (uBAM), an automatic deep learning algorithm designed to analyze behavior by identifying and emphasizing deviations. A key component of this method is the unsupervised learning of posture and behavior representations, which facilitates an objective evaluation of movement. In addition to detecting and measuring behavioral deviations, we introduce a generative model that visually enhances subtle behavioral differences directly within a video, bypassing the need for keypoints or annotations. Critical to this magnification process, even across different subjects, is the separation of appearance and behavior. Our evaluations on rodents and human patients with neurological disorders highlight the broad applicability of this approach. Furthermore, integrating optogenetic stimulation with our unsupervised behavior analysis demonstrates its potential as a non-invasive diagnostic tool linking functionality to brain plasticity.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0659", "problem_id": "06590001", "content": "Research on complex networks represents a major advancement in contemporary science, contributing to diverse fields such as social sciences, biology, physics, and computer science. These networks influence human behavior through applications like social media platforms, search engines, and recommendation systems, with their models and algorithms deeply integrated into modern society. A common approach for modeling such networks involves generating low-dimensional Euclidean embeddings of nodes, where closeness indicates edge probability. However, we challenge the prevailing assumption that these embeddings effectively represent key characteristics of complex networks. Specifically, we examine two empirically validated properties—low degree and high clustering coefficients—and mathematically demonstrate that any dot product-based embedding capable of replicating these features must have a rank nearly proportional to the number of nodes. This finding implies that widely used methods like Singular Value Decomposition and node2vec cannot accurately model essential structural elements of real-world networks. Additionally, our empirical analysis of various dot product-based embedding techniques reveals their inability to preserve triangular relationships in network structures.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0660", "problem_id": "06600001", "content": "This paper introduces a learning-based approach for generating new perspectives of intricate scenes using only unorganized, real-world photographs. Expanding on Neural Radiance Fields (NeRF), which employs the weights of a multilayer perceptron to represent a scene's density and color relative to 3D coordinates, we tackle NeRF's limitations in uncontrolled environments. While NeRF performs well with images of static subjects under controlled conditions, it struggles to model common real-world elements present in uncontrolled images, such as inconsistent lighting or temporary occlusions. To overcome these challenges, we present several enhancements to NeRF, facilitating precise reconstructions from unstructured image sets acquired from the internet. We evaluate our system, named NeRF-W, on internet photo collections of well-known landmarks, and show temporally consistent novel view renderings that achieve a level of photorealism exceeding previous methods.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0661", "problem_id": "06610001", "content": "This paper introduces Multiresolution Graph Networks (MGN) and Multiresolution Graph Variational Autoencoders (MGVAE) for multiresolution and equivariant graph learning and generation. MGN uses higher-order message passing at each resolution to encode a graph, concurrently learning to partition it into distinct clusters and coarsen it to a lower resolution. Based on MGN, MGVAE establishes a hierarchical generative model to variationally autoencode the hierarchy of coarsened graphs. The suggested framework maintains end-to-end permutation equivariance concerning node order. The presented methods demonstrate efficacy in various generative tasks, such as link prediction on citation graphs, unsupervised molecular representation learning for molecular property prediction, molecular generation, general graph generation, and graph-based image generation.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0662", "problem_id": "06620001", "content": "Graph Neural Networks (GNNs) have demonstrated remarkable efficacy across diverse prediction tasks at the node, edge, and graph levels in numerous fields. Current methodologies primarily address static graphs; however, many graphs evolve dynamically, with edges and node or edge attributes changing over time. Accounting for this evolution is crucial for learning node representations in dynamic graphs. This paper introduces a Temporal Multilayered Position-aware Graph Neural Network (TMP-GNN), a node embedding technique designed for dynamic graphs that integrates temporal relation interdependence into the embedding process. We assess TMP-GNN's performance using two distinct representations of temporal multilayered graphs, benchmarking it against prevalent GNNs in node-level prediction tasks. Furthermore, we integrate TMP-GNN into a deep learning framework for estimating missing data, comparing its performance against both comparable GNNs from our earlier experiment and a baseline method. Empirical results on four real-world datasets reveal up to a 58% reduction in ROC AUC for pairwise node classification and a 96% reduction in MAE for missing feature estimation, particularly for graphs characterized by a high node count and low average connectivity.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0663", "problem_id": "06630001", "content": "The ability to analyze and recognize driving styles is crucial for advancements in intelligent transportation systems and vehicle calibration. This study introduces a new framework for driving style analysis, which leverages fundamental driving patterns derived from real-world driving data. To accomplish this, a Bayesian nonparametric learning technique, based on a hidden semi-Markov model (HSMM), is employed to extract these fundamental patterns from time-series driving data, without requiring prior knowledge of their quantity. This Bayesian nonparametric method utilizes a hierarchical Dirichlet process (HDP) to infer the unknown number of smooth dynamical modes within the HSMM, thereby generating the primitive driving patterns. These patterns are then clustered and labeled with behavioral semantics, based on drivers' physical and psychological perception thresholds. For each driver, 75 primitive driving patterns are learned and semantically labeled in car-following scenarios. To demonstrate the effectiveness of the HDP-HSMM approach for learning primitive driving patterns, it is compared against two alternative Bayesian nonparametric methods: HDP-HMM and sticky HDP-HMM. The naturalistic driving data used in this study was collected from the University of Michigan Safety Pilot Model Deployment (SPDM) database, encompassing 18 drivers. Individual driving styles are analyzed based on the distributional characteristics of the learned primitive driving patterns, and the differences in driving styles among drivers are quantified using the Kullback-Leibler divergence. The experimental results confirm that the proposed primitive pattern-based method facilitates a semantic understanding of driver behaviors and driving styles.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0664", "problem_id": "06640001", "content": "Gesture recognition is a field of growing interest because of its wide range of potential applications. Despite recent advances in multi-modal learning, current techniques do not adequately integrate spatio-temporal modalities to fully leverage their synergistic effects for gesture recognition. This is partly because existing network architectures, which are manually designed, are inefficient for joint multi-modal learning. This paper introduces a novel neural architecture search (NAS)-based approach for RGB-D gesture recognition, comprising two main elements: 1) improved temporal representation using a proposed 3D Central Difference Convolution (3D-CDC) family, designed to capture extensive temporal context by aggregating temporal difference information; and 2) optimized backbone architectures for multi-sampling-rate branches and lateral connections between different modalities. The resulting multi-modal, multi-rate network offers a novel way to interpret the relationship between RGB and depth modalities and their temporal dynamics. Extensive experiments conducted on three standard datasets (IsoGD, NvGesture, and EgoGesture) demonstrate state-of-the-art performance in both single- and multi-modality scenarios. The code is available at https://github.com/ZitongYu/3DCDC-NAS.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0665", "problem_id": "06650001", "content": "Deep structured output learning exhibits significant potential for applications such as semantic image segmentation. We propose a novel and efficient approach to deep structured model learning, demonstrating how deep Convolutional Neural Networks (CNNs) can be utilized to estimate the messages involved in message passing inference within structured prediction frameworks that employ Conditional Random Fields (CRFs). By using CNNs for message estimation, we eliminate the necessity of learning or assessing potential functions needed for message computation. This greatly enhances the efficiency of the learning process, as traditional structured learning with CRF and CNN potentials requires costly inference for each stochastic gradient iteration. The output dimension of the network for message estimation corresponds directly to the number of classes, unlike the output for general CNN potential functions in CRFs, which grows exponentially with respect to the order of the potentials. As a result, CNN-based message learning involves a smaller number of network parameters and offers improved scalability for scenarios with numerous classes. We apply our approach to semantic image segmentation on the PASCAL VOC 2012 dataset, achieving a 73.4 intersection-over-union score on its test set, marking the highest reported result for methods that exclusively utilize the VOC training images. This excellent outcome underscores the effectiveness and applicability of our CNN message learning approach.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0666", "problem_id": "06660001", "content": "Generating consensus segmentations from multiple expert annotations is crucial for medical image segmentation. This paper introduces a new method that leverages graph cuts (GC) and semi-supervised learning (SSL) to achieve this. Unlike common techniques that employ iterative Expectation Maximization (EM) which can be susceptible to local minima, our approach utilizes a self-consistency (SC) score, calculated using low-level image features, to assess annotator consistency. SSL is then applied to predict absent annotations, incorporating both global features and local image consistency. The SC score is also integrated as a penalty cost within a second-order Markov random field (MRF) cost function, which is optimized using graph cuts to determine the final consensus label. This graph cut optimization avoids iterative processes by directly finding a global maximum. Experiments conducted on synthetic images, real-world data from Crohn's disease patients, and retinal images demonstrate that our segmentation method achieves superior accuracy and consistency compared to existing techniques.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0667", "problem_id": "06670001", "content": "This study investigates inverse reinforcement learning (IRL) for autonomous robot navigation by leveraging semantic observations. The goal is to deduce a cost function that rationalizes expert behavior using only observed states and control trajectories. We introduce a map encoder that estimates semantic class probabilities from observations and a cost encoder, implemented as a deep neural network operating on semantic features. Since the expert's cost function is not directly measurable, the model parameters are optimized by minimizing the discrepancy between demonstrated controls and a policy derived from the estimated cost. This optimization employs a closed-form subgradient computed selectively over relevant states using a motion planner. Our method demonstrates successful adherence to traffic rules in the CARLA autonomous driving simulator by interpreting semantic observations of vehicles, sidewalks, and lane markings.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0668", "problem_id": "06680001", "content": "This study focuses on achieving high-accuracy three-dimensional object detection in the context of autonomous driving, proposing the Multi-View 3D networks (MV3D) framework. MV3D integrates both LIDAR point cloud data and RGB images to predict oriented three-dimensional bounding boxes, utilizing a compact multi-view representation to encode the sparse three-dimensional point cloud. The framework consists of two primary subnetworks: one dedicated to generating three-dimensional object proposals and another for fusing multi-view features. The proposal generation subnetwork efficiently produces three-dimensional candidate boxes from the bird's eye view representation of the three-dimensional point cloud. A deep fusion scheme is designed to combine region-wise features from multiple views, facilitating interactions between intermediate layers of different paths. As demonstrated by experiments on the KITTI benchmark, the proposed approach surpasses the state-of-the-art by approximately 25% and 30% in terms of average precision (AP) for three-dimensional localization and detection tasks, respectively. Furthermore, for two-dimensional detection, the approach yields a 10.3% higher AP than the state-of-the-art on the hard data subset among LIDAR-based methods, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0669", "problem_id": "06690001", "content": "The emergence of deep learning has facilitated enhanced image classification through sophisticated discriminative models. Nevertheless, the increased complexity of these deep models necessitates extensive labeled datasets to ensure effective generalization during training. Applying these classification models to medical images can readily lead to overfitting due to the limited availability of training data, a prevalent challenge in medical image analysis. This paper introduces and examines a reinforced classifier designed to improve generalization when trained with limited data. Drawing inspiration from reinforcement learning principles, the proposed classifier updates its parameters using generalization-feedback derived from a subset of the training data, rather than relying solely on the conventional cross-entropy loss function. We assess the improvements offered by the proposed classifier by applying it to three distinct classification problems, comparing its performance against standard deep classifiers incorporating existing overfitting-prevention techniques. The results demonstrate not only an overall enhancement in classification performance but also notable generalized learning characteristics, highlighting the potential of this classifier for medical classification tasks.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0670", "problem_id": "06700001", "content": "Deep neural networks are often trained with a significantly higher number of parameters than the available training data. Surprisingly, recent studies show that this overparameterization not only enhances the training of large models but also aids in developing compact models, particularly through pruning or sparsification. This work provides theoretical insights into these observations by analyzing the high-dimensional behavior of model pruning in overparameterized settings, addressing whether it is preferable to train a small model directly or first train a larger model and then prune. We demonstrate scenarios where, even with prior knowledge of the most informative features, training a large model followed by pruning outperforms training solely on known features. This results in a novel double descent phenomenon for sparse models, where increasing the initial model size while maintaining target sparsity enhances test accuracy beyond the overparameterization threshold. Additionally, our analysis highlights the advantages of retraining in relation to feature correlations. These effects are observed in simpler models like linear and random-features models, and our methodology extends high-dimensional analysis tools to precisely describe the asymptotic behavior of overparameterized least-squares. Empirical validation on neural networks supports the insights derived from theoretical examination of these simpler models.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0671", "problem_id": "06710001", "content": "This paper introduces a method for spatially identifying activities within video frames, accommodating scenarios where individuals engage in multiple concurrent actions. The proposed technique incorporates both the temporal context of the scene and the interrelationships between the actions of identified individuals. A temporal recurrent neural network (RNN) models the temporal context, while a graph RNN captures the action relationships. These networks undergo simultaneous training, and the resulting approach demonstrates state-of-the-art performance on the AVA dataset.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0672", "problem_id": "06720001", "content": "Artificial intelligence (AI) systems, particularly those employing Deep Learning (DL), have recently gained prominence due to their exceptional performance in domains like image processing, natural language processing, and speech recognition. However, despite the high accuracy of these DL models, their decision-making processes often lack transparency. Consequently, Interpretability methods have emerged as a valuable tool for understanding how these models arrive at their conclusions. Explaining DL models is especially crucial in the medical field, where experts must provide justifications for their diagnoses to patients. In this paper, we introduce an explanation-guided training approach that leverages Layer-wise relevance propagation (LRP) to guide the model's attention toward the most relevant image regions. We validated our method on a convolutional neural network (CNN) model designed for classifying low-grade and high-grade gliomas. The experimental results demonstrate the potential of integrating interpretation techniques into the model training process.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0673", "problem_id": "06730001", "content": "Recent advancements in reinforcement learning (RL) exploration have introduced progressively intricate solutions, often sacrificing generality in the process. Empirical evidence indicates that in diverse environments, certain complex exploration techniques are surpassed by simpler approaches like ε-greedy. This study introduces an exploration method that maintains the simplicity of ε-greedy while minimizing dithering. The key insight is that ε-greedy’s primary weakness lies in its short-term action selection, hindering escape from local optima. To address this, we present a temporally extended ε-greedy variant that repeats actions for random intervals. Surprisingly, many duration distributions enhance exploration across various domains, with ecological models of animal foraging behavior inspiring a particularly effective class of distributions.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0674", "problem_id": "06740001", "content": "Recent CNN-based approaches to salient object detection (SOD) primarily depend on optimizing cross-entropy loss (CELoss), with the resulting saliency maps often assessed using the F-measure. This study explores an intriguing question: is it possible to uniformly apply the F-measure formulation during both training and evaluation for SOD? We introduce the relaxed F-measure, a reformulation of the traditional F-measure that is differentiable concerning the posterior and can be seamlessly integrated as a loss function at the end of CNNs. Unlike the standard cross-entropy loss, which experiences a significant drop in gradients within saturated regions, our proposed loss function, referred to as FLoss, maintains substantial gradients even as activations near their targets. This characteristic enables FLoss to continually drive the network towards creating polarized activations. Extensive experiments on various well-known datasets demonstrate that FLoss significantly surpasses existing state-of-the-art methods. Notably, due to the polarized predictions generated, our approach effectively yields high-quality saliency maps without the need for meticulous threshold optimization, showcasing notable benefits in practical applications.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0675", "problem_id": "06750001", "content": "Fully convolutional networks (FCNs) currently represent the most advanced techniques in semantic segmentation. Nevertheless, their reliance on extensive receptive fields and numerous pooling layers leads to blurred outputs and reduced spatial resolution in deeper layers. Consequently, FCNs often generate segmentation results with imprecise object boundary localization. While previous research has tackled this issue through post-processing methods, such as employing color-based Conditional Random Fields (CRFs) on FCN outputs, these methods introduce additional parameters and low-level features that are challenging to optimize and incorporate into the original network structure. Furthermore, the common use of color-based pixel affinities in most CRFs is not ideal for semantic segmentation, often resulting in spatially fragmented predictions. To address these challenges, we propose a Boundary Neural Field (BNF), a global energy model that combines FCN predictions with boundary cues. This boundary information serves to improve semantic segment coherence and enhance object localization. Initially, we demonstrate that the convolutional filters within semantic FCNs offer effective features for boundary detection. Subsequently, we utilize the predicted boundaries to establish pairwise potentials within our energy model. Finally, we reveal that our energy formulation decomposes semantic segmentation into a series of binary problems, which can be simplified for effective global optimization. Our comprehensive experiments demonstrate that minimizing our global boundary-based energy yields significant improvements over existing globalization techniques, both in quantitative metrics and qualitative assessments.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0676", "problem_id": "06760001", "content": "Applications that engage with the physical environment, like augmented reality or robotic manipulation, necessitate a thorough comprehension of the position and orientation of nearby objects. In this study, we introduce a novel technique for estimating the 6 Degree of Freedom (DoF) or 6D pose of objects from one RGB image. This method can be integrated with an object detection and segmentation technique to assess, enhance, and monitor the pose of the objects by aligning the input image with generated images.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0677", "problem_id": "06770001", "content": "Cutting-edge machine learning algorithms are meticulously refined to achieve optimal predictions, which often leads to the development of complex models. Although these intricate models significantly outperform simpler, more interpretable alternatives, they frequently present challenges in understanding their operational mechanisms, leaving us with a \"black box\" dilemma. In this study, we propose an accessible method to elucidate the behavior of any predictive model, applicable to both regression and classification tasks. By examining the partial derivatives of a given model relative to its inputs, we can gather the necessary information for interpretation. We illustrate this approach by analyzing convolutional and multi-layer neural networks within the realm of natural language processing.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0678", "problem_id": "06780001", "content": "We introduce a framework known as Molecule Deep Q-Networks (MolDQN) aimed at optimizing molecules by integrating specialized knowledge from chemistry with advanced reinforcement learning methods, specifically double Q-learning and randomized value functions. Our approach involves direct modifications to molecules, guaranteeing 100% chemical validity. Additionally, we do not employ pre-training on any datasets to eliminate potential biases resulting from dataset selection. To address challenges encountered in medicinal chemistry lead optimization, we enhance our model using multi-objective reinforcement learning, focusing on maximizing drug-likeness while preserving resemblance to the original molecule. Moreover, we illustrate the trajectory through chemical space for molecule optimization to clarify the model's functionality.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0679", "problem_id": "06790001", "content": "We propose a novel self-supervised approach to learn visual correspondence from unlabeled video data, leveraging cycle-consistency over time as a supervisory signal to learn visual representations from the ground up. During training, our model develops a feature map representation that facilitates cycle-consistent tracking, which is then utilized at test time to identify nearest neighbors across both spatial and temporal domains. The learned representation demonstrates broad generalizability, achieving strong performance on a variety of visual correspondence tasks - including video object segmentation, keypoint tracking, and optical flow estimation - without requiring fine-tuning. Notably, our method surpasses existing self-supervised techniques and exhibits competitive performance with strongly supervised methods, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0680", "problem_id": "06800001", "content": "Many algorithms utilizing deep learning techniques for generating 3D point sets are limited to producing clouds with a predetermined number of points. Additionally, these methods generally necessitate large networks with numerous parameters, complicating the training process. In this study, we introduce an auto-encoder framework capable of encoding and decoding point clouds of any size, showcasing its efficiency in enhancing sparse point clouds. Notably, our approach employs fewer than half the parameters compared to leading architectures, yet achieves superior performance. We will ensure our code base is fully accessible.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0681", "problem_id": "06810001", "content": "Numerous machine learning challenges in natural language processing can be formulated as sequence labeling tasks. We introduce Gaussian process models utilizing pseudo-likelihood approximation for sequence labeling, offering a Bayesian learning framework through kernel methods. The pseudo-likelihood approach effectively captures long-range dependencies between sequence output elements while maintaining computational feasibility. Inference in this model is conducted using an efficient variational Gaussian approximation technique. Additionally, we develop an iterative algorithm that leverages information from adjacent labels for accurate predictions. The model's capacity to handle long-range dependencies enhances its applicability across diverse sequence labeling tasks. Experimental results on various sequence labeling datasets confirm the effectiveness of our proposed method.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0682", "problem_id": "06820001", "content": "This study employs 3D Convolutional Neural Networks for volumetric medical image segmentation. While deep neural networks excel in 2D vision tasks, their application to 3D data remains difficult due to sparse annotated datasets and computational constraints. To address these issues, we introduce an innovative coarse-to-fine 3D framework that efficiently processes volumetric data while maintaining performance. Our 3D approach significantly surpasses 2D methods by utilizing comprehensive spatial information across all three dimensions. Evaluations on datasets containing both healthy and diseased pancreases demonstrate state-of-the-art performance measured by the Dice-Srensen Coefficient (DSC). Specifically, on the NIH pancreas segmentation dataset, our method exceeds the prior best results by an average of 2%, with the most challenging cases showing a 7% improvement to nearly 70% DSC, highlighting its clinical reliability.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0683", "problem_id": "06830001", "content": "Reinforcement learning benefits from options, which enable agents to break down tasks into hierarchical subtasks, potentially accelerating learning and planning processes. Yet, independently discovering useful sets of options remains a significant challenge. This paper explores leveraging representation learning techniques to facilitate option discovery, particularly focusing on eigenoptions—derived from representations that capture diffusive information flow in the environment. We enhance existing eigenoption discovery algorithms to accommodate stochastic transitions and environments lacking predefined features. Our proposed method identifies eigenoptions while learning non-linear state representations directly from pixel inputs, drawing on advancements in deep reinforcement learning and the connection between proto-value functions and the successor representation. We validate our approach using classic tabular domains for clarity and Atari 2600 games to showcase its effectiveness.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0684", "problem_id": "06840001", "content": "This paper presents an unsupervised learning method for the automated detection, summarization, and modification of artistic styles extracted from extensive painting datasets. Our technique employs archetypal analysis, an unsupervised learning methodology similar to sparse coding but with a geometric underpinning. By applying this analysis to deep image representations from a collection of artworks, we derive a visually interpretable dictionary of archetypal styles. Following model training, the style of a novel image, defined by the local statistics of deep visual features, is approximated using a sparse convex combination of archetypes. This approximation facilitates the identification of the archetypal styles present in the input image and their relative contributions. Furthermore, our method enables manipulation of the latent archetypal decomposition coefficients, facilitating various stylistic modifications, including style enhancement, style transfer, and interpolation between multiple archetypes.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0685", "problem_id": "06850001", "content": "This study introduces a novel framework, termed Generative Transfer Network (GTNet), designed to tackle the challenge of zero-shot object detection (ZSD). The GTNet architecture comprises two primary components: an Object Detection Module, capable of acquiring extensive knowledge from seen domains, and a Knowledge Transfer Module, which utilizes a feature synthesizer to generate features for unseen classes, subsequently used to fine-tune the Object Detection Module's classification layer. To effectively synthesize features that capture both intra-class variance and Intersection over Union (IoU) variance for each unseen class, an IoU-Aware Generative Adversarial Network (IoUGAN) is developed as the feature synthesizer, seamlessly integrable into the GTNet framework. IoUGAN is composed of three distinct unit models: the Class Feature Generating Unit (CFU), Foreground Feature Generating Unit (FFU), and Background Feature Generating Unit (BFU), which collectively generate class-specific features with intra-class and IoU variance. The efficacy of the proposed method is validated through experiments conducted on three publicly available datasets, with the results indicating that it outperforms existing state-of-the-art ZSD approaches.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0686", "problem_id": "06860001", "content": "Distance Metric Learning (DML) aims to discover an embedding space that effectively distinguishes data points by minimizing the distance between similar instances and maximizing the distance between dissimilar ones. This paper focuses on Semi-Supervised DML (SSDML), which tackles the challenge of learning a metric using a limited number of labeled examples in conjunction with a large pool of unlabeled data. SSDML is crucial due to the impracticality of labeling all instances in extensive datasets. Despite its significance, recent research on SSDML, particularly in deep learning contexts, is limited, with most existing methods relying on traditional linear Mahalanobis metric learning. To address this gap, this paper introduces a novel deep learning-based approach to SSDML. Specifically, a stochastic, graph-based method is proposed to propagate affinity relationships from labeled to unlabeled data, which are subsequently used to generate triplet constraints for metric learning. Furthermore, an orthogonality constraint is imposed on the metric parameters to prevent model collapse and enhance performance.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0687", "problem_id": "06870001", "content": "The disparity between the difficulty of proposing a viable solution to a problem and assessing its quality is a fundamental challenge in machine learning and related disciplines. This disparity underlies numerous selection-based methods, which employ a generator system to propose potential solutions and an evaluator system to rank and select the most suitable ones. This study applies this approach to the tasks of panoptic image segmentation and class-agnostic parts segmentation, utilizing a dual convolutional neural network architecture comprising a generator network that proposes diverse segments corresponding to objects, stuff, and parts within an image, and an evaluator network that selects the optimal segments to merge into the final segmentation map. By adopting a trial-and-error evolutionary strategy, a generator network that produces segments with low average accuracy but high variability can still yield satisfactory results when paired with a precise evaluator network. The generator network, which incorporates a Pointer net, predicts the segment region containing a given point in the image, and generating and evaluating each segment individually is crucial, as it substantially reduces the number of required guesses compared to evaluating the entire segmentation map. An independent region-specific classification network is used to classify the selected segments, enabling class-agnostic segmentation that can handle unfamiliar categories not present in the training data. The efficacy of this method was evaluated on the COCO Panoptic segmentation benchmark, yielding results comparable to those achieved by basic semantic segmentation and Mask-RCNN methods, and was also applied to the task of splitting objects from unseen classes into parts, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0688", "problem_id": "06880001", "content": "The susceptibility of deep neural networks to adversarial perturbations, juxtaposed with their widespread adoption, has spurred significant research into verifying the robustness of neural network classifiers. Despite the NP-completeness of determining the minimum adversarial distortion for ReLU-activated networks, establishing a non-trivial lower bound on this distortion remains a viable approach for ensuring provable robustness. Prior research, however, has predominantly concentrated on basic, fully-connected architectures (multilayer perceptrons) and has been constrained to ReLU activations. Addressing these limitations, we introduce CNN-Cert, a versatile and efficient framework designed to certify the robustness of general convolutional neural networks. Our framework exhibits generality by accommodating diverse architectures, including convolutional layers, max-pooling layers, batch normalization layers, and residual blocks, alongside a range of activation functions. Furthermore, our approach achieves efficiency through leveraging the specific structure inherent in convolutional layers, resulting in speed improvements of up to 17 and 11 times compared to existing certification algorithms (e.g. Fast-Lin, CROWN) and a 366-fold speed-up relative to the dual-LP method, all while maintaining comparable or superior verification bounds. CNN-Cert also generalizes state-of-the-art algorithms like Fast-Lin and CROWN. Empirical evaluations demonstrate that our proposed method surpasses existing lower-bound-based certification algorithms in both bound quality and computational speed.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0689", "problem_id": "06890001", "content": "Conventional image instance segmentation typically involves initial object detection followed by segmentation within the detected bounding box. Contemporary deep learning techniques, such as Mask R-CNN, execute these tasks concurrently. Nevertheless, the distinctive characteristics of the \"human\" category, which can be effectively represented by pose skeletons, are often overlooked. Human pose skeletons offer enhanced instance differentiation in scenarios with significant occlusion compared to bounding boxes. In this paper, we introduce an innovative pose-based instance segmentation framework designed for humans, which delineates instances using human pose information rather than relying on proposal region detection. Our results indicate that this pose-based method achieves superior accuracy compared to state-of-the-art detection-based methods for human instance segmentation and exhibits improved handling of occlusion. Furthermore, the scarcity of publicly available datasets featuring heavily occluded humans with detailed annotations poses a challenge that has received limited attention. To address this gap, we introduce a novel benchmark dataset, \"Occluded Human (OCHuman)\", specifically designed to focus on occluded humans and provide comprehensive annotations, including bounding boxes, human poses, and instance masks. This dataset comprises 8110 meticulously annotated human instances across 4731 images, with an average MaxIoU of 0.67 per person, establishing OCHuman as a complex and challenging resource for human instance segmentation research. The primary goal of this dataset is to highlight occlusion as a significant problem meriting further investigation.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0690", "problem_id": "06900001", "content": "Unsupervised domain adaptation seeks to develop a robust classifier for a target domain using a labeled source dataset and an unlabeled target dataset. A key challenge in this field is addressing the 'domain shift' issue, which has led researchers to focus on aligning the distribution of the two domains. The generative adversarial network (GAN) has been shown to effectively capture data distributions, prompting the proposal of a novel, adversarial learning-based model for unsupervised domain adaptation. This approach utilizes a shared encoder for both the source and target domains, which, aided by an adversarial discriminator, extracts domain-invariant representations. The incorporation of center loss with labeled source data enhances the discriminative power of the learned features. Furthermore, the conditional distribution of the two domains is aligned to promote feature discrimination in the target domain. Notably, this method jointly learns feature representations for both domains, unlike previous studies that rely on fixed, pre-trained encoders for source feature extraction. By sharing the encoder, the model eliminates the need to identify image sources during testing, thereby broadening its applicability. The proposed method is evaluated on several unsupervised domain adaptation benchmarks, yielding superior or comparable results to state-of-the-art outcomes, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0691", "problem_id": "06910001", "content": "Natural signals exhibit both discrete spatial patterns and their gradual changes. Prior research has employed Lie groups and representation theory to represent continuous image transformations, while sparse coding has been utilized to learn pattern dictionaries from natural signals. This paper introduces a Bayesian generative model that integrates these approaches to autonomously separate spatial patterns from their continuous transformations. Images are represented as a sparse combination of shape components subjected to a transformation parameterized by *n* continuous variables. The model learns both the shape components and transformations from the data, adapting to its inherent symmetries, with the restriction that the transformations constitute a representation of an *n*-dimensional torus. When trained on a dataset of controlled geometric transformations of specific MNIST digits, the model successfully recovers these transformations along with the digits themselves. Furthermore, training on the complete MNIST dataset demonstrates the model's ability to learn fundamental digit shapes and natural transformations like shearing and stretching present in the data.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0692", "problem_id": "06920001", "content": "Pre-training plays a vital role in training deep neural networks effectively. Traditional pre-training techniques typically involve training basic models, such as restricted Boltzmann machines, and progressively stacking them to construct deeper architectures. While this layer-wise approach is well-supported both theoretically and empirically, it faces challenges when applied to models lacking a distinct hierarchical structure, such as recurrent neural networks (RNNs). This study introduces a novel pre-training strategy based on knowledge transfer learning. Unlike incremental layer-wise training, the proposed method optimizes the entire model simultaneously using a simplified objective function, leveraging soft targets generated by a pre-trained teacher model. This approach is model-agnostic, making it suitable for pre-training highly intricate architectures. Empirical validation on a speech recognition task confirmed that complex RNNs can be effectively trained using a less sophisticated deep neural network (DNN) model. Additionally, integrating this method with conventional layer-wise pre-training yields further performance improvements.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0693", "problem_id": "06930001", "content": "In the context of graph structures, bipartite networks comprise nodes from two distinct domains, with edges representing interactions between these domains. While numerous network embedding methods have been developed to learn node representations in general graphs, including those with homogeneous and heterogeneous node and edge types, as well as some tailored to bipartite networks, they are insufficient for modeling multiplex bipartite networks, which exhibit multiple interaction types and node attributes, such as those found in e-commerce. The complexity of real-world multiplex bipartite networks is further compounded by sparsity and imbalanced node distributions, posing significant modeling challenges. To address this, we propose an unsupervised Dual HyperGraph Convolutional Network (DualHGCN) model, which transforms the multiplex bipartite network into two homogeneous hypergraph sets and leverages spectral hypergraph convolutional operators, combined with intra- and inter-message passing strategies, to facilitate information exchange within and across domains, thereby learning effective node embeddings. Our model is evaluated on four real-world datasets, using link prediction and node classification tasks, and the results of our extensive experiments show that DualHGCN outperforms existing state-of-the-art methods, demonstrating robustness to varying levels of sparsity and node distribution imbalances.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0694", "problem_id": "06940001", "content": "Consider the problem of decomposing an observed matrix into the sum of a low-rank component and a sparse component representing outliers, with the objective of retrieving each component from their combination. This type of decomposition is applicable to several numerical challenges, such as system identification, latent variable graphical modeling, and principal components analysis. We investigate the conditions that enable the recovery of this decomposition through the minimization of both the \\ell_1 norm and the trace norm. Our primary focus is determining the maximum number of outliers permissible while still enabling accurate recovery via convex programming. The recovery guarantees we establish surpass those of prior research, and critically, our analysis does not rely on the assumption of random spatial distribution of outliers, unlike related matrix completion analyses.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0695", "problem_id": "06950001", "content": "The development of models for video action classification is advancing swiftly. Nonetheless, the efficacy of these models can still be significantly enhanced through the ensemble of identical models trained on different modalities (e.g., Optical flow). Unfortunately, utilizing multiple modalities during inference incurs a high computational cost. Recent studies focus on how to harness the benefits of multi-modality within a single RGB model. However, there remains potential for further enhancement. In this paper, we investigate various techniques to incorporate ensemble capabilities into a unified model. We demonstrate that appropriate initialization and mutual modality learning improve the performance of single-modality models. Consequently, we attain state-of-the-art results on the Something-Something-v2 benchmark.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0696", "problem_id": "06960001", "content": "Humans have the remarkable capability to identify and isolate moving objects even amid numerous items, intricate background structures, the movement of the observer, and instances of camouflage. This motion detection occurs almost instantaneously. Although there have been significant advancements in motion segmentation recently, we still appear to lag behind human performance. In this study, we derive a novel likelihood function based on fundamental principles to evaluate the probability of an optical flow vector based on an object's 3D motion direction. This likelihood employs a unique combination of both the angle and magnitude of the optical flow to enhance the understanding of the actual motions of objects. By leveraging this new likelihood along with several innovative approaches to initialization, we create a motion segmentation algorithm that significantly outperforms existing state-of-the-art techniques. We evaluate our method against five leading algorithms on two well-established benchmarks, as well as a newly introduced dataset of camouflaged animals, aiming to advance the field of motion segmentation further.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0697", "problem_id": "06970001", "content": "The task of Online Multi-Object Tracking (MOT) from video footage poses significant challenges in the field of computer vision, having been the subject of extensive research over the years. While many existing MOT algorithms rely on the Tracking-by-Detection (TBD) paradigm, often integrated with machine learning techniques to minimize manual parameter tuning, these approaches typically require large amounts of labeled data, such as bounding boxes, which can be costly to obtain for video content. Furthermore, the TBD framework is often suboptimal due to its non-end-to-end nature, treating detection and tracking as separate tasks rather than a unified process. To address these limitations, we introduce a novel Tracking-by-Animation framework, which utilizes a differentiable neural model to track objects across input frames and then reconstructs these objects into animated frames, with learning driven by reconstruction error via backpropagation. Additionally, we propose a Reprioritized Attentive Tracking method to enhance the robustness of data association. Experimental results on both synthetic and real-world video datasets demonstrate the effectiveness of our proposed model, with further details available at our project page: https://github.com/zhen-he/tracking-by-animation.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0698", "problem_id": "06980001", "content": "Deep learning typically demands substantial amounts of data in terms of both quantity and diversity. Nonetheless, many remote sensing applications are constrained by the availability of training datasets, with only a limited portion being labeled. In this paper, we examine three cutting-edge strategies in deep learning that address this issue. The first is transfer learning, which involves transferring certain elements, such as features, from one domain to another. The second strategy is unsupervised learning, like autoencoders, which function with unlabeled data. The final approach discussed is generative adversarial networks, capable of producing realistic data that can deceive both deep learning systems and humans. This article aims to highlight this concern, guide readers to relevant existing research, and emphasize the current gaps that require attention.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0699", "problem_id": "06990001", "content": "Elderly individuals often encounter numerous challenges while performing daily living activities due to factors such as age, sensory decline, loneliness, and cognitive changes, which increase the risk of falls. Obtaining real-life fall data is a challenging task, and as a result, simulated falls have become a common approach to evaluate proposed methodologies. A review of existing literature reveals that many researchers rely on raw and energy features, specifically time domain features, of signal data due to their discriminatory capabilities. However, real-life fall signals may be noisier than current simulated data, which could significantly impact the accuracy of results based on raw features. This study utilizes frequency domain Fourier coefficient features, which are constructed using Fast Fourier Transform, to distinguish between various daily human activities, offering robustness to noise and rotation invariance. The methodology is evaluated using two supervised classifiers, kNN and SVM, and two publicly available datasets are employed for benchmark analysis, with the kNN classifier yielding more discriminatory results. The performance is assessed using standard metrics, including Standard Accuracy (SA), Macro Average Accuracy (MAA), Sensitivity (SE), and Specificity (SP), demonstrating that the proposed method outperforms energy features and is competitive with raw features, while also surpassing a recent deep learning approach that did not utilize data augmentation methods, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0700", "problem_id": "07000001", "content": "We introduce a new conformal prediction method designed for scenarios where training data is scarce. Conformal prediction offers a set of potential output candidates, ensuring the true answer is included with a high degree of certainty, instead of providing only one prediction. To address the issue of impractically large prediction sets common with limited training data, we reformulate conformal prediction as a meta-learning problem across interchangeable sets of related tasks, resulting in significantly more precise prediction sets while preserving the desired marginal coverage guarantees. Our conformalization algorithm is efficient, straightforward to implement, and adaptable to various underlying models, learning techniques, and datasets. The efficacy of our method is shown through experiments on several few-shot classification and regression problems spanning natural language processing, computer vision, and computational chemistry for drug discovery.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0701", "problem_id": "07010001", "content": "Point cloud completion involves reconstructing full 3D shapes from partial observations represented as point sets. Existing techniques employ encoder-decoder neural networks to generate the entire point cloud, including redundant information by reproducing the already known input geometry. This work introduces an end-to-end neural architecture that specifically predicts only the missing regions and combines them with the input. The framework consists of two networks: one for predicting the absent geometry from the partial input and another for merging and refining the combined point sets. Evaluations on the ShapeNet dataset demonstrate that our approach surpasses current state-of-the-art methods in completion accuracy. The implementation and experimental details are accessible at \\url.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0702", "problem_id": "07020001", "content": "The detection and identification of extreme weather phenomena in extensive climate simulations represent a critical issue for risk management, guiding governmental policy-making, and enhancing our fundamental comprehension of the climate system. Recent studies indicate that fully supervised convolutional neural networks (CNNs) can achieve sufficient accuracy in classifying recognized types of extreme weather events, provided there is an ample supply of labeled data. Nevertheless, numerous spatially localized climate patterns, such as hurricanes, extra-tropical cyclones, weather fronts, and blocking events, are of interest. The currently available labeled data for these patterns often has shortcomings, such as being restricted to specific years or geographic locations and containing false negatives. This type of climatic data presents several intriguing challenges for machine learning. We introduce a multichannel spatiotemporal CNN architecture designed for semi-supervised bounding box prediction and exploratory data analysis. Our findings demonstrate that this method can utilize temporal information and unlabeled data to enhance the localization of extreme weather events. Additionally, we investigate the representations generated by our model to gain insights into this significant data. To support machine learning research in this domain and to aid in understanding and alleviating the impacts of climate change, we present the ExtremeWeather dataset, which is accessible at extremeweatherdataset.github.io, with the corresponding code available at https://github.com/eracah/hur-detect.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0703", "problem_id": "07030001", "content": "Person re-identification (re-ID) continues to pose difficulties in real-world applications, as it demands that a trained model adapts to entirely unseen target data despite cross-domain variations. While generative adversarial models have recently gained popularity for increasing training data diversity, these methods frequently struggle to generalize across different domains due to a misalignment between the generative process and discriminative feature learning. To improve model generalization, we present an end-to-end domain adaptive attention network that simultaneously performs cross-domain image translation and discriminative re-ID feature learning within a unified framework. To bridge the domain gap, we incorporate an attention mechanism that focuses on modifying the background while preserving the person's identity during image translation. By concentrating attention on non-identity regions, the subject's distinguishing features remain intact. Our integrated learning approach demonstrates substantial performance gains compared to existing state-of-the-art techniques across multiple benchmark datasets.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0704", "problem_id": "07040001", "content": "The Transformer architecture, widely used in natural language processing, relies on self-attention to model relationships between tokens; however, the quadratic time complexity of self-attention presents a computational bottleneck. While several techniques exist to mitigate this issue, matrix approximation is a prominent strategy. For instance, Nystr\\\"omformer employs the Nystr\\\"om method to approximate the softmax function by leveraging a subset of columns from a symmetric positive semidefinite (SPSD) matrix to create a faster approximation. Nevertheless, the Nystr\\\"om approximation's accuracy is limited when the SPSD matrix's spectrum exhibits a slow decay, resulting in a low-rank approximation. This paper introduces an alternative approximation method with a superior error bound compared to the Nystr\\\"om method, while maintaining the same time complexity of O\\left(\\right) as Nystr\\\"omformer.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0705", "problem_id": "07050001", "content": "The field of natural language processing has greatly benefited from unsupervised representation learning methods, such as word embeddings, whereas similar techniques have not yet been widely adopted in 3D vision. Notably, physical 3D environments exhibit a comparable semantic structure to textual data, where words and objects are surrounded by semantically related entities. Leveraging this structural similarity, our research focuses on learning low-dimensional vector representations that capture the semantic meaning of objects. To achieve this, we employ an unsupervised algorithm to mine a dataset of scanned 3D spaces, utilizing point clouds as a flexible and general representation for 3D data, which are then encoded into vector representations. Our approach enhances the ability of clustering algorithms to differentiate between distinct semantic classes by incorporating contextual information, and interpolation experiments demonstrate that our algorithm generates continuous and meaningful object embeddings, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0706", "problem_id": "07060001", "content": "Recent progress in deep learning has significantly enhanced the performance of computer vision tasks such as object detection and segmentation. However, in critical real-world scenarios like autonomous driving, the consequences of erroneous object predictions can be severe. Conventional deep learning models, including YOLO architectures, often exhibit excessive confidence in their outputs and fail to account for uncertainty when processing out-of-distribution data. This paper introduces an efficient method for quantifying uncertainty in object detection and segmentation by leveraging Monte-Carlo DropBlock (MC-DropBlock) inference. The technique involves applying drop-block operations to convolutional layers during both training and inference phases in models like YOLO. This transforms the network into a Bayesian convolutional neural network capable of estimating epistemic uncertainty, while aleatoric uncertainty is modeled using a Gaussian likelihood. Through out-of-distribution experiments, we validate that MC-DropBlock enhances YOLO models' generalization, calibration, and uncertainty estimation in object detection and segmentation tasks.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0707", "problem_id": "07070001", "content": "The capacity to leverage previous experiences to swiftly address unfamiliar challenges is a distinctive feature of biological learning systems and holds significant practical relevance for artificial systems. Recent literature in meta reinforcement learning has concentrated on optimizing the learning process itself. This paper presents an alternative approach that is conceptually straightforward, broadly applicable, modular, and builds on recent advancements in off-policy learning. Our framework draws inspiration from concepts in probabilistic inference and integrates robust off-policy learning with a behavior prior—representing a default behavior that narrows the solution space and provides a bias for exploration—as well as a representation for the value function, both of which are easily acquired from various training tasks in a multi-task context. Our method demonstrates competitive adaptation performance on hold-out tasks when compared to meta reinforcement learning benchmarks and is capable of scaling to complex scenarios with sparse rewards.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0708", "problem_id": "07080001", "content": "Remote photoplethysmography (rPPG), a non-contact technique for monitoring cardiac activity, holds significant promise in various fields such as telemedicine. Current end-to-end rPPG and heart rate (HR) estimation approaches using facial video analysis often struggle in unconstrained environments (e.g., under motion or poor lighting). This study investigates the limitations of existing end-to-end models in challenging scenarios and introduces AutoHR, a robust baseline framework for remote HR estimation utilizing neural architecture search (NAS). The approach consists of three key components: 1) an optimized backbone architecture incorporating Temporal Difference Convolution (TDC) to extract rPPG-relevant temporal features across frames; 2) a composite loss function integrating time- and frequency-domain constraints; and 3) spatio-temporal data augmentation techniques to enhance feature learning. Extensive evaluations across three benchmark datasets demonstrate the method's superior performance in both intra- and cross-dataset validation.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0709", "problem_id": "07090001", "content": "Braille has empowered the visually impaired community to read and write; however, it has also created a divide due to the general difficulty that non-Braille users face in comprehending Braille scripts. This challenge has motivated researchers to develop Optical Braille Recognition methods aimed at translating Braille documents into natural language. This study is primarily focused on bridging the communication divide in academic institutions by converting the personal documents of blind students. The proposed solution is a cost-effective and efficient method that utilizes a smartphone camera to digitize Braille documents. For any provided Braille image, a dot detection mechanism utilizing Hough transform is introduced, which remains robust against skewness, noise, and other interferences. The identified dots are then grouped into Braille cells through a distance-based clustering algorithm. Following this, the standard physical characteristics of each Braille cell are assessed for feature extraction and recognition as natural language characters. A thorough evaluation of this method on a dataset comprising 54 Braille scripts has resulted in an accuracy of 98.71%.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0710", "problem_id": "07100001", "content": "This study presents a new algorithm for transductive inference in higher-order MRFs, utilizing a variable classifier to parameterize unary energies. The problem is formulated as a joint optimization involving both continuous classifier parameters and discrete label variables. Unlike previous methods like convex relaxations, our strategy effectively separates the objective function into discrete and continuous components, employing an efficient optimization technique inspired by ADMM. This method maintains the integrality of discrete labels and ensures global convergence to a critical point. Experimental results on tasks such as video object segmentation using the DAVIS dataset and interactive image segmentation highlight the benefits of our approach.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0711", "problem_id": "07110001", "content": "Graph-based approaches have proven effective in various machine learning and pattern classification applications. These techniques represent semi-structured data as graphs, with nodes denoting basic elements (such as parts, key points, or segments) and edges capturing the connections between these elements. However, such non-vectorial graph representations cannot be directly used with standard machine learning methods unless they undergo an initial conversion into vector form through explicit or implicit embedding. This transformation must maintain robustness against variations within the same class while ensuring strong discriminative power. We introduce a novel high-order stochastic graphlet embedding (SGE) technique that converts graphs into vector space representations. Our key innovation involves a stochastic search mechanism that efficiently explores and samples high-order graphlets from a given graph. By progressively increasing the order of these graphlets, we capture both local elements and their increasingly intricate relationships. To construct the graph representation, we analyze the distribution of these graphlets using specialized hash functions that accurately group isomorphic graphlets with minimal collision risk. When integrated with maximum margin classifiers, these graphlet-based representations significantly enhance pattern comparison and recognition performance, as demonstrated by extensive testing on standard benchmark datasets.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0712", "problem_id": "07120001", "content": "We note that the conventional log likelihood training objective for a Recurrent Neural Network (RNN) model applied to time series data aligns with a variational Bayesian training objective when appropriate generative and inference models are selected. This viewpoint could inspire advancements in both RNNs and variational Bayesian frameworks. We suggest one such advancement, utilizing multiple particles for the RNN's hidden state, which facilitates a natural portrayal of uncertainty or multimodality.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0713", "problem_id": "07130001", "content": "Domain adaptive person re-identification (re-ID) presents significant challenges, particularly when the identities of individuals in target domains are unfamiliar. Current approaches generally focus on transferring image styles or aligning feature distributions between domains; however, they fall short of effectively utilizing the abundant unlabeled data available in target domains. This paper introduces an innovative augmented discriminative clustering (AD-Cluster) method that not only estimates but also enhances person clusters in target domains, thereby strengthening the discriminative capability of re-ID models via these augmented clusters. AD-Cluster is developed through a process of iterative density-based clustering, adaptive sample augmentation, and learning of discriminative features. It encompasses an image generator and a feature encoder that strive to maximize diversity within clusters in the sample space while minimizing the distance within clusters in the feature space through an adversarial min-max framework. Ultimately, AD-Cluster significantly improves the diversity of sample clusters and enhances the discrimination ability of re-ID models. Extensive evaluations on Market-1501 and DukeMTMC-reID demonstrate that AD-Cluster surpasses existing state-of-the-art methods by substantial margins.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0714", "problem_id": "07140001", "content": "An empirical evaluation was conducted to compare ICA and PCA methods using two simulated noisy time series with different distribution parameters and noise levels. Overall, ICA outperforms PCA as it incorporates higher-order statistical moments of the data distribution. Conversely, PCA demonstrates significant sensitivity to the degree of signal correlations.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0715", "problem_id": "07150001", "content": "Molecule representation learning (MRL) techniques seek to map molecules into real-valued vector spaces. Current approaches based on SMILES (Simplified Molecular-Input Line-Entry System) or GNNs (Graph Neural Networks) either struggle to capture structural details from SMILES strings or overly focus on GNN architectures while compromising generalization. We introduce a method that leverages chemical reactions to enhance molecular representation learning. Our core concept enforces equivalence between reactant and product embeddings in chemical equations, ensuring their vector sums remain equal. This constraint effectively organizes the embedding space and enhances generalization. Our framework is architecture-agnostic, compatible with any GNN-based molecule encoder. Evaluations show superior performance across multiple tasks, including a 17.4% absolute Hit@1 improvement in reaction prediction, 2.3% absolute AUC gain in property prediction, and 18.5% relative RMSE reduction in graph-edit-distance prediction compared to existing methods. The implementation is accessible at https://github.com/hwwang55/MolR.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0716", "problem_id": "07160001", "content": "Spatiotemporal forecasting is crucial for numerous applications within intelligent transportation systems (ITS), including route optimization, navigation, and traffic regulation. Deep spatiotemporal graph neural networks (GNNs) excel in traffic prediction by modeling spatial and temporal dependencies. Investigating the functionality of GNN-based forecasting, along with their vulnerabilities and robustness, is vital for practical deployment. For instance, if spatiotemporal GNNs are susceptible to manipulation in real-world traffic prediction, malicious actors could exploit these weaknesses, leading to severe congestion or even city-wide disruptions. While prior research has shown that deep neural networks (DNNs) are prone to adversarial perturbations in domains like image classification and graph representation, existing attack methods are not directly applicable to spatiotemporal forecasting due to its causal structure and spatiotemporal dynamics. To address this limitation, we introduce the Spatially Focused Attack (SFA), which targets a single node to compromise spatiotemporal GNNs. Our approach involves inverse estimation to resolve causality constraints, followed by genetic algorithms combined with a universal attack method to identify the most vulnerable node. Perturbations are then generated by solving an optimization problem based on inverse estimation. Experiments on real-world traffic datasets demonstrate that SFA-generated perturbations on a single node can propagate across a significant portion of the graph.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0717", "problem_id": "07170001", "content": "A variant of the Conditional Generalized Adversarial Network (CGAN) can be employed to train a deep neural network to replicate the outcomes of sequence processing. Although previous studies have demonstrated that CGAN can enhance results even for deterministic sequences, where a single output corresponds to a given input, our experiments using CGAN on deterministic geophysical processing sequences unexpectedly failed to yield significant improvements over the L_p loss. This paper provides a theoretical explanation for this phenomenon, progressing from non-deterministic to deterministic cases, and presents an adversarial approach to training a content loss, which achieved superior results on our dataset, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0718", "problem_id": "07180001", "content": "An image represents more than a mere assembly of objects; it functions as a graph in which objects are interconnected via spatial and semantic relationships. Incorporating relational reasoning modules, such as the non-local module \\cite, can enhance object detection performance. Existing approaches typically deploy these specialized modules either within a single layer of the bottom-up pathway or among already-detected objects. We demonstrate that relational reasoning is more effectively modeled through a coarse-to-fine strategy and introduce a new framework that sequentially applies a non-local module to progressively higher-resolution feature maps in the top-down stream. This approach facilitates the natural propagation of information from larger objects to their smaller counterparts. Additionally, applying the module to fine-grained feature maps enables interactions among small objects, leveraging repeated instances of the same class. However, due to the high memory demands of the non-local module, its direct application to high-resolution feature maps is impractical in current implementations. To address this, we redesigned the non-local module, optimizing its memory usage and computational efficiency, thereby enabling its integration at any network stage. We also enhanced the module by incorporating relative spatial information in a way compatible with our efficient design. Our method demonstrates its efficacy by boosting small object detection performance on COCO, achieving a 1-2 AP improvement over Faster and Mask RCNN and a 1 AP gain compared to using the non-local module in the bottom-up stream.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0719", "problem_id": "07190001", "content": "This research investigates face recognition (FR) and normalization specifically within surveillance imagery, a challenging area with significant implications for law enforcement. Although conventional FR has advanced, surveillance FR has received comparatively less attention. To address this disparity, we introduce a Feature Adaptation Network (FAN) designed for simultaneous FR and normalization in surveillance contexts. Our normalization approach primarily focuses on enhancing image resolution, aligning with face super-resolution techniques. Unlike existing face super-resolution methods that rely on paired training data with pixel-level correspondence—often absent in real-world low-resolution and high-resolution face scenarios—FAN utilizes both paired and unpaired data. This is achieved by separating features into identity and non-identity components and adjusting the distribution of identity features, overcoming the limitations of current face super-resolution methods. Furthermore, we present a random scale augmentation strategy to facilitate the learning of identity features that are robust to variations in resolution, offering advantages over previous fixed scale augmentation techniques. Comprehensive experiments conducted on the LFW, WIDER FACE, QUML-SurvFace, and SCface datasets demonstrate the efficacy of our method for surveillance FR and normalization.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0720", "problem_id": "07200001", "content": "This paper introduces GATSBI, a generative model designed to convert a sequence of unprocessed observations into a structured latent space, effectively encapsulating the spatio-temporal context surrounding an agent's actions. In vision-based decision-making, agents encounter intricate, high-dimensional observations characterized by interactions between multiple entities, necessitating a robust scene representation that identifies key components and maintains consistency over time. GATSBI employs unsupervised object-centric scene representation learning to distinguish between the active agent, the static background, and passive objects. It subsequently models interactions to reflect causal relationships between these decomposed entities, enabling the prediction of physically realistic future states. The proposed model demonstrates generalization across diverse environments featuring dynamic interactions between various robots and objects. Experimental results demonstrate that GATSBI outperforms state-of-the-art methods in both scene decomposition and video prediction.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0721", "problem_id": "07210001", "content": "Retrosynthetic planning is a core challenge in chemistry that involves identifying reaction sequences to produce a desired molecule. Recent advances have employed search algorithms enhanced by deep neural networks (DNNs) to generate potential reaction pathways. However, current approaches fall short as they fail to fully account for two critical constraints: (a) pathways must consist of feasible real-world reactions, and (b) they should rely on accessible \"building block\" molecules. To address these limitations, we introduce an end-to-end framework that trains DNNs to produce reaction pathways with these desired characteristics. Our approach leverages a self-improving mechanism where the model learns from its own successful pathways. Additionally, we present an innovative reaction augmentation method using a forward reaction model. Experimental results show that our method increases the success rate of retrosynthetic planning from 86.84% to 96.32% without compromising the DNN's ability to predict valid reactions.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0722", "problem_id": "07220001", "content": "The effectiveness of simple algorithms in tackling complex reinforcement learning problems can be significantly enhanced by leveraging a suitable representation, as demonstrated in this study. Here, simplicity is embodied in tabular Q-Learning, while a good representation is achieved through a learned state abstraction, and the challenges are posed by continuous control tasks. The primary contribution of this work is the development of an algorithm capable of abstracting a continuous state-space into a discrete representation, which can then be applied to novel, unseen problems, thereby facilitating efficient learning. Theoretical analysis reveals that the learned abstractions ensure a bounded value loss, and experimental results confirm that these abstractions enable tabular Q-Learning to learn efficiently in new tasks, as shown in Figure A, B, C (References [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0723", "problem_id": "07230001", "content": "The automatic generation of natural language captions for images is a significant area of research. Current leading models can produce sentences that closely resemble human language. However, these models generally offer a holistic depiction of the scene without focusing on specific objects of interest or the emotional dynamics among these objects within the image. Marketing firms, on the other hand, need to highlight these essential characteristics of a scene. In our study, the objects of interest are consumer products, typically recognized by their logos and linked to particular brands. From a marketing perspective, it is crucial to assess the emotional context associated with a branded product, indicating whether it carries a positive or negative connotation. We tackle the challenge of identifying brands within images and generating relevant captions by presenting a modified image captioning network. Additionally, we incorporate a third output modality that yields real-valued ratings for images. Our network is trained with a classification-aware loss function that encourages the production of sentences highlighting brand identification. We validate our model using a dataset featuring images of human interactions with branded products. The proposed network enhances mean class accuracy by 24.5 percent and significantly elevates the quality of captions generated for images of branded products due to the inclusion of the third output modality.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0724", "problem_id": "07240001", "content": "One of the most critical challenges in machine learning is addressing missing data, with zero imputation being a straightforward approach that replaces missing entries with zero. Nevertheless, numerous studies have demonstrated that this method yields suboptimal results when training neural networks, although the underlying causes of these performance declines have not been adequately explained. This paper introduces the concept of the variable sparsity problem (VSP), a phenomenon characterized by significant fluctuations in the output of a predictive model in response to varying rates of missingness in the input data, which adversely impacts model performance. Through theoretical analysis, we propose Sparsity Normalization (SN), a simple yet effective technique designed to mitigate the effects of missingness by directly addressing the VSP. Our experimental evaluation of SN on a range of benchmark datasets, as seen in Figure A, B, C, confirms that accounting for input-level sparsity improves model performance and stabilizes neural network training, as discussed in References [citation].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0725", "problem_id": "07250001", "content": "Exploration techniques in reinforcement learning for Atari 2600 games have concentrated on challenging environments like Montezuma's Revenge (Bellemare et al., 2016). Bonus-based exploration, which enhances environmental rewards to promote exploration, has achieved above-average human performance in these settings. This study re-evaluates prevalent bonus-based methods within a unified framework. By integrating Rainbow (Hessel et al., 2018) with various exploration bonuses, we assess performance on Montezuma's Revenge, Bellemare et al.'s suite of hard exploration games with sparse rewards, and the entire Atari 2600 library. Results indicate that while exploration bonuses improve scores on Montezuma's Revenge, they offer no significant advantage over the simpler \\epsilon-greedy approach. Moreover, methods excelling on Montezuma's Revenge often lag behind \\epsilon-greedy on easier Atari 2600 games, a conclusion that persists even with hyperparameter tuning. Furthermore, none of the surveyed methods benefit from increased training data (1 billion frames, compared to Rainbow's 200 million) on Bellemare et al.'s hard exploration games. These findings suggest that recent improvements on Montezuma's Revenge may stem more from architectural changes than superior exploration strategies, and that progress in Atari 2600 exploration research may be obscured by focusing on a single game.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0726", "problem_id": "07260001", "content": "The concept of arbitrary style transfer holds substantial research significance and potential for practical applications, aiming to transform a content image into a stylized version that captures the color tone and expressive brushstroke patterns of a reference style painting while preserving the intricate structural details of the original content. To achieve this, style transfer methods typically commence by learning representations of both content and style from their respective reference images, subsequently utilizing these representations to guide the generation of stylized images. This paper introduces a novel multi-adaptation network architecture, comprising two self-adaptation modules and one co-adaptation module, which adaptively separates content and style representations: the position-wise self-attention in the content self-adaptation module enhances content representation, while the channel-wise self-attention in the style self-adaptation module refines style representation; meanwhile, the co-adaptation module recalibrates the style representation distribution based on the content representation distribution by computing non-local similarities between disentangled content and style features. Furthermore, a newly designed disentanglement loss function allows the network to effectively extract dominant style patterns and precise content structures, enabling adaptability to diverse input images. Extensive qualitative and quantitative experiments, as shown in Figure A, B, C, and cited in References [1, 2], demonstrate the superiority of the proposed multi-adaptation network over existing state-of-the-art style transfer approaches, according to citations [3, 4].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0727", "problem_id": "07270001", "content": "In recent years, deep fakes have gained significant popularity, largely due to their enhanced realism. This has prompted a need to assess humans' capacity to differentiate between authentic and artificial face images when faced with advanced creation technologies. We outline the design and findings of a perceptual study we conducted, in which a varied and extensive group of participants viewed synthetic face images generated by leading Generative Adversarial Networks (specifically, PG-GAN, StyleGAN, and StyleGAN2). The results of this experiment strongly challenge our confidence in the ability to distinguish real faces from those created by contemporary AI.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0728", "problem_id": "07280001", "content": "This paper argues that relational algebra offers a cohesive framework for representing and manipulating statistical-relational entities, analogous to the role of linear algebra in conventional single-table machine learning. Given that relational algebra is realized through SQL, the foundation of relational database management systems, we introduce FACTORBASE, a system that employs SQL as a high-level scripting language for statistical-relational learning of graphical model structures, to validate our claim. FACTORBASE is designed to treat statistical models as integral components within a database environment. The implementation demonstrates that the SQL constructs in FACTORBASE promote rapid, modular, and dependable program development. Empirical results from six benchmark databases suggest that utilizing database system capabilities enables scalable model structure learning.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0729", "problem_id": "07290001", "content": "This study introduces a rapid and precise coordinate regression technique for facial alignment. Departing from conventional approaches that rely on fully connected layers to transform feature maps into landmark coordinates, we incorporate a structure coherence module to explicitly model the relationships between facial landmarks. Given the inherent geometric structure of human faces, interconnections among facial regions offer valuable information for accurate landmark localization. However, densely connected layers tend to overutilize these relationships, obscuring critical cues within excessive connections. To address this, our structure coherence component employs a dynamic sparse graph to selectively propagate features among the most relevant landmarks. Additionally, we introduce a new objective function called Soft Wing loss to enhance precision. Comprehensive evaluations on widely-used benchmarks—WFLW, COFW, and 300W—validate the method's efficacy, showing superior speed and performance. Notably, our technique exhibits exceptional robustness in difficult scenarios, yielding remarkably low failure rates (0% and 2.88%) on COFW and WFLW datasets.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0730", "problem_id": "07300001", "content": "Current techniques for detecting arbitrarily shaped text in natural settings often struggle with two main problems: 1) fragmented detections due to gaps within a text instance, and 2) imprecise detections of arbitrarily shaped text in varying background environments. To overcome these limitations, we introduce a new approach called Intra- and Inter-Instance Collaborative Learning (I3CL). Specifically, to mitigate the first problem, we introduce a convolutional module utilizing multiple receptive fields to collaboratively learn improved character and gap feature representations at both local and extended ranges within a text instance. Addressing the second problem, we develop an instance-based transformer module to leverage dependencies between different text instances, and a global context module to utilize semantic context from the shared background, enabling the collaborative learning of more distinctive text feature representations. Consequently, I3CL effectively integrates intra- and inter-instance dependencies within a unified, end-to-end trainable framework. Furthermore, to maximize the use of unlabeled data, we employ an effective semi-supervised learning method that leverages pseudo-labels through an ensemble strategy. Empirical evaluations demonstrate that the proposed I3CL achieves state-of-the-art performance on three demanding public datasets, achieving an F-measure of 77.5% on ICDAR2019-ArT, 86.9% on Total-Text, and 86.4% on CTW-1500. Notably, our I3CL with the ResNeSt-101 backbone achieved the top rank on the ICDAR2019-ArT leaderboard. The source code will be made publicly available.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0731", "problem_id": "07310001", "content": "Conventional video super-resolution techniques primarily aim to reconstruct high-resolution video frames from their low-resolution counterparts, often overlooking the impact of compression. Given that most online and mobile videos are compressed, sometimes significantly due to bandwidth constraints, we introduce a novel compression-informed video super-resolution model designed to generate high-resolution video while mitigating compression-induced artifacts. Our model incorporates three key modules for video super-resolution: bi-directional recurrent warping, detail-preserving flow estimation, and Laplacian enhancement, each specifically designed to address compression-related characteristics such as intra-frame positioning and output frame smoothness. To rigorously assess performance, we performed comprehensive experiments across standard datasets, utilizing diverse compression rates to mirror realistic video scenarios. The results demonstrate that our method excels not only in recovering high-resolution content from uncompressed frames within benchmark datasets but also in achieving state-of-the-art super-resolution performance for compressed videos, as evidenced by various quantitative metrics. Furthermore, we evaluated the method's efficacy and robustness by simulating video streaming from YouTube.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0732", "problem_id": "07320001", "content": "In various computer vision tasks such as image captioning, visual question answering, and person search, the acquisition of discriminative feature representations at both the image and text levels presents a crucial yet complex challenge. This complexity arises from significant word variability in the text domain and the difficulties involved in accurately assessing the distance between features from the two modalities. Previous research has primarily concentrated on addressing the latter issue by developing loss functions that enhance feature representation learning but often neglect the intricacies of textual input. To address this gap, we propose TIMAM: a Text-Image Modality Adversarial Matching method that learns modality-invariant feature representations through adversarial and cross-modal matching objectives. Furthermore, we show that BERT, a publicly available language model capable of extracting word embeddings, can be effectively utilized in the text-to-image matching context. Our proposed approach achieves state-of-the-art performance in cross-modal matching across four widely-used publicly available datasets, yielding absolute rank-1 accuracy improvements ranging from 2% to 5%.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0733", "problem_id": "07330001", "content": "Age estimation is gaining traction due to its diverse applications in medicine. While numerous studies have explored human age estimation using biomedical images, to our knowledge, mammograms have not been utilized for this purpose. This research aims to develop an artificial intelligence model for age estimation based on mammogram images. Given the scarcity of publicly available mammography datasets with age information, a web crawler was employed to gather thumbnail mammogram images and corresponding age data from the Digital Database for Screening Mammography. Unfortunately, the original images in this dataset are inaccessible due to malfunctioning retrieval software. Deep learning features were then extracted from the collected dataset, and a Random Forests regressor was trained to automatically estimate age. The model's performance was evaluated using the mean absolute error, yielding an average error of approximately 8 years across 10 tests with random sample selections. This paper demonstrates the potential of this method for imputing missing age values. To further validate the advantages of our approach, logistic and linear regression models were applied to an independent dataset. Furthermore, this paper introduces the freely accessible Mini-DDSM dataset.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0734", "problem_id": "07340001", "content": "The recognition of human actions from skeletal data has garnered significant attention, driven by the Graph Convolutional Network's (GCN) ability to effectively model non-Euclidean structural data. Nevertheless, existing GCN methods often rely on pre-defined graphs that remain fixed throughout the network, potentially overlooking implicit correlations between joints. Furthermore, conventional spectral GCNs are typically approximated using one-order hops, which may not adequately capture higher-order connections. To overcome these limitations, this study leverages Neural Architecture Search (NAS) to propose the first automatically designed GCN for skeleton-based action recognition. By introducing multiple dynamic graph modules that capture spatial-temporal correlations between nodes, as well as multi-hop modules to mitigate the representational capacity constraints imposed by one-order approximations, the search space is significantly expanded. Additionally, an efficient evolution strategy is developed to search for an optimal architecture, balancing sampling and memory efficiency. The resulting architecture demonstrates the efficacy of higher-order approximations and dynamic graph modeling with temporal interactions, a topic that has received limited attention previously. Extensive experiments conducted on two large-scale datasets validate the performance of the searched model, yielding state-of-the-art results, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0735", "problem_id": "07350001", "content": "Convolutional neural networks (CNNs) have significantly impacted machine learning, particularly in computer vision and temporal pattern recognition. Their adaptability in vision tasks stems from max-pooling operations applied to convolutions, which achieve translation invariance. In biological systems, temporal representations in the mammalian brain rely on temporal basis functions organized in a geometric series, ensuring even logarithmic time distribution. This work presents a Scale-Invariant Temporal History Convolution network (SITHCon), incorporating a logarithmically-distributed temporal memory. Applying max-pooling to this memory enables temporal scale-invariance. When evaluated against a Temporal Convolution Network (TCN), both models perform well on classification and regression tasks involving univariate and multivariate time series f(t). However, only SITHCon generalizes to rescaled inputs f(at) without retraining. Drawing insights from neuroscience and psychology, this approach could enable more efficient large-scale networks with enhanced training speed and generalization, even with reduced parameter counts.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0736", "problem_id": "07360001", "content": "Reinforcement learning frequently uses feature vectors to model real-world states, though not all features are relevant for a given task. We introduce Feature Selection Explore and Exploit (FS-EE), a method that dynamically identifies essential features during the learning of a Factored Markov Decision Process. Under reasonable conditions, we demonstrate that its sample complexity depends only on the in-degree of the relevant features' dynamics, not the in-degree of all features. This leads to significantly improved sample efficiency when the necessary features have a lower in-degree than the complete feature set.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0737", "problem_id": "07370001", "content": "This paper presents two broadly applicable regularization techniques that involve directly adjusting weight matrices. The first technique, Weight Reinitialization, employs a simplified Bayesian assumption while partially resetting a sparse group of parameters. The second technique, Weight Shuffling, adds a non-white noise that is invariant to entropy and weight distribution to the parameters, which can also be viewed as an ensemble method. We assess the effectiveness of these methods using benchmark datasets like MNIST, CIFAR-10, and the JSB Chorales database, in addition to tasks involving time series modeling. Our findings indicate improvements in both the performance and entropy of the networks examined. Additionally, we have made our code accessible through a GitHub repository (https://github.com/rpatrik96/lod-wmm-2019).", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0738", "problem_id": "07380001", "content": "We tackle structured output prediction by training a deep value network (DVN) to accurately assess the task loss across various output configurations for a specific input. After training the model, we use gradient descent on the continuous relaxations of the output variables during inference to identify outputs with favorable scores from the value network. In the context of image segmentation, the value network takes an image along with a segmentation mask as inputs and predicts a scalar value that estimates the intersection over union between the input and the ground truth masks. For multi-label classification, the goal of the DVN is to accurately predict the F1 score for all possible label configurations. The DVN architecture demonstrates state-of-the-art performance in both multi-label prediction and image segmentation benchmarks.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0739", "problem_id": "07390001", "content": "Instance-based interpretation techniques have been extensively explored for supervised learning approaches due to their ability to elucidate predictions made by opaque neural networks. Yet, their application and understanding in unsupervised learning contexts remain limited. This study explores influence functions [20], a prominent instance-based interpretation technique, within the framework of variational auto-encoders (VAE), a category of deep generative models. We rigorously define the counterfactual inquiry addressed by influence functions in this scenario and theoretically analyze their insights into training data effects on traditional unsupervised learning techniques. Additionally, we propose VAE-TracIn, an efficient and theoretically grounded method inspired by Pruthi et al. [28], tailored for VAEs. The effectiveness of VAE-TracIn is subsequently assessed through comprehensive quantitative and qualitative experiments on multiple real-world datasets.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0740", "problem_id": "07400001", "content": "Spectral techniques have long been fundamental in fields like machine learning and scientific computing, relying on spectral decomposition to derive basis functions that represent key problem structures. Principal component analysis (PCA), a widely used spectral approach, employs the leading eigenvectors of the covariance matrix for tasks such as dimensionality reduction, effectively distinguishing signal from noise. Despite their utility, matrix-based spectral methods like PCA face constraints, as they rely on pairwise moments, implicitly assuming Gaussian data distributions and struggling with non-Gaussian data influenced by hidden variables. Many real-world datasets contain latent factors—such as document topics or disease causes—that remain unobserved. By expanding spectral decomposition to higher-order moments, we show that a broader class of latent variable models can be learned efficiently. Tensors, which represent these higher-order moments, encapsulate richer information than pairwise matrices and can uncover latent structures missed by matrix techniques, such as non-orthogonal components. Leveraging these properties enables provable unsupervised learning for diverse latent variable models. Additionally, we discuss computational strategies for efficient tensor decomposition and present Tensorly, a Python-based library with a user-friendly interface for tensor operations. Its adaptable back-end supports frameworks like NumPy, PyTorch, TensorFlow, and MXNet, enabling multi-GPU and CPU processing while seamlessly integrating with deep-learning capabilities.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0741", "problem_id": "07410001", "content": "This article explores the interplay between Machine Learning and Control Theory, highlighting how Control Theory offers valuable frameworks and methodologies for Machine Learning, while Machine Learning techniques can address complex control challenges. The first section examines the relationship between reinforcement learning and Markov Decision Processes, which represent discrete-time control scenarios. The second section discusses supervised learning and its connection to static optimization, noting that deep learning—an extension of supervised learning—can be interpreted as a control problem. The third section analyzes the ties between stochastic gradient descent and mean-field theory. Subsequently, the fourth and fifth sections investigate machine learning applications in stochastic control problems, with an emphasis on deterministic cases to simplify the explanation of numerical algorithms.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0742", "problem_id": "07420001", "content": "Human pose estimation (HPE) plays a crucial role in analyzing the visual storytelling and body motions of figures in artistic works, including Greek vase paintings. However, current HPE techniques often fail to adapt effectively across different domains, leading to inaccurate pose recognition. To address this, we present a two-stage solution: (1) modifying a dataset of natural images with labeled person and pose annotations to resemble Greek vase paintings through style-transfer techniques. Our approach incorporates perceptually-driven style transfer training to ensure visual coherence. Subsequently, we refine the base model using this stylized dataset. Our results demonstrate that style-transfer learning enhances state-of-the-art performance on unannotated data, achieving over 6% improvement in both mean average precision (mAP) and mean average recall (mAR). (2) To further boost performance, we developed ClassArch, a small annotated dataset of 6th–5th century BCE Greek vase paintings. Fine-tuning the style-transferred model with this dataset yields additional gains. A detailed ablation study examines the impact of varying style intensities, showing the model’s ability to generalize across domain-specific styles. We also introduce a pose-based image retrieval system to validate our method’s efficacy.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0743", "problem_id": "07430001", "content": "Consider n points x_1,\\ldots,x_n in a finite-dimensional Euclidean space, each assigned one of two colors. We assume the existence of a separating hyperplane, represented by its unit normal vector w, which ensures that points sharing the same color are situated on the same side of the hyperplane. The effectiveness of this hyperplane is evaluated through its margin \\gamma(w), defined as the shortest distance from any point x_i to the hyperplane. This paper demonstrates that the margin function \\gamma adheres to a nonsmooth Kurdyka-Lojasiewicz inequality with an exponent of 1/2. This finding has significant implications. For instance, let \\gamma^ denote the maximum achievable margin for the problem, and w^ represent the hyperplane parameter that achieves this margin. For any other separating hyperplane with parameter w, we define d(w):=\\|w-w^\\| as the Euclidean distance between w and w^, also referred to as the bias of w. From the established KL-inequality, we can infer that (\\gamma^-\\gamma(w)) / R \\le d(w) \\le (2-\\gamma(w))/\\gamma^}, where R:=\\max_i \\|x_i\\| indicates the furthest distance of the points x_i from the origin. As a result, for any optimization method (whether gradient-based or not), the bias of the iterations converges at least as rapidly as the square root of the margin's convergence rate. Thus, our findings offer a universal framework for assessing the implicit bias of any algorithm concerning its margin, especially in contexts where tailored analyses may not exist: it is sufficient to determine a satisfactory convergence rate for the margin, a generally much simpler endeavor.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0744", "problem_id": "07440001", "content": "Visual attention is crucial for image understanding and has proven useful in creating natural language descriptions of images. Conversely, recent research indicates that language related to an image can influence visual attention within a scene during cognitive processing. Drawing on this, we present a text-guided attention model for image captioning that learns to direct visual attention using corresponding captions. The model employs an exemplar-based learning method, retrieving associated captions for each image from the training dataset and leveraging them to learn attention on visual features. This attention model facilitates the description of detailed scene states by effectively differentiating between small or easily confused objects. The model's efficacy is validated on the MS-COCO Captioning benchmark, where it achieves state-of-the-art results according to standard metrics.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0745", "problem_id": "07450001", "content": "Graph Neural Networks (GNNs) have made significant strides in the realm of graph representation learning. However, existing GNNs often necessitate loading the complete attributed graph into the network for processing, an assumption that may be impractical in scenarios with limited memory, particularly when dealing with large attributed graphs. This paper introduces a novel Binary Graph Convolutional Network (Bi-GCN), which converts both the network parameters and input node features into binary forms. Additionally, we modify the standard matrix multiplications to binary operations to enhance processing speed. The theoretical analysis indicates that the Bi-GCN significantly decreases memory usage by approximately 30 times for both network parameters and input data, while also increasing inference speed by about 47 times on citation networks. Furthermore, we propose a new gradient approximation-based back-propagation method to effectively train the Bi-GCN. Extensive experimental results show that our Bi-GCN achieves performance comparable to full-precision baselines. Moreover, our binarization technique can be readily adapted to other GNNs, as evidenced by our experimental findings.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0746", "problem_id": "07460001", "content": "In the realm of hierarchical reinforcement learning, a significant obstacle lies in identifying suitable low-level policies. We introduce an unsupervised learning framework, inspired by the asymmetric self-play method from Sukhbaatar et al. (2018), which autonomously acquires effective representations of sub-goals within the environment and develops a corresponding low-level policy to execute them. A higher-level policy can subsequently guide the lower-level one by producing a series of continuous sub-goal vectors. We assess our model in Mazebase and Mujoco environments, including the demanding AntGather task. The visual representations of the sub-goal embeddings illustrate a coherent breakdown of tasks in the environment. In quantitative terms, our method demonstrates significant performance improvements compared to non-hierarchical approaches.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0747", "problem_id": "07470001", "content": "Generative models have recently gained renewed interest due to adversarial learning techniques. Generative adversarial networks (GANs) comprise a sample generation model and a discriminator that differentiates between real and artificial samples. When paired with convolutional layers (for the discriminator) and de-convolutional layers (for the generator), they excel at producing images, particularly natural scenes. However, fully connected layers introduce global dependencies in the generated images, which can cause significant and widespread variations in the output from minor local changes in the input noise. This study introduces architectures based on fully convolutional networks, including dilated layers, which are tailored to create globally ergodic images—those free from global dependencies. Experimental results demonstrate that these architectures are effective for generating natural textures, such as geological formations.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0748", "problem_id": "07480001", "content": "Recent developments in deep neural networks (DNN) have greatly improved the real-time identification of anomalous data within IoT applications. Nonetheless, the challenge of balancing complexity, accuracy, and delay remains: while more intricate DNN models yield better accuracy, most IoT devices struggle to handle the computational requirements, and transferring this load to the cloud results in increased latency. This paper tackles this issue by introducing an adaptive anomaly detection framework utilizing hierarchical edge computing (HEC). We begin by creating multiple DNN models for anomaly detection, each with varying complexity, and link them to specific HEC layers. Subsequently, we devise an adaptive model selection strategy framed as a contextual-bandit problem, which is addressed using a reinforcement learning policy network. Additionally, we implement a parallelism policy training approach to enhance the training efficiency by leveraging distributed models. An HEC testbed is established with actual IoT devices, and our contextual-bandit methodology is implemented and assessed using both univariate and multivariate IoT datasets. When compared to baseline and state-of-the-art methods, our adaptive solution demonstrates the optimal accuracy-delay balance on the univariate dataset and achieves superior accuracy and F1-score on the multivariate dataset, with only a marginally longer delay than the best (yet rigid) option.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0749", "problem_id": "07490001", "content": "This paper presents a novel self-supervised approach to graph representation learning, leveraging the inherent structure of networked data as a means of supervision. By drawing inspiration from human social interactions, where any two entities in a connected network can potentially interact through paths of varying lengths, we propose that the global context of each node encompasses all other nodes in the graph. We explore the potential of this global context to serve as a source of supervisory signals for learning informative node representations. To achieve this, a neural network is trained to predict the relative contextual position of randomly selected node pairs, with the underlying assumption that the resulting representations will capture the graph's global topology and effectively characterize node similarities and differences. The efficacy of our approach is demonstrated through extensive experiments on node classification, clustering, and link prediction benchmarks, where it outperforms numerous state-of-the-art unsupervised methods and, in some cases, even surpasses the performance of supervised counterparts.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0750", "problem_id": "07500001", "content": "While machine learning serves as a valuable tool to support clinicians in medical analysis, the substantial need for labeled data to train supervised methods makes development processes time-consuming and resource-heavy. Most clinical data exists in the form of detailed written reports containing critical patient information, yet leveraging this natural language data for conventional model development is complicated by its inherent complexity. This study introduces a model pipeline that employs an unsupervised method to train an encoder-language model—a recurrent network—to produce document encodings. These encodings serve as input features for a decoder-classifier model, significantly reducing the labeled data required to distinguish between fine-grained disease categories with high accuracy. The language model was trained on 218,159 unlabeled radiology reports from Massachusetts General Hospital, achieving a final loss of 1.62. Classification models were then trained on three labeled head CT datasets—large vessel occlusion (n=1403), acute ischemic stroke (n=331), and intracranial hemorrhage (n=4350)—to detect various findings directly from radiology reports, yielding AUCs of 0.98, 0.95, and 0.99, respectively. The generated encodings can also be combined with imaging data to develop models capable of processing diverse modalities. By automating feature extraction from text, this approach accelerates model development and enhances the integration of textual data, making clinical reports a more practical and effective input for comprehensive and precise deep learning models.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0751", "problem_id": "07510001", "content": "In recent years, technological progress has led to increasingly sedentary lifestyles in urban environments, contributing to rising obesity rates among younger populations. Obesity is associated with numerous health risks, including diabetes, cardiovascular conditions, and hypertension. Machine learning has demonstrated significant potential across diverse fields such as predictive analytics, healthcare, medical imaging, and sentiment analysis. This study proposes a framework employing machine learning algorithms—Random Forest, Decision Tree, XGBoost, Extra Trees, and KNN—to develop models for predicting obesity classification, body weight, and fat percentage regression based on multiple parameters. Additionally, hyperparameter optimization techniques, including Genetic Algorithm, Random Search, Grid Search, and Optuna, were evaluated to enhance model accuracy. The web-based framework, developed using Python Flask, offers features such as personalized diet and exercise plans alongside a progress-tracking dashboard. An IoT-enabled smart scale is also integrated to monitor caloric and macronutrient intake.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0752", "problem_id": "07520001", "content": "Current reinforcement learning approaches are hindered by inefficient sampling and unsafe exploration strategies, rendering it impractical to train robotic policies solely on physical hardware. To mitigate the challenge of transferring knowledge from simulated to real-world environments, this study proposes a novel approach that leverages meta-learning to develop adaptable policies capable of navigating diverse dynamic scenarios, while also utilizing a task-specific trajectory generation model to facilitate efficient exploration of the action space. The efficacy of this method is assessed through simulated domain adaptation and an examination of the latent space structure during the adaptation process, as seen in Figure A, B, C (as cited in [citation]). The policy is then implemented on a KUKA LBR 4+ robot, where its performance is evaluated on a task involving hitting a hockey puck at a target, demonstrating more consistent and stable domain adaptation compared to the baseline, ultimately yielding improved overall performance, as discussed in References.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0753", "problem_id": "07530001", "content": "This study introduces \"GENet,\" a novel deep learning architecture that integrates a multi-layer network structure with a graph embedding framework. The initial layer employs basic unsupervised learning techniques like PCA/LDA to extract low-level features. Subsequent layers incorporate cascaded dimensionality reduction modules based on the graph embedding approach. A linear SVM classifier is then utilized to categorize the reduced-dimensional features. Experimental results demonstrate that this method achieves superior classification performance on the CMU-PIE, ORL, and Extended Yale B datasets.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0754", "problem_id": "07540001", "content": "Recent years have seen the emergence of two promising algorithms: the deep Q-network (DQN) and return-based reinforcement learning, which have shown potential in addressing complex sequential decision problems and leveraging sample trajectories, respectively. This paper introduces a novel framework, termed R-DQN, which integrates DQN with most return-based reinforcement learning algorithms, demonstrating that the incorporation of return-based reinforcement learning can significantly enhance the performance of traditional DQN. To further refine R-DQN, a two-measurement strategy is developed to quantitatively assess policy discrepancy, with established bounds within the proposed framework. The strategy enables algorithms to accurately capture the trace coefficient and yield a closer approximation to return, as evidenced by experiments conducted on a range of tasks from the OpenAI Gym library, which validate the effectiveness of the proposed measurements and show that algorithms incorporating this strategy surpass state-of-the-art methods.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0755", "problem_id": "07550001", "content": "Numerous machine learning models have been applied to microwave imaging challenges, yet these approaches typically disregard the underlying imaging geometry. Integrating the physical configuration of the imaging array into the network architecture has proven difficult, often leading to impractical, data-hungry models. This study introduces a graph-based representation of the microwave imaging array, enabling the proposed architectures to account for the physical setup and inherent symmetries, thereby reducing data demands. Graph convolution and attention mechanisms are employed to address fully-connected graphs associated with multi-static arrays. The effectiveness of this graph-based approach is assessed through experimental validation in the context of brain anomaly detection using microwave imaging.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0756", "problem_id": "07560001", "content": "Visual explanations help humans interpret the decision-making process of Deep Convolutional Neural Networks (CNNs), though they typically do not enhance model performance. This study examines attention maps for visual explanation, highlighting regions with high response values that are crucial for image recognition. By integrating an attention mechanism that concentrates on key image regions, CNN performance can be significantly improved. We present the Attention Branch Network (ABN), an extension of top-down visual explanation models that incorporates a branch structure with an attention mechanism. ABN is adaptable to various image recognition tasks through its attention branch and can be trained end-to-end for both visual explanation and recognition. Evaluations on tasks such as image classification, fine-grained recognition, and multiple facial attribute recognition demonstrate that ABN surpasses baseline models in accuracy while producing attention maps for interpretability. Our code is available at https://github.com/machine-perception-robotics-group/attention_branch_network.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0757", "problem_id": "07570001", "content": "In safety-critical applications like autonomous driving, it is crucial for Convolutional Neural Networks (CNNs) to be resilient against inevitable image degradation, including noise. Unlike previous research focused on full-image classification, we investigate robust prediction in the context of dense semantic segmentation. Building on the understanding that enhancing network bias towards object shapes can improve output robustness, we propose a novel training approach designed to increase this shape bias. Our method, termed \"Painting-by-Numbers\", involves alpha-blending portions of RGB training images with artificially generated images, where each class label is assigned a fixed, randomly selected color unlikely to occur in real-world images, thereby compelling the network to rely more heavily on shape cues. We validate the efficacy of our training approach using DeepLabv3+ with various backbones, including MobileNet-V2, ResNets, and Xception, and evaluate its performance on the Cityscapes dataset. Compared to training with clean data, our approach yields superior results in 74% of cases across 16 types of image corruptions and 5 network backbones, with only marginal declines in performance where it underperforms a model trained without our schema, while achieving significant performance gains of up to 25% for certain corruptions, such as noise.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0758", "problem_id": "07580001", "content": "We introduce average Localisation-Recall-Precision (aLRP), a comprehensive, limited, balanced, and ranking-oriented loss function applicable to both classification and localisation problems in object detection. aLRP builds upon the Localisation-Recall-Precision (LRP) performance metric (Oksuz et al., 2018), drawing inspiration from how Average Precision (AP) Loss translates precision into a ranking-based loss framework for classification (Chen et al., 2020). aLRP offers several notable benefits: (i) it is the inaugural ranking-based loss function designed for both classification and localisation tasks; (ii) the application of ranking across both tasks inherently promotes high-quality localisation corresponding to high-precision classification; (iii) it guarantees a demonstrable balance between positive and negative instances; (iv) unlike the average of approximately six hyperparameters found in the loss functions of leading detectors, aLRP Loss contains only one hyperparameter, which we did not adjust during practical use. On the COCO dataset, aLRP Loss enhances its ranking-based predecessor, AP Loss, by nearly 5 AP points, achieving 48.9 AP without test time augmentation and surpassing all one-stage detectors. Code available at: https://github.com/kemaloksuz/aLRPLoss.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0759", "problem_id": "07590001", "content": "We investigate the sample complexity associated with model-based reinforcement learning (RL) within general contextual decision processes that necessitate strategic exploration to identify a near-optimal policy. We develop novel algorithms for RL utilizing a generic model class and examine their statistical characteristics. The sample complexity of our algorithms is determined by a new structural parameter termed the witness rank, which we demonstrate to be relatively small in various relevant scenarios, such as factored MDPs. Furthermore, we establish that the witness rank does not exceed the recently introduced Bellman rank parameter, which influences the sample complexity of the model-free algorithm OLIVE (Jiang et al., 2017), the only other algorithm proven to be sample-efficient for global exploration at this level of generality. In the specific scenario of factored MDPs, we provide an exponential lower bound for a broad class of model-free methods, including OLIVE, and when integrated with our algorithmic findings, this reveals an exponential distinction between model-based and model-free RL in certain rich-observation contexts.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0760", "problem_id": "07600001", "content": "The video object segmentation (VOS) task, which involves distinguishing foreground objects from their background in videos, has garnered significant interest in recent years. This paper presents a novel, end-to-end trainable deep neural network architecture, termed convolutional gated recurrent Mask-RCNN, designed to address the semi-supervised VOS challenge. By leveraging the strengths of both Mask-RCNN, an instance segmentation network, and Conv-GRU, a visual memory module, our approach effectively tackles the complexities of VOS. The instance segmentation network generates instance masks, while the visual memory module selectively propagates information across multiple instances, handling changes in appearance, scale, pose, and inter-object occlusions. Following offline and online training using solely instance segmentation losses, our method yields satisfactory outcomes without requiring post-processing or synthetic video data augmentation, as evidenced by experimental results on the DAVIS 2016 dataset and DAVIS 2017 dataset, demonstrating its efficacy in video object segmentation.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0761", "problem_id": "07610001", "content": "Advances in generative modeling, particularly generative adversarial networks, have enabled the large-scale synthesis and modification of digital media. These machine-generated deepfakes are increasingly exploited by bad actors to distort public discourse. Current efforts to verify media authenticity primarily focus on detecting deepfakes, but the adversarial nature of generative frameworks implies that improvements in detection will drive the creation of more convincing deepfakes. Consequently, creators of generative models face heightened scrutiny from stakeholders combating misinformation. However, these models also offer numerous beneficial uses, underscoring the necessity for tools that promote responsible generative modeling while mitigating harmful misuse. Our method optimizes the entropy source of each generative model to probabilistically trace a deepfake back to its origin. Testing our approach on facial synthesis, we achieve 97.62% attribution accuracy, with robustness against perturbations and adversarial attacks. We examine the ethical implications of our work, outline potential applications, and emphasize the need for stronger regulations to ensure ethical generative modeling practices. Additionally, we advocate for plausible deniability for model developers and introduce a secondary framework that enables them to substantiate claims of non-involvement in disputed media creation.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0762", "problem_id": "07620001", "content": "Current investigations into vision and language grounding for robotic navigation have primarily concentrated on enhancing model-free deep reinforcement learning models within simulated environments. Nevertheless, these models neglect the complexities of real-world settings and frequently struggle to adapt to novel scenarios. This paper presents a groundbreaking approach to reconciling the disparity between theoretical studies and practical applications by introducing a pioneering hybrid reinforcement learning model. This model synergistically combines model-free and model-based reinforcement learning techniques to tackle real-world vision-language navigation challenges. A key component of our approach is the look-ahead module, which seamlessly integrates a predictive policy model with an environmental model that forecasts subsequent states and rewards, as shown in Figure A. Our experimental findings, detailed in Figure B and Table C (citations: [1], [2]), demonstrate that the proposed methodology substantially surpasses baseline performance and yields optimal results on the Room-to-Room dataset, while also exhibiting enhanced generalizability when applied to previously unseen environments.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0763", "problem_id": "07630001", "content": "Survival models find utility across diverse domains, including cancer treatment protocol design. Despite the proliferation of statistical and machine learning techniques aimed at refining survival prediction accuracy, the reliable quantification of prediction uncertainty has been largely overlooked. Contemporary models often lack transparency and trustworthiness, frequently exhibiting unwarranted confidence, particularly when confronted with test data dissimilar to the training set, and even in instances of incorrect predictions. To address these limitations, we introduce a Bayesian survival modeling framework designed to enhance prediction accuracy and improve uncertainty quantification. Our methodology integrates variational inference for uncertainty estimation, neural multi-task logistic regression for modeling nonlinear and time-dependent risks, and a sparsity-promoting prior to effectively manage high-dimensional data.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0764", "problem_id": "07640001", "content": "Hybrid modeling, which integrates first principle and machine learning approaches, is an increasingly prominent area of research. While hybrid models demonstrate impressive results in academic scenarios, various technical difficulties still impede their application in real-world settings. Introducing NeuralFMUs, a combination of a FMU, a numerical ODE solver, and an ANN, facilitates the integration of diverse first principle models from multiple modeling tools into hybrid frameworks. This work addresses the hybrid modeling of a complex real-world case: beginning with a simplified 1D fluid model of the human cardiovascular system (focusing on the arterial side), the goal is to derive overlooked physical phenomena such as arterial elasticity from data. We will demonstrate that the hybrid modeling process is more user-friendly, requires less expertise, and is thus less susceptible to errors compared to exclusive reliance on first principle modeling. Moreover, the developed hybrid model shows enhanced computational performance relative to a traditional first principle white-box model, while meeting the accuracy demands for the relevant hemodynamic parameters. The techniques discussed are presented in a broad context, and the illustrated case study can serve as a reference for other modeling and simulation endeavors in and beyond the medical field.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0765", "problem_id": "07650001", "content": "Image captioning is an Artificial Intelligence task that combines computer vision with natural language processing, aiming to produce textual descriptions for images. This task has multiple applications, including descriptions for assistive technologies and image indexing in search engines, making it a significant area of research within AI. However, similar to many tasks in this field, it relies on large sets of images that are labeled through human annotation, a process that can be labor-intensive, costly, fraught with errors, and challenging to execute in certain scenarios (such as with medical imaging). To alleviate the dependency on labeled data, we explore self-supervised learning, where the labels are derived from the data intrinsic to the images themselves. This approach poses challenges due to the dual nature of the task, as images and captions are derived from different modalities and typically processed by distinct types of networks. Therefore, defining a fully self-supervised solution remains unclear. The research into how this could enable captioning akin to existing self-supervision in image recognition is ongoing. In our project, we implement an encoder-decoder architecture where the encoder is a convolutional neural network (CNN) trained on the OpenImages dataset, learning image features in a self-supervised manner through a rotation pretext task. The decoder, which is a Long Short-Term Memory (LSTM) network, is trained concurrently with the image captioning model on the MS COCO dataset and is tasked with producing the captions. Our GitHub repository is available at: https://github.com/elhagry1/SSL_ImageCaptioning_RotationPrediction.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0766", "problem_id": "07660001", "content": "The development of graphical causal inference, initially explored by Judea Pearl, originated in the realm of artificial intelligence, with minimal overlap with machine learning for an extended period. This article explores the existing and potential connections between these fields, providing an introduction to fundamental concepts. It posits that the most challenging unresolved issues in machine learning and AI are inherently tied to causality, and elucidates the emerging understanding of these relationships within the field.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0767", "problem_id": "07670001", "content": "While video object detection has shown promising results on desktop GPUs, its current architecture remains too resource-intensive for mobile devices. Additionally, it is uncertain whether the core concepts of sparse feature propagation and multi-frame feature aggregation remain effective under severe computational constraints. This paper introduces a lightweight network architecture tailored for mobile video object detection, utilizing a lightweight image object detector on selected key frames. A compact network, Light Flow, is developed to establish inter-frame correspondences, while a flow-guided GRU module efficiently aggregates features on key frames. Non-key frames employ sparse feature propagation. The entire network supports end-to-end training and achieves 60.2% mAP at 25.6 fps on mobile platforms (e.g., HuaWei Mate 8).", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0768", "problem_id": "07680001", "content": "Capturing spatio-temporal information is crucial for distinguishing between real and fake faces in video sequences, with subtle movements (e.g., eye blinks, mouth movements, and head movements) across frames being particularly important. In this paper, we introduce a CNN-LSTM network designed for face anti-spoofing that emphasizes motion cues across video frames. Initially, highly discriminative features are extracted from video frames using a standard Convolutional Neural Network (CNN). Subsequently, these features are used as inputs to a Long Short-Term Memory (LSTM) network to model temporal dynamics. To facilitate the detection of subtle motions during training, Eulerian motion magnification is applied as a preprocessing step to amplify facial expressions. Additionally, an attention mechanism is integrated into the LSTM to enable selective focus on dynamic frames within the video clips. Experimental results on the Replay Attack and MSU-MFSD databases demonstrate that the proposed approach achieves state-of-the-art performance and exhibits improved generalization capabilities compared to several other established methods.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0769", "problem_id": "07690001", "content": "Recent research on model-agnostic explanations for black-box machine learning models has indicated that the interpretability of intricate models can be achieved without sacrificing accuracy or model adaptability. However, the optimal type of explanation—such as linear models, decision trees, or rule lists—remains undefined, as diverse tasks and models may benefit from different explanation formats. Rather than selecting a single representational family, this study introduces \"programs\" as model-agnostic explanations. We demonstrate that concise programs offer expressiveness and intuitiveness as explanations and can be generalized across various existing interpretable families. We introduce a prototype program induction technique based on simulated annealing, which approximates the local behavior of black-box classifiers around a particular prediction using random perturbations. Finally, we present an initial application on small datasets, demonstrating that the generated explanations are both intuitive and accurate for a variety of classifiers.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0770", "problem_id": "07700001", "content": "We introduce an innovative approach for unsupervised image-to-image translation that integrates a novel attention mechanism and a trainable normalization technique in an end-to-end framework. The attention module directs the model to prioritize significant regions differentiating source and target domains using attention maps generated by an auxiliary classifier. Unlike prior attention-based techniques limited to handling geometric transformations, our method effectively translates images needing both global modifications and substantial structural alterations. Additionally, our Adaptive Layer-Instance Normalization (AdaLIN) enables dynamic adjustment of shape and texture transformations through dataset-dependent learned parameters. Experiments demonstrate that our approach outperforms current state-of-the-art models with fixed architectures and hyper-parameters. Our code and datasets are accessible at https://github.com/taki0112/UGATIT or https://github.com/znxlwm/UGATIT-pytorch.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0771", "problem_id": "07710001", "content": "This study concentrates on developing unsupervised representation learning techniques for recognizing actions based on skeletal data. Traditional methods, which rely on sequential prediction to learn action representations, often struggle to capture comprehensive semantic information. To overcome this limitation, a novel framework called Prototypical Contrast and Reverse Prediction (PCRP) is introduced, enabling the learning of both low-level details, such as body posture in each frame, and high-level patterns, like motion sequences, through reverse sequential prediction. Additionally, PCRP incorporates action prototypes to implicitly encode the semantic similarities among action sequences. By treating action prototypes as latent variables, PCRP is formulated as an expectation-maximization problem, involving an iterative process of determining prototype distributions through clustering action encodings (E-step) and optimizing the encoder using the proposed ProtoMAE loss to refine action encodings and perform reverse prediction (M-step). Experimental results on the N-UCLA, NTU 60, and NTU 120 datasets demonstrate that PCRP surpasses current state-of-the-art unsupervised methods and even outperforms some supervised approaches, with the code available at https://github.com/Mikexu007/PCRP.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0772", "problem_id": "07720001", "content": "Humans possess the ability to interpret natural language descriptions and mentally visualize the corresponding scenes, such as envisioning the layout and furniture arrangement of a house based on a textual description. While the computer vision community has explored the automatic generation of real-world images from text descriptions, this concept has not been applied to document images like floor plans until now. Previous attempts have focused on synthesizing floor plans from sketches or using data-driven models, but this work represents the first endeavor to automatically generate building floor plan images from textual descriptions. In this approach, a natural language description of a house's internal structure and furniture arrangement serves as the input, and the output is a 2D floor plan image. Experiments were conducted using publicly available benchmark floor plan datasets, resulting in the successful rendering of realistic floor plan images from English language descriptions, as shown in Figure A (Reference [1], Figure B [2], and Figure C [3]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0773", "problem_id": "07730001", "content": "By 2050, approximately two-thirds of the global population is expected to reside in cities, according to United Nations estimates, highlighting the urgency for sustainable urban planning and observation. While existing urban footprint datasets offer detailed city boundaries, they often fail to capture critical details about spatial distribution, patterns, and urban attributes. The Local Climate Zone (LCZ) classification system presents a standardized approach to characterize urban structures and features. Although global LCZ mapping efforts exist, they face limitations such as reduced accuracy, inconsistent labeling, and domain adaptation issues. This research addresses these challenges by creating a tailored LCZ dataset for major Korean cities, employing a multi-scale convolutional neural network. The findings reveal that the proposed custom LCZ approach, combined with deep learning, outperforms traditional community-based LCZ mapping using machine learning and transfer learning from the global So2Sat dataset in terms of accuracy.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0774", "problem_id": "07740001", "content": "This work introduces a unified framework for the design and analysis of distributional reinforcement learning (DRL) algorithms, focusing on the recursive estimation of return distribution statistics. By recognizing that DRL algorithms can be broken down into a statistical estimator and a method for generating a return distribution that aligns with the estimated statistics, we gain a new perspective on these algorithms. This insight enables us to re-examine existing DRL algorithms and develop a novel algorithm, EDRL, which estimates the expectiles of the return distribution. To validate our analysis, we compare EDRL to existing methods across various Markov decision processes (MDPs) and also propose a deep reinforcement learning variant, ER-DQN, which is evaluated on the Atari-57 suite of games, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0775", "problem_id": "07750001", "content": "This paper presents the motivation and development of source traces for temporal difference (TD) learning within a tabular framework. Source traces, akin to eligibility traces, focus on potential rather than immediate histories, enabling the propagation of TD errors to likely causal states, thereby enhancing generalization speed. They can be regarded as the model-based, backward perspective of successor representations (SR), offering many similar advantages. This perspective introduces several novel concepts. Firstly, a TD(λ)-like source learning algorithm is introduced, and its convergence is established. Secondly, an innovative algorithm for constructing the source map (or SR matrix) is created, demonstrating superior performance compared to earlier algorithms. Lastly, the paper investigates various methodologies for utilizing the source/SR model, revealing that source traces can be effectively integrated with other model-based strategies such as Dyna and experience replay.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0776", "problem_id": "07760001", "content": "To enhance the generalization of deep learning models, advanced data augmentation techniques, including regional dropout methods, have been extensively investigated. Regional dropout, a common technique, encourages models to attend to less salient areas by randomly eliminating image regions, thereby improving regularization. Nevertheless, this removal of information can be detrimental. Current techniques propose randomly cutting and combining patches and their corresponding labels from training images to leverage the benefits of regional dropout without introducing meaningless pixels in the augmented data. We contend that random patch selection may not always capture enough relevant object information, and mixing labels based on these less informative patches can lead to the learning of unintended feature representations. To address this, we introduce SaliencyMix, which employs a saliency map to carefully choose a representative image patch and integrates it with the target image, promoting the learning of more suitable feature representations. On ImageNet classification, SaliencyMix achieves top-1 error rates of 21.26% and 20.09% for ResNet-50 and ResNet-101 architectures, respectively, and enhances model robustness against adversarial attacks. Furthermore, models trained using SaliencyMix improve object detection performance. The source code is available at https://github.com/SaliencyMix/SaliencyMix.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0777", "problem_id": "07770001", "content": "Traffic sign detection has been extensively studied in computer vision, yet current advanced techniques primarily focus on common sign categories that are adequately covered in training datasets. This work addresses the challenge of identifying and classifying uncommon traffic signs by leveraging artificially generated training data. The synthetic data is created by inserting computer-generated sign images into authentic photographs. Three novel techniques are introduced to ensure visual coherence between the synthetic signs and their surroundings, utilizing contemporary generative adversarial network (GAN) frameworks. These approaches enable realistic integration of rare traffic sign types not present in the original training data. Additionally, a variational autoencoder is employed to generate feasible placements for new signs within images. Experimental results show that combining our synthetic data with real-world samples enhances the performance of both classification and detection systems.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0778", "problem_id": "07780001", "content": "Frequently, there is a need to forecast numerous interdependent variables that are also influenced by observed factors. Structured prediction approaches integrate classification techniques with graphical modeling, leveraging the latter's capacity to represent multivariate data efficiently and the former's strength in utilizing extensive input features for prediction. This tutorial focuses on conditional random fields (CRFs), a widely used probabilistic framework for structured prediction, which has found extensive use in domains such as natural language processing, computer vision, and bioinformatics. It covers inference and parameter estimation techniques for CRFs, along with practical considerations for deploying them at scale. Designed to be accessible even without prior familiarity with graphical modeling, the tutorial aims to benefit practitioners across diverse disciplines.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0779", "problem_id": "07790001", "content": "The process of annotating extensive datasets for training contemporary convolutional neural networks is often impractically costly and labor-intensive for numerous practical applications, prompting the exploration of alternative approaches. One such alternative involves training models on labeled synthetic datasets and then applying them to real-world scenarios, but this method frequently struggles to generalize effectively due to the domain bias that exists between synthetic and real datasets. Although various unsupervised domain adaptation (UDA) methods have been proposed to mitigate this issue, the majority of these methods are limited to simple classification tasks. This paper introduces a novel UDA model designed to tackle the more intricate object detection problem within the context of autonomous driving, leveraging both pixel-level and feature-level transformations to facilitate cross-domain detection. Our model can be trained end-to-end to optimize performance and utilizes objectives from generative adversarial networks and cycle consistency loss for image translation in pixel space, as seen in Figure A. Additionally, to combat potential semantic inconsistencies, we propose a region proposal-based feature adversarial training approach, which preserves target object semantics while minimizing domain shifts, as shown in Figure B, and our method's effectiveness is validated through extensive experiments on multiple datasets, including those referenced in [citation], yielding results that demonstrate its robustness and superiority.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0780", "problem_id": "07800001", "content": "The contemporary understanding of multi-task learning involves utilizing a single model to perform multiple similar tasks, ranging from classifying handwritten characters across various alphabets to mastering multiple Atari games through reinforcement learning, with these tasks typically being of a similar nature. This study aims to further expand the scope of multi-task learning by incorporating heterogeneous tasks into a unified learning framework. To achieve this, we first establish a formal definition of a multi-network model, outlining its essential components and characteristics that enable adaptability to diverse tasks. We then develop a prototype model that integrates three distinct tasks - classification, regression, and data sampling - and evaluate its performance, demonstrating its capabilities. Building on the insights gained from this analysis, we identify key open challenges and potential avenues for future research that can unlock the full potential of our proposed model definition, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0781", "problem_id": "07810001", "content": "Fuzzy time series forecasting techniques have gained significant attention from researchers due to their ability to predict future values without relying on the rigid assumptions inherent in traditional time series forecasting methods. Among these, non-stochastic approaches are particularly favored for their capacity to yield more accurate forecasting results. The performance of a forecasting method is typically influenced by four key factors: the number and length of intervals used to partition the universe of discourse, the fuzzification rules or feature representation of the crisp time series, the method employed to establish fuzzy logic rules between input and target values, and the defuzzification rule used to obtain the crisp forecasted value. Focusing on the first two factors to enhance forecasting accuracy, this study proposes a novel non-stochastic fuzzy time series forecasting method that utilizes interval index number and membership value as input features to predict future values. A simple rounding-off range and suitable step size method are introduced to determine the optimal number of intervals, while a fuzzy c-means clustering process is used to divide the universe of discourse into intervals of varying lengths. The method establishes fuzzy logic rules using a support vector machine (SVM), and its performance is evaluated through a simulated study on five widely used real-time series, with comparisons made to recently developed models, as well as an alternative implementation using a multi-layer perceptron (MLP) instead of SVM, with the results showing improved forecasting accuracy based on two performance measures, RSME and SMAPE, Figure A, B, C, (References, citations remain unchanged).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0782", "problem_id": "07820001", "content": "This paper introduces Rerouted Behavior Improvement (RBI), a Reinforcement Learning (RL) policy improvement algorithm designed to mitigate the impact of Q-function evaluation errors, which frequently arise from learning Q-values with limited historical data. Recognizing that greedy policies and even constrained policy optimization methods can incur an improvement penalty (negative policy improvement) by overlooking these errors, RBI diminishes rapid policy adjustments for infrequently sampled, low-probability actions to minimize this penalty. This strategy effectively prevents catastrophic performance decline and reduces regret when learning from batch data. A two-armed bandit problem with Gaussian rewards illustrates that RBI also enhances data efficiency when the optimal action exhibits high variance. Evaluated within the Atari Learning Environment across two distinct tasks: (1) learning from data generated by multiple behavior policies and (2) iterative RL, RBI demonstrates advantages over greedy policies and other constrained policy optimization algorithms, establishing itself as a safer and more data-efficient learning paradigm. The RBI implementation is available in an anonymous Github repository: https://github.com/eladsar/rbi.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0783", "problem_id": "07830001", "content": "Recent progress in Natural Language Processing has led to machines exceeding human capabilities in various tasks, such as Question Answering. However, most deep learning approaches for Question Answering focus on domains with extensive datasets and well-established research. The Nuclear and Atomic energy field remains relatively untapped regarding the use of unlabeled data for practical applications. To address the data scarcity, a new dataset was constructed using 7000 nuclear domain research papers. This study aims to enhance nuclear domain knowledge, assessed via the Nuclear Question Answering Dataset (NQuAD), developed by nuclear experts as part of this research. NQuAD comprises 612 questions based on 181 paragraphs randomly selected from the IGCAR research paper corpus. This paper introduces Nuclear Bidirectional Encoder Representational Transformers (NukeBERT), incorporating a novel method for constructing a BERT vocabulary suitable for tasks with limited training data. Experiments on NQuAD demonstrated that NukeBERT significantly outperformed BERT, validating the proposed methodology. Due to the high computational cost of training NukeBERT, the pretrained weights and NQuAD will be open-sourced to encourage further research in the nuclear domain.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0784", "problem_id": "07840001", "content": "Biomanufacturing is vital for economic support and public health; however, it encounters significant hurdles such as complexity, high variability, extended lead times, and extremely scarce process data, particularly concerning personalized cell and gene biotherapeutics. In light of these difficulties, we suggest an interpretable semantic bioprocess probabilistic knowledge graph and devise a risk and sensitivity analysis based on game theory for the production process to enable quality-by-design and stability management. By examining the causal links and interactions among critical process parameters and quality attributes (CPPs/CQAs), we develop a Bayesian network-driven probabilistic knowledge graph that depicts the intricate causal interdependencies among all relevant factors. Additionally, we implement a Shapley value-based sensitivity analysis, which effectively measures the contribution of each input factor to the outputs (i.e., productivity, product quality). Given that the bioprocess model coefficients are derived from limited observational data, we derive the Bayesian posterior distribution to assess model uncertainty and enhance the Shapley value-based sensitivity analysis to assess the influence of estimation uncertainty from each set of model coefficients. Consequently, the proposed bioprocess risk and sensitivity analyses can pinpoint bottlenecks, guide reliable process specifications, maximize \"informative\" data collection, and enhance production stability.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0785", "problem_id": "07850001", "content": "The increasing prevalence of Deep Neural Networks (DNNs) in automation systems can be attributed to recent breakthroughs in standardizing key components, including architecture, optimization methods, and regularization techniques. This study aims to contribute to a deeper understanding of Spectral Normalization (SN) and its potential to standardize regularization across a broader spectrum of Deep Learning models, adopting an empirical approach. Through a series of experiments, we investigate the training dynamics of SN in comparison to the widely-used Batch Normalization (BN), revealing that SN enhances gradient sparsity and regulates gradient variance. However, our findings also indicate that SN is susceptible to a phenomenon known as the mean-drift effect, which hinders its performance. To address this issue, we propose a novel weight reparameterization technique, termed Mean Spectral Normalization (MSN), which effectively mitigates the mean drift and substantially improves network performance. Notably, our model demonstrates a ~16% increase in speed compared to BN in practical applications, while requiring fewer trainable parameters. Furthermore, we evaluate the efficacy of MSN across a range of CNN architectures, including 3-layer CNN, VGG7, and DenseNet-BC, as well as unsupervised image generation tasks using Generative Adversarial Networks (GANs), highlighting its versatility in embedded automation tasks.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0786", "problem_id": "07860001", "content": "Hyperspectral Image (HSI) data can be effectively represented using graphs, which also facilitate semi-supervised classification by propagating labels among neighboring nodes. This paper introduces a new framework for HSI classification with minimal labeled data, drawing on multi-view graph learning and graph signal processing techniques. Starting with a superpixel-segmented HSI, we develop a robust and efficient graph construction and label propagation approach for semi-supervised learning (SSL). Given the critical role of the graph in classification performance, especially given the inherent complexity of HSI data, we focus on optimizing the graph representation. Our work makes two key contributions: first, we propose a multi-stage, edge-efficient semi-supervised graph learning framework for HSI that incorporates label information via pseudo-label features during graph construction. Second, we refine the integration of multiple superpixel features in the graph using pseudo-labels, extending the initial framework while reducing dependence on extensive parameter tuning. Through comprehensive experiments, we show that our methods outperform existing state-of-the-art techniques.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0787", "problem_id": "07870001", "content": "This paper details Sighthound's fully automated system for recognizing vehicle make, model, and color. The system's core is a computationally efficient deep convolutional neural network that achieves cutting-edge performance across multiple benchmarks. The deep network is trained using a large, multi-million image dataset labeled via a semi-automated approach. The system's performance is evaluated against public datasets and an internal test dataset, demonstrating significant outperformance compared to existing methods. The model is accessible to developers via the Sighthound Cloud API at https://www.sighthound.com/products/cloud.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0788", "problem_id": "07880001", "content": "The detection of adversarial attacks, especially in their early phases, presents a considerable hurdle. This paper introduces an attack-agnostic detection mechanism for interactive recommendation systems powered by reinforcement learning. Initially, adversarial examples are generated to demonstrate their varied distributions. Subsequently, recommendation systems are enhanced through the integration of a deep learning-based classifier, trained on the generated data, to identify potential attacks. An analysis of the intensity and prevalence of these adversarial examples is conducted, and the model's efficacy is assessed using standard datasets and diverse crafting techniques. Empirical evaluations reveal the effectiveness of most adversarial attacks, with both their strength and frequency influencing overall attack performance. Notably, strategically timed attacks achieve comparable performance levels with a significantly reduced attack frequency (ranging from 1/3 to 1/2). Furthermore, the study demonstrates that a black-box detector, trained using a specific crafting method, exhibits generalization capabilities across multiple crafting methods.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0789", "problem_id": "07890001", "content": "Current video summarization techniques primarily emphasize the sequential or structural attributes of video data. Nevertheless, they often lack focus on the summarization task's inherent nature. In this paper, we introduce a meta-learning approach called MetaL-TDVS, designed to perform task-driven video summarization by directly investigating the underlying summarization mechanism across different videos. Specifically, MetaL-TDVS seeks to uncover the latent mechanism in video summarization by framing it as a meta-learning challenge, thereby enhancing the generalization capabilities of the trained model. MetaL-TDVS treats the summarization of each video as an individual task, leveraging the knowledge acquired from summarizing other videos to improve the summarization of new videos. Moreover, MetaL-TDVS employs a two-fold back propagation method during model updates, compelling the model optimized on one video to achieve high accuracy on another video during each training iteration. Comprehensive experiments conducted on standard datasets reveal that MetaL-TDVS outperforms several state-of-the-art methods in terms of both performance and generalization ability.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0790", "problem_id": "07900001", "content": "Gym-ANM is a Python package intended to streamline the creation of reinforcement learning (RL) environments for simulating active network management (ANM) problems within electricity networks. This paper details the procedures for developing novel environments and for programming interactions with existing environments. Furthermore, it presents ANM6-Easy, an environment developed to exemplify typical ANM challenges. The document concludes by considering Gym-ANM's prospective influence on the research and academic sectors. The aim is that this package will foster collaboration between power system engineers and RL researchers in the pursuit of control algorithms for upcoming energy systems.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0791", "problem_id": "07910001", "content": "This study investigates the potential of deep learning algorithms in predicting occult invasive disease from breast MR images following a core needle biopsy diagnosis of ductal carcinoma in situ (DCIS). The research, approved by the institutional review board, examined dynamic contrast-enhanced fat-saturated T1-weighted MRI sequences from 131 patients with biopsy-confirmed DCIS, excluding those with preoperative therapy or prior breast cancer history. Two deep learning methods were evaluated for detecting occult invasive components later identified during surgical excision. The first approach employed transfer learning, fine-tuning the GoogleNet model (pre-trained on ImageNet) with DCIS images. The second method extracted deep features using a pre-trained network and applied a support vector machine (SVM) for prediction. Performance was assessed via 10-fold cross-validation and ROC curve analysis (AUC). The deep features approach with GoogleNet and polynomial kernel SVM achieved superior classification (AUC = 0.70, 95% CI: 0.58–0.79), while transfer learning yielded an AUC of 0.53 (95% CI: 0.41–0.62). The findings suggest convolutional neural networks may help identify occult invasive disease in DCIS patients initially diagnosed via core needle biopsy.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0792", "problem_id": "07920001", "content": "The task of decomposing multi-object scenes in an unsupervised manner is a rapidly evolving area of research within the field of representation learning, yet existing models struggle to effectively utilize dynamic cues inherent in video data, despite notable advancements in static scene analysis. To address this limitation, we introduce a novel iterative inference framework that operates across both spatial and temporal domains, enabling the joint modeling of complex object representations and explicit temporal relationships between latent variables spanning multiple frames. By integrating 2D-LSTM, temporally conditioned inference and generation, and iterative amortized inference for posterior refinement, our approach yields improved decomposition quality, captures object dynamics, and facilitates individual object trajectory prediction. Notably, our model maintains high accuracy even in the absence of color information, and its capabilities in decomposition, segmentation, and prediction are demonstrated to surpass state-of-the-art performance on multiple benchmark datasets, including a newly curated dataset that will be made publicly available.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0793", "problem_id": "07930001", "content": "We present the Region Adaptive Graph Fourier Transform (RA-GFT) for the compression of attributes in 3D point clouds. The RA-GFT is a multiresolution transform that combines block transforms localized in space. We assume that the points are structured by a set of nested partitions depicted as a rooted tree. At each level of resolution, attributes are handled in clusters through block transforms. Each block transform yields one approximation (DC) coefficient and multiple detail (AC) coefficients. The DC coefficients are moved up the tree to the next lower resolution level, allowing the procedure to recur until reaching the root. Due to the variation in point numbers across clusters, each block transform needs to reflect the relative significance of each coefficient. To address this, we introduce the \\mathbf-normalized graph Laplacian and suggest employing its eigenvectors as the block transform. The RA-GFT offers superior complexity-performance trade-offs compared to earlier methods, specifically outperforming the Region Adaptive Haar Transform (RAHT) by as much as 2.5 dB with minimal complexity overhead.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0794", "problem_id": "07940001", "content": "Remote sensing and automated earth monitoring play a crucial role in addressing global challenges, including disaster prevention, land management, and climate change mitigation. Despite the abundance of remote sensing data, a significant portion remains unlabeled, making it unsuitable for supervised learning techniques. Transfer learning can alleviate the data requirements for deep learning models; however, many existing methods are pretrained on ImageNet, and their applicability to remote sensing imagery is uncertain due to domain disparities. In this study, we introduce Seasonal Contrast (SeCo), a robust pipeline designed to utilize unlabeled data for in-domain pre-training of remote sensing representations. The SeCo pipeline comprises two components: an organized method for collecting extensive, unlabeled, and uncurated remote sensing datasets that feature images from various locations on Earth over different time periods, and a self-supervised algorithm that leverages temporal and spatial invariance to develop transferable representations for remote sensing tasks. Our empirical results demonstrate that models trained with SeCo outperform those pretrained on ImageNet and other leading self-supervised learning techniques across several downstream tasks. The datasets and models resulting from SeCo will be publicly available to promote transfer learning and accelerate advancements in remote sensing applications.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0795", "problem_id": "07950001", "content": "The concept of transfer learning enables the utilization of knowledge acquired from prior tasks, known as the source task, and applies it to novel tasks or domains, referred to as the target task, provided they exhibit some degree of commonality. Two crucial factors that influence the efficacy of transfer learning models are the magnitude of the target dataset and the degree of distributional similarity between the source and target domains. Despite their importance, the significance of these factors has not been thoroughly examined. This study undertakes an experimental investigation into the effects of target dataset size and source-target domain similarity on model performance. The results indicate that increased data availability consistently yields improved performance, with a linear correlation between model performance and the logarithm of data size, until data exhaustion occurs. Furthermore, as the disparity between source and target domains increases, larger datasets are required, and fine-tuning surpasses feature extraction in terms of performance. Conversely, when source and target domains are similar and dataset sizes are limited, fine-tuning and feature extraction yield comparable performance. By initiating a quantitative analysis of the impact of data volume and domain similarity on transfer learning, this research aims to stimulate further exploration into the role of data in developing more accurate statistical models, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0796", "problem_id": "07960001", "content": "The exploration-exploitation dilemma has been a significant challenge in reinforcement learning for an extended period. In this paper, we introduce a novel strategy to automatically achieve a balance between these two aspects. Our approach is based on the Soft Actor-Critic (SAC) algorithm, which incorporates an \"entropy temperature\" to harmonize the primary task reward with the policy entropy, thereby managing the balance between exploitation and exploration. It has been empirically demonstrated that SAC is highly dependent on this hyperparameter, and the subsequent work (SAC-v2), which employs constrained optimization for automatic tuning, presents certain drawbacks. The essence of our method, referred to as Meta-SAC, is the application of metagradient techniques combined with an innovative meta objective to automatically adjust the entropy temperature within SAC. We demonstrate that Meta-SAC delivers promising results across several Mujoco benchmarking tasks and surpasses SAC-v2 by over 10% in one of the most complex challenges, humanoid-v2.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0797", "problem_id": "07970001", "content": "Research in human-computer interaction has emphasized object detection, with skin area detection being crucial for various applications, including face recognition, human motion detection, and image classification, such as distinguishing between pornographic and nude images, etc. While numerous studies on skin detection have utilized images of individuals from African, Mongolian, and Anglo-Saxon ethnic backgrounds for training and testing, the skin tones of Indian sub-continentals have received relatively little attention as a distinct focus. This study aims to address this gap by conducting a comparative analysis of three image segmentation approaches using images of individuals from the Indian sub-continent, with the goal of optimizing detection criteria and identifying effective parameters for skin area detection. The results of the experiments indicate that an approach based on the HSV color model yields the most favorable outcomes for detecting skin areas in Indian sub-continental images, achieving a true positive rate of 91.1% and a true negative rate of 88.1%.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0798", "problem_id": "07980001", "content": "Developing cyber-physical systems (CPSs) is complex due to the extensive search space encompassing diverse configurations and component values. Efficient exploration of this design space is essential to identify architectures and component specifications that fulfill system objectives. To tackle this challenge, we frame CPS design as a multi-objective optimization problem and introduce DISPATCH, a two-phase approach for effective design space exploration. Initially, a genetic algorithm explores discrete component values for architecture selection or component choice, producing a preliminary design even before achieving system targets. Subsequently, inverse design refines component values in a continuous space to satisfy varied system requirements. A neural network serves as a surrogate model for inverse design, transformed into a mixed-integer linear program to enable active learning for efficient sampling in continuous space. DISPATCH is validated on electrical circuit benchmarks, including two-stage and three-stage transimpedance amplifiers. Simulations demonstrate a 5-14x improvement in sample efficiency over reinforcement learning-based synthesis methods while producing circuits with superior performance (maximum bandwidth/minimal area) compared to those generated by reinforcement learning, Bayesian optimization, or manual design.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0799", "problem_id": "07990001", "content": "In recent years, monocular depth inference has emerged as a viable alternative to costly time-of-flight sensors, yet it still faces challenges related to scale acquisition and implementation complexity. This study proposes an unsupervised learning approach that can accurately predict depth maps, egomotion, and camera intrinsics from a sequence of monocular images using a single network. By integrating spatial and temporal geometric constraints, our method effectively resolves scale factors for depth and pose, which are enforced through reconstruction loss functions during training. Notably, our single-network architecture can be trained using only unlabeled stereo sequences, thereby reducing implementation overhead compared to existing methods. The results show that our approach achieves strong performance on the KITTI driving dataset, surpassing current state-of-the-art methods, and offers faster training times due to its reduced network complexity, as seen in Figure A, B, C (citations remain unchanged).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0800", "problem_id": "08000001", "content": "In stochastic dynamic environments with uncertain probabilistic parameters, robustness is crucial for sequential decision-making. This work explores the application of robust Markov Decision Processes (RMDPs) within reinforcement learning to derive policies that ensure provable worst-case performance. Recognizing that the ambiguity set significantly influences the quality and robustness of an RMDP solution and that current ambiguity sets often produce overly conservative solutions, we introduce RSVF. RSVF reduces conservatism while maintaining equivalent worst-case guarantees by 1) incorporating a Bayesian prior, 2) optimizing both the size and positioning of the ambiguity set, and, crucially, 3) loosening the constraint that the set must represent a confidence interval. Theoretical analysis confirms the safety of RSVF, and empirical findings highlight its potential for practical application.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0801", "problem_id": "08010001", "content": "Human action recognition, a crucial application of computer vision, has been extensively researched over the years, with skeleton-based approaches gaining significant attention lately due to their robustness and superior performance. Nevertheless, current skeleton-based methods overlook the potential interactions between individuals, despite the fact that one person's action is often influenced by another, particularly in complex scenarios. To address this limitation, this paper presents a novel approach to human action recognition in complex events, leveraging group-skeleton-based methods. The proposed method employs multi-scale spatial-temporal graph convolutional networks (MS-G3Ds) to extract features from the skeletons of multiple individuals, utilizing not only traditional key point coordinates but also key point speed values to enhance performance. The distance values between the reference person and other individuals are then embedded into the extracted features using multilayer perceptrons (MLPs), and all features are fused and classified using another MS-G3D. To mitigate class imbalance issues, the network is trained with a focal loss, as part of the solution to the Large-scale Human-centric Video Analysis in Complex Events Challenge. Experimental results on the HiEve dataset demonstrate the superiority of the proposed method over other state-of-the-art approaches, as shown in Figure A, B, C (References [1], [2], [3]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0802", "problem_id": "08020001", "content": "The 2D Least Median of Squares (LMS) method is widely used in robust regression due to its strong outlier resistance; it can tolerate up to 50% outlier contamination in the dataset without compromising the estimator's accuracy. Given that the computational complexity of 2D LMS estimation is proven to be \\Omega(n^2), where n represents the number of data points, and considering the parallel processing capabilities of graphics processing units (GPUs), we introduce a rapid, parallel, GPU-accelerated algorithm for LMS calculation. We present a CUDA-based algorithm for LMS computation, demonstrating its superior speed compared to the most efficient single-threaded CPU algorithm currently available. Initially, we detail the proposed methodology and evaluate its performance. Subsequently, we illustrate its application in enhancing the Hough Transform (HT) for efficient line detection in images with significant noise. Comparative analysis against standard HT-based line detection techniques reveals that our method effectively addresses their limitations, offering improvements in both efficiency and accuracy.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0803", "problem_id": "08030001", "content": "The proliferation of data has introduced significant challenges in recommender systems, necessitating the development of scalable and high-performance recommendation methods. Singular Value Decomposition (SVD)-based algorithms have been effectively utilized to enhance recommendation quality. This paper expands upon the SVD technique to improve scalability and performance through 1) multi-threading, 2) the use of Graphical Processing Units, and 3) distributed computation. We introduce block based matrix factorization (BMF) integrated with SVD, which allows us to combine the benefits of SVD over basic matrix factorization (MF) with the parallelism and scalability afforded by BMF. The Compute Unified Device Architecture (CUDA) platform and associated hardware were employed to utilize GPU capabilities in conjunction with block based SVD, showcasing improvements in both performance and memory utilization.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0804", "problem_id": "08040001", "content": "We introduce a motion prediction framework that utilizes a unique structured map representation and interactions between actors and maps. Rather than converting vectorized maps into raster images, we generate a lane graph directly from raw map data to maintain structural integrity. To address the intricate topology and extended dependencies of the lane graph, we develop LaneGCN, an enhanced graph convolution method incorporating multiple adjacency matrices and along-lane dilation. For modeling actor-map interactions, we employ a fusion network with four distinct interaction types: actor-to-lane, lane-to-lane, lane-to-actor, and actor-to-actor. Combining LaneGCN with these interactions enables our model to generate precise and plausible multi-modal trajectory predictions. Our method achieves superior performance compared to existing approaches on the large-scale Argoverse motion forecasting benchmark.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0805", "problem_id": "08050001", "content": "Echocardiography serves as a vital prenatal diagnostic tool for the early identification of fetal congenital heart diseases (CHDs). Among echocardiographic images, the four-chamber (FC) view is particularly significant and readily obtainable. The automatic evaluation of FC views greatly enhances the early detection of CHDs. The initial phase of this automated process involves pinpointing the fetal four essential heart chambers within an ultrasound (US) image. However, this task is fraught with challenges due to various factors, including the presence of numerous speckles in US images, the small size and variable positions of the fetal cardiac chambers, and the indistinct categories caused by the similarities among them. These issues impede the extraction of robust and distinctive features, thereby compromising the accurate localization of fetal cardiac anatomical chambers. To address this, we first introduce a multistage residual hybrid attention module (MRHAM) to enhance feature learning. Subsequently, we unveil an enhanced YOLOv4 detection model, termed MRHAM-YOLOv4-Slim. Specifically, the standard residual identity mapping in the MRHAM-YOLOv4-Slim backbone is substituted with the MRHAM, allowing for the precise localization of the four key chambers in fetal FC views. Comprehensive experiments validate that our proposed approach surpasses existing state-of-the-art methods, achieving a precision of 0.919, recall of 0.971, F1 score of 0.944, mAP of 0.953, and frames per second (FPS) of 43.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0806", "problem_id": "08060001", "content": "Time series forecasting is a pervasive problem that has significant implications for numerous real-world applications, underscoring the need for accurate and reliable predictive models. This paper presents a novel forecasting framework that integrates deep autoregressive models with a Spectral Attention (SA) module, which effectively fuses global and local frequency domain information within the model's embedded space by representing the time series embedding as a random process in the spectral domain, thereby enabling the identification of global trends and seasonal patterns. The proposed architecture leverages two spectral attention models, one global and one local to the time series, to incorporate this information into the forecasting process and apply spectral filtering to mitigate noise in the time series. The resulting Spectral Attention Autoregressive Model (SAAM) boasts several desirable attributes, including seamless integration into established forecast architectures, a relatively low parameter count, and the generation of interpretable results that enhance forecasting accuracy. Empirical evaluations of SAAM on several well-known forecast datasets, as seen in Figure A, B, C, demonstrate its competitive performance relative to state-of-the-art approaches, as discussed in References [citation].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0807", "problem_id": "08070001", "content": "The task of universal style transfer involves transforming a content image into the visual style of any reference image, encompassing both artistic and photorealistic transformations. Typically, existing approaches involve training an auto-encoder (AE) to reconstruct an image using deep features, and then incorporating pre-defined style transfer modules into the AE reconstruction process to modify the deep features and transfer the style. However, these methods often require multiple iterations of time-consuming AE reconstruction to achieve satisfactory stylization results. In contrast, this work aims to develop innovative neural network architectures built upon AE to enable rapid style transfer with reduced artifacts and distortions in a single pass of end-to-end inference. To achieve this, two novel network architectures, ArtNet and PhotoNet, are proposed to enhance artistic and photorealistic stylization, respectively. As demonstrated by extensive experiments, ArtNet produces images with fewer artifacts and distortions compared to state-of-the-art artistic transfer algorithms, while PhotoNet improves photorealistic stylization results by generating sharp images that faithfully preserve the rich details of the input content, Figure A, B, C. Furthermore, ArtNet and PhotoNet achieve significant speed-ups of 3X to 100X over state-of-the-art algorithms, making them particularly advantageous for large content images, as shown in References [citation].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0808", "problem_id": "08080001", "content": "The application of large-scale optimal transport in machine learning has been significantly expanded due to recent breakthroughs, yet current approaches often fall short by either not explicitly deriving the transport map or being incompatible with general cost functions. This paper introduces an end-to-end methodology for tackling large-scale optimal transport, which not only directly computes the transport map but also accommodates general cost functions. By utilizing stochastic neural networks to model the transport map and leveraging adversarial training to enforce constraints on marginal distributions, our proposed framework offers flexibility. Furthermore, it can be adapted to learn Monge maps or optimal bijections by incorporating cycle-consistency constraints. The efficacy and superiority of our approach are validated through its outstanding performance in various large-scale, real-world applications, such as domain adaptation, image-to-image translation, and color transfer, as demonstrated against existing methods.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0809", "problem_id": "08090001", "content": "This study introduces a novel geometric and probabilistic framework for synchronizing correspondences among multiple object or image sets. We propose two key algorithms: (1) Birkhoff-Riemannian L-BFGS, which systematically optimizes the relaxed cycle consistency loss that is combinatorially challenging, and (2) Birkhoff-Riemannian Langevin Monte Carlo for sampling on the Birkhoff Polytope and assessing solution confidence. Our approach begins by exploring the newly established Riemannian geometry of the Birkhoff Polytope, followed by the development of a probabilistic synchronization model using a Markov Random Field (MRF). Leveraging first-order retraction operators, we then reformulate the problem as a stochastic differential equation and introduce innovative integrators. Experimental results on both synthetic and real-world datasets demonstrate that our method delivers superior multi-graph matching performance with accelerated convergence and robust uncertainty quantification.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0810", "problem_id": "08100001", "content": "The increasing digitization of hospital records enables the use of textual medical data, such as electronic health records (EHRs) initially collected for non-epidemiological purposes. Manual data search and analysis are labor-intensive; therefore, natural language processing (NLP) tools have been utilized to automate information extraction from EHRs, enabling data structuring and statistical analysis. Current methods often require synonym or ontology dictionaries, which are primarily available in English and lack local or custom notations. This study presents a custom NLP system, developed by oncologists and data scientists, for processing clinical reports of breast cancer patients. The system combines standard text mining techniques with an advanced synonym detection method, facilitating comprehensive analysis through the retrieval of indicators like medical history, tumor characteristics, therapeutic responses, recurrences, and prognosis. The method's adaptability allows for the easy acquisition of new indicators, facilitating retrospective studies with significantly reduced manual effort. This language-agnostic method, which does not require biomedical annotators or pre-defined ontologies, achieves good extraction accuracy for various concepts of interest, as validated against a manually structured file, and without the need for existing corpora with local or new notations.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0811", "problem_id": "08110001", "content": "This study investigates the optimization of non-convex composite functions, comprising multiple inner and outer finite-sum functions, which is a problem pertinent to various significant applications, including nonlinear embedding and reinforcement learning, as seen in Figure A. The existing methods, such as stochastic gradient descent (SGD) and stochastic variance reduced gradient (SVRG) descent, can be utilized to address this issue, but they often result in high query complexity, particularly when dealing with a substantial number of inner component functions, as discussed in [1]. By leveraging variance-reduced techniques, we develop two novel algorithms that substantially enhance query complexity in scenarios with numerous inner component functions. Notably, our work is the first to provide a comprehensive query complexity analysis for non-convex stochastic composition, as shown in Figure B, and our experiments corroborate the efficacy of the proposed algorithms and theoretical framework, consistent with the findings in [2] and Figure C.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0812", "problem_id": "08120001", "content": "Numerous few-shot learning methods have been developed within the meta-learning framework, which enables learning from diverse tasks and generalizing to new ones. While these meta-learning approaches yield satisfactory results when all samples are drawn from identical distributions (i.i.d. observations), they often struggle in real-world applications where few-shot learning is susceptible to data shift, meaning samples within and across tasks may originate from different data distributions. Since most existing few-shot learning methods do not account for data shift, their performance deteriorates when the data distribution changes. Addressing data shift in few-shot learning is challenging due to the limited number of labeled samples per task. To tackle this issue, we introduce a novel metric-based meta-learning framework that leverages a knowledge graph to extract both task-specific and task-shared representations, allowing it to mitigate data shift within and between tasks by combining these representations. Our proposed model is assessed on established benchmarks and two newly created challenging datasets, and the evaluation results show its outstanding performance, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0813", "problem_id": "08130001", "content": "The super-resolution problem is inherently ill-posed, as a single low-resolution image can be derived from numerous high-resolution images, resulting in a one-to-many stochastic mapping. To address this challenge, we propose a Variational Sparse framework for Super-Resolution (VSpSR) that leverages neural networks to capture the non-local self-similarity of natural images. Our approach, based on the principle that small patches of high-resolution images can be sparsely represented using an over-complete dictionary, introduces a two-branch module called VSpM. This module consists of one branch that extracts patch-level basis from low-resolution inputs and another branch that infers pixel-wise variational distributions related to sparse coefficients. By iteratively sampling these coefficients, we can generate multiple sparse representations, ultimately producing diverse high-resolution images. Preliminary results from the NTIRE 2021 challenge on learning SR space indicate that our team, FudanZmic21, achieved a 7th place ranking according to the released scores, and the VSpSR implementation is available at https://zmiclab.github.io/.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0814", "problem_id": "08140001", "content": "Land cover maps are a crucial component in various environmental research and management applications, and while machine learning techniques can automatically generate these maps, they often require extensive training data to ensure high accuracy, which may not always be readily available. To address this limitation, researchers employ domain adaptation (DA) techniques, where a model trained on data from a different region, or source domain, is adapted for use in the target domain. This paper focuses on semi-supervised DA, where a limited number of labelled samples are available in the target domain. We introduce Sourcerer, a novel Bayesian-inspired, deep learning-based semi-supervised DA approach for land cover mapping using SITS data, which leverages a convolutional neural network initially trained on a source domain and further refined on the target domain with a custom regularizer that dynamically adjusts the model's adaptation to the target data based on the availability of labelled samples. Our evaluation of Sourcerer on Sentinel-2 time series images, comparing it to two state-of-the-art semi-supervised DA techniques and four baseline models, demonstrates its superior performance across different source-target domain pairings and labelled target data quantities, with notable results showing that Sourcerer's initial accuracy, 74.2%, surpasses the next-best state-of-the-art method even when trained on 20,000 labelled target instances.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0815", "problem_id": "08150001", "content": "Child trafficking is a global crisis, affecting over four million children annually, many of whom are subjected to sexual exploitation. To address this issue, we partnered with UK Police and Global Emancipation Network to create a machine learning pipeline prototype designed to help identify children in intercepted images. This study concentrates on images of children in school uniforms to determine their school of origin, a task traditionally performed manually by law enforcement, requiring substantial time and resources. By automating parts of the school identification process, we aim to expedite child identification efforts. Our pipeline incorporates two machine learning models: (i) to detect the presence of a school uniform in a child's image, and (ii) to identify characteristics of various school uniform components (e.g., the color and texture of shirts, sweaters, and blazers). We detail the procedures for data collection, labeling, model development, and validation, along with strategies for effectively searching schools based on model predictions.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0816", "problem_id": "08160001", "content": "This paper presents our approach to addressing the Traffic4cast 2020 challenge, which, similar to its 2019 counterpart, required participants to devise algorithms capable of forecasting future traffic conditions in major cities. Our strategy involved a two-pronged approach, focusing on the critical aspects of feature and loss function design, which yielded substantial enhancements to the top-performing U-Net solution from the previous year. Additionally, we investigated the application of Graph Neural Networks (GNNs) and developed a novel ensemble GNN architecture that surpassed the GNN solution from the preceding year, although it still fell short of matching the performance of U-Nets, with potential reasons for this disparity being examined. Ultimately, our final submission, which combined our U-Net and GNN models, secured the 4th place position in the Traffic4cast 2020 competition.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0817", "problem_id": "08170001", "content": "Representation learning, also known as embedding learning, has proven effective for modeling large-scale semantic knowledge graphs. This approach involves transforming the knowledge graph into a tensor structure, where entries are estimated using latent representations of generalized entities. Traditionally, knowledge graphs are viewed as static, expanding only by adding new links as facts emerge, while the underlying truth values of these links remain constant. This study focuses on knowledge graphs where the validity of triples is time-dependent. We propose that all modifications to the knowledge graph occur through events, which serve as the primary mechanism for updates. We develop an event prediction model that incorporates both background knowledge from the graph and recent event data. By forecasting future events, our model anticipates potential changes in the knowledge graph, effectively capturing its temporal evolution. Experimental results show strong performance in clinical settings, recommendation systems, and sensor network applications.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0818", "problem_id": "08180001", "content": "This paper explores Salient Object Subitizing, which involves predicting both the presence and quantity of salient objects in an image based on overall contextual information. This task draws inspiration from the human capacity to rapidly and precisely determine the number of items within the subitizing range (1-4). To facilitate research in this area, we introduce a new dataset comprising approximately 14,000 everyday images, annotated for salient objects via online crowdsourcing. Our results indicate that a fully trained Convolutional Neural Network (CNN) achieves prediction accuracy comparable to human performance in distinguishing images containing zero or one salient object. Furthermore, the model demonstrates performance significantly above chance for images with multiple salient objects, without necessitating object localization. We also introduce a technique to enhance CNN subitizing model training through the use of synthetic images. Experimental results validate the accuracy and generalizability of our CNN subitizing model, as well as its utility in salient object detection and image retrieval.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0819", "problem_id": "08190001", "content": "Deep learning denoisers have recently surpassed the performance of traditional methods like BM3D. These denoisers are commonly trained by minimizing the mean squared error (MSE) between the deep neural network (DNN) output and a clean ground truth image, emphasizing the need for high-quality ground truth data. However, acquiring such data is often difficult. We introduce a technique employing Stein's unbiased risk estimator (SURE) to train DNN denoisers using only noisy images corrupted by Gaussian noise. Results show that our SURE-based approach achieves performance comparable to networks trained with ground truth for both grayscale and color images. Furthermore, we present a SURE-based refining method utilizing a noisy test image to enhance performance, outperforming BM3D, deep image prior, and often networks trained with ground truth. We also explored the potential of extending our SURE-based methods to the Poisson noise model.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0820", "problem_id": "08200001", "content": "This study presents NeXtVLAD, an innovative and efficient network architecture designed to consolidate frame-level features into a streamlined feature vector for large-scale video classification. Essentially, it involves breaking down a high-dimensional feature into several lower-dimensional vectors using attention mechanisms, followed by the application of NetVLAD aggregation across time. The NeXtVLAD method proves to be both effective and parameter-efficient in capturing temporal data. In the second Youtube-8M video understanding challenge, a single NeXtVLAD model comprising fewer than 80 million parameters achieved a GAP score of 0.87846 on the private leaderboard. A combination of three NeXtVLAD models reached a score of 0.88722, placing third among 394 competing teams. The code is accessible at https://github.com/linrongc/youtube-8m.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0821", "problem_id": "08210001", "content": "This study introduces a hierarchical compositional AND-OR model for interpretable image synthesis by optimizing the generator network's sparsity. The methodology employs a hierarchy of scene-objects-parts-subparts-primitives for image representation, where a scene encompasses various types (i.e., OR), and each type is made up of multiple objects (i.e., AND). This recursive structure continues through the hierarchy until reaching the primitive level (for instance, wavelet-like bases). To implement this AND-OR hierarchy in image synthesis, we develop a generator network comprising two key components: (i) Each hierarchical layer is represented by an over-complete collection of convolutional basis functions, utilizing existing convolutional neural architectures to build the hierarchy, and (ii) Sparsity-inducing constraints are applied during end-to-end training, resulting in a sparsely activated and sparsely connected AND-OR model derived from the initial densely connected generator network. A simple sparsity-inducing constraint is employed, allowing only the top-k basis functions to activate at each layer (with k as a hyper-parameter). The learned basis functions also facilitate image reconstruction to elucidate the input images. The proposed approach is evaluated on four benchmark datasets, with results indicating that it successfully learns meaningful and interpretable hierarchical representations, yielding superior quality in image synthesis and reconstruction compared to baseline methods.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0822", "problem_id": "08220001", "content": "Electronic health records (EHR) exhibit sparsity and irregularity due to inconsistent measurement intervals and varying clinical variables at each observation. This study introduces a self-attention-based approach for multi-view feature integration from irregular multivariate time series without imputation. We present a novel Multi-Integration Attention Module (MIAM) to capture intricate patterns in irregular temporal data, explicitly modeling relationships among observed values, missing indicators, and time intervals between observations. Our method leverages implicit human knowledge embedded in the data regarding measurement selection and timing. Furthermore, we incorporate an attention-based decoder during training to impute missing values, enhancing representation learning of multi-view observation relationships for predictive tasks. Experimental validation on MIMIC-III and PhysioNet 2012 datasets demonstrates superior performance over state-of-the-art methods for in-hospital mortality prediction.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0823", "problem_id": "08230001", "content": "This study introduces a framework for interactive video object segmentation (VOS) in real-world scenarios, enabling users to iteratively annotate selected frames. A segmentation algorithm then refines the masks based on these annotations. Unlike prior interactive VOS approaches that rely on ground truth to identify frames with poor evaluation metrics—an impractical requirement during testing—this work argues that such frames may not necessarily yield the greatest performance gains. Instead, the frame selection problem is modeled as a Markov Decision Process, where a deep reinforcement learning agent is trained to recommend optimal frames. This learned agent autonomously identifies the most valuable frames, enhancing the practicality of interactive VOS. Evaluations on public datasets demonstrate the agent's effectiveness without modifying existing VOS algorithms. Our data, code, and models are available at https://github.com/svip-lab/IVOS-W.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0824", "problem_id": "08240001", "content": "The safety of individuals in various public spaces, such as subway stations, universities, airports, shopping malls, and city squares, has become a pressing concern, highlighting the need for intelligent event detection systems. Developing methods to detect abnormal behavior and enable prompt action to prevent undesirable activities is crucial. However, high crowd density in these areas poses significant challenges, including occlusion, which renders individual tracking and analysis impossible, as illustrated in Fig. 1. Moreover, accurately representing individual behavior within crowds is a complex issue. To address these challenges, a novel approach is proposed, involving the division of frames into smaller patches to extract motion patterns and demonstrate motion in each patch. This method utilizes KLT corners as consolidated features to describe moving regions, which are then tracked using the optical flow method. By modeling the distribution of motion information in each patch as a Gaussian distribution, parameters of the Gaussian model are formulated as a motion pattern descriptor, effectively embedding motion patterns.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0825", "problem_id": "08250001", "content": "This study introduces a novel approach for estimating the direction and color of a scene's light source from a single image, leveraging two key concepts: firstly, a newly created synthetic dataset that exhibits pronounced shadow effects, analogous to the SID dataset, and secondly, a deep learning architecture trained on this dataset to predict the light source's direction and color. The proposed method demonstrates impressive results on synthetic images and is further validated through a preliminary procedure that enables the estimation of light positions in the Multi-Illumination dataset, thereby confirming the model's effectiveness in real-world scenarios as well.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0826", "problem_id": "08260001", "content": "Can machine learning enhance decision-making for environmental conservation in a rapidly changing world? This paper explores the application of _reinforcement learning_ (RL), a specialized branch of machine learning, to address complex conservation challenges. RL is particularly suited for environmental and global change issues due to three key advantages: (1) it emphasizes designing agents that actively engage with dynamic and uncertain environments, (2) it operates effectively without requiring extensive datasets, and (3) it integrates existing models, simulations, and accumulated knowledge rather than replacing them. The study offers both conceptual and technical insights into RL’s applicability to ecological and conservation problems, illustrated through case studies such as fisheries quota allocation and ecological tipping point management. Additionally, four appendices with annotated code serve as a practical resource for researchers interested in implementing or advancing these methods.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0827", "problem_id": "08270001", "content": "We introduce two novel evaluation metrics tailored to the class-conditional image generation setting, which are derived by extending the widely-used Inception Score (IS) and Fre'chet Inception Distance (FID) metrics. A theoretical examination reveals the underlying rationale for each proposed metric, establishing a connection to their unconditional counterparts, with the relationship taking the form of a product for IS and an upper bound for FID. Through an extensive empirical analysis, we compare these new metrics to their unconditional counterparts and other existing metrics, and apply them to assess the performance of current generative models, yielding valuable insights into their strengths and weaknesses, including issues such as unlearned classes and mode collapse.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0828", "problem_id": "08280001", "content": "ObSuRF is introduced, a technique that transforms a single image of a scene into a 3D model composed of multiple Neural Radiance Fields (NeRFs), each representing a distinct object. The method employs an encoder network to generate a set of latent vectors, each characterizing an object within the scene, in a single forward pass. These vectors individually condition a NeRF decoder, which determines the geometry and appearance of each object. To enhance computational efficiency during training, a novel loss function is introduced, enabling NeRF training on RGB-D data without requiring explicit ray marching. The model's performance is validated on three 2D image segmentation benchmarks, demonstrating results comparable to or exceeding state-of-the-art methods. Furthermore, its application to two multi-object 3D datasets, including a multiview adaptation of CLEVR and a new dataset utilizing ShapeNet models, reveals that ObSuRF, after being trained on RGB-D views of training scenes, can effectively reconstruct the 3D geometry of a scene from a single image and segment it into individual objects, even without specific segmentation supervision.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0829", "problem_id": "08290001", "content": "This paper introduces ST3D++, a self-training methodology incorporating a comprehensive pseudo-label denoising framework for unsupervised domain adaptation in 3D object detection. ST3D++ focuses on minimizing pseudo-label noise during generation and mitigating the detrimental effects of these noisy labels on model training. Initially, the 3D object detector undergoes pre-training on the labeled source domain utilizing random object scaling (ROS), a technique designed to reduce pseudo-label noise in the target domain caused by object scale bias inherent in the source domain. Subsequently, the detector is iteratively refined through alternating phases of pseudo-label generation and detector training using pseudo-labeled target domain data. A hybrid quality-aware triplet memory is integrated into the pseudo-label generation process to enhance the quality and consistency of the generated labels. Concurrently, a source data-assisted training strategy and a curriculum data augmentation policy are introduced during model training to effectively correct noisy gradient directions and prevent model overfitting to noisy pseudo-labeled data. These targeted design elements enable the detector to be trained on meticulously refined pseudo-labeled target data with denoised training signals, thereby facilitating effective adaptation to a target domain without requiring annotations. The method's performance is evaluated on four 3D benchmark datasets (i.e., Waymo, KITTI, Lyft, and nuScenes) across three standard categories (i.e., car, pedestrian and bicycle). ST3D++ demonstrates state-of-the-art results in all tested scenarios, significantly exceeding the corresponding baseline (e.g., 9.6% \\sim 38.16% on Waymo \\rightarrow KITTI in terms of AP_}), and even surpassing fully supervised oracle performance on the KITTI 3D object detection benchmark with target prior. Code will be available.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0830", "problem_id": "08300001", "content": "Creating point clouds, such as molecular structures, with arbitrary rotations, translations, and variations continues to pose significant challenges. Neural networks incorporating symmetry-invariant layers have demonstrated the ability to optimize training objectives efficiently with limited data. Inspired by this, we introduce an architecture capable of generating valid Euclidean distance matrices that inherently maintain invariance under object rotation and translation. Driven by the objective of generating molecular structures in Cartesian space, we employ this architecture to develop a Wasserstein GAN with a permutation-invariant critic network. This approach enables the one-shot generation of molecular structures by producing Euclidean distance matrices that can be embedded in three-dimensional space.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0831", "problem_id": "08310001", "content": "This paper presents a Federated Learning (FL) simulation platform designed for Acoustic Model training, marking the first known application of FL techniques to Speech Recognition tasks given their inherent complexity. The modular architecture of the platform enables its adaptation to various tasks, while a novel hierarchical optimization framework and two gradient aggregation methods are introduced, achieving nearly tenfold faster convergence than existing distributed or FL approaches such as BMUF and FedAvg. Beyond accelerating convergence, the hierarchical optimization enhances flexibility in the training process. Additionally, a dynamic gradient aggregation algorithm, leveraging data-driven weight inference, serves as a gradient quality regularizer. The platform also includes an unsupervised FL-specific training pipeline. Evaluations on the LibriSpeech task demonstrate a 7x speedup and 6% Word Error Rate reduction (WERR) over baseline results, while session adaptation yields a 20% WERR improvement compared to a production-ready LAS model. The proposed FL system surpasses conventional distributed training benchmarks in both convergence efficiency and model accuracy.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0832", "problem_id": "08320001", "content": "A severe snowfall event in 2020, exacerbated by climate change, resulted in a major disruption, with 2,000 vehicles stranded on a highway for three days, and 10 vehicles involved in a billiard accident due to icy road conditions. To mitigate such hazards, road managers need to provide timely warnings to drivers about snow-covered areas. This research presents a deep learning-based approach that utilizes live image post-processing to automatically generate a snow hazard ratio indicator. The method involves two key steps: first, a generative adversarial network, specifically pix2pix, is used to reveal the road surface obscured by snow; second, semantic segmentation is applied using DeepLabv3+ with MobileNet as the backbone to distinguish between snow-covered and road surface classes. The trained networks are then used to compute the road-to-snow rate hazard index, which quantifies the extent of snow cover on the road surface. The effectiveness of this approach is demonstrated through its application to 1,155 live snow images from a cold region in Japan, highlighting the practical utility and robustness of the proposed method, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0833", "problem_id": "08330001", "content": "This study presents an innovative interval type-2 fuzzy neural network designed to generate non-separable fuzzy rules with adaptable shapes. To capture uncertainty, the shapes of the fuzzy sets are regarded as uncertain. Consequently, a new variant of interval type-2 fuzzy sets, modeled on a general Gaussian framework, is introduced to create various shapes (such as triangular, bell-shaped, and trapezoidal). To account for interactions among input variables, input vectors are reformulated into new feature spaces populated with uncorrelated variables suitable for defining each fuzzy rule. These new features are subsequently processed through a fuzzification layer, utilizing the proposed interval type-2 fuzzy sets with adaptable shapes. This results in the establishment of interval type-2 non-separable fuzzy rules that appropriately consider local variable interactions and uncertainty. For the process of type reduction, the contributions from the upper and lower firing strengths of each fuzzy rule are adaptively selected independently. The Levenberg-Marquardt optimization method is employed to train various network parameters. The efficacy of the proposed approach is evaluated using both clean and noisy datasets to demonstrate its capacity to manage uncertainty. Furthermore, this paradigm is effectively applied to real-world time-series forecasting, regression challenges, and nonlinear system identification. Experimental results indicate that our proposed model surpasses other methods while maintaining a simpler structure.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0834", "problem_id": "08340001", "content": "This work introduces a straightforward and efficient hyperparameter optimization algorithm, drawing inspiration from Boolean function analysis methods, particularly suitable for high-dimensional scenarios such as training neural networks with numerous hyperparameters. The algorithm, based on iterative compressed sensing techniques for orthogonal polynomials, relies solely on uniform hyperparameter sampling, facilitating parallelization. Empirical evaluations involving deep neural network training on Cifar-10 demonstrate that the proposed algorithm surpasses state-of-the-art methods like Hyperband and Spearmint, achieving superior solutions, even surpassing manual tuning in certain instances. Regarding total runtime, encompassing hyperparameter sampling and computation, the algorithm exhibits a speed improvement of at least tenfold compared to Hyperband and Bayesian Optimization, and an eightfold improvement over Random Search. Furthermore, the method offers provable performance guarantees and provides the first advancement in decision tree learning sample complexity in over two decades, yielding the first quasi-polynomial time algorithm for learning noisy decision trees with polynomial sample complexity.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0835", "problem_id": "08350001", "content": "Modern Convolutional Neural Networks (CNN) have demonstrated remarkable performance in demanding image recognition and object detection benchmarks, greatly increasing research interest in these approaches. However, comparisons between various CNN techniques and earlier shallow representations like the Bag-of-Visual-Words and Improved Fisher Vector remain insufficiently explored. This study provides a systematic assessment of these newer methods, examining diverse deep architectures while standardizing evaluation conditions and highlighting critical implementation considerations. Our analysis reveals key characteristics of CNN-based representations, such as the ability to substantially reduce output layer dimensionality without compromising accuracy. Additionally, we demonstrate transferable elements between deep and shallow approaches, notably showing that data augmentation strategies typically used for CNNs can similarly enhance the performance of shallow methods. The experimental code and models are publicly released for reproducibility.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0836", "problem_id": "08360001", "content": "This paper investigates the use of convolutional neural networks for simultaneously detecting objects in static images and predicting their 3D orientations. We analyze various feature representations for oriented objects and the energy functions that enable a network to learn these representations. The selection of representation is critical, as object pose follows a continuous structure, whereas object category is a discrete variable. We assess these methods on the Pascal3D+ benchmark for combined object detection and pose estimation, employing Average Viewpoint Precision as the metric. Our results demonstrate that discretizing viewpoints and applying a classification-based approach yields state-of-the-art performance in joint object detection and pose estimation, substantially surpassing prior baselines on this benchmark.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0837", "problem_id": "08370001", "content": "Graph neural networks, while effective in semi-supervised node classification, are susceptible to adversarial attacks, a common issue in deep learning. Current research primarily explores creating resilient GNN models or identifying graph adversarial attacks. The concept of immunization against such attacks on graphs, however, remains largely unexplored. This paper introduces and defines the graph adversarial immunization problem, which involves strategically immunizing a small percentage of node pairs, whether connected or not, to enhance the graph's verifiable robustness against various adversarial attacks. To address this, we present AdvImmune, an efficient algorithm employing meta-gradient optimization in a discrete manner, thereby avoiding the high computational cost of combinatorial optimization. Evaluations on two citation networks and one social network reveal that AdvImmune significantly increases the proportion of robust nodes by 12%, 42%, and 65% respectively, using a modest immunization budget of just 5% of edges.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0838", "problem_id": "08380001", "content": "Recent research has focused on multimodal information retrieval (IR) systems, particularly in the domain of outside knowledge visual question answering (OKVQA), which integrates text corpora, knowledge graphs, and images. Nevertheless, widely used OKVQA datasets exhibit significant shortcomings. A considerable number of queries fail to evaluate the integration of cross-modal information effectively, as some are image-independent, rely on speculation, necessitate optical character recognition (OCR), or can be resolved using only the image. Furthermore, the prevalence of overlapping answers between training and testing sets enables effective frequency-based guessing. Consequently, it is challenging to ascertain whether current state-of-the-art systems capitalize on these dataset biases rather than genuinely inferring answers, due to their lack of transparency and interpretable reasoning processes. Another key limitation is the dataset's exclusive focus on quantitative evaluation of end-to-end answer retrieval, neglecting the assessment of accurate semantic interpretation of the input query. To address these issues, we identify a core structural pattern in OKVQA, termed S3 (select, substitute, and search), and develop a novel dataset and challenge centered on this pattern. Specifically, the questioner identifies an entity within the image and poses a question about that entity, which can only be answered by referencing a knowledge graph or corpus passage that mentions the entity. Our challenge includes (i) OKVQAS3, a subset of OKVQA annotated according to the S3 structural pattern, and (ii) S3VQA, a newly constructed dataset. We also introduce S3, a neural OKVQA system designed with structural transparency, which directly tackles our challenge dataset and demonstrates superior performance compared to existing baseline models.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0839", "problem_id": "08390001", "content": "The success of deep learning in computer vision has been significantly influenced by the availability of large-scale datasets. Recent studies indicate that expanding dataset sizes can further enhance object detection performance. This paper presents the EuroCity Persons dataset, featuring extensive, diverse, and precise annotations of pedestrians, cyclists, and other riders in urban traffic environments. Collected from moving vehicles across 31 cities in 12 European countries, the dataset comprises over 47300 images with 238200 manually labeled person instances, making it nearly ten times larger than previous benchmarking datasets. Additionally, it includes over 211200 annotations for person orientation. Four state-of-the-art deep learning models (Faster R-CNN, R-FCN, SSD, and YOLOv3) are optimized to establish baselines for this new benchmark. Experiments evaluate the generalization of these detectors when trained on the new dataset, examining factors such as training set size, dataset diversity (day vs. night, geographic regions), annotation detail (e.g., object orientation), and annotation quality. The study also investigates error sources and outlines future research directions.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0840", "problem_id": "08400001", "content": "Professionals across various sectors, including healthcare, economics, and education, are keen to leverage machine learning for better decision-making. The challenges posed by the cost and feasibility of conducting experiments, coupled with a significant rise in electronic data storage, highlight the issue of assessing decisions using non-experimental observational data. This work is situated within that context. Specifically, we focus on estimating individual-level causal effects, such as how a particular patient responds to different medications, based on documented contexts, decisions, and outcomes. We establish generalization bounds for the error in estimated effects utilizing distance metrics between groups subjected to varying treatments, which permits sample re-weighting. We outline the conditions under which our bound is precise and discuss its connection with findings in unsupervised domain adaptation. Informed by our theoretical outcomes, we develop representation learning algorithms aimed at minimizing our bound by regularizing the treatment group distance generated by the representation, while promoting information sharing among treatment groups. Furthermore, we enhance these algorithms to concurrently learn a weighted representation to further decrease treatment group distances. Ultimately, an experimental assessment using both real and synthetic data validates the effectiveness of our proposed representation framework and regularization approach.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0841", "problem_id": "08410001", "content": "This paper presents a novel approach to image synthesis using Generative Adversarial Networks (GANs) that incorporate Capsule Networks, leveraging the principle of positional-equivariance of features to enhance the critic's ability to capture spatial relationships between image features. By utilizing Capsule Networks, which outperform traditional Convolutional Neural Networks (CNNs) in this context, our proposed GAN architectures demonstrate accelerated learning of the data manifold, resulting in the generation of visually accurate images with significantly fewer training samples and epochs compared to conventional GANs and variants that rely on CNNs. Furthermore, our analysis encompasses both quantitative evaluations of the generated images and an in-depth examination of the limitations of GAN architectures that employ CNN critics, shedding light on the factors contributing to their reduced coverage and diversity, as seen in Figure A, B, C (References [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0842", "problem_id": "08420001", "content": "This paper introduces Earliness-Aware Deep Convolutional Networks (EA-ConvNets), a complete deep learning system engineered for the early classification of time series data. Different from prevailing techniques that rely on pre-defined feature sets, this framework simultaneously learns features through a deep hierarchy that captures crucial time series characteristics and employs a dynamic truncation model. This model enhances the deep feature learning architecture by focusing on the initial segments of each time series. As a result, the framework achieves superior early prediction accuracy compared to existing state-of-the-art early time series classification methods, while remaining competitive with leading time series classification algorithms that process full time series data. This approach represents, to our knowledge, the first instance of data-driven (deep) feature learning applied to early time series classification. Comprehensive experiments conducted on benchmark datasets demonstrate the method's significant performance gains over current state-of-the-art techniques. Furthermore, the deep shapelets learned exhibit high interpretability, providing valuable insights into the underlying characteristics of time series data.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0843", "problem_id": "08430001", "content": "High-resolution mapping of cellular and tissue structures lays the groundwork for the development of interpretable machine-learning models in computational pathology. Deep learning techniques can generate precise mappings when supplied with sufficient labeled instances for training and validation. However, the generation of a substantial amount of quality labels poses a significant challenge in computational pathology due to the extensive time and effort required from pathologists. In this paper, we present a strategy for involving groups of medical students and pathologists to create a dataset comprising over 220,000 annotations of cell nuclei in breast cancer cases. We demonstrate how annotations suggested by a weak algorithm can enhance the accuracy of annotations made by non-experts and provide valuable data for training segmentation algorithms while minimizing the need for extensive manual tracing. We thoroughly investigate interrater agreement and outline adjustments made to the MaskRCNN model to enhance cell mapping. Additionally, we introduce a method referred to as Decision Tree Approximation of Learned Embeddings (DTALE), which utilizes nucleus segmentations and morphological features to increase the transparency of nucleus classification models. The annotation dataset generated through this research is freely accessible for algorithm development and benchmarking at: https://sites.google.com/view/nucls.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0844", "problem_id": "08440001", "content": "Recent developments have introduced convolutional neural network (CNN)-based techniques for denoising hyperspectral images (HSIs), with significant attention on unsupervised methods like deep image prior (DIP), as they eliminate the need for training data. However, DIP experiences a semi-convergence issue, necessitating termination at a specific iteration informed by the ground-truth image. In this study, we present the spatial-spectral constrained deep image prior (S2DIP) designed for the removal of mixed noise in HSIs. We enhance DIP by integrating it with a spatial-spectral total variation (SSTV) component to maintain the spatial-spectral local smoothness of the HSI, along with an \\ell_1-norm term to effectively address complex sparse noise. The S2DIP method capitalizes on the expressive capabilities of deep CNNs without requiring training data while exploiting the inherent structures of HSIs and noise through carefully designed priors, thus circumventing the semi-convergence issue and demonstrating greater stability compared to DIP. Furthermore, our approach significantly improves the HSI denoising performance of the DIP framework. To implement our denoising model, we have developed an algorithm based on the alternating direction multiplier method. Comprehensive experimental results show that S2DIP surpasses both optimization-driven and supervised CNN-based HSI denoising methods currently regarded as state-of-the-art.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0845", "problem_id": "08450001", "content": "Binary grid masks are widely employed for instance segmentation, exemplified by Mask R-CNN's mask predictions on a 28x28 grid. While low-resolution grids often lack detailed capture, high-resolution grids substantially increase training complexity. This paper introduces a novel mask representation, DCT-Mask, which utilizes the discrete cosine transform (DCT) to encode high-resolution binary grid masks into a compact vector. DCT-Mask seamlessly integrates into most pixel-based instance segmentation methods, delivering considerable performance improvements across various frameworks, backbones, datasets, and training schedules without requiring pre-processing or pre-training, and with minimal impact on runtime. Notably, the method exhibits enhanced improvements with higher-quality annotations and more complex backbones. Furthermore, an analysis of the method's performance from the perspective of mask representation quality reveals that DCT-Mask's effectiveness stems from its ability to achieve high-quality mask representation with low complexity. Code is available at https://github.com/aliyun/DCT-Mask.git.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0846", "problem_id": "08460001", "content": "Detecting anomalies in images is crucial for various sectors, including healthcare for disease identification and manufacturing for quality control. Manual image inspection, particularly over prolonged and repetitive periods, is labor-intensive and prone to missing irregularities. Artificial neural networks excel at handling straightforward, repetitive tasks, sometimes surpassing human performance. This study explores diverse deep learning approaches—both supervised and unsupervised—for anomaly detection in a quality assurance scenario, using the MVTec dataset. We evaluate three models: a CNN for supervised detection, KD-CAE for autoencoder-based detection, NI-CAE for noise-induced detection, and a DCGAN for image reconstruction. Experimental results show KD-CAE outperforms CNN and NI-CAE on most datasets, while NI-CAE excels on the Transistor dataset. Although we developed a DCGAN for synthetic training data, computational constraints and limitations in extending AnoGAN's mechanics restricted its use to image generation. Our findings highlight the superiority of unsupervised methods for anomaly detection, particularly when labeled data is scarce or unavailable.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0847", "problem_id": "08470001", "content": "Attention mechanisms are commonly employed to enhance feature representations by identifying discriminative features. This study introduces an extension of attention mechanisms to weakly supervised object localization (WSOL) through the dual-attention guided dropblock module (DGDM), designed to capture informative and complementary visual patterns. The DGDM consists of two main components: channel attention guided dropout (CAGD) and spatial attention guided dropblock (SAGD). The CAGD evaluates channel interdependencies by ranking attention weights, prioritizing top-k high-magnitude channels while retaining certain low-valued elements to adapt to their potential importance during training. Meanwhile, the SAGD removes discriminative information by erasing contiguous feature map regions instead of isolated pixels, encouraging the model to focus on less discriminative regions for classification. Additionally, it helps differentiate foreground objects from background areas to mitigate attention misdirection. Experimental evaluations confirm that the proposed approach attains state-of-the-art localization performance.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0848", "problem_id": "08480001", "content": "Graph Neural Networks (GNNs) have demonstrated encouraging results across a range of practical applications. Nevertheless, recent research has indicated that GNNs are susceptible to adversarial threats. This paper investigates a newly proposed realistic attack scenario involving graphs known as the graph injection attack (GIA). In the GIA framework, the adversary is restricted from altering the existing link structures and node attributes of the input graph; instead, the attack involves inserting adversarial nodes. We provide an analysis of the topological vulnerabilities of GNNs in the context of GIA, from which we introduce the Topological Defective Graph Injection Attack (TDGIA) for more efficient injection attacks. TDGIA initially employs a topological defective edge selection strategy to identify original nodes to which the injected nodes will connect. It subsequently formulates a smooth feature optimization objective to create features for these injected nodes. Comprehensive experiments on large-scale datasets demonstrate that TDGIA consistently and significantly surpasses various attack baselines when targeting numerous defense GNN models. Remarkably, the reduction in performance of the target GNNs due to TDGIA exceeds twice the impact of the most effective attack method identified among hundreds of entries in KDD-CUP 2020.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0849", "problem_id": "08490001", "content": "Recent progress in Reinforcement Learning, achieved by integrating established theoretical principles with Deep Learning methodologies, has resulted in significant advancements across various artificial intelligence applications, thereby establishing Deep Reinforcement Learning (DRL) as an active research area. This study examines contemporary DRL algorithms, emphasizing their theoretical underpinnings, practical constraints, and empirically observed characteristics.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0850", "problem_id": "08500001", "content": "Hashing techniques, which convert similar high-dimensional data into binary hashcodes characterized by short Hamming distances, have garnered significant interest because of their efficient storage and rapid retrieval capabilities. Pairwise similarity, commonly used for retrieval purposes, is a key element in the design of many supervised hashing algorithms. To address the challenge of labeling all data pairs, semi-supervised hashing has emerged as a method for learning effective codes using a limited number of labeled pairs in conjunction with a large quantity of unlabeled data. Current approaches often construct graphs to represent the dataset's structure; however, these methods struggle with complex data because graph construction relies on data representations, and obtaining accurate representations for complex data is challenging. This paper introduces a novel teacher-student semi-supervised hashing framework where the student network is trained using pairwise information generated by the teacher network. By adhering to the smoothness assumption, the network ensures consistent distances for similar data pairs, leading to comparable retrieval results for neighboring queries. Experiments conducted on extensive datasets demonstrate that the proposed method significantly outperforms supervised baselines and surpasses existing state-of-the-art semi-supervised hashing methods.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0851", "problem_id": "08510001", "content": "Existing benchmarks for optical flow algorithms assess estimation either by directly comparing predicted flow fields with the actual ground truth or indirectly by using these predicted fields for frame interpolation and then evaluating the resulting frames against the original ones. In the latter scenario, objective quality metrics such as mean squared error are commonly utilized. However, it is widely acknowledged that the true quality perceived by users cannot be completely determined by such straightforward metrics. Consequently, we undertook a subjective quality assessment study utilizing crowdsourcing for the interpolated frames generated by one of the optical flow benchmarks, specifically the Middlebury benchmark. We gathered forced-choice paired comparisons between the interpolated images and their corresponding ground truth. To enhance observers' sensitivity in discerning subtle differences in these comparisons, we introduced a novel technique for full-reference quality assessment known as artefact amplification. Using the crowdsourced data, we reconstructed absolute quality scale values based on Thurstone's model, resulting in a re-ranking of the 155 algorithms in relation to the visual quality of the interpolated frames. This re-ranking emphasizes the importance of visual quality assessment as an additional evaluation criterion for optical flow and frame interpolation benchmarks, while also providing a basis for developing innovative image quality assessment (IQA) methods focused on the perceptual quality of interpolated images. As an initial step, we proposed a new full-reference method called WAE-IQA, which, by giving weight to local differences between an interpolated image and its ground truth, slightly outperformed the best existing FR-IQA approach in the literature.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0852", "problem_id": "08520001", "content": "Recent studies have shown that fine-tuning and joint training—two widely used methods for transfer learning—do not consistently enhance performance on downstream tasks. Our primary objective is to gain insights into the circumstances under which fine-tuning and joint training may be inefficient or detrimental for transfer learning. To investigate this, we developed semi-synthetic datasets where the source task can be addressed using either source-specific features or transferable features. Our findings indicate that (1) pre-training may lack motivation to acquire transferable features, and (2) joint training can simultaneously capture source-specific features while overfitting to the target. To surpass fine-tuning and joint training, we introduce Meta Representation Learning (MeRLin), which focuses on acquiring transferable features. MeRLin utilizes a meta-learning approach to ensure that a supplementary model on top of the representations trained with target data also performs effectively on validation data for the target. Additionally, we demonstrate that MeRLin can recover the true target model when utilizing a quadratic neural network parameterization alongside a source distribution that includes both transferable and source-specific features. In contrast, pre-training and joint training are mathematically proven to fail in learning transferable features within the same distribution. Empirically, MeRLin outperforms existing state-of-the-art transfer learning techniques across a range of practical vision and NLP transfer learning benchmarks.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0853", "problem_id": "08530001", "content": "The development of deep learning-based automatic colorization techniques has seen significant progress, yet these methods still face challenges in few-shot learning scenarios, where they are hindered by their reliance on substantial amounts of training data. To address this limitation, this study introduces MemoPainter, a memory-augmented colorization model capable of generating high-quality colorizations using limited data, with a particular strength in capturing and accurately colorizing rare instances. Furthermore, a novel threshold triplet loss is proposed, facilitating the unsupervised training of memory networks without requiring class labels. The results of experiments demonstrate the superiority of the proposed model in both few-shot and one-shot colorization tasks, showcasing its potential to overcome existing limitations in data-intensive colorization methods.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0854", "problem_id": "08540001", "content": "Building on the demonstrated effectiveness of Two-Dimensional Convolutional Neural Networks (2D CNNs) in image recognition, researchers have sought to adapt them for video analysis. A key challenge in using 2D CNNs for videos is that identical kernels process all frames, potentially leading to redundant spatial feature extraction and overlooking important temporal variations. To address this, we propose two solutions: 1) A Progressive Enhancement Module (PEM) that sequentially filters channels to prioritize distinctive features across frames, reducing redundancy, and 2) A Temporal Diversity Loss (TD Loss) that encourages kernels to focus on inter-frame differences rather than static visual patterns. Evaluations on Something-Something V1 and V2 show performance gains of 2.4% and 1.3% over leading methods, while experiments on Kinetics further demonstrate improvements over state-of-the-art 2D-CNN-based approaches.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0855", "problem_id": "08550001", "content": "The prevailing deep learning frameworks heavily rely on vast amounts of annotated data, but the annotations' quality can be inconsistent due to variability among labelers. To investigate these discrepancies, multi-observer studies have been undertaken, where the same data is labeled multiple times, revealing the impact on critical applications such as medical image analysis. This process exacerbates the already laborious and expertise-intensive annotation task. Alternatively, automated annotation methods leveraging NLP algorithms have emerged as a viable option, utilizing existing diagnostic reports from clinical systems. Although these algorithms generate labels of varying quality, often noisier than those from human labelers, this paper demonstrates how to harness noisy annotations from different algorithm-based labelers to enhance the learning of classification tasks. By introducing the concept of attention-on-label, which dynamically samples superior label sets during training, and designing a meta-training-based label-sampling module that identifies the most beneficial labels through additional back-propagation, we showcase the effectiveness of this approach. The attention-on-label scheme is applied to a synthetic noisy CIFAR-10 dataset and yields superior results, with a 3-5% increase in average AUCs for multiple disease classifications, on both the hospital-scale MIMIC-CXR dataset and the hand-labeled OpenI dataset, outperforming conventional training paradigms.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0856", "problem_id": "08560001", "content": "This paper introduces the Hue-Net, a novel deep learning framework designed for intensity-based image-to-image translation tasks, leveraging a technique called network augmentation to construct differentiable intensity histograms from images. The framework incorporates differentiable representations of 1D cyclic and 2D joint histograms, which are utilized to define loss functions based on cyclic Earth Mover's Distance (EMD) and Mutual Information (MI), as seen in Figure A. The Hue-Net is applied to color transfer problems, where the goal is to render a source image in the colors of a target image, without relying on supervised pixel-to-pixel learning since the desired output does not exist. To achieve this, the framework employs the HSV color-space and an intensity-based loss function built on the EMD between the cyclic hue histograms of the output and target images, as illustrated in Figure B. Additionally, a semantic-based loss function is defined using a differentiable approximation of the MI between the source and output images, as shown in Figure C (citations). The combination of histogram loss functions with an adversarial loss enables the generation of semantically meaningful and realistic images, yielding promising results across various datasets, and is discussed further in References.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0857", "problem_id": "08570001", "content": "We tackle the challenge of executing intricate bimanual robotic manipulation tasks involving multiple objects with limited reward signals. These complex tasks can be broken down into sub-tasks that various robots can perform either concurrently or sequentially to enhance efficiency. While earlier reinforcement learning methods have primarily concentrated on the compositional aspects of sub-tasks, two critical issues are often overlooked when developing cooperative strategies for two robots: (i) domination, where one robot attempts to complete a task independently, leaving the other inactive; and (ii) conflict, as one robot can disrupt another's workspace while performing different sub-tasks at the same time. To address these challenges, we introduce a novel approach termed disentangled attention, which offers an intrinsic regularization that encourages the two robots to concentrate on distinct sub-tasks and objects. We assess our methodology across four bimanual manipulation tasks. The experimental findings indicate that our intrinsic regularization effectively mitigates domination and minimizes conflicts within the policies, resulting in significantly more effective cooperative strategies compared to all baseline methods. For further details and videos, please visit our project page at https://mehooz.github.io/bimanual-attention.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0858", "problem_id": "08580001", "content": "Scene comprehension in computer vision involves interconnected tasks such as object classification, detection, segmentation, and depth estimation. By jointly training these tasks, their inherent relationships can be exploited through consistency losses, enabling mutual supervision. This approach not only enhances performance but also minimizes reliance on labeled data while incorporating unsupervised or simulated datasets. We introduce a distributed training algorithm with task-level parallelism, ensuring high asynchrony and robustness for scalable learning across multiple tasks or extensive input data. Our framework is validated on tasks including depth and normal prediction, semantic segmentation, 3D motion and ego-motion estimation, as well as object tracking and 3D detection in point clouds, showing notable improvements, particularly with limited labeled data.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0859", "problem_id": "08590001", "content": "The development of reinforcement learning (RL) algorithms with function approximation has led to a multitude of new methods, each with distinct algorithmic characteristics and implementation specifics. However, the interplay between algorithmic innovations and implementation details has made it challenging to pinpoint the sources of performance gains across different algorithms. This study focuses on a subset of off-policy inference-based actor-critic algorithms, including MPO, AWR, and SAC, with the goal of disentangling their algorithmic contributions from implementation decisions. By deriving these algorithms from a unified control-as-inference objective, we categorize them as either Expectation-Maximization (EM) or Kullback-Leibler (KL) divergence minimization-based, treating other specifications as implementation details. Our extensive ablation studies reveal significant performance degradation when implementation details are not properly matched with algorithmic choices, highlighting the co-adaptation and co-evolution of certain implementation details with specific algorithms. Notably, our results show that certain implementation details, such as tanh Gaussian policy and network sizes, are highly adapted to specific algorithmic types, whereas others, like layer normalization and ELU, are critical for MPO's performance and can also be transferred to SAC, yielding noticeable gains. Our work aims to facilitate future research by clarifying the sources of performance improvements across multiple algorithms, enabling researchers to build upon both the algorithmic and implementational advancements of others.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0860", "problem_id": "08600001", "content": "Topic modeling extends the clustering problem by assuming observations arise from multiple latent factors, where, for example, document words are produced by a combination of active topics rather than a single one. While this offers enhanced representational capabilities, it introduces a more complex unsupervised learning challenge: estimating topic probability vectors (word distributions for each topic) given only observed words and hidden topics. We introduce a straightforward and effective learning method that reliably estimates parameters for various mixture models, including latent Dirichlet allocation (LDA). For LDA, the method accurately recovers both topic probability vectors and topic priors using only trigram statistics (third-order moments estimable from three-word documents). This approach, named Excess Correlation Analysis (ECA), employs spectral decomposition of low-order moments (third and fourth order) using two singular value decompositions (SVDs). The algorithm's scalability stems from performing SVD operations on k\\times k matrices, where k represents the number of latent factors (e.g., topics), instead of the higher-dimensional observed space d (typically d \\gg k).", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0861", "problem_id": "08610001", "content": "This paper explores the Correlation Clustering functional, a crucial component of unsupervised learning that integrates both positive and negative relationships between data points. The research presents a two-pronged contribution: firstly, a theoretical examination of the functional is provided, and secondly, novel optimization algorithms are introduced that can efficiently handle large-scale problems comprising over 100,000 variables, which are currently intractable using existing methods. The theoretical analysis offers a probabilistic generative interpretation of the functional, validating its inherent ability to perform model selection. Additionally, an analogy is drawn between optimizing this functional and minimizing Potts energy, enabling the proposal of several new optimization algorithms that leverage the functional's model-selection capability to automatically determine the underlying cluster count. The performance of these algorithms is evaluated against existing methods using both synthetic and real data, and two new applications are proposed: unsupervised face identification and interactive multi-object segmentation via rough boundary delineation, as shown in Figure A, B, C (citations remain unchanged).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0862", "problem_id": "08620001", "content": "Retrosynthesis, aimed at identifying suitable reactants to produce a target compound, is a growing field in deep learning research. Although current methods have achieved notable success, they often fail to account for reactant availability—such as stability or commercial accessibility—or adapt to unfamiliar reaction templates (chemical transformation rules). This study introduces a novel solution by reframing retrosynthesis as a reactant selection task from a predefined pool of purchasable molecules. We present RetCL (retrosynthesis via contrastive learning), an efficient framework that ranks candidate molecules using selection scores derived from graph neural networks. Additionally, we develop a contrastive training strategy incorporating hard negative mining to optimize these scores. Experimental results highlight the advantages of our selection-based method. For instance, with 671k candidate reactants from the USPTO dataset, RetCL attains a top-1 exact match accuracy of 71.3% on the USPTO-50k benchmark, outperforming a recent transformer-based model (59.6%). Furthermore, RetCL exhibits strong generalization to unseen templates across diverse scenarios, unlike template-dependent approaches.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0863", "problem_id": "08630001", "content": "Neural attention (NA) has emerged as a fundamental element in sequence-to-sequence models, achieving top-tier results in complex tasks like abstractive document summarization (ADS) and video captioning (VC). NA mechanisms compute context vectors by adaptively aggregating weighted combinations of deterministic input sequence encodings across extended temporal spans. Drawing on recent advances in amortized variational inference (AVI), this study proposes treating soft-attention (SA) generated context vectors as latent variables, with their approximate finite mixture model posteriors derived through AVI. We hypothesize that this approach could enhance generalization performance, consistent with prior applications of AVI in deep learning. To validate our framework, we conduct experiments on demanding ADS, VC, and MT benchmarks, demonstrating its superior performance compared to existing state-of-the-art methods.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0864", "problem_id": "08640001", "content": "The application of machine learning techniques to unprocessed numerical time series data is hindered by significant drawbacks, including a pronounced sensitivity to hyperparameter settings and the initialization of random weights. To address these issues, this study proposes a hybrid approach that integrates a recurrent neural network with a dimensionality-reducing symbolic representation, with a focus on time series forecasting. The results demonstrate that the incorporation of symbolic representation can mitigate some of the inherent limitations of traditional methods, potentially enabling faster training times while maintaining forecast accuracy.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0865", "problem_id": "08650001", "content": "The goal of reinforcement learning is to determine an optimal policy through environmental interactions, but this often necessitates an impractically large number of samples to learn complex behaviors. Traditional methods employing policy gradients for local search typically rely on random perturbations, resulting in high variance estimates and suboptimal sample complexity. In contrast, Bayesian optimization actively selects informative samples by constructing a probabilistic surrogate of the objective function from previous samples. This paper integrates these two approaches by introducing an algorithm that leverages a probabilistic model of the objective function and its gradient to decide where to query a noisy zeroth-order oracle, thereby improving gradient estimates. The proposed algorithm, a new type of policy search method, is evaluated against existing black-box algorithms, demonstrating improved sample complexity and reduced variance in extensive empirical evaluations on synthetic objectives, as well as highlighting the advantages of active sampling on popular RL benchmarks, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0866", "problem_id": "08660001", "content": "Automated recognition of human emotions and facial expressions is a crucial capability for intelligent robotics, enhancing human-robot interaction and collaboration. While deep learning methods excel in controlled laboratory settings, they often struggle with accuracy in unconstrained real-world scenarios. Facial action units (AU), which encode fine-grained facial movements, offer a solution by resolving ambiguous expressions. This study investigates the relationship between action units and facial expressions, proposing an AU-Expression Knowledge Constrained Representation Learning (AUE-CRL) framework to derive AU representations without requiring AU labels and adaptively apply them to improve expression recognition. The framework utilizes AU-expression correlations to train AU classifiers, enabling AU representation learning without annotations. Additionally, it employs a knowledge-guided attention mechanism to selectively utilize AU representations based on these correlations. This approach enhances facial feature extraction by identifying localized discriminative and complementary patterns for expression recognition. Evaluations on challenging unconstrained datasets confirm the framework’s superiority over existing state-of-the-art techniques. Codes and trained models are available at https://github.com/HCPLab-SYSU/AUE-CRL.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0867", "problem_id": "08670001", "content": "Autonomous robotic skill acquisition facilitates the development of versatile robots capable of mastering diverse behaviors without substantial manual intervention. Nevertheless, current robotic skill learning techniques often necessitate compromises to achieve practical real-world application, such as relying on manually crafted policy or value function representations, initializing from human demonstrations, instrumenting the training environment, or enduring protracted training durations. This paper introduces a novel reinforcement learning algorithm designed for manipulation skill acquisition, capable of training general-purpose neural network policies with reduced human involvement, while preserving rapid and efficient learning in stochastic environments. Our methodology expands upon the guided policy search (GPS) algorithm, recasting the reinforcement learning problem as supervised learning guided by a computational instructor, thus eliminating the need for human demonstrations. Unlike previous GPS techniques that require consistent initial states for system reset after each episode, our method accommodates randomized initial states, broadening its applicability to environments lacking deterministic reset capabilities. We evaluate our approach against established policy search methods in simulated environments, demonstrating its capacity to train high-dimensional neural network policies with comparable sample efficiency to prior GPS methods. Furthermore, we present empirical results obtained using a PR2 robotic manipulator in a real-world setting.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0868", "problem_id": "08680001", "content": "The field of Neural Architecture Search (NAS) has experienced significant growth, enabling the automatic exploration of network architectures within a defined input space. A key component of the NAS framework involves encoding architectural structures, which comprise computational blocks, including operations and their interconnections. However, existing methods often struggle to effectively capture architectural structural properties or rely on manually designed vectors to represent operator information. This paper introduces a novel approach, replacing fixed operator encoding with learnable representations within the optimization process, allowing for a more nuanced capture of operation relationships and yielding more accurate and smoother architectural representations, ultimately leading to enhanced end-task performance. As demonstrated through extensive evaluation on the ENAS benchmark, the proposed operation embeddings facilitate the generation of highly accurate models, achieving state-of-the-art results, and our method produces top-performing architectures exhibiting similar operation and graph patterns, underscoring a strong correlation between architectural structure and performance.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0869", "problem_id": "08690001", "content": "Recurrent Neural Networks (RNNs) have been extensively utilized in natural language processing and computer vision, with the Hierarchical Multi-scale RNN (HM-RNN) being a recently proposed variant that can automatically learn hierarchical temporal structures from data. This study extends the application of HM-RNN to the computer vision task of action recognition, addressing the challenge of discovering input-output relationships in sequence-to-sequence models, such as RNNs, when dealing with static inputs. To overcome this limitation, an attention mechanism can be employed to extract relevant information from inputs, thereby facilitating the modeling of input-output relationships. By integrating HM-RNN with the attention mechanism, a novel attention network, termed Hierarchical Multi-scale Attention Network (HM-AN), is proposed for action recognition. The Gumbel-softmax method is utilized to implement temporal boundary detectors and stochastic hard attention mechanisms, while an adaptive temperature training method is applied to mitigate the negative effects of temperature sensitivity in Gumbel-softmax, ultimately enhancing system performance. Experimental results show that HM-AN outperforms LSTM with attention in the vision task, and visualization of the learned features reveals that HM-AN can effectively capture both attention regions in images and hierarchical temporal structures, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0870", "problem_id": "08700001", "content": "This paper presents DensePoint, a comprehensive dataset of densely sampled and annotated point clouds featuring over 10,000 individual objects across 16 categories, created by integrating various data from two existing datasets. Each point cloud within DensePoint comprises 40,000 points, with each point linked to two types of information: RGB values and part annotations. Furthermore, we introduce a technique for colorizing point clouds utilizing Generative Adversarial Networks (GANs). This network enables the generation of colors for point clouds of single objects simply by inputting the point cloud. Experiments conducted with DensePoint reveal distinct boundaries in the point clouds that separate different parts of an object, indicating that the proposed network is capable of producing reasonably accurate colors. Our dataset is available to the public on the project page.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0871", "problem_id": "08710001", "content": "We utilize the commonly perceived drawbacks of deep learning techniques—such as substantial computational demands, reliance on extensive datasets, lack of interpretability, sensitivity to hyperparameter selection, tendency to overfit, and susceptibility to adversarial attacks—to develop a secure and efficient approach for training neural networks remotely across unprotected communication channels.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0872", "problem_id": "08720001", "content": "We introduce an innovative method for disentangling the generative factors of variation that underlie a specific collection of observations. Our approach is founded on the premise that the (unknown) low-dimensional manifold representing the data space can be explicitly expressed as a product of submanifolds. This definition facilitates the development of a unique weakly-supervised algorithm that aims to identify the unknown explanatory factors behind the data. During training, our algorithm necessitates only pairs of non i.i.d. data samples that possess at least one, potentially multidimensional, generative factor of variation. We do not require prior knowledge regarding the nature of these transformations and impose no restrictive assumptions on the characteristics of each subspace. Our method is straightforward to implement and applicable to diverse data types (ranging from images to 3D surfaces) subject to arbitrary transformations. In addition to conventional synthetic benchmarks, we demonstrate our technique in demanding real-world scenarios, where we achieve competitive results compared to existing leading methods.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0873", "problem_id": "08730001", "content": "In recent years, advancements in branch predictor (BP) effectiveness have slowed, and innovative concepts in BP design have become scarce, highlighting the need for new approaches in this field. This paper posits that examining BPs through the lens of Reinforcement Learning (RL) enables systematic analysis and exploration of BP designs. We outline the application of the RL framework to branch predictors, demonstrate that existing predictors can be effectively represented within this framework, and investigate two RL-based adaptations of traditional BPs.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0874", "problem_id": "08740001", "content": "This study presents a systematic approach to designing attention-based models for end-to-end pose estimation, leveraging a sequential prediction framework to learn spatial models with enhanced temporal consistency. By integrating conventional networks and constraints into the attention mechanism, we demonstrate how to effectively learn long-range dependencies for pose estimation tasks. Our methodology enables the adaptation of temporal receptive fields through a multi-scale structure of dilated convolutions, allowing for flexibility and scalability in handling arbitrary video sequences as input. Furthermore, the proposed architecture can be easily modified to facilitate real-time performance in a causal model, and can seamlessly incorporate existing 2D pose estimation systems, such as Mocap libraries, in a straightforward manner. The results show that our method achieves state-of-the-art performance, outperforming existing approaches with a mean per joint position error of 33.4 mm on the Human3.6M dataset.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0875", "problem_id": "08750001", "content": "Recent progress in machine learning (ML) indicates its capability to rapidly create sophisticated predictive models. Nevertheless, in sectors with stringent regulations like financial technology (Fintech), anxieties have emerged regarding the possibility of ML systems exhibiting discriminatory behavior towards particular protected demographics or individuals. In response, researchers have developed several mathematical metrics for fairness and algorithms to reduce bias. This study examines the concealed technical debts and difficulties associated with creating equitable ML systems within a Fintech production setting. We investigate numerous phases within the ML system development and deployment lifecycle that necessitate consideration for fairness. To pinpoint these hidden technical debts in the creation of fair ML systems for Fintech, we concentrate on critical pipeline stages, including data preparation, model development, system monitoring, and production integration. Our findings suggest that ensuring fairness in production-ready ML systems within Fintech demands dedicated engineering efforts throughout the ML system lifecycle. Furthermore, we offer several preliminary measures to alleviate these technical debts when deploying fair ML systems in production.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0876", "problem_id": "08760001", "content": "Lifelong learning, which addresses the challenge of continual learning with sequentially arriving tasks, has garnered increasing interest within the computer vision field. The goal of lifelong learning is to create a system capable of assimilating new tasks while preserving its effectiveness on previously mastered tasks. Nevertheless, deep neural networks face two significant challenges in lifelong learning: catastrophic forgetting and capacity constraints. To address these challenges, we propose a Multi-task based lifelong learning approach through a nonexpansive AutoML framework called Regularize, Expand and Compress (REC), drawing inspiration from recent advancements in automated neural network architecture design. REC consists of three phases: 1) it continually assimilates sequential tasks without relying on the data from learned tasks by implementing a novel multi-task weight consolidation (MWC) algorithm; 2) it expands the network to enhance lifelong learning through network-transformation based AutoML, thereby improving potential model capabilities and performance; and 3) it compresses the expanded model following the acquisition of each new task to ensure both efficiency and performance are upheld. Our proposed MWC and REC algorithms demonstrate superior performance compared to other lifelong learning methods across four distinct datasets.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0877", "problem_id": "08770001", "content": "Imagine an assistive system designed to help visually impaired individuals navigate using speech and tactile cues. Current navigation technologies, whether robotic, portable, or wearable, often function uniformly without considering individual user differences. Our real-world observations highlight the necessity of tailoring guidance to accommodate varying mobility skills, reducing confusion and errors. To tackle this challenge in scalable system design, we introduce a model-based reinforcement learning framework that personalizes user-system interactions. For efficient adaptation to new users, we employ a weighted experts model to overcome data-efficiency constraints in deep transfer learning. Experiments using real-world navigation data from blind users demonstrate that our method enhances long-term behavior prediction (up to 20 seconds ahead) by better accounting for personal mobility traits, obstacle interactions, and navigation goals, while also enabling rapid adaptation during initial learning phases with limited data.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0878", "problem_id": "08780001", "content": "Recurrent graph convolutional neural networks have proven to be highly effective in processing spatiotemporal signals, and newly developed graph neural network architectures are continually being assessed on conventional tasks like traffic and weather forecasting. To provide an alternative evaluation framework, this paper introduces the Chickenpox Cases in Hungary dataset as a novel benchmark for comparing the efficacy of various graph neural network architectures. Through our experiments on time series analysis and forecasting, we show that the Chickenpox Cases in Hungary dataset is a suitable platform for evaluating the predictive accuracy and forecasting abilities of state-of-the-art recurrent graph neural network architectures.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0879", "problem_id": "08790001", "content": "Deep clustering techniques integrate representation learning with clustering by simultaneously optimizing both a clustering loss and a non-clustering loss. These approaches employ a deep neural network for representation learning in conjunction with a clustering network. Rather than adhering to this traditional framework to enhance clustering outcomes, we suggest a more straightforward method that focuses on optimizing the entanglement of the generated latent code representation from an autoencoder. We characterize entanglement as the proximity of pairs of points within the same class or structure compared to pairs of points across different classes or structures. To quantify the entanglement of data points, we utilize the soft nearest neighbor loss and enhance it by incorporating an annealing temperature factor. Our proposed method achieved test clustering accuracies of 96.2% on the MNIST dataset, 85.6% on the Fashion-MNIST dataset, and 79.2% on the EMNIST Balanced dataset, surpassing the performance of our baseline models.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0880", "problem_id": "08800001", "content": "Air traffic control involves real-time decision-making that is crucial for safety within rapidly changing and uncertain environments. Currently, a human air traffic controller oversees numerous aircraft operating within a specified airspace sector. Given the increasing complexity of air traffic, both from traditional commercial aircraft and low-altitude vehicles like drones and eVTOLs, there is a pressing need for an autonomous air traffic control system that can handle high-density traffic while ensuring safe distances between planes. We introduce a deep multi-agent reinforcement learning framework capable of detecting and resolving aircraft conflicts in a complex, stochastic, and dynamic en-route sector that features multiple intersections and merging points. Our framework employs an actor-critic model, A2C, which integrates the loss function from Proximal Policy Optimization (PPO) to enhance learning stability. Additionally, it adopts a centralized learning yet decentralized execution approach, where a single neural network is developed and utilized by all agents within the environment. Our findings indicate that this framework is both scalable and efficient for managing a substantial influx of aircraft, thereby achieving exceptionally high traffic throughput while maintaining safety. We validate our model through extensive simulations conducted in the BlueSky environment, demonstrating that it successfully resolves 99.97% of conflicts at intersections and 100% at merging points in extremely high-density air traffic conditions.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0881", "problem_id": "08810001", "content": "In the realm of unsupervised representation learning, it is customary to leverage labeled data to assess the quality of the learned representations, which in turn informs key aspects of the training process, such as the selection of data augmentation policies. Nevertheless, this approach is not feasible for real-world datasets that lack labels, as is often the case in sensitive domains like medical imaging. This study demonstrates that evaluating learned representations using a self-supervised image rotation task exhibits a strong correlation (rank correlation > 0.94) with traditional supervised evaluation methods. This correlation is consistently observed across a wide range of augmentation policies, training settings, and network architectures. Furthermore, an algorithm, referred to as SelfAugment, is introduced to enable the efficient and automatic selection of augmentation policies without relying on supervised evaluations, yielding results comparable to those obtained through exhaustive supervised evaluations, all without utilizing labeled data.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0882", "problem_id": "08820001", "content": "The concept of an adversarial example refers to a manipulated input designed to mislead a system into producing an incorrect output during testing. While adversarial examples have been successfully created for classifiers, their existence for detectors has not been previously demonstrated. The potential consequences of such examples are significant, as they could be exploited to create security vulnerabilities in environments such as roads with smart vehicles. This study presents a novel construction method that effectively deceives two widely-used detectors, Faster RCNN and YOLO, into misclassifying inputs. Notably, the existence of these examples is unexpected, given the distinct differences between attacking classifiers and detectors, as well as the complex structure of detectors, which must autonomously identify bounding boxes with limited accuracy, making it likely that adversarial patterns would be disrupted. Our results show that the constructed adversarial examples exhibit robust generalizability across digital sequences, despite requiring substantial perturbations, and can also be realized as physical objects that retain their adversarial properties.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0883", "problem_id": "08830001", "content": "This paper introduces O-CNN, a Convolutional Neural Network (CNN) framework that utilizes octrees for 3D shape analysis. Leveraging the octree representation of 3D shapes, the approach uses the average normal vectors of a 3D model, sampled from the smallest leaf octants, as input. It then applies 3D CNN operations specifically to those octants containing the 3D shape's surface. To optimize storage and processing, a new octree data structure is implemented for efficient management of octant information and CNN features within the graphics memory, enabling full GPU-based training and evaluation of the O-CNN. Adaptable to various CNN architectures and 3D shape representations, O-CNN restricts computations to surface-containing octants, resulting in quadratic growth of memory and computational demands relative to octree depth, thus enabling the processing of high-resolution 3D models. Comparative evaluations against existing 3D CNN methods highlight O-CNN's efficiency and effectiveness across object classification, shape retrieval, and shape segmentation tasks.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0884", "problem_id": "08840001", "content": "The increasing popularity of graph neural networks has led to their widespread adoption and impressive performance across various disciplines. Nevertheless, the majority of existing algorithms are limited by their assumption of pairwise relationships between objects, which does not account for the complex, higher-order interactions present in many real-world applications. To address this limitation, we propose the integration of two novel, end-to-end trainable operators - hypergraph convolution and hypergraph attention - into the graph neural network framework, enabling the effective learning of deep embeddings from high-order graph-structured data. Hypergraph convolution provides a foundational formulation for performing convolutional operations on hypergraphs, while hypergraph attention enhances the representational capacity of the model by incorporating an attention mechanism, as shown in Figure A, B, C (References [1], [2]). By leveraging these two operators, graph neural networks can be readily extended to accommodate more flexible and nuanced relationship structures, making them applicable to a broader range of domains where non-pairwise relationships are prevalent, with experimental results on semi-supervised node classification demonstrating the efficacy of hypergraph convolution and hypergraph attention (citations [3], [4]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0885", "problem_id": "08850001", "content": "Generative Adversarial Networks (GANs) have demonstrated significant utility in Computer Vision, but their potential remains largely untapped within space science and planetary exploration. This work introduces methodologies for processing planetary data acquired by the Chang'E-4 mission and proposes a Neural Style Transfer framework, leveraging Cycle-consistency from rendered images. The experimental validation is performed in relation to the Iris Lunar Rover, a Carnegie Mellon-led nano-rover scheduled for lunar deployment in 2021, marking the first American unmanned rover on the Moon.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0886", "problem_id": "08860001", "content": "Recent research has focused on adaptive stochastic gradient algorithms in Euclidean spaces. However, analogous investigations on Riemannian manifolds are less developed, more restricted, and pose significant challenges due to the inherent non-linearity and lack of a standard coordinate system. In machine learning, many relevant manifolds are represented as matrices possessing row and column subspaces, which may also contain implicit manifold constraints. For instance, the Grassmann manifold comprises column subspaces. Therefore, algorithms developed for manifold optimization should preserve this rich structure instead of simply converting matrices into vectors. We introduce new stochastic gradient algorithms tailored for problems on Riemannian matrix manifolds, leveraging the row and column subspaces of gradients. We rigorously prove the convergence of our algorithms, demonstrating a convergence rate of order (\\log (T)/), where T represents the number of iterations. Experimental results validate the effectiveness of our proposed algorithms across various applications.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0887", "problem_id": "08870001", "content": "Video summarization poses a significant challenge in the field of computer vision, as it seeks to pinpoint key frames or sequences within extensive video inputs. This paper introduces a novel attention-based framework designed for video summarization, particularly suited for complex video data. Distinguishing itself from preceding research that solely applies attention mechanisms to frame correspondence, our proposed multi-concept video self-attention (MC-VSA) model identifies salient regions by leveraging both temporal and conceptual video features, thereby capturing contextual diversity across time and space to enhance summarization. By incorporating consistency between the original video and its summary, our framework can seamlessly handle both labeled and unlabeled data, rendering it highly suitable for real-world applications. Comprehensive experiments conducted on two benchmarks, as shown in Figure A, B, C, validate the efficacy of our model through quantitative and qualitative assessments, and establish its superiority over existing state-of-the-art methods, as discussed in References [citation].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0888", "problem_id": "08880001", "content": "Cutting-edge techniques approach pedestrian attribute recognition as a multi-label image classification challenge. Previous approaches often disregard or inadequately represent the spatial information related to person attributes by merely segmenting the entire body. In this study, we present a weakly-supervised framework for attribute localization. Utilizing GoogLeNet, we initially uncover a set of mid-level attribute features through specifically designed detection layers, where a max-pooling based weakly-supervised object detection method is employed to train these layers using only image-level labels, eliminating the necessity for bounding box annotations of pedestrian attributes. Subsequently, attribute labels are derived through the regression of the detection response magnitudes. Lastly, we infer the locations and approximate shapes of pedestrian attributes by clustering a combination of activation maps from the detection layers, with the fusion weights determined by the correlation strengths between each attribute and its associated mid-level features. Comprehensive experiments conducted on the two largest pedestrian attribute datasets, namely the PETA dataset and the RAP dataset, demonstrate that our proposed method performs competitively in attribute recognition compared to other leading approaches. Additionally, we visualize the results of attribute localization to better understand the features of our proposed methodology.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0889", "problem_id": "08890001", "content": "Graph convolutional networks (GCNs) represent a robust deep learning framework for analyzing graph-structured data, demonstrating exceptional performance across diverse real-world applications. However, existing GCN models are predominantly shallow due to the over-smoothing issue. This paper investigates the development and evaluation of deeper graph convolutional architectures, introducing GCNII, an enhanced version of the standard GCN that incorporates two straightforward yet impactful techniques: [technique 1] and [technique 2]. Both theoretical analysis and experimental results confirm that these methods mitigate over-smoothing effectively. Empirical evaluations reveal that the deep GCNII model surpasses leading approaches in semi-supervised and fully supervised tasks. Code is available at https://github.com/chennnM/GCNII.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0890", "problem_id": "08900001", "content": "The inherent acoustic attributes of objects can serve as valuable indicators for learning comprehensive representations, enabling effective object detection and tracking. By leveraging the synchronized occurrence of audiovisual events in video footage, it is possible to pinpoint objects within the visual field by monitoring environmental sounds alone. However, existing methods have been restricted to static camera setups and single-object detection, with limited robustness due to their reliance on RGB images, which are vulnerable to variations in illumination and weather. This study introduces the innovative MM-DistillNet framework, a self-supervised approach that utilizes multiple teacher networks incorporating diverse modalities, including RGB, depth, and thermal images, to extract complementary information and transfer knowledge to a single audio-based student network. A novel loss function, MTA, is proposed to facilitate self-supervised knowledge distillation from multimodal teachers, alongside a pretext task for the audio student that eliminates the need for manual annotations. The introduction of a large-scale multimodal dataset comprising over 113,000 synchronized frames across RGB, depth, thermal, and audio modalities enables extensive experimentation, demonstrating the superiority of the proposed approach over state-of-the-art methods in detecting multiple objects using sound alone, even in mobile settings, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0891", "problem_id": "08910001", "content": "This study introduces a novel approach to deriving informative representations of complex 3-dimensional objects, such as point clouds, leveraging deep generative architectures. Our method enables the acquisition of meaningful 3D shape representations, which can be utilized for various challenging tasks, including the generation, reconstruction, compression, and clustering of 3D points. Unlike existing methods that employ separate, decoupled models for representation learning and generation, our approach presents the first end-to-end solution, allowing for the simultaneous learning of a latent representation space and the generation of 3D shapes from it. Furthermore, our model is capable of learning compact, meaningful binary descriptors through adversarial training in the latent space. By extending the deep Adversarial Autoencoder (AAE) model to accommodate 3D inputs and outputs, we develop the 3D Adversarial Autoencoder (3dAAE), which yields either binary or continuous latent spaces that capture a significantly larger portion of the training data distribution due to our end-to-end training regime. The quantitative evaluation of 3dAAE demonstrates state-of-the-art performance in 3D points clustering and 3D object retrieval.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0892", "problem_id": "08920001", "content": "To mitigate global temperature increases, carbon capture and storage (CCS) plays a crucial role in decarbonizing the atmosphere, and a novel framework leveraging unsupervised learning has been developed to identify potential sites for long-term carbon dioxide storage by generating diverse subsurface geologic volumes. This approach employs generative adversarial networks to create geologic volumes, which are then refined using an additional neural network that samples the posterior distribution of a trained generator based on sparse physical measurements. Furthermore, these generative models are enhanced through Bayesian inversion, incorporating historic dynamic fluid flow data to improve the accuracy of forecasts regarding the storage capacity of injected carbon dioxide, ultimately informing the selection of suitable storage sites.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0893", "problem_id": "08930001", "content": "Accurate monitoring of lesion or tumor growth is crucial, and lesion segmentation on computed tomography (CT) scans plays a vital role in this process. However, manual segmentation is impractical due to its time-consuming and costly nature, as well as the requirement for specialized expertise. Currently, the response evaluation criteria in solid tumors (RECIST) is widely used as a surrogate, despite its limitations in providing detailed lesion information, and is readily available in hospitals' picture archiving and communication systems (PACS), making it a potential source of weak supervision for 2D lesion segmentation. This paper presents a weakly-supervised lesion segmentation approach using a convolutional neural network (CNN), which initializes lesion masks from RECIST measurements and refines them through co-segmentation, leveraging lesion similarities. Specifically, an attention-based co-segmentation model is employed to learn more distinctive features from image pairs. The proposed method is evaluated on the NIH DeepLesion dataset, yielding significant improvements in lesion segmentation performance, as evidenced by a 4.0% increase in Dice score (from 85.8% to 89.8%, Figure A, B, C) (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0894", "problem_id": "08940001", "content": "Infographics utilize textual, graphical, and visual components to efficiently convey information. This study investigates the automated interpretation of infographic images through Visual Question Answering. To achieve this, we introduce InfographicVQA, a novel dataset featuring a varied set of infographics accompanied by question-answer pairs in natural language. These questions necessitate integrated reasoning across document layout, text, graphical elements, and data visualizations. The dataset is designed to emphasize questions that demand fundamental reasoning and basic arithmetic abilities. We assess the performance of two robust baseline models, built upon current multi-modal VQA approaches, to define initial performance benchmarks for this novel task. The dataset, code, and leaderboard will be accessible at http://docvqa.org.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0895", "problem_id": "08950001", "content": "This brief paper addresses an optimized implementation for Tensorflow. The acceleration compared to the standard implementation is realized through the streamlining of the graph for both forward and backward passes.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0896", "problem_id": "08960001", "content": "Knowledge distillation (KD) is a popular technique in deep learning for image classification, where a student model's performance is enhanced using knowledge transferred from a teacher model. However, KD has been scarcely explored for image regression with scalar outputs, and no existing KD method is versatile enough to handle both classification and regression tasks. Furthermore, current KD methods often necessitate careful selection or modification of teacher and student architectures, which limits their practical scalability. Addressing these limitations, especially the common issue of limited labeled data in KD scenarios, this paper introduces a novel unified KD framework, cGAN-KD, based on conditional generative adversarial networks (cGANs). Unlike conventional KD approaches, cGAN-KD transfers knowledge through samples generated by cGANs, enabling its application to both classification and regression, ensuring compatibility with other KD methods, and reducing sensitivity to architectural choices. By leveraging advances in cGAN methodologies, along with custom subsampling and filtering, cGAN-KD exhibits strong performance even with limited labeled data. We also derive an error bound for student models trained with cGAN-KD, providing a theoretical justification for its effectiveness and practical implementation guidelines. Extensive experiments on CIFAR-10 and Tiny-ImageNet demonstrate that integrating state-of-the-art KD methods into cGAN-KD achieves superior results. Additional experiments on RC-49 and UTKFace confirm cGAN-KD's efficacy in image regression, a domain where current KD methods are not applicable.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0897", "problem_id": "08970001", "content": "This study focuses on enhancing the tracking of road users in urban environments by introducing a constraint programming (CP) solution for the data association stage within the tracking-by-detection framework of multiple object tracking (MOT). The CP approach offers greater efficiency in resolving data association compared to graph-based techniques and effectively manages the combinatorial complexity arising from multi-frame analysis. Concentrating solely on data association, our MOT technique employs basic image features—namely, detection centroids and colors per frame. Constraints are applied to these features and broader MOT challenges, such as maintaining color consistency across trajectories and limiting inter-frame motion. Pre-processing filters eliminate potential detections before CP application, while post-processing removes spurious trajectories generated by the solver. Evaluated on a motorized vehicle tracking dataset, our method achieves superior performance over leading approaches in the UA-DETRAC benchmark.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0898", "problem_id": "08980001", "content": "We propose a new category of localized atomic environment representations, derived from the Coulomb matrix, and integrate these with the Gaussian approximation potential framework to develop LC-GAP, a machine learning-based system for generating atomic potentials. The efficacy of LC-GAP is demonstrated through its application to the QM7, QM7b, and GDB9 biomolecular datasets, where it achieves chemical accuracy in predicting atomization energies for larger molecules beyond the training set, and also accurately predicts various other atomic properties for QM7b, consistent with recent literature. Notably, the optimal representation exhibits linear dimensionality with respect to the number of atoms in the local environment, thereby enhancing both predictive accuracy and computational efficiency compared to existing Coulomb matrix-based approaches.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0899", "problem_id": "08990001", "content": "The compression of neural networks has garnered significant interest recently due to the high computational demands of contemporary deep learning models. This study aims to facilitate the transfer of knowledge from a precise and complex model to a more compact counterpart. Our contributions are threefold: (i) we present an adversarial network compression method that enables the smaller student network to replicate the larger teacher network's behavior, eliminating the necessity for labels during the training process; (ii) we introduce a regularization method to avoid a trivially effective discriminator while maintaining the network’s capacity; and (iii) our technique is applicable across various teacher-student model configurations. Through comprehensive evaluations on five standard datasets, we demonstrate that our student network experiences minimal accuracy loss, outperforms other knowledge transfer methods, and exceeds the results of the same network trained with labeled data. Furthermore, we achieve state-of-the-art outcomes when compared to other compression techniques.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0900", "problem_id": "09000001", "content": "The goal of link prediction in knowledge graphs is to forecast missing links between entities, but existing approaches are often constrained to a transductive setting and struggle to handle unseen entities. Recently introduced subgraph-based models offer an alternative by predicting links based on the subgraph structure surrounding a candidate triplet, yet they rely heavily on extensive training data and perform suboptimally on relationships with limited triplets. To address this, we introduce Meta-iKG, a novel meta-learning framework for inductive relation reasoning that leverages local subgraphs to facilitate the transfer of subgraph-specific information and accelerate the learning of transferable patterns through meta gradients. This enables the model to rapidly adapt to relationships with only a few known facts in inductive settings. Furthermore, we enhance traditional meta-learning with a large-shot relation update procedure, allowing Meta-iKG to generalize effectively to both few-shot and large-shot relations. Our evaluation of Meta-iKG on inductive benchmarks from NELL and Freebase demonstrates its superiority over state-of-the-art methods in both few-shot scenarios and standard inductive settings, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0901", "problem_id": "09010001", "content": "This study investigates the application of Transfer Learning (TL) to create traffic flow prediction models in data-scarce environments. By leveraging existing high-quality predictive models through TL, it becomes possible to develop accurate models with limited datasets. The research examines three distinct scenarios of data absence, employing TL techniques alongside Deep Learning (DL) approaches for traffic forecasting. Conventional batch learning methods are evaluated against TL-based models using real-world traffic flow data obtained from loop sensors operated by the Madrid City Council (Spain). Furthermore, Online Learning (OL) techniques are implemented to continuously update models after each prediction, allowing adaptation to evolving traffic patterns and incremental learning from new data. The experimental findings highlight the benefits of combining transfer and online learning for traffic forecasting, while providing valuable insights into their effectiveness relative to the volume of training data available at the target location.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0902", "problem_id": "09020001", "content": "The identification of all relevant attributes, both strong and weak, is a crucial aspect of feature selection, particularly in time series classification and regression for industrial applications such as predictive maintenance and production line optimization, where multiple time series and meta-information are associated with each label or regression target. To address this challenge, we propose a scalable and efficient feature extraction algorithm that filters features early in the machine learning pipeline based on their significance to the classification or regression task, while also controlling the expected percentage of selected but irrelevant features. By combining established feature extraction methods with a feature importance filter, our algorithm achieves low computational complexity, enables initialization with limited domain knowledge, and can be easily parallelized, making it highly scalable and grounded in well-established non-parametric hypothesis tests. The effectiveness of our proposed algorithm is demonstrated through benchmarking on all binary classification problems in the UCR time series classification archive, as well as on time series data from a production line optimization project and simulated stochastic processes exhibiting qualitative changes in dynamics, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0903", "problem_id": "09030001", "content": "This study explores training a 3D object detector for novel object categories using only 2D bounding box annotations for these categories, concurrently leveraging knowledge from 3D bounding box annotations of existing categories. We introduce a transferable semi-supervised 3D object detection model designed to train a 3D object detector network using a dataset containing two distinct sets of object categories: strongly labeled categories with both 2D and 3D bounding box labels, and weakly labeled categories with only 2D bounding box labels. Specifically, we propose a relaxed reprojection loss, a box prior loss, and a Box-to-Point Cloud Fit network to facilitate the effective transfer of 3D information from the strongly labeled categories to the weakly labeled categories during training, thereby enabling 3D object detection in the weakly labeled categories during inference. Empirical evaluations demonstrate that our proposed method surpasses baseline methods and achieves competitive performance compared to fully-supervised methods on the SUN-RGBD and KITTI datasets. Additionally, results indicate that our Box-to-Point Cloud Fit network enhances the performance of fully-supervised methods on both datasets.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0904", "problem_id": "09040001", "content": "This research addresses the challenge of robust zero-shot planning in dynamic stochastic environments. We examine time-varying Markov Decision Processes (MDPs) and analyze Model-Based Reinforcement Learning approaches under these conditions. Our framework rests on two key assumptions: 1) the environment changes continuously with a limited rate of variation; 2) the current model is available at each decision point, though its future progression remains unknown. Our contributions are fourfold: 1) we formalize a specialized category of MDPs termed Non-Stationary MDPs (NSMDPs), introducing the concept of regular evolution through Lipschitz-Continuity assumptions for transition and reward functions over time; 2) we investigate a planning agent that utilizes the current environmental model without knowledge of its future changes, prompting a worst-case analysis where the environment acts adversarially; 3) building on this perspective, we develop the Risk-Averse Tree-Search (RATS) algorithm, a zero-shot Model-Based technique analogous to Minimax search; 4) we empirically demonstrate RATS' advantages and benchmark its performance against established Model-Based algorithms.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0905", "problem_id": "09050001", "content": "Recent advancements in deep learning models have surpassed the performance of leading seasonal forecasting techniques, particularly in predicting the El Niño-Southern Oscillation (ENSO). Nevertheless, existing deep learning models, which primarily utilize convolutional neural networks, face challenges in interpretability and may struggle to capture extensive atmospheric patterns. In contrast, graph neural networks (GNNs) excel at representing large-scale spatial dependencies and offer enhanced interpretability through their explicit modeling of information flow via edge connections. We introduce the inaugural use of GNNs in seasonal forecasting, accompanied by a unique graph connectivity learning module that allows our GNN model to simultaneously learn substantial spatial interactions alongside the ENSO forecasting objective. Our model, \\graphino, demonstrates superior performance compared to cutting-edge deep learning models for forecasts extending up to six months. Moreover, we provide evidence that our model is more interpretable, as it identifies logical connectivity structures that align with the ENSO anomaly pattern.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0906", "problem_id": "09060001", "content": "Recently, sampling techniques have been effectively utilized to improve the quality of samples produced by Generative Adversarial Networks (GANs). Nevertheless, they often exhibit low sample efficiency in real-world applications due to the independent nature of the proposal sampling from the generator. In this study, we introduce REP-GAN, an innovative sampling technique that facilitates general dependent proposals by reparameterizing the Markov chains within the generator's latent space. We provide theoretical evidence that our reparameterized proposal supports a closed-form Metropolis-Hastings acceptance ratio. Through comprehensive experiments conducted on both synthetic and real datasets, we illustrate that REP-GAN significantly enhances sample efficiency while simultaneously achieving superior sample quality.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0907", "problem_id": "09070001", "content": "In numerous real-world datasets derived from social networks, complex higher-order relationships exist among data points that cannot be adequately represented using traditional graph modeling techniques. To address this limitation, utilizing hypergraphs, which provide a more general framework, is a logical approach to modeling such social networks. This paper presents a novel geometric framework for hyperedges in hypergraphs, enabling the capture of intricate higher-order relationships among data points. Additionally, building upon this geometric foundation, we propose a new methodological approach - the nearest neighbors method in hypergraphs - designed to facilitate the analysis of sociological datasets, as seen in Figure A, B, C (References: [1], [2], [3]; citations: [4], [5]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0908", "problem_id": "09080001", "content": "The analysis of extensive geophysical observational data gathered from a range of sophisticated sensors on different satellite platforms enhances our comprehension of the geophysical system. For example, convolutional neural networks (CNN) have proven effective in estimating the intensity of tropical cyclones (TC) using satellite data with a fixed time interval (e.g., every 3 hours). However, to obtain more prompt (under 30 minutes) and precise TC intensity evaluations, a deep learning model is necessary to accommodate temporally diverse satellite observations. Specifically, infrared (IR1) and water vapor (WV) imagery is accessible every 15 minutes, whereas passive microwave rain rate (PMW) data is available approximately every 3 hours. Additionally, the visible (VIS) channel is significantly impacted by noise and sunlight, complicating its application. Consequently, we introduce an innovative framework that integrates a generative adversarial network (GAN) with CNN. This model makes use of all available data, including VIS and PMW, during the training stage, but relies solely on the high-frequency IR1 and WV data to produce intensity estimates during the prediction stage. Experimental findings indicate that the hybrid GAN-CNN framework achieves accuracy on par with the leading models while enhancing the maximum estimation frequency from 3 hours to less than 15 minutes.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0909", "problem_id": "09090001", "content": "Semantic segmentation map editing has emerged as a versatile intermediary for image generation, offering robust support across diverse tasks. This study focuses on enhancing the quality of edited segmentation maps based on semantic inputs. While existing approaches widely employ global and local adversarial losses to improve image generation quality, they often encounter boundary misalignment issues within mask regions. To overcome this challenge, we introduce MExGAN, a framework for semantic segmentation map editing that incorporates a novel Multi-Expansion (MEx) loss, implemented through adversarial losses on MEx regions. These regions predominantly consist of generated mask areas while including minority boundary sections from the original context. For improved efficiency and stability, we also propose an Approximated MEx (A-MEx) loss. Unlike prior models that use partial images for training data construction—resulting in performance limitations—MExGAN utilizes complete images. Comprehensive evaluations on semantic segmentation map editing and natural image inpainting demonstrate competitive performance across four datasets.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0910", "problem_id": "09100001", "content": "Deep reinforcement learning (DRL) applied to Markov decision processes (MDPs) with continuous action spaces typically involves training parametric policies by optimizing estimated policy gradients (PGs). Studies indicate that the effectiveness of PG-based algorithms is significantly influenced by the bias-variance tradeoffs inherent in PG estimation and utilization. One strategy to optimize this tradeoff combines on-policy and off-policy gradient estimations, though current PG merging techniques often suffer from low sample efficiency and incompatibility with deterministic policy training. To overcome these limitations, this work presents elite PGs, enhancing their variance reduction through elitism and policy consolidation, which leverages behavioral insights from high-performing trajectories. Additionally, we introduce a two-step approach for integrating elite PGs with conventional PGs, extending traditional interpolation merging. Theoretical and empirical analyses demonstrate that both two-step and interpolation merging adjust bias-variance tradeoffs in policy training, allowing efficient utilization of elite PGs while minimizing their performance effects. Experimental results further reveal that two-step merging surpasses interpolation merging and leading algorithms across six benchmark control tasks.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0911", "problem_id": "09110001", "content": "Research in scene text recognition has been largely motivated by the goal of achieving superior performance on public benchmarks, leading to significant advancements. Yet, a detailed analysis uncovers a concerning trend: while current state-of-the-art methods excel at recognizing words within their vocabulary, they struggle with out-of-vocabulary terms—a phenomenon termed \"vocabulary reliance.\" This paper introduces an analytical framework to thoroughly examine vocabulary reliance in scene text recognition, revealing several critical insights: (1) All existing methods display some degree of vocabulary reliance; (2) Attention-based decoders generalize poorly to unfamiliar words, whereas segmentation-based decoders effectively leverage visual features; (3) Context modeling is closely intertwined with prediction layers. These observations offer valuable guidance for future research. Additionally, we present a straightforward yet efficient mutual learning approach that enables collaborative training between attention-based and segmentation-based models, mitigating vocabulary reliance and enhancing overall recognition accuracy.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0912", "problem_id": "09120001", "content": "The task of responding to questions that take into account multiple forms of context poses significant challenges, as it necessitates the seamless fusion of diverse data sources. Current methods, however, only facilitate limited interactions between these sources within a single attention hop. This paper introduces the Holistic Multi-modal Memory Network (HMMN) framework, which comprehensively accounts for the interplay between various input sources, including multi-modal context and question, at each hop, and also incorporates answer choices during the context retrieval phase. By doing so, the proposed framework successfully consolidates multi-modal context, question, and answer information, resulting in the retrieval of more informative context for question answering. The HMMN framework yields state-of-the-art accuracy on the MovieQA dataset, as evidenced by Figure A, and extensive ablation studies, such as those presented in Figure B and Figure C, underscore the crucial role of holistic reasoning and the contributions of different attention strategies, as discussed in References [citation].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0913", "problem_id": "09130001", "content": "Deep neural networks for segmentation typically demand extensive pixel-level annotations, which are labor-intensive to produce. This study introduces a multi-task learning approach to mitigate this requirement by framing segmentation as a series of progressively refined approximation subproblems. The proposed framework integrates three components: 1) a segmentation task utilizing pixel-level ground truth masks for a limited subset of images, 2) a recursive approximation task that refines partial object regions through data-driven mask evolution, and 3) auxiliary tasks leveraging sparse annotations to enhance feature learning. Training primarily relies on rough partial masks—lacking precise boundaries—rather than complete segmentations. The approximation task progressively extends these partial regions toward object boundaries by leveraging insights from the segmentation task in a data-driven manner. The method is trained with minimal precisely labeled images and abundant coarse annotations, enabling cost-effective data collection. Its effectiveness is validated across applications involving microscopy and ultrasound images.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0914", "problem_id": "09140001", "content": "One of the simplest classifiers available in machine learning is the Nearest Neighbour Classifier, which classifies data by locating the closest neighbours to a given query and using them to ascertain the class of the query. This classification method is especially relevant today, as the concerns regarding poor run-time performance have diminished due to the enhanced computational capabilities available. This paper provides a comprehensive examination of Nearest Neighbour classification techniques, emphasizing similarity assessment (distance), challenges in finding nearest neighbours, and methods for reducing data dimensionality. This document represents the second edition of a previously published technical report, incorporating new sections on similarity measures for time-series, retrieval speed enhancements, and intrinsic dimensionality. Additionally, an Appendix is provided that grants access to Python code for the essential methods discussed.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0915", "problem_id": "09150001", "content": "Time series classification plays a critical role in data analysis and often serves as a foundation for subsequent tasks. Existing approaches predominantly rely on either shape-based classification with distance metrics or feature-based methods after identifying domain-specific features. However, it is frequently overlooked that certain classes are more effectively distinguished using features, while others benefit from shape-based discrimination. Consequently, selecting one approach over the other inevitably leads to suboptimal performance for some classes. This study introduces a novel time series classification model that dynamically incorporates both shape and feature-based measures as needed. Our method autonomously determines the optimal approach for each class and selects the most reliable classifier during query processing. Empirical evaluation on real-world datasets confirms that our approach achieves statistically significant improvements in classification accuracy.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0916", "problem_id": "09160001", "content": "Although GANs have proven effective for generating realistic images, their potential for non-synthesis tasks remains largely unexplored. This study investigates whether GANs learn meaningful object structures while attempting to replicate them and introduces a straightforward yet efficient GAN-based method for semantic part segmentation, requiring only one labeled example alongside an unlabeled dataset. The approach involves extracting pixel-wise representations from a trained GAN and using them as feature vectors for segmentation. Experiments reveal that GAN-derived representations are inherently discriminative, yielding results on par with supervised baselines that rely on far more labeled data. This innovative application of GANs suggests a broader framework for unsupervised representation learning with potential uses across various tasks. Additional findings can be found at https://repurposegans.github.io/.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0917", "problem_id": "09170001", "content": "As 3D surface data acquisition systems become increasingly widespread, reliably identifying particular shapes from collected data has grown in significance. Statistical shape models, trained on databases of pristine shape instances, enable this extraction even amid noise and occlusions. This paper examines, evaluates, and contrasts various statistical models, ranging from those assessing geometric variation globally to those focusing on local variations. We begin by surveying existing applications of these models in prior research, then formally define and theoretically analyze them, considering both statistical and computational dimensions. Subsequently, we conduct comprehensive experimental comparisons for model fitting tasks and provide insights into which model types are more suitable for specific applications. Leveraging the abundance of high-quality datasets, we focus on extracting human faces from distorted data as our primary case study.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0918", "problem_id": "09180001", "content": "Neural networks, adept at precisely modeling the functional relationship between a physical system's inputs and outputs, are increasingly used for surrogate modeling in scientific fields. Because these networks possess a large number of parameters, substantial training data is typically needed. To mitigate overfitting and enhance generalization, regularization techniques, such as \\ell_1- and \\ell_2-norm penalties on the parameters, are commonly employed, along with pruning connections to increase network sparsity. This paper investigates the impact of sparsity-inducing \\ell_1-regularization on neural network training when limited high-fidelity data is available. Addressing the limitations of standard \\ell_1-regularization, two enhanced \\ell_1-regularization methods are examined, incorporating parameter information from an identical network trained on lower-fidelity model data. These bi-fidelity approaches extend transfer learning, leveraging knowledge from extensive low-fidelity datasets to improve training efficiency with limited high-fidelity data. These strategies are compared against two \\ell_1-regularization techniques that rely solely on the high-fidelity dataset. The effectiveness of the proposed bi-fidelity \\ell_1-regularization methods is demonstrated through three numerical examples involving uncertainty propagation in physical systems, where they achieve errors an order of magnitude smaller than networks trained exclusively on high-fidelity data.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0919", "problem_id": "09190001", "content": "A novel approach to object localization is introduced, leveraging pose estimation and camera calibration techniques. By capturing multiple 2D images of the object, 3D coordinates are estimated and subsequently used to calibrate the camera, involving calculations of intrinsic and extrinsic parameters to correct for lens distortion, determine object size, and calculate camera position. A transformation strategy is also outlined for estimating 3D pose from 2D images. The methodology is implemented in MATLAB, with validation experiments conducted to assess the accuracy of both pose estimation and camera calibration, as presented in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0920", "problem_id": "09200001", "content": "Vehicle re-identification (ReID) is increasingly important for autonomous driving perception systems. However, a comprehensive solution for vehicle-mounted surround-view systems is currently lacking. This paper addresses two key challenges: (i) the difficulty of identifying vehicles across time in single-camera views due to fisheye distortion, occlusion, and truncation, and (ii) significant appearance variations in multi-camera views of the same vehicle. To overcome these issues, we introduce a complete vehicle Re-ID solution. Our approach includes a novel quality evaluation mechanism that balances tracking box drift and target consistency. Furthermore, we leverage an attention-based Re-ID network coupled with a spatial constraint strategy to improve performance across different cameras. Experimental results demonstrate that our solution achieves state-of-the-art accuracy in real-time. The code and an annotated fisheye dataset will be released to benefit the research community.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0921", "problem_id": "09210001", "content": "Deep learning is widely applied in various fields of computer vision, typically necessitating extensive datasets for initial training followed by fine-tuning for smaller-scale tasks. Action recognition is among the many domains benefiting from deep learning techniques. Although Convolutional Neural Networks are commonly used with RGB and optical flow frames, recurrent networks like LSTM are often employed for training on temporal sequences of 3D skeletal joint data. This paper introduces a novel approach that transforms sequences of 3D skeleton joints into texture-like representations using mathematically grounded kernel methods. These representations serve as the initial layer in a conventional CNN, such as ResNet-50, and are integrated into a supervised domain adaptation framework to transfer knowledge from a source to a target dataset. By exploiting Kinect-based data beyond single-dataset training, our method surpasses basic fine-tuning approaches that naively combine datasets. Specifically, we leverage overlapping classes across datasets, linking data points of the same class through supervised domain adaptation principles. Our approach achieves state-of-the-art performance on three publicly available benchmarks.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0922", "problem_id": "09220001", "content": "Recent advances in facial image manipulation have been substantial, yet existing methods are limited by their reliance on predefined face attributes or their inability to allow users to interactively edit images. To address these limitations, this study introduces MaskGAN, a novel framework designed to facilitate diverse and interactive face manipulation. The core idea underlying MaskGAN is the utilization of semantic masks as an intermediate representation, enabling flexible and faithful face manipulation. This framework consists of two primary components: the Dense Mapping Network (DMN) and Editing Behavior Simulated Training (EBST). The DMN learns to map styles between user-modified masks and target images, allowing for diverse generation outcomes, while EBST simulates user editing behaviors on source masks, enhancing the framework's robustness to various manipulated inputs through the introduction of dual-editing consistency as an auxiliary supervision signal. A large-scale, high-resolution face dataset, CelebAMask-HQ, with fine-grained mask annotations, is constructed to support extensive research. Comprehensive evaluations of MaskGAN on attribute transfer and style copy tasks demonstrate its superior performance compared to state-of-the-art methods, with the code, models, and dataset available at https://github.com/switchablenorms/CelebAMask-HQ.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0923", "problem_id": "09230001", "content": "The application of modern adiabatic quantum computers (AQC) has already been established in tackling complex combinatorial optimisation problems across multiple scientific disciplines. However, their use in computer vision has been relatively limited, with only a handful of demonstrations to date. This study provides an overview of AQC and proposes a novel algorithm designed to address correspondence problems in point sets, which can be effectively executed on AQC platforms. Notably, the algorithm achieves a subquadratic computational complexity during the state preparation phase. The experimental evaluation, conducted through simulated sampling, yields successful outcomes in transformation estimation and point set alignment, as illustrated in Figure A, B, C (as seen in References [citation]). A comparative analysis of the solutions and their corresponding energy values is also presented, offering insights into the differences between them.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0924", "problem_id": "09240001", "content": "The registration of multimodal images, including optical, LiDAR, SAR, and map data, has long been a daunting task in the remote sensing community due to the substantial nonlinear intensity differences between these data types. This paper introduces a rapid and robust matching framework that leverages local descriptors to facilitate multimodal registration. The framework begins by extracting local descriptors, such as Histogram of Oriented Gradient (HOG), Local Self Similarity (LSS), or Speeded-Up Robust Feature (SURF), at each pixel to create a pixel-wise feature representation of the image. A similarity measure is then defined based on this feature representation in the frequency domain using the 3 Dimensional Fast Fourier Transform (3DFFT) technique, which is followed by a template matching scheme to identify control points between images. Additionally, a novel pixel-wise feature representation, termed channel features of orientated gradients (CFOG), is proposed, which extends the traditional pixel-wise HOG descriptors and exhibits superior matching performance and computational efficiency. The key benefits of the proposed framework include its ability to represent structural similarity using pixel-wise feature descriptions and its high computational efficiency, courtesy of the 3DFFT technique. Experimental results on various types of multimodal images demonstrate the proposed framework's superior matching performance compared to state-of-the-art methods, as seen in Figure A, B, C, and cited in References [citation]. The framework has been successfully integrated into the software products of a Chinese listed company, and the MATLAB code is available in this manuscript.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0925", "problem_id": "09250001", "content": "Feature learning is essential for addressing complex challenges in areas such as speech, computer vision, and natural language processing. This paper introduces an innovative category of matrix and tensor-valued features that can be pre-trained with unlabeled data. We propose effective algorithms for retrieving discriminative information based on these pre-trained features and labeled data for related tasks. Our features derive from higher-order score functions, which reflect local fluctuations in the probability density function of the input data. We provide a theoretical framework to delineate the nature of the discriminative information that can be gathered from score-function features when combined with labeled data. Furthermore, we utilize efficient spectral decomposition techniques (applied to both matrices and tensors) to extract the discriminative elements. The benefit of using tensor-valued features lies in their ability to capture more comprehensive discriminative information through overcomplete representations. Consequently, we introduce a new framework for leveraging generative models of the input in the context of discriminative learning.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0926", "problem_id": "09260001", "content": "Weight sharing is a popular technique for accelerating architecture performance estimation by constructing a supernet encompassing all architectures as submodels, thereby avoiding individual training. However, the actual benefits of weight sharing for Neural Architecture Search (NAS) remain debated due to discrepancies between supernet optimization and NAS objectives. To investigate this, we present an extensive analysis across five search spaces: NAS-Bench-101, NAS-Bench-201, DARTS-CIFAR10, DARTS-PTB, and ProxylessNAS. Our findings indicate that weight sharing is effective in certain search spaces but not universally. Furthermore, we pinpoint biases influencing this behavior and the inherent capabilities of weight sharing. This research aims to guide future NAS studies in effectively harnessing the advantages of weight sharing.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0927", "problem_id": "09270001", "content": "The susceptibility of deep neural networks to adversarial examples has sparked a surge in research on adversarial attacks and defenses in recent years. Nevertheless, certain assumptions about adversarial attacks and object detection methods have become commonplace in the field. This study offers a novel perspective by investigating the effects of universal perturbations on object detection at the class level, with a focus on a meticulously curated dataset related to autonomous driving. Using the Faster-RCNN object detector, we examine images from five categories - person, car, truck, stop sign, and traffic light - from the COCO data set, applying careful perturbations via the Universal Dense Object Suppression algorithm. Our findings reveal a resilience ranking of these categories to universal perturbations, from most to least resilient: person, car, traffic light, truck, and stop sign. To our knowledge, this is the first instance of such a ranking being established, which holds significant implications for the security of autonomous vehicle datasets and object detection in general.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0928", "problem_id": "09280001", "content": "The development of deep learning models is heavily dependent on the availability of large-scale annotated datasets, which inevitably fall short of capturing the vast variability of real-world scenarios, thereby limiting the neural networks' capacity to generalize due to the constrained visual and semantic information present in their training data. This thesis emphasizes the importance of designing deep learning architectures capable of operating effectively in unfamiliar visual domains and recognizing novel semantic concepts. The first part of this thesis explores strategies for enabling deep models to generalize across new visual domains by leveraging knowledge transfer from labeled source domains to unlabeled target domains, demonstrating the applicability of batch-normalization variants to various scenarios, including domain adaptation, domain generalization, continuous domain adaptation, and predictive domain adaptation, where target domain information is only available through metadata. The second part focuses on extending the knowledge of pre-trained deep models to novel semantic concepts without requiring access to the original training set, addressing scenarios such as sequential multi-task learning, open-world recognition, and incremental class learning in semantic segmentation, while also mitigating the issue of semantic shift in the background class. Ultimately, this thesis tackles the challenging problem of developing a model that can recognize images of unseen concepts in unseen domains, given images from multiple domains and semantic categories, and proposes a novel approach based on domain and semantic mixing of inputs and features, representing a promising initial step towards resolving this complex issue.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0929", "problem_id": "09290001", "content": "Research on generative adversarial networks (GANs) has experienced rapid growth over the past few years, with significant contributions primarily in the computer vision domain, particularly in the generation of realistic images and videos. However, the application of GANs has expanded beyond computer vision to other areas, including time series and sequence generation, which has emerged as a new and promising area of research. As efforts continue to develop high-quality, diverse, and private time series data, this paper provides a comprehensive review of GAN variants tailored for time series applications, introducing a taxonomy that categorizes these models into discrete-variant GANs and continuous-variant GANs for handling discrete and continuous time series data, respectively. The review encompasses the latest literature, including architectures, results, and applications, as well as an examination of popular evaluation metrics and their applicability across different use cases. Additionally, the paper discusses privacy considerations and potential safeguards for protecting sensitive data, with the goal of providing a clear and concise overview of the current state-of-the-art research in this field and its potential applications in real-world technologies.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0930", "problem_id": "09300001", "content": "Deep neural networks often produce representations that are rich in information yet unclear regarding the specific information they encode. We present a probabilistic modeling approach that develops two distinct deep representations: an invariant representation reflecting the class information of the data and an equivariant representation capturing the symmetry transformation that characterizes the specific data point within the class manifold (equivariant in the sense that it changes naturally with symmetry transformations). This method relies mainly on the strategic routing of data through these two latent variables, making it conceptually clear, straightforward to implement, and broadly applicable to any data consisting of discrete classes of continuous distributions (such as objects in images, topics in language, or individuals in behavioral data). Our approach demonstrates qualitatively convincing representation learning and competitive quantitative performance in both supervised and semi-supervised contexts compared to similar modeling techniques in the literature, requiring minimal fine-tuning.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0931", "problem_id": "09310001", "content": "Deep learning techniques have made substantial advancements in various tasks, including classification, detection, and segmentation. Ensemble learning, which combines multiple complementary models, is a well-established method for further enhancing performance. While ensemble learning is readily applicable to classification tasks through methods such as averaging or voting, its application to tasks like object detection is more challenging due to the variable quantity and complexity of outputs. To address this limitation, this paper introduces Predictive Ensemble Learning (PEL), a novel approach that leverages the predictive capabilities of deep neural networks to directly identify the top-performing model from a set of base models for each test example, effectively reducing ensemble learning to a traditional classification problem. The effectiveness of PEL is demonstrated through its application to scene text detection, a task for which existing ensemble learning strategies are inadequate, resulting in significant performance improvements over individual state-of-the-art models and non-maximum suppression-based model fusion. The experimental results highlight the potential of PEL to predict model performance based on a query example, paving the way for its extension to ensemble learning in various complex tasks, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0932", "problem_id": "09320001", "content": "Offline reinforcement learning involves training a policy to optimize cumulative rewards using a predetermined dataset. Existing conservative strategies typically involve either regularizing the behavior policy or estimating a lower bound for the value function. However, excessive conservatism can hinder the policy's ability to generalize, especially when dealing with heterogeneous datasets, leading to performance deterioration. To mitigate this, we introduce a reinforcement learning approach designed to reduce conservativeness. Our method trains the policy to focus more on under-represented samples within the static dataset, thereby addressing data imbalance. Simultaneously, we derive a more precise lower bound for the value function compared to prior techniques, enabling the discovery of potentially optimal actions. As a result, our method effectively manages the skewed distribution inherent in the dataset and produces a value function that more closely approximates the true expected value function. Empirical evaluations demonstrate that our proposed method surpasses state-of-the-art techniques in established D4RL offline reinforcement learning benchmarks and custom-designed mixed datasets.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0933", "problem_id": "09330001", "content": "Seed phenotyping involves evaluating the morphological traits of seeds to forecast their growth patterns, resilience, and productivity across diverse environmental settings. This study explores the implementation and practicality of advanced object detection and segmentation models, specifically Mask R-CNN and YOLO (You Only Look Once), for seed phenotyping using Tensorflow. A significant challenge in this approach is the necessity for extensive training datasets. Although acquiring numerous seed images is demanding, these images must also be annotated to outline seed boundaries and formatted for neural network processing. While free annotation tools exist, manual annotation is highly time-consuming. To address this issue, domain randomization—leveraging models trained on synthetic images for real-world applications—is examined. Furthermore, transfer learning, which applies insights from one problem to another, is employed, with networks initialized using pre-trained weights from ImageNet and COCO datasets. Experiments involving various parameters are conducted on five seed varieties: canola, rough rice, sorghum, soy, and wheat.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0934", "problem_id": "09340001", "content": "Video comprehension requires a model to capture the dynamic relationship between static visual elements and their temporal evolution: The model should predict potential future developments from a single frame while also decomposing a video into its initial static content and supplementary dynamic features absent in the first frame. This implies a bidirectional correspondence between video sequences and their constituent static imagery plus residual data. Unlike conventional stochastic image-to-video generation, this framework establishes a deterministic mapping between residual vectors and video sequences, producing varied outputs only when sampling from the residual space. The method employs a conditional invertible neural network (cINN) to separately model static and dynamic video components, enabling precise video generation. Evaluations across four distinct video datasets confirm the approach’s efficacy in producing high-quality, diverse synthetic videos. Our project page is available at https://bit.ly/3t66bnU.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0935", "problem_id": "09350001", "content": "The slope difference distribution (SDD) is calculated for a one-dimensional curve, demonstrating robustness in determining the partitioning point for logical curve separation, as well as in calculating the clustering center of each separated curve segment. Initially proposed for image segmentation, SDD has shown superior performance compared to existing methods, with comparative Matlab codes available on Matlab Central for verification. Furthermore, the contour of an object exhibits similarities to its histogram, making feature detection via SDD from the object contour a viable approach. This work defines SDD features, which provide a sparse representation of the object contour, and utilizes these features to construct a reference model for each object, enabling model matching for online object recognition. The experimental results are highly promising, with SDD achieving 100% accuracy in gesture recognition for the NUS dataset and the near-infrared dataset, as well as 100% accuracy in object recognition for the Kimia 99 dataset, as shown in Figure A, B, C (see References [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0936", "problem_id": "09360001", "content": "Many real-world classification problems exhibit example-dependent cost-sensitive characteristics, where misclassification costs vary across individual examples, not just between classes. Traditional classification approaches overlook these varying costs, instead assuming a uniform cost for misclassification errors. Previous research has introduced methods that incorporate financial costs into algorithm training, with the example-dependent cost-sensitive decision tree algorithm yielding the most significant savings. This paper presents a novel framework for ensembles of example-dependent cost-sensitive decision trees, which involves generating multiple trees from random subsamples of the training data and combining them using three distinct approaches, including two newly proposed cost-sensitive methods: cost-sensitive weighted voting and cost-sensitive stacking, the latter of which is based on cost-sensitive logistic regression. The proposed framework is evaluated against state-of-the-art example-dependent cost-sensitive techniques, including cost-proportionate sampling, Bayes minimum risk, and cost-sensitive decision trees, using five databases from four real-world applications: credit card fraud detection, churn modeling, credit scoring, and direct marketing, with the results demonstrating that the proposed algorithms achieve higher savings across all databases, as shown in Figure A, B, C (References [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0937", "problem_id": "09370001", "content": "Neural network and machine learning models are gaining increasing traction in space physics research, particularly in predicting geomagnetic indices. Traditional evaluation metrics like root-mean-square error (RMSE) and Pearson correlation coefficient are commonly used, yet they often overlook critical behavioral patterns. To demonstrate these limitations, we developed a long short-term memory neural network to forecast the disturbance storm time index at time t with a 1- to 6-hour horizon, trained on OMNIWeb data. While correlation coefficient and RMSE suggested performance on par with recent studies, visual analysis revealed that the predictions resembled those of a persistence model. This study introduces a novel approach to detect temporal shifts between time series, such as persistence model outputs versus observations. The proposed method, leveraging Dynamical Time Warping, effectively identifies persistence-like behavior and validates visual assessments of the neural network's predictions. Additionally, alternative training strategies are investigated to mitigate persistence effects in the model's outputs.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0938", "problem_id": "09380001", "content": "Visual tracking is susceptible to interference from similar background elements. These challenging distractors, despite their relative scarcity among negative samples, significantly contribute to target drift and model degradation, necessitating focused attention during online tracking and model updating. To improve tracking reliability, we introduce a cascaded regression tracker comprising two successive phases. Initially, a computationally efficient convolutional regression is employed to eliminate a large number of easily distinguishable negative candidates. Subsequently, a ridge regression based on discrete sampling is implemented to rigorously examine the remaining difficult samples, acting as a substitute for fully-connected layers and leveraging its closed-form solution for rapid learning. Comprehensive evaluations performed on 11 demanding tracking datasets, including OTB-2013, OTB-2015, VOT2018, VOT2019, UAV123, Temple-Color, NfS, TrackingNet, LaSOT, UAV20L, and OxUvA, demonstrate that the proposed approach attains state-of-the-art results on common benchmarks, while maintaining real-time operational speed.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0939", "problem_id": "09390001", "content": "This paper investigates the often overlooked, yet significant, impact of point sampling techniques within point cloud Generative Adversarial Networks (GANs). Empirical evaluations reveal that discriminators with low sensitivity to sampling methods (e.g., PointNet-Max) generate shape point clouds exhibiting point clustering artifacts. Conversely, discriminators highly sensitive to sampling (e.g., PointNet++, DGCNN) struggle to facilitate the creation of valid shapes. To characterize the diverse sampling sensitivities of discriminators, we introduce the concept of a sampling spectrum. Furthermore, we analyze how various evaluation metrics balance the influence of sampling patterns against geometric fidelity, and we propose several perceptual metrics that constitute a sampling spectrum of metrics. Utilizing the defined sampling spectrum, we identify a balanced, sampling-aware baseline discriminator, PointNet-Mix, which substantially enhances existing point cloud generators across sampling-related metrics. Our findings suggest that discriminator design, rather than generator design, represents the primary limitation in point cloud GAN performance, contrary to the focus of recent studies. This research offers recommendations and resources for developing discriminators in future work. The code will be released to promote further investigation.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0940", "problem_id": "09400001", "content": "Numerous real-world applications, such as human activity recognition (HAR) within IoT, can be structured as a multi-task multi-view learning challenge. Each individual task encompasses multiple common feature views obtained from various sources, which may be either similar or different. A common technique in recent methodologies is to implement a conventional hard/soft sharing strategy during the initial phase for each view across tasks to reveal shared knowledge, based on the premise that all views are conditionally independent. However, multiple views across tasks may indeed be interrelated in practice. Additionally, traditional supervised approaches may fall short when labeled data is limited. To address these issues, we propose a new framework, ASM2TV, for semi-supervised multi-task multi-view learning. We introduce a novel concept called the gating control policy, a learnable task-view-interacted sharing policy that dynamically identifies the most suitable candidate shared block for any view across tasks, thereby revealing more nuanced task-view interactions and enhancing inference efficiency. Notably, our proposed gathering consistency adaptation process fully leverages extensive amounts of unlabeled fragmented time-series data, rendering it a versatile framework applicable to a broad array of scenarios. Experiments conducted on two distinct real-world HAR benchmark datasets gathered from various subjects and sources showcase the superiority of our framework compared to existing state-of-the-art solutions.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0941", "problem_id": "09410001", "content": "In computer vision, local windows are commonly employed, typically with their centers aligned to the processed pixels. However, we demonstrate that this standard practice is not always optimal. Centering the window on a pixel situated on an edge is a primary source of edge blurring in many filtering algorithms. Addressing this, we introduce a novel Side Window Filtering (SWF) method that aligns the window's side or corner with the processed pixel. The SWF technique, while straightforward, is theoretically sound and practically efficient. We illustrate how numerous conventional linear and nonlinear filters can be readily implemented within the SWF framework. Comprehensive evaluations demonstrate that incorporating the SWF principle substantially enhances edge preservation, achieving state-of-the-art results in image smoothing, denoising, enhancement, structure-preserving texture-removing, mutual-structure extraction, and HDR tone mapping. Beyond image filtering, we extend the SWF principle to other applications utilizing local windows, using colorization by optimization as an example to show that SWF effectively mitigates artifacts like color leakage associated with traditional implementations. Considering the widespread use of window-based operations in computer vision, the SWF technique holds the potential to improve a wide array of applications.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0942", "problem_id": "09420001", "content": "The recent achievements of NeRF and similar implicit neural representation techniques have paved a new way for continuous image representation, allowing pixel values to be inferred from neural network models within a continuous spatial domain rather than being retrieved from stored discrete 2D arrays. While the recent study LIIF has shown that this innovative method can perform well in arbitrary-scale super-resolution tasks, the resulting upscaled images often suffer from structural distortions due to inaccurate predictions of high-frequency textures. In response, we introduce UltraSR, a straightforward yet powerful new network architecture that integrates spatial coordinates and periodic encoding with implicit neural representation. Through thorough experimentation and ablation studies, we demonstrate that spatial encoding is a crucial element needed for achieving high-accuracy implicit image functions. UltraSR establishes new state-of-the-art results on the DIV2K benchmark across all super-resolution scales when compared to previous best-performing methods. Additionally, it shows exceptional performance on other standard benchmark datasets, surpassing earlier approaches in nearly all tests. Our code will be available at https://github.com/SHI-Labs/UltraSR-Arbitrary-Scale-Super-Resolution.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-0943", "problem_id": "09430001", "content": "The fundamental challenge in machine learning revolves around the ability of learned models to generalize to new, unseen data, a problem that remains poorly understood, particularly for over-parameterized neural network models that dominate the field. The complexity of this issue is exacerbated by the non-convex nature of the underlying learning problems, making it difficult to comprehensively assess the generalization capabilities of these models. To address this knowledge gap, we propose a comprehensive framework for characterizing the asymptotic generalization error of single-layer neural networks, which encompasses a broad range of non-linearities and is applicable to both regression and classification tasks. This framework allows for the examination of key factors influencing model performance, including the impact of over-parameterization and non-linearity, as well as the effects of loss function selection, initialization strategies, and regularization techniques. Furthermore, our model accounts for discrepancies between training and test data distributions, and we demonstrate its utility through the analysis of specific cases, such as linear and logistic regression, providing a rigorous and analytical explanation of the phenomenon observed in generalized linear models.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0944", "problem_id": "09440001", "content": "This paper explores a machine-learning application using large datasets to predict infrequent occurrences within semi-structured machine log files. These log files, generated rapidly and in substantial quantities by NORC's computer-assisted telephone interviewing (CATI) network during survey operations, are analyzed to achieve highly accurate predictions. By carefully integrating natural language processing (NLP) and data-mining techniques, we develop efficient learning and prediction models to classify rare error messages within these logs, despite the absence of source code, current documentation, or dictionaries. Specifically, our straightforward yet potent method, which involves preallocating features for learning from imbalanced data combined with naive Bayes classifiers, shows potential for broader application in supervised or semi-supervised learning and prediction tasks for other crucial events, such as cyberattack detection.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0945", "problem_id": "09450001", "content": "This study introduces a new approach for detecting key computational pathways in randomly wired neural networks prior to training. The method involves pruning the computational graph using a node mass probability function, which incorporates local graph metrics and is adjusted by hyperparameters generated by a reinforcement learning-driven controller network. By applying Ricci curvature, edges with minimal significance are eliminated before converting the graph into a neural network. The results demonstrate a nearly 35% reduction in floating-point operations (FLOPs) per forward pass without compromising performance. Additionally, the technique effectively regularizes randomly wired networks using structural attributes alone, and the beneficial traits observed in one network extend to others. The approach yields networks with superior performance at comparable compression levels relative to pruning based on lowest-magnitude weights. To our knowledge, this represents the first effort in pruning randomly wired neural networks and the inaugural use of Ricci curvature as a topological pruning criterion.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0946", "problem_id": "09460001", "content": "This study presents a method for enabling machines to interpret visual and textual data by autonomously learning the relationship between sentences and potentially flawed video segments, thereby eliminating the need for manual labeling. We introduce a self-supervised learning structure designed to extract cross-modal data and implement a unique adversarial learning component to specifically address inaccuracies in videos where subtitles may not accurately reflect the visual content. To support this research, we provide a new dataset, `ApartmenTour,' consisting of numerous online videos paired with subtitles. Experimental evaluations involving bidirectional sentence-video retrieval demonstrate that the proposed model attains state-of-the-art results, outperforming established baselines in both retrieval tasks. The dataset is available for download at https://github.com/zyj-13/WAL.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0947", "problem_id": "09470001", "content": "OpenSpiel provides a suite of environments and algorithms designed for advancing research in general reinforcement learning and game-based search and planning. It accommodates n-player scenarios, including single- and multi-agent settings, covering zero-sum, cooperative, and general-sum games, as well as one-shot and sequential interactions with turn-based or simultaneous moves, and both perfect and imperfect information. Additionally, it supports classic multiagent environments like grid worlds (with partial or full observability) and social dilemmas. The platform also offers tools for assessing learning dynamics and standard evaluation metrics. This document functions as both a guide to the codebase and a primer on key terms, foundational ideas, and algorithms in reinforcement learning, computational game theory, and search.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0948", "problem_id": "09480001", "content": "Restricted Boltzmann Machines (RBMs) are versatile unsupervised learning tools utilized to derive generative models that capture the underlying distributions of data. Typically, RBMs are trained using the Contrastive Divergence (CD) learning algorithm, which approximates the gradient of the log-likelihood of the data. Although a basic reconstruction error is commonly employed to evaluate the adequacy of the CD algorithm's approximation, its reliability has been questioned by several researchers (Schulz et al., 2010; Fischer & Igel, 2010). Given the limited exploration of alternative methods in the existing literature, this study explores straightforward alternatives to the reconstruction error, aiming to promptly identify declines in log-likelihood during the learning process.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0949", "problem_id": "09490001", "content": "This paper introduces a real-time method for estimating 3D face pose with six degrees of freedom (6DoF) directly from images, bypassing initial face detection or landmark localization steps. The core idea is based on the premise that determining the 6DoF rigid transformation of a face is less complex than facial landmark detection, a common approach for 3D face alignment, and that 6DoF provides richer information than face bounding boxes. The contributions include: (a) An efficient Faster R-CNN-based model is presented that is trained to directly regress 6DoF pose for all faces in an image, removing the need for preliminary face detection. (b) A methodology for converting and maintaining pose consistency between the original input image and arbitrarily cropped regions during both training and evaluation phases is detailed. (c) A demonstration of how face poses can effectively substitute traditional bounding box labels for training is provided. Experiments conducted on AFLW2000-3D and BIWI datasets demonstrate that the proposed method achieves real-time performance and exceeds the state of the art (SotA) in face pose estimation. Notably, the method also outperforms SotA models of similar complexity on the WIDER FACE detection benchmark, even though it is not specifically trained using bounding box labels.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0950", "problem_id": "09500001", "content": "Air traffic control presents a complex operational challenge where decision support systems can effectively enhance human expertise. This paper introduces a novel intelligent decision-making framework that employs multi-agent reinforcement learning (MARL) to provide dynamic, real-time aircraft speed adjustments. The system aims to improve air traffic controllers' ability to guide aircraft effectively, mitigating congestion and near-miss incidents while enhancing arrival punctuality. We introduce a new deep ensemble MARL technique designed to capture the intricacies of air traffic control by efficiently mediating between a local kernel-based RL model and a more comprehensive deep MARL model. The proposed method is validated using an open-source air traffic management simulator from Eurocontrol. Comprehensive empirical evaluations using a real-world dataset of thousands of aircraft demonstrate the viability of multi-agent RL for en-route air traffic control, with our deep ensemble MARL approach outperforming three state-of-the-art benchmark methods significantly.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0951", "problem_id": "09510001", "content": "Reinforcement learning, a subfield of artificial intelligence, encompasses a set of techniques where a learning agent improves its performance through iterative experimentation. Unlike supervised learning, this approach does not rely on pre-defined labels; the agent learns by continuously interacting with its environment. The agent begins in a given state, executes an action, transitions to a new state, and receives a reward based on the result. Various methodologies, such as Q-learning, aim to optimize the cumulative reward, leading to a policy that dictates the optimal action for each state. This process can be mathematically represented as a Markov decision process and, while R packages exist for related tasks, a dedicated reinforcement learning package is currently lacking. To address this gap, this paper introduces the ReinforcementLearning package, illustrating its use for reinforcement learning in R. The package offers a highly adaptable structure applicable to diverse problems, as demonstrated through standard examples from the literature (e.g. finding optimal game strategies).", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0952", "problem_id": "09520001", "content": "Recent advancements in semi-supervised learning have demonstrated significant success in utilizing both labeled and unlabeled data, though most approaches assume models begin with random initialization. This study integrates semi-supervised learning with transfer learning, presenting a more practical and effective framework that leverages pre-trained models from a source domain alongside labeled and unlabeled target domain data. To maximize the benefits of pre-trained weights and unlabeled target samples, we propose adaptive consistency regularization, featuring two key elements: Adaptive Knowledge Consistency (AKC) for aligning predictions between source and target models, and Adaptive Representation Consistency (ARC) for ensuring coherence in the target model across labeled and unlabeled data. The selection of samples for regularization is dynamically adjusted based on their relevance to the target task. Extensive experiments on benchmarks like CIFAR-10, CUB-200, and MURA, using an ImageNet pre-trained ResNet-50 model, demonstrate that our approach surpasses leading semi-supervised methods such as Pseudo Label, Mean Teacher, and FixMatch. Additionally, our technique complements existing methods, yielding further enhancements when combined with MixMatch and FixMatch. Our code is available at https://github.com/SHI-Labs/Semi-Supervised-Transfer-Learning.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0953", "problem_id": "09530001", "content": "Addressing partially-observable Markov decision processes (POMDPs) is essential for the effective deployment of deep reinforcement learning (DRL) in real-world robotic applications, where agents often operate with limited environmental awareness. This paper introduces graph convolutional memory (GCM), a DRL-based approach for tackling POMDPs. Distinct from recurrent neural networks (RNNs) or transformers, GCM integrates domain-specific prior knowledge into the memory retrieval mechanism through a knowledge graph. This graph-based encapsulation of priors allows GCM to tailor its performance to specific tasks while maintaining broad applicability across various DRL challenges. Utilizing graph convolutions, GCM derives hierarchical graph features, mirroring the function of image features extracted by a convolutional neural network (CNN). Experimental results demonstrate that GCM surpasses the performance of long short-term memory (LSTM), gated transformers for reinforcement learning (GTrXL), and differentiable neural computers (DNCs) in control, long-term non-sequential recall, and 3D navigation tasks, all while requiring substantially fewer parameters.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0954", "problem_id": "09540001", "content": "Neural architecture search (NAS) has demonstrated significant advancements in the design of deep neural networks. Differentiable architecture search transforms the task of exploring discrete architectures into a hyperparameter optimization challenge that can be addressed using gradient descent. Nevertheless, concerns have been raised about the efficacy and generalization of gradient-based methods for tackling non-convex architecture hyperparameter optimization issues. In this study, we present L^NAS, which intelligently learns to optimize and adjust architecture hyperparameters utilizing an actor neural network informed by the distribution of high-performing architectures from the search history. We propose a quantile-driven training method that effectively trains L^NAS within an actor-critic framework through continuous-action reinforcement learning. Experimental results indicate that L^NAS achieves state-of-the-art performance on the NAS-Bench-201 benchmark, as well as in the DARTS and Once-for-All MobileNetV3 search spaces. Furthermore, we demonstrate that the search policies derived from L^NAS exhibit generalizability and transferability across various training datasets with little need for fine-tuning.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0955", "problem_id": "09550001", "content": "Recent advancements in unsupervised domain translation have demonstrated remarkable results using Generative Adversarial Networks (GANs) and extensive unpaired training data. Nevertheless, current domain translation approaches tend to be single-use, failing to retain learned knowledge and hindering model adaptation to new domains. Addressing this limitation, we investigate unsupervised domain translation through a meta-learning lens. We introduce the Meta-Translation GAN (MT-GAN) model, designed to identify effective initializations for translation models. During meta-training, MT-GAN undergoes training on both a primary translation task and a generated dual translation task. A cycle-consistency meta-optimization objective is incorporated to enhance generalization capabilities. The efficacy of our model is validated across ten varied two-domain translation tasks and several face identity translation tasks. The results indicate that our method substantially surpasses existing domain translation techniques, particularly when each domain is limited to ten or fewer training samples.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0956", "problem_id": "09560001", "content": "We introduce a novel deep convolutional neural network architecture, termed What-Where Nets (WW-Nets), which leverages the implicit object location knowledge embedded in network connection weights to inform selective attention mechanisms for object detection tasks. Drawing inspiration from the human visual pathway's structure, our framework integrates two complementary networks: the \"What Network\" and the \"Where Network\". The \"What Network\" focuses on directing selective attention to pertinent image regions, while the \"Where Network\" utilizes this information to identify and classify objects of interest, mirroring the ventral and dorsal streams in the brain, which process \"what\" and \"where\" information, respectively. This approach is evaluated against state-of-the-art algorithms on the PASCAL VOC 2007 and 2012 and COCO object detection challenge datasets, as well as compared to human \"ground-truth\" attention through an eye-tracking experiment on human subjects using images from PASCAL VOC 2007, revealing intriguing relationships between human overt attention and information processing in WW-Nets. Our results demonstrate that the proposed method outperforms other object detection approaches, often by a significant margin, with the code and eye-tracking ground-truth dataset available at: https://github.com/mkebrahimpour.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0957", "problem_id": "09570001", "content": "The swift advancement of the Industrial Internet of Things (IIoT) necessitates the digitalization of industrial production to optimize network performance. Digital Twin technology offers a viable solution for facilitating the digital transformation of IIoT by generating virtual representations of physical assets. Nevertheless, ensuring network efficiency in IIoT environments presents considerable difficulties due to devices with limited resources, unpredictable tasks, and diverse resource types. Computation offloading can effectively utilize distributed resources within IIoT networks to decrease energy usage and improve data processing speed. This paper introduces Digital Twin Networks (DTN), a novel framework for constructing network topologies and modeling stochastic task arrivals in IIoT systems. Subsequently, we formulate the stochastic computation offloading and resource allocation challenge as a means of minimizing long-term energy consumption. Given that the formulated problem is stochastic, we apply Lyapunov optimization to convert it into a deterministic problem for each time slot. Finally, we introduce an Asynchronous Actor-Critic (AAC) algorithm to ascertain the best stochastic computation offloading strategy. The presented results show that the proposed method considerably surpasses the performance of the benchmarks.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0958", "problem_id": "09580001", "content": "Although significant progress has been made in image synthesis, current approaches frequently struggle with handling large geometric transformations. A prominent example is pose-conditioned person image generation, where the quality of synthesis heavily depends on accurately recognizing and modeling varying transformations across body parts. Traditional generative models, primarily relying on local convolutions, often fail to address critical challenges such as severe occlusions, viewpoint variations, or drastic appearance shifts that arise from arbitrary pose alterations. To overcome these issues stemming from geometric variability and spatial distortions, we introduce a Soft-Gated Warping Generative Adversarial Network (Warping-GAN), operating in two stages: 1) generating a target segmentation map based on the desired pose to provide structural guidance for image synthesis, and 2) employing a soft-gated warping-block within the Warping-GAN to map and transfer textures from the source image to the synthesized segmentation. This approach adaptively adjusts transformation intensity according to different target poses while maintaining computational efficiency and modularity, allowing seamless integration into various architectures. Extensive evaluations, including human perceptual studies and quantitative metrics, confirm that Warping-GAN surpasses all existing techniques on two large-scale datasets.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0959", "problem_id": "09590001", "content": "This study introduces a framework and evaluation standard for visual reasoning in robotics, specifically targeting small object grasping and manipulation tasks. The framework emphasizes deducing object characteristics from visual and textual inputs, covering household items along with their attributes, functions, natural language descriptions, and question-answer pairs for reasoning tasks, supplemented by semantic scene representations. Additionally, a technique for creating synthetic data is proposed to expand the benchmark to new objects or environments, accompanied by a more rigorous evaluation protocol compared to current datasets. The approach employs symbolic program execution for reasoning, utilizing separated visual and textual representations to run symbolic programs that simulate the algorithm's reasoning steps. Experimental evaluations on the proposed benchmark reveal limitations in existing datasets, which could result in inaccurate assessments of visual reasoning system performance when compared to state-of-the-art methods.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0960", "problem_id": "09600001", "content": "We introduce Disease-oriented image embedding with pseudo-scanner standardization (DI-PSS), a novel framework designed to enhance content-based image retrieval (CBIR) for clinical brain MRI databases. This approach integrates two key components: data harmonization and dimensionality reduction. DI-PSS employs skull stripping and CycleGAN-based transformations to align brain images with a standardized reference scanner, followed by 3D convolutional autoencoders (3D-CAE) incorporating deep metric learning to derive low-dimensional embeddings that better capture disease-specific features. Evaluated on T1-weighted MRIs from the Alzheimer's Disease Neuroimaging Initiative and the Parkinson's Progression Markers Initiative, our method significantly reduced embedding variability across different scanners and datasets. Specifically, PSS decreased the distance variability between Alzheimer's disease (AD) and clinically normal (CN) cases by 15.8–22.6%, and between AD and Parkinson's disease (PD) by 18.0–29.9%. These improvements facilitate more effective disease classification, as demonstrated by 6.2% and 10.7% increases in average accuracy and macro-F1 scores, respectively, for AD/CN classification using spectral clustering. Given its ability to harmonize images from diverse MRI scanners not included in training data, DI-PSS shows strong potential for application to large collections of legacy MRIs acquired under heterogeneous conditions.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0961", "problem_id": "09610001", "content": "This paper introduces a new dataset aimed at analyzing traffic accidents. The objective is to address the shortage of public data available for research focused on automatic spatio-temporal annotations related to road safety. Analysis of the dataset revealed a marked decline in object detection performance for pedestrians, attributed to both the size of the objects and the complexity of various scenes. To tackle this issue, we suggest incorporating contextual information into the standard Faster R-CNN framework via Context Mining (CM) and Augmented Context Mining (ACM), enhancing the detection accuracy for smaller pedestrians. Our experiments show a notable increase in object detection accuracy, with improvements of +8.51% for CM and +6.20% for ACM. Lastly, we illustrate the effectiveness of accident prediction within our dataset through the implementation of Faster R-CNN and an Accident LSTM architecture, achieving an average Time-To-Accident of 1.684 seconds and a Precision of 47.25%. Our Webpage for the paper is https://goo.gl/cqK2wE.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0962", "problem_id": "09620001", "content": "It is widely acknowledged that many machine learning models, despite performing well on benchmark datasets, struggle to generalize to real-world scenarios due to biased training data that inadequately represents real-world events. While generative models are also susceptible to this issue, recent progress in generative adversarial networks (GANs) suggests potential improvements in synthesizing realistic and diverse images. This raises the question of whether generative modeling of photos is now a fully resolved problem. Our findings indicate that while current GANs effectively adapt to standard datasets, they still lack the capacity to comprehensively model the visual manifold. Specifically, we examine their ability to accommodate basic transformations like camera movements and color variations. Results show that while the models reflect inherent biases present in their training datasets (e.g., a tendency towards centered objects), they also demonstrate some generalization capabilities. By manipulating the latent space, we can shift the distribution while maintaining the realism of the generated images. We propose that the extent of distributional shift is correlated with the breadth of the training data distribution. Consequently, we conduct experiments to assess the limitations of GAN transformations and introduce methods to alleviate this issue. The code is available at: https://ali-design.github.io/gan_steerability/", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0963", "problem_id": "09630001", "content": "In the context of performative prediction, where predictions inform decision-making and subsequently impact the distribution of future data, existing research has primarily concentrated on identifying performatively stable models, which represent the equilibrium points of iterative retraining processes. Nevertheless, such stable solutions often fall short of optimality when assessed in terms of performative risk, which refers to the loss incurred by the decision maker upon model deployment. This paper redirects the focus from solely pursuing performative stability to directly optimizing performative risk. We delineate a set of inherent properties of the loss function and the distribution shift induced by the model, under which the performative risk exhibits convexity, a characteristic that cannot be inferred solely from the convexity of the loss function. Additionally, we propose algorithms that exploit these structural assumptions to optimize performative risk with enhanced sample efficiency compared to generic derivative-free convex optimization methods, as seen in Figure A (Reference [1], Figure B [2], and Citation [3]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0964", "problem_id": "09640001", "content": "This paper investigates deep face recognition in open-set conditions, where effective face features should exhibit a smaller maximum intra-class distance compared to the minimum inter-class distance within an appropriate metric space. Hyperspherical face recognition, a promising research direction, has garnered significant interest and become a primary area of focus. SphereFace, a foundational work in this area, aimed to learn face embeddings with considerable inter-class angular margins. However, SphereFace's practical application is limited by significant training instability. To mitigate this, we present a unified framework for understanding large angular margins in hyperspherical face recognition. Using this framework, we expand upon SphereFace and introduce SphereFace-R, an enhanced version demonstrating improved training stability. Specifically, we introduce two novel methods for implementing the multiplicative margin and analyze SphereFace-R using three distinct feature normalization schemes (no feature normalization, hard feature normalization, and soft feature normalization). Furthermore, we propose a \"characteristic gradient detachment\" implementation strategy to stabilize training. Comprehensive experiments demonstrate that SphereFace-R consistently outperforms or performs comparably to state-of-the-art methods.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0965", "problem_id": "09650001", "content": "Optimizing computational efficiency is crucial when implementing machine learning models for real-time time series forecasting. While such algorithms automatically update model parameters using input data, they still rely on user-defined hyperparameters, which play a key role in determining prediction performance. Traffic data, commonly gathered through online sensors, exhibits serial correlation and may undergo gradual distribution shifts over time. A common approach to address this involves periodic hyperparameter re-tuning, though this introduces computational overhead. This study introduces a computationally efficient and theoretically grounded online hyperparameter optimization method for Kernel Ridge regression, specifically tailored for traffic prediction. Experimental results using actual traffic data demonstrate that the proposed method reduces computation time by up to sevenfold compared to alternative tuning approaches while maintaining or improving prediction accuracy.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0966", "problem_id": "09660001", "content": "Facial pose estimation has garnered significant interest in various applications, including human-robot interaction, gaze estimation, and driver monitoring, with end-to-end deep learning-based approaches becoming increasingly prevalent. Nevertheless, a major obstacle hindering the progress of facial pose estimation is the scarcity of sufficient training data, particularly for large poses. Motivated by the similarity in appearance of faces under similar poses, we propose a novel formulation of facial pose estimation as a label distribution learning problem, where each face image is associated with a Gaussian label distribution rather than a single label. To this end, we develop a convolutional neural network trained with a multi-loss function on the AFLW dataset and 300W-LP dataset, enabling direct prediction of facial poses from color images. Our approach is evaluated through extensive experiments on several prominent benchmarks, including AFLW2000, BIWI, AFLW, and AFW, demonstrating a substantial improvement over existing state-of-the-art methods, as shown in Figure A, B, C (References [1], [2], and [3]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0967", "problem_id": "09670001", "content": "Network biology has proven effective in elucidating intricate disease mechanisms, particularly in cancer. However, constructing disease-specific networks necessitates extensive knowledge, which remains limited despite recent advances in human cancer biology. Deep learning offers a promising avenue to address this challenge, but its conventional reliance on grid-like structured data has hindered its application to human disease subtype classification. The emergence of graph-based deep learning techniques presents an opportunity to capitalize on network biology analyses. In this work, we introduce a hybrid model integrating: 1) a graph convolutional neural network (graph CNN) and 2) a relation network (RN). The graph CNN component learns expression patterns of cooperative gene communities, while the RN component learns associations between these patterns. We applied this model to the PAM50 breast cancer subtype classification task, a clinically relevant benchmark. Experimental results in both subtype classification and patient survival analysis demonstrate that our method significantly outperforms existing approaches. This work represents a crucial initial step towards realizing personalized medicine.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0968", "problem_id": "09680001", "content": "This study focuses on inference and forecasting for multiple interdependent time series, incorporating a selection of concurrent predictors for each target series. Using a structural time-series framework, Bayesian methods are applied for model estimation, prediction, and variable selection, building upon recent advancements in univariate analysis. The Bayesian approach in this multivariate context mitigates overfitting while effectively modeling dependencies across series through various state components. The framework allows flexible selection of components and predictors tailored to each target series, with a cyclical component addressing short-term volatility induced by external shocks. Comprehensive simulations assess estimation precision and predictive performance, followed by an empirical analysis involving one-step-ahead forecasts of maximum log returns for a portfolio comprising four major financial stocks. Results from both simulations and empirical tests demonstrate the superiority of this multivariate model over three benchmarks: an independent series model, ARIMAX, and MARIMAX.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0969", "problem_id": "09690001", "content": "Although numerous investigations have examined the connection between inter-rater variability and deep learning model uncertainty in medical image segmentation, the influence of individual rater characteristics remains poorly understood. This research measures rater style through bias and consistency metrics and assesses their effects on deep learning model training. Utilizing two publicly available multi-rater datasets—one for brain multiple sclerosis lesions and another for spinal cord grey matter segmentation—the study reveals significant correlations (R^2 = 0.60 and 0.93) between rater bias and model uncertainty. Additionally, the role of label fusion among raters' annotations is investigated, demonstrating that multi-center consensus labels outperform single-center agreements in reducing uncertainty, as rater style tends to be specific to individual centers.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0970", "problem_id": "09700001", "content": "Recent advancements in Deep Convolutional Neural Networks (CNN) have led to notable successes in single image super-resolution (SR). Nonetheless, current CNN-based techniques rely on artificially created low-resolution (LR) and high-resolution (HR) image pairs for training, which limits their effectiveness in real-world scenarios where the transition from HR to LR is considerably more intricate than what is artificially generated. To address this issue, we introduce a real-world LR images guided bi-cycle network for single image super-resolution. This approach utilizes bidirectional structural consistency to train both the degradation and SR reconstruction networks in an unsupervised manner. Specifically, we develop a degradation network that simulates the real-world degradation process from HR to LR through generative adversarial networks, using these generated realistic LR images alongside actual HR images for the training of the SR reconstruction network in the first cycle. In the subsequent reverse cycle, the consistency of real-world LR images is leveraged to enhance the stability of training for both the SR reconstruction and degradation networks. Extensive evaluations on synthetic and real-world images indicate that our proposed algorithm outperforms leading single image SR techniques.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0971", "problem_id": "09710001", "content": "A frequent problem in systems and control engineering involves classifying time series data obtained from dynamic systems, with applications in areas like health monitoring, fault identification, and quality assurance. This task becomes more difficult when a system's underlying model is unavailable, measurement errors exist, and lengthy signals require analysis. This paper introduces a novel non-parametric classifier utilizing topological signatures to overcome these challenges. The proposed model learns classes by employing weighted kernel density estimates (KDEs) on persistent homology diagrams. It then uses Sinkhorn divergences on the diagram KDE space to assess proximity and predict labels for new trajectories. The results demonstrate the method's ability to accurately differentiate between states of chaotic systems with similar parameters and its resilience to noisy data.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0972", "problem_id": "09720001", "content": "This study introduces a novel deep neural network architecture for detecting salient objects, which enhances the incorporation of local and global image context within, around, and beyond the objects of interest. The core concept involves adaptively propagating and aggregating image context features across the entire feature maps, utilizing variable attenuation. To accomplish this, a spatial attenuation context (SAC) module is designed, which recursively translates and aggregates context features with distinct attenuation factors, and then learns weights to integrate the aggregated features in an adaptive manner. By integrating this module into a deep network, termed SAC-Net, to process individual layers, the network can be trained end-to-end, allowing for optimization of context features for salient object detection. Experimental results demonstrate that the proposed method outperforms 29 state-of-the-art approaches on six commonly used benchmark datasets, yielding favorable outcomes both quantitatively and visually.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-0973", "problem_id": "09730001", "content": "Estimating the three-dimensional pose of humans is a vital task in various domains, including the analysis of human behavior, applications involving augmented and virtual reality, and the development of self-driving technologies. In real-world settings, videos often feature multiple individuals who may be partially occluded, captured by cameras that are free to move, posing significant challenges for 3D human pose estimation due to the scarcity of datasets with precise 3D annotations. To address this, we introduce a novel approach utilizing a temporal regression network equipped with a gated convolution module, capable of converting 2D joint information into 3D while simultaneously recovering joints that are occluded. Additionally, a straightforward yet effective localization method is employed to translate normalized poses into global trajectories. We validate the efficacy of our method by introducing a new dataset, referred to as MMHuman, which comprises footage of multiple individuals with significant occlusions captured by moving cameras, with 3D ground truth data obtained through a precise motion capture system. Experimental results on both the static-camera-based Human3.6M dataset and our moving-camera-based dataset demonstrate that our proposed approach surpasses most current state-of-the-art methods for estimating 3D poses from 2D data, particularly in scenarios involving heavy occlusions, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0974", "problem_id": "09740001", "content": "In recent years, deep learning techniques for processing relational data have become increasingly prevalent. Unlike conventional relational learning approaches that employ first-order logic for data representation, these deep methods transform symbolic relational data into Euclidean vector spaces. While they provide improved scalability, they merely approximate relational structures numerically and support fewer reasoning tasks. This work presents a new relational representation learning framework that integrates the strengths of both paradigms. Drawing inspiration from auto-encoding, the proposed approach utilizes first-order logic for data representation and employs logic programs—rather than neural networks—to map between original and latent representations. Learning is formulated as a constraint optimization problem solvable with existing tools. By using logic as the representation language, the framework achieves greater accuracy (through exact rather than approximate representation), enhanced flexibility, and improved interpretability compared to deep learning methods. Experimental results demonstrate the advantages of these latent representations in relational learning applications.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0975", "problem_id": "09750001", "content": "The accurate identification of ocean fronts is crucial for both marine fisheries and national security, as these fronts can concentrate nutrients and influence underwater sound transmission. Existing methods for detecting ocean fronts often suffer from limited accuracy or are primarily designed for binary classification, neglecting the diverse characteristics of various ocean fronts across different marine environments. To address these limitations, we introduce a semantic segmentation network, termed the Location and Seasonality Enhanced Network (LSENet), designed for pixel-level, multi-class ocean front detection. Our network incorporates a channel supervision unit that integrates the inherent seasonal patterns of ocean fronts with contextual information to enhance detection precision. Furthermore, we implement a location attention mechanism that dynamically assigns attention weights based on the typical geographic locations of ocean fronts, thereby further boosting the accuracy of multi-class detection. Empirical evaluations demonstrate that our proposed method outperforms existing semantic segmentation techniques and current representative ocean front detection methods, showcasing its superior effectiveness.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-0976", "problem_id": "09760001", "content": "This study demonstrates that the computational complexity of the Iterative Thresholding and K-residual-Means (ITKrM) algorithm for dictionary learning can be substantially decreased through the application of dimensionality reduction techniques, specifically those based on the Johnson-Lindenstrauss lemma, which can be efficiently implemented using the fast Fourier transform. The proposed Iterative compressed-Thresholding and K-Means (IcTKM) algorithm enables rapid dictionary learning, and its convergence properties are examined. It is shown that IcTKM can recover, with high probability, an incoherent and overcomplete dictionary consisting of K atoms from training signals with a sparsity level of S, through the embedding of both the training data and dictionary into a lower-dimensional space of dimension m, where m is less than d. Notably, local stability in recovery is achieved with an embedding dimension as low as m = O(S \\log^4 S \\log^3 K), effectively mitigating the data dimensionality bottleneck that hinders the computational efficiency of ITKrM, resulting in a reduction in computational cost by a factor of O(m/d). The theoretical findings are supported by numerical simulations, which highlight the efficacy and efficiency of IcTKM in learning dictionaries from high-dimensional datasets.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-0977", "problem_id": "09770001", "content": "Decentralized online learning, characterized by online learning within decentralized networks, is gaining increasing interest because it enables data providers to collaboratively address online challenges without disclosing sensitive data to external entities or other providers. This collaboration commonly involves the exchange of models, such as recommendation models, among neighboring data providers. However, the existing best regret bound for decentralized online learning algorithms, \\Ocal, is considered inadequate, as it can be achieved without any inter-network communication. This motivates an investigation into the conditions under which communication enhances regret reduction in decentralized online learning. In this study, we decompose each loss function into adversarial and stochastic components. Utilizing this characterization, we demonstrate that decentralized online gradient (DOG) achieves a regret bound of \\OcalG + \\sigma}, where G quantifies the adversarial component within the private data (or local loss function) and \\sigma quantifies the randomness within the private data. This bound indicates that sharing private information can leverage the randomness inherent in private data to improve performance. Furthermore, this paper addresses dynamic regret, a more applicable metric for tracking user interest dynamics. Empirical evaluations are presented to support our theoretical analysis.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0978", "problem_id": "09780001", "content": "As the focus on human-induced alterations to our environment and its wildlife increases, there is a growing emphasis on the sustainable and responsible extraction of resources. To protect marine wildlife, the Norwegian government has deemed it essential to establish a comprehensive overview of the presence and abundance of various wildlife species in the Norwegian fjords and adjacent seas. This paper presents an application and analysis of an object detection method aimed at identifying fish in camera images. The data utilized is sourced from an underwater data station located at Fulehuk in Norway. We employ the You Only Look Once (YOLO) version 3 algorithm and compile a dataset comprising 99,961 images, achieving a mean Average Precision (mAP) of approximately 0.88. Additionally, we explore intermediate results within the YOLO framework to gain insights into its object detection efficacy.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0979", "problem_id": "09790001", "content": "A policy is considered robust if it optimizes the reward while taking into account a poor or even adversarial model. This study introduces two novel robustness criteria related to action uncertainty. We examine two situations where the agent aims to execute an action a, and (i) with probability \\alpha, an opposing adversarial action \\bar a is executed, or (ii) an adversary applies a perturbation to the chosen action in a continuous action space. Our findings demonstrate that these criteria correspond to typical uncertainty forms in robotic applications, such as sudden force occurrences, and we propose algorithms for the tabular case. Building on these algorithms, we extend our methodology to deep reinforcement learning (DRL) and conduct thorough experiments across various MuJoCo environments. The results reveal that our method not only yields robust policies but also enhances performance in the absence of perturbations. This extension suggests that action-robustness may serve as an implicit form of regularization in reinforcement learning challenges.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0980", "problem_id": "09800001", "content": "This study re-examines the stochastic variance-reduced policy gradient (SVRPG) approach introduced by Papini et al. (2018) for reinforcement learning. We present an enhanced convergence analysis, demonstrating that SVRPG can locate an \\epsilon-approximate stationary point of the performance function using O(1/\\epsilon^) trajectories, improving upon the previous best result of O(1/\\epsilon^2) by a factor of O(1/\\epsilon^). Key to our analysis are (i) a more precise variance bound for importance sampling weights, showing that variance depends on the parameter distance between policies, and (ii) a detailed examination of epoch length and batch size parameters to substantially decrease the required trajectories per SVRPG iteration. Experimental results on benchmark reinforcement learning tasks validate our theoretical findings regarding batch sizes.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0981", "problem_id": "09810001", "content": "This paper provides a concise introduction to Artificial Intelligence, focusing on the practical development of an object detection and tracking algorithm. Detecting and tracking rapidly moving objects is essential for AI applications such as autonomous vehicles, sports analytics, robotics, and object counting. The study introduces \"CueNet,\" a Fully Convolutional Neural Network designed to reliably detect and track the cueball in a labyrinth game. CueNet V1 processes a single input image, whereas CueNet V2 analyzes three sequential 240 x 180-pixel images to generate a probability heatmap for the cueball's position. To evaluate robustness, the network was tested on a video containing various distractions. CueNet V1 achieved 99.6% accuracy in locating the cueball across frames, while CueNet V2 reached 99.8% accuracy on the same test data.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-0982", "problem_id": "09820001", "content": "We introduce a new unsupervised representation learning model termed Robust Block-Diagonal Adaptive Locality-constrained Latent Representation (rBDLR). The rBDLR model effectively recovers multi-subspace structures while simultaneously extracting salient features that preserve adaptive locality. Utilizing a Frobenius-norm based latent low-rank representation framework, rBDLR learns both coding coefficients and salient features together, enhancing robustness against outliers and errors in the input data, while adaptively maintaining the local information of salient features and ensuring the coefficients exhibit block-diagonal structures. To bolster robustness, we conduct the latent representation and adaptive weighting within a clean data space. To achieve block-diagonal coefficients, we apply auto-weighting by minimizing the reconstruction error associated with salient features, constrained by a block-diagonal regularization term. This approach guarantees the acquisition of a strictly block-diagonal weight matrix, thereby enabling salient features to maintain their adaptive locality-preserving characteristics. By reducing the discrepancy between the coefficient and weight matrices, we can derive a block-diagonal coefficients matrix that facilitates the propagation and exchange of valuable information between salient features and coefficients. Comprehensive experimental results highlight the advantages of rBDLR in comparison to other leading methods.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0983", "problem_id": "09830001", "content": "Deep reinforcement learning techniques often face challenges in environments where rewards are scarce. A notable approach for enhancing exploration in such scenarios is to mimic trajectories from human demonstrators. Typically, these demonstrations are gathered under controlled conditions, meaning the agent has a full understanding of the environment setup as well as the demonstrator's actions and rewards. In this paper, we introduce a two-stage approach that addresses these issues by utilizing noisy, unaligned video data without requiring access to the aforementioned information. Initially, we develop a method to align unstructured videos from various sources into a unified representation using self-supervised learning objectives that integrate both temporal and multimodal aspects (such as visual and auditory signals). Subsequently, we incorporate a single YouTube video into this representation to formulate a reward function that drives the agent to replicate human gameplay. This innovative one-shot imitation method enables our agent to surpass human performance on notoriously challenging exploration games like Montezuma's Revenge, Pitfall!, and Private Eye, even in the absence of any environment-specific rewards.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0984", "problem_id": "09840001", "content": "We introduce an algorithm utilizing posterior sampling (also known as Thompson sampling) that attains nearly optimal worst-case regret bounds for a communicating Markov Decision Process (MDP) with an unknown but finite diameter. Our primary contribution is a high-probability regret upper bound of \\tilde(DS) for any communicating MDP characterized by S states, A actions, and diameter D. The regret is measured by comparing the algorithm's cumulative reward to the expected reward of an optimal infinite-horizon undiscounted average reward policy over a time horizon T. This finding closely aligns with the established lower bound of \\Omega(). Additionally, we derive new insights into the anti-concentration properties of the Dirichlet distribution, which may have broader relevance.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0985", "problem_id": "09850001", "content": "Generating visually appealing designs often involves overlaying text on images, a process that can be automated by intelligently determining the optimal position, orientation, and style of the text based on the background image's content. This challenge, which we term \"copyspace detection,\" differs from simple foreground-background separation. We present solutions developed using both single-stage and two-stage object detection techniques, trained on meticulously labeled data. This workshop will explore these algorithms for copyspace detection and illustrate their integration into generative design models and pipelines, exemplified by systems like Einstein Designer.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0986", "problem_id": "09860001", "content": "Graph-based deep learning has advanced significantly, particularly in adapting Convolutional Neural Networks (CNNs) to graph data. CNNs commonly employ alternating convolutional and pooling layers, where pooling layers downsample the grid, trading spatial or temporal resolution for enhanced feature dimensionality. While generalized convolution operators for graphs are well-explored and effective, hierarchical graph coarsening remains difficult due to the absence of spatial locality and inherent node ordering in graphs. This paper introduces two primary contributions: first, a differential module for computing structural similarity features from the adjacency matrix, applicable across various algorithms. Second, the integration of these features with a modified pooling layer, DiffPool arXiv:1806.08804, resulting in a proposed SimPool layer. This is accomplished by merging network reduction via structural graph similarity with hierarchical localized pooling. Experiments show that, within a Graph Neural Network architecture, SimPool generates node cluster assignments that more closely mimic the locality-preserving pooling operations of CNNs operating on standard grid receptive fields. Furthermore, experimental findings indicate the utility of these features in inductive graph classification, without increasing the number of parameters.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-0987", "problem_id": "09870001", "content": "Active contour models are prevalent in image segmentation, with the level set method (LSM) being a favored solution approach that uses a level set function to implicitly define the contour. Nevertheless, the LSM is computationally intensive and prone to numerical instability, often necessitating supplementary regularization or re-initialization. In this work, we introduce an alternative contour representation using characteristic functions to formulate geodesic active contours, leading to an efficient iterative convolution-thresholding method (ICTM). The ICTM offers simplicity and improved efficiency compared to the LSM, while retaining the advantageous characteristics of level set-based methods. Experimental results on 2D synthetic, 2D ultrasound, 3D CT, and 3D MR images for nodule, organ, and lesion segmentation indicate that the proposed method achieves competitive or superior segmentation performance relative to the LSM, coupled with a substantial increase in speed.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-0988", "problem_id": "09880001", "content": "Learning curves offer valuable insights into how a learner's generalization performance varies with the size of the training dataset. These curves serve as a critical tool for tasks such as model selection, predicting the impact of additional training data, and optimizing the computational efficiency of model training and hyperparameter tuning. This review traces the historical origins of the concept, presents a precise definition of learning curves, and briefly addresses fundamental aspects like their estimation. A key focus is a thorough survey of existing research on the characteristics of learning curves, highlighting empirical and theoretical findings that suggest well-behaved patterns, often following power-law or exponential trends. The discussion extends to Gaussian processes, examining their intricate curve shapes and the factors affecting them. Notably, instances of poorly behaved curves, where performance degrades with increased data, are also explored. The review concludes by identifying unresolved questions that merit further empirical and theoretical study. Overall, it emphasizes the remarkable diversity of learning curves and the absence of a universal model to describe them.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0989", "problem_id": "09890001", "content": "This study examines the efficacy of a straightforward approach to addressing the prevalent challenge of deep learning in medical image analysis, where labeled training data is scarce. By leveraging surrogate supervision, a process that involves assigning artificial labels to readily available unlabeled medical images, we pre-train a deep neural network model for a target medical image analysis task that lacks sufficient labeled training data. Our investigation employs three surrogate supervision schemes - rotation, reconstruction, and colorization - across four distinct medical imaging applications, encompassing both classification and segmentation tasks for 2D and 3D medical images. The key outcomes of our research reveal that: 1) pre-training with surrogate supervision yields positive results when training sets are limited; 2) deep models initialized with weights pre-trained via surrogate supervision outperform their counterparts trained from scratch, implying that surrogate supervision pre-training is a viable precursor to training deep 3D models; and 3) pre-training models within the medical domain using surrogate supervision surpasses transfer learning from unrelated domains, such as natural images, highlighting the practical benefits of exploiting abundant unlabeled medical image data.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-0990", "problem_id": "09900001", "content": "Recently, numerous deep learning techniques for 3D face reconstruction have emerged; however, their application within gaming remains limited. Existing character customization systems in games either necessitate significant manual adjustment of facial features by players to achieve their preferred appearance or offer restricted options for facial shape and texture. This paper presents an automated method for creating character faces that predicts both facial shape and texture from a single portrait and can be incorporated into most established 3D games. While methods based on the 3D Morphable Face Model (3DMM) can accurately reconstruct 3D faces from individual images, the topological structure of 3DMM meshes differs from that employed in most games. Achieving high-quality texturing with current methods requires extensive facial texture datasets for training, creation of which is labor-intensive and time-consuming. Moreover, datasets collected in controlled environments may not generalize effectively to real-world conditions. To address these challenges, we introduce 1) an economical method for acquiring facial textures, 2) a shape transfer technique that adapts 3DMM mesh geometry for use in games, and 3) an innovative framework for training networks designed for 3D game face reconstruction. Our approach not only generates richly detailed and lifelike game characters that closely resemble the input portrait but also mitigates the effects of lighting variability and occlusions. Experimental results demonstrate that our method surpasses existing leading techniques used in gaming.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-0991", "problem_id": "09910001", "content": "A major challenge in integrated circuit (IC) design lies in the frequent execution of resource-intensive SPICE simulations, especially during intricate chip testing and verification processes. Pseudo transient analysis (PTA) has emerged as a leading continuation SPICE solver, though its effectiveness depends heavily on the configured pseudo-parameters. This study introduces BoA-PTA, a Bayesian optimization-enhanced PTA approach that significantly enhances simulation speed and convergence without introducing additional errors. Notably, the method eliminates the need for pre-computation or offline training, making it adaptable for accelerating both ongoing repetitive simulations and entirely new circuit analyses. BoA-PTA leverages advanced machine learning techniques, including deep learning, Gaussian processes, Bayesian optimization, non-stationary monotonic transformation, and parameterized variational inference. Evaluated across 43 benchmark circuits against state-of-the-art SPICE solvers, BoA-PTA achieves an average 2.3x (peak 3.5x) speed improvement compared to the original CEPTA.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-0992", "problem_id": "09920001", "content": "Fourier methods have established themselves as a reliable and effective means of data processing, with a history of successful applications. In response to the growing importance of memory and computational efficiency in mobile and embedded systems, this work proposes a hybrid approach that integrates Fourier methods with recurrent neural network architectures. By leveraging the short-time Fourier transform, it is possible to process multiple samples simultaneously in an efficient manner. Furthermore, the application of low-pass filtering enables reductions in model weights. The efficacy of this approach is demonstrated through the prediction of time series data, including both synthetic data generated from the chaotic Mackey-Glass differential equation and real-world datasets comprising power load and motion capture data.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-0993", "problem_id": "09930001", "content": "This study introduces an innovative deep learning framework designed for anatomical segmentation and automated landmark detection. The approach specifically addresses the complex task of segmenting the mandible in cone-beam computed tomography (CBCT) scans and locating its nine anatomical landmarks within geodesic space. The methodology involves three interconnected stages: first, a deep neural network architecture with optimized regularization and hyper-parameters is developed to achieve precise segmentation without requiring data augmentation or extensive post-processing. Second, landmark localization is performed directly on the geodesic space to handle sparsely distributed anatomical points. Third, a long short-term memory (LSTM) network is employed to detect closely positioned landmarks, a task that conventional detection networks often struggle with. The fully automated system demonstrated superior performance compared to existing mandible segmentation and landmarking techniques, particularly in cases of craniofacial anomalies and pathological conditions. Evaluation was conducted using a clinically representative CBCT dataset of 50 patients exhibiting significant craniomaxillofacial (CMF) variability, alongside qualitative assessments on 250 diverse scans. Additionally, the framework was validated on the MICCAI Head-Neck Challenge (2015) dataset, achieving state-of-the-art results. A detailed analysis of hyper-parameter selection, including pooling and activation functions, further underscores the robustness of the proposed networks.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-0994", "problem_id": "09940001", "content": "Constant-curvature Riemannian manifolds (CCMs) serve as effective embedding spaces in various applications due to their non-Euclidean structure, which naturally captures data properties such as hierarchy and circularity. This paper presents the CCM adversarial autoencoder (CCM-AAE), a probabilistic generative model designed to represent data distributions on a CCM. The approach aligns the aggregated posterior of the CCM-AAE with a predefined CCM probability distribution, enabling the encoder to implicitly learn CCM-based representations that deceive the discriminator network. Additionally, geometric constraints are explicitly enforced by optimizing the embeddings' membership degree to the CCM during training. Unlike prior works that focus solely on hyperspherical or hyperbolic manifolds, our framework provides a unified solution adaptable to CCMs of varying curvatures. We validate the model's performance on three geometrically complex datasets: semi-supervised classification on MNIST, link prediction on citation networks, and molecular graph generation using QM9. The results demonstrate superior performance over both Euclidean and non-Euclidean autoencoder baselines across all evaluated tasks.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-0995", "problem_id": "09950001", "content": "Machine learning (ML) has demonstrated its effectiveness by achieving human-level performance across diverse applications; however, its inherent black-box nature hinders the interpretability of results. Although current explainable ML techniques show potential, their primary focus on formulating interpretability as an optimization problem results in iterative and computationally intensive processes, thereby restricting their use in real-time scenarios. This paper introduces a new framework designed to accelerate explainable ML using Tensor Processing Units (TPUs). Capitalizing on the relationship between matrix convolution and Fourier transform, the framework leverages the inherent capabilities of TPUs for efficient matrix computations. The key contributions of this work are threefold: (1) This paper presents the first known effort to enable hardware acceleration of explainable ML using TPUs. (2) The proposed approach is versatile, applicable to a broad range of ML algorithms, and enables real-time outcome interpretation through effective TPU-based acceleration. (3) Comprehensive experiments reveal that the proposed method achieves significant speed improvements, offering approximately a ten-fold reduction in both classification time (averaging 25x) and interpretation time (averaging 13x) when compared to existing state-of-the-art methods.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0996", "problem_id": "09960001", "content": "In this study, we identify two types of redundancies when employing vision transformers (ViT) for image recognition tasks. Firstly, maintaining a consistent number of tokens across the entire network results in redundant features at the spatial level. Secondly, there is redundancy in the attention maps utilized by various transformer layers. To address these observations, we introduce PSViT: a vision transformer that incorporates token Pooling and attention Sharing to minimize redundancy, thereby significantly improving feature representation and achieving a favorable speed-accuracy balance. Specifically, token pooling in our PSViT refers to the process of reducing the number of tokens spatially. Additionally, attention sharing is implemented between adjacent transformer layers to leverage the highly correlated attention maps. We then establish a streamlined set of potential configurations for different token pooling and attention sharing strategies. From this proposed set, the token quantity in each layer and the selection of layers for attention sharing can be considered hyper-parameters that are automatically optimized based on the data. Experimental results indicate that our method can enhance accuracy by as much as 6.6% in ImageNet classification compared to DeiT.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-0997", "problem_id": "09970001", "content": "Selecting the most suitable kernel is essential in numerous Machine Learning applications. Gaussian Process stands out as an advanced method for both regression and classification that is significantly dependent on a kernel function. Nonetheless, in the existing literature on Gaussian Processes, kernels have typically been either specifically designed without a systematic approach, chosen from a pre-established collection, or explored within a predetermined space of kernel compositions. In this work, we introduce a Genetic-Programming approach that formulates a kernel function as a tree of basic mathematical expressions. This structure allows for the modeling of a broader array of kernels, leading to the discovery of potentially superior solutions, despite introducing new challenges. Our proposed algorithm effectively addresses these issues and identifies kernels that closely represent the data's characteristics. This technique has been evaluated across multiple real-world time-series extrapolation tasks, yielding enhancements over current state-of-the-art performance while simplifying the kernels' complexity.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-0998", "problem_id": "09980001", "content": "Current leading techniques for LiDAR segmentation in extensive driving scenarios frequently convert point clouds into a 2D representation for processing with 2D convolutions. While effective, this approach compromises the inherent 3D topology and geometric relationships of the data. Employing 3D voxelization and 3D convolutional networks presents an intuitive solution; however, our findings indicate limited performance gains in outdoor point clouds due to their sparsity and variable density. Addressing these challenges, we introduce a novel framework utilizing cylindrical partitioning and asymmetrical 3D convolution networks to leverage 3D geometric patterns while preserving the point cloud's inherent characteristics. Additionally, a point-wise refinement module mitigates inaccuracies introduced by voxel-based label encoding. Evaluations on the SemanticKITTI and nuScenes datasets demonstrate that our method achieves state-of-the-art performance, securing the top position on the SemanticKITTI leaderboard and surpassing existing methods on nuScenes by approximately 4%. The proposed 3D framework also exhibits strong generalization capabilities in LiDAR panoptic segmentation and LiDAR 3D detection tasks.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-0999", "problem_id": "09990001", "content": "Machine learning relies on data, yet obtaining and labeling real-world datasets is often difficult, costly, and time-intensive. Furthermore, once real data is collected, modifications are nearly impossible (e.g., altering room lighting), complicating the analysis of how various data attributes influence performance. This paper introduces AI Playground (AIP), an open-source tool built on Unreal Engine for the generation and labeling of virtual image datasets. AIP facilitates capturing the same image under varying conditions (e.g., fidelity, lighting) and producing different ground truths (e.g., depth or surface normals) with ease. It is highly extensible and can be utilized with or without programming knowledge. To evaluate our tool, we created eight datasets that were identical except for differences in lighting and fidelity. We trained deep neural networks to predict (1) depth values, (2) surface normals, or (3) object labels, and analyzed each network's performance both within and across datasets. Among other discoveries, we found that sensitivity to various settings varies depending on the problem. We corroborated previous studies that indicated segmentation models are highly sensitive to fidelity, but also discovered they are equally sensitive to lighting conditions. In contrast, depth and normal estimation models appear to be less influenced by fidelity or lighting, showing greater sensitivity to the image structure. Finally, we evaluated our depth-estimation networks on two real-world datasets, achieving results comparable to those obtained from training on real data alone, thereby validating that our virtual environments are sufficiently realistic for real-world applications.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1000", "problem_id": "10000001", "content": "This paper investigates 3D object detection utilizing RGB-D data in both indoor and outdoor environments. Unlike existing approaches that primarily utilize images or 3D voxels, which can distort inherent 3D structures and invariances, our method directly processes raw point clouds extracted from RGB-D scans. A central challenge in this approach is the efficient localization of objects within extensive point clouds (region proposal). To address this, our technique integrates established 2D object detectors with sophisticated 3D deep learning techniques to enhance object localization, thus achieving both efficiency and high recall, even for small objects. By learning directly from raw point clouds, our method accurately estimates 3D bounding boxes, even in cases of significant occlusion or sparse point density. Evaluations conducted on the KITTI and SUN RGB-D 3D detection benchmarks demonstrate that our method significantly surpasses current state-of-the-art performance, while also maintaining real-time processing speeds.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1001", "problem_id": "10010001", "content": "Object detection is crucial for numerous vision-based applications, yet there is often a trade-off between detection accuracy and speed. A key factor limiting the performance of rapid detectors is their difficulty in identifying small objects. To overcome this challenge, we introduce MRFSWSnet, a weakly-supervised segmentation network that employs multiple receptive fields and prioritizes small objects for efficient detection. The network utilizes a multiple receptive fields (MRF) block to assign varying weights to different spatial regions of an object and its surrounding background, thereby improving feature discrimination. Additionally, a specialized weakly-supervised segmentation module is incorporated to focus exclusively on small objects during auxiliary training, enhancing their detection accuracy. Comprehensive evaluations on PASCAL VOC and MS COCO datasets demonstrate the method’s effectiveness. Notably, with a 300x300 input resolution, MRFSWSnet attains 80.9% mAP on VOC2007 test while processing each frame in 15 milliseconds, establishing it as the leading real-time detector.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1002", "problem_id": "10020001", "content": "Real-world data sets frequently comprise a mix of continuous and discrete measurements, posing significant challenges in terms of pre-processing and understanding the relationships between variables. The initial phase of data mining, which involves gaining insight into the data, is hindered by the vast search space that arises when no assumptions are made about the data, leading to a super-exponential increase in complexity with the number of variables. To address this issue, we introduce graphical hypergeometric networks (HNet), a novel approach that leverages statistical inference to identify significant associations between variables, ultimately yielding a network that illuminates the intricate relationships between them. HNet is capable of processing unstructured data sets in their raw form and producing a network consisting of directed or undirected edges between nodes, which represent the variables. The efficacy of HNet is evaluated using well-known data sets, as well as synthetic data sets with established ground truth, and its performance is compared to that of Bayesian structure learning, as seen in Figure A, B, C (References [citation]). The results demonstrate that HNet achieves high accuracy in detecting node links, with an average MCC score of 0.33 + 0.0002 (P<1x10-6) for the Alarm data set, outperforming the average MCC score of 0.52 + 0.006 (P<1x10-11) obtained through Bayesian structure learning, and significantly surpassing the MCC score of 0.004 + 0.0003 (P=0.49) resulting from random edge assignment. Overall, HNet offers a robust solution for analyzing mixed data types, scaling effortlessly with the number of variables, and facilitating in-depth examination of the detected associations, with the implementation available at https://erdogant.github.io/hnet/.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1003", "problem_id": "10030001", "content": "Recent studies have shown that mobile devices can estimate gaze by analyzing images from the front-facing camera without requiring specialized hardware, offering numerous applications in human-computer interaction, medical diagnosis, and accessibility, such as hands-free gaze input for patients with motor disorders. However, existing methods are hindered by their reliance on collecting user data, a time-consuming and costly process that is difficult to scale across devices. Although attempts have been made to generate eye region data using 3D models that simulate various head poses and camera settings, these models lack realism. This paper presents an improved approach, utilizing a generative adversarial framework to create a large, diverse, and realistic dataset of high-resolution images with accurate gaze labels, which can also complete missing image parts and operate on extended eye regions. By leveraging this synthesized dataset without additional real-user training data, we achieve state-of-the-art results in estimating 2D gaze position on mobile devices, and demonstrate improved cross-device generalization, as well as enhanced robustness to varied head poses, blur, and distances.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1004", "problem_id": "10040001", "content": "The increasing use of electronic health records has heightened the demand for predictive models capable of handling clinical time-series data efficiently. Deep neural networks, particularly those using Recurrent Neural Network (RNN) architectures with Long Short-Term Memory (LSTM) units, have set new performance benchmarks in various clinical prediction tasks. However, the sequential nature of RNNs limits their ability to leverage parallel computing, making them less efficient, especially when managing extensive sequences. In contrast, modern architectures that rely exclusively on attention mechanisms have demonstrated substantial success in transduction tasks within natural language processing (NLP) while offering improved computational efficiency. This paper introduces the novel use of attention models for clinical time-series analysis, eliminating the need for recurrence. We present the (Simply Attend and Diagnose) architecture, which integrates a masked self-attention approach along with positional encoding and dense interpolation techniques to maintain temporal order. Additionally, we create a multi-task version of this model to enable concurrent inference across multiple diagnostic tasks. Utilizing the latest MIMIC-III benchmark datasets, our findings reveal that the proposed method achieves leading performance across all tasks, surpassing both LSTM models and traditional baselines that rely on hand-crafted features.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1005", "problem_id": "10050001", "content": "This work introduces a straightforward and formally rigorous theory of rational decision-making, encompassing sequential decisions that influence the environment. The theory possesses a geometric nature, facilitating visualization and comprehension of the arguments. The framework assumes complete decision makers with a fully defined preference set. The core finding demonstrates that a completely rational decision maker inherently possesses a probabilistic model of the environment. A countable variant of this result clarifies the distinction between countable and finite additivity, illustrating its dependence on the geometry of the preference space. This is accomplished by establishing a productive link between rationality and the Hahn-Banach Theorem. The presented theory can be interpreted as a formalization and expansion of the betting odds perspective on probability, as proposed by Ramsey and De Finetti.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1006", "problem_id": "10060001", "content": "The utilization of video feedback in surgical procedures offers valuable insights and serves as the primary sensory cue for surgeons, underscoring the importance of scene understanding in computer-assisted interventions and post-operative analysis. A critical component of this capability is the accurate identification and localization of surgical instruments and anatomical structures, which can be achieved through semantic segmentation. Although deep learning has significantly enhanced semantic segmentation techniques in recent years, its effectiveness is contingent upon the availability of labeled datasets for training purposes. To address this need, this study presents a dataset for the semantic segmentation of cataract surgery videos, comprising annotated images from the publicly available CATARACTS challenge dataset, as seen in Figure A, B, C. Furthermore, the performance of several state-of-the-art deep learning models for semantic segmentation is evaluated on this dataset, with the dataset available for public access at https://cataracts.grand-challenge.org/CaDIS/, as referenced in [citations].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1007", "problem_id": "10070001", "content": "Processing high-velocity data streams necessitates the creation of models that deliver swift and precise predictions. While deep neural networks represent the leading edge in numerous machine learning applications, their efficacy in real-time data streaming contexts remains an underexplored research area. Recent initiatives, however, have aimed to modify intricate deep learning models for streaming applications by lowering their processing speeds. The asynchronous dual-pipeline deep learning framework is designed to facilitate simultaneous prediction on incoming instances and model updates through two distinct layers. This study aims to evaluate the effectiveness of various deep architecture types for data streaming classification within this framework. We examine models including multi-layer perceptrons, recurrent, convolutional, and temporal convolutional neural networks across multiple time-series datasets treated as streams. The results demonstrate that convolutional architectures provide superior performance regarding both accuracy and efficiency.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1008", "problem_id": "10080001", "content": "The FlowNet has shown that optical flow estimation can be approached as a learning problem; however, traditional methods still define the leading standards for flow quality. In particular, FlowNet struggles to match the performance of variational methods, especially with minor displacements and real-world data. In this paper, we promote the idea of end-to-end optical flow learning and significantly enhance its effectiveness. The substantial gains in quality and speed stem from three key contributions: first, we emphasize the significance of the training data presentation schedule. Second, we introduce a stacked architecture that incorporates the warping of the second image using intermediate optical flow. Third, we address small displacements by creating a sub-network specifically designed for minor motions. FlowNet 2.0 is only slightly slower than its predecessor, but it reduces the estimation error by over 50%. It matches the performance of leading methods while operating at interactive frame rates. Additionally, we provide faster variants capable of computing optical flow at up to 140fps, achieving accuracy similar to the original FlowNet.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1009", "problem_id": "10090001", "content": "Deep learning, exemplified by artificial deep neural networks (DNNs), has demonstrated remarkable success across various domains involving text, images, videos, and graphs. Despite this, the opaque nature of DNNs remains a significant barrier to their broader adoption in critical applications like medical diagnosis and treatment. Given the immense potential of deep learning, there has been growing research interest in interpreting neural networks. This paper presents a systematic review of recent studies on understanding neural network mechanisms, outlines interpretability applications, particularly in medicine, and explores future research directions, including connections to fuzzy logic and brain science, based on a comprehensive taxonomy.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1010", "problem_id": "10100001", "content": "Deep neural networks (DNNs) achieve strong performance in image classification, yet those trained on datasets with co-occurrence bias may depend on incorrect features for decision-making, negatively impacting their transferability. To address this, we introduce an interactive approach that guides classifiers to concentrate on user-specified regions, reducing the effects of co-occurrence bias. Experiments on the CelebA dataset demonstrate that a pre-trained AlexNet, fine-tuned using Grad-CAM results, successfully focuses on targeted facial attributes.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1011", "problem_id": "10110001", "content": "This study examines the development of above-ground biomass (AGB) prediction maps using synthetic aperture radar (SAR) intensity images. The aim is to enhance traditional regression models that utilize SAR intensity, which have been limited by the small number of in situ AGB measurements. Although collecting these measurements can be expensive, data obtained from airborne laser scanning (ALS) sensors show a strong correlation with AGB. Consequently, we suggest employing AGB predictions derived from ALS data as proxy response variables for the SAR data in a sequential modeling approach, significantly increasing the volume of training data. To model the relationship between SAR intensity and ALS-predicted AGB, we recommend utilizing a conditional generative adversarial network (cGAN), specifically the Pix2Pix convolutional neural network. This approach allows for the recreation of existing ALS-based AGB prediction maps. The synthesized ALS-based AGB predictions are then assessed both qualitatively and quantitatively in comparison to ALS-based AGB predictions produced by a conventional non-sequential regression model trained in the same region. The findings indicate that the proposed framework effectively captures the features of the actual data, suggesting that ALS-guided generative models represent a promising strategy for predicting AGB from SAR intensity. Future research in this domain has the potential to yield large-scale and cost-effective AGB predictions.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1012", "problem_id": "10120001", "content": "We present SyntheticFur, a novel dataset designed for machine learning applications, comprising ray-traced synthetic fur renders along with associated rasterized input buffers and simulation files. Using Houdini, we generated around 140,000 procedurally created images and 15 simulations, featuring fur groomed on diverse skin primitives and animated with various motions under predefined lighting conditions. Additionally, we showcase the dataset's utility in neural rendering by training a conditional generative adversarial network with perceptual loss, which enhances fur graphics using cost-effective input buffers. This high-fidelity dataset aims to foster advancements in neural rendering techniques for diverse applications.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1013", "problem_id": "10130001", "content": "Transformers have emerged as the leading framework in natural language processing due to their capacity for large-scale pretraining followed by efficient adaptation to specialized tasks through fine-tuning. The Vision Transformer pioneered the direct application of transformer models to image inputs, showing that such architectures can match convolutional networks in benchmark classification tasks. Nevertheless, the high computational demands of attention mechanisms restrict their use to low-resolution inputs. For advanced tasks like detection or segmentation, preserving high-resolution input is essential to accurately capture and represent fine-grained details. This leads to the critical inquiry of whether transformer-based models like the Vision Transformer can extend beyond classification tasks. Our study confirms that Vision Transformers can serve as an effective backbone for detection tasks when paired with a standard detection head, yielding competitive results on COCO. The proposed ViT-FRCNN model exhibits key transformer characteristics, such as strong pretraining capabilities and rapid fine-tuning. Additionally, it outperforms conventional detection backbones in several areas, including handling out-of-domain images, detecting large objects more effectively, and reducing dependency on non-maximum suppression. We consider ViT-FRCNN a significant advancement toward achieving pure-transformer solutions for complex vision challenges like object detection.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1014", "problem_id": "10140001", "content": "Generative adversarial networks (GANs) are capable of producing high-resolution images from latent space representations. While current research explores image editing through latent code manipulation, it typically focuses on simple attribute modifications. This paper introduces a novel approach that facilitates manipulation based on complex, multidimensional conditions like keypoints and captions. The proposed algorithm identifies a new latent code that aligns with the desired condition, leveraging the Surrogate Gradient Field (SGF) generated by an auxiliary mapping network. To provide a quantitative assessment, a metric is introduced to measure the disentanglement of manipulation techniques. Extensive experiments on facial attribute adjustment reveal that the proposed method achieves superior disentanglement compared to state-of-the-art approaches. Furthermore, the method's applicability to diverse conditional modalities is demonstrated by altering intricate image characteristics, including keypoints and captions.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1015", "problem_id": "10150001", "content": "Generative Adversarial Networks (GANs) integrated with self-supervised tasks have demonstrated encouraging outcomes in both unconditional and semi-supervised image generation. In this work, we introduce a self-supervised method (LT-GAN) aimed at enhancing the quality and diversity of generated images by evaluating the transformations induced by the GAN (i.e., the changes in the generated images resulting from perturbations in the generator's latent space). Specifically, the self-supervised task is designed to determine whether the latent transformation applied to one pair of images—comprising a generated image and its transformed counterpart—matches that of another pair. This auxiliary loss thus incentivizes the generator to create images that are differentiable by the auxiliary network, subsequently fostering the generation of semantically coherent images concerning the latent transformations. We validate the effectiveness of this pretext task by enhancing image generation quality, as measured by FID, across state-of-the-art models in both conditional and unconditional frameworks on the CIFAR-10, CelebA-HQ, and ImageNet datasets. Additionally, we provide empirical evidence that LT-GAN improves controlled image editing capabilities for CelebA-HQ and ImageNet compared to baseline models. We also illustrate that our proposed LT self-supervision task can be successfully integrated with other leading training strategies for additional advantages. As a result, we achieve a new state-of-the-art FID score of 9.8 for conditional image generation on CIFAR-10.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1016", "problem_id": "10160001", "content": "This study presents novel attention-driven convolutional neural networks designed for hyperspectral image band selection. The method leverages convolutional activations across multiple network layers, employing gating mechanisms to pinpoint the most spectrally relevant regions. These modular attention modules are straightforward to integrate and support end-to-end training via gradient descent. Extensive experimental results demonstrate that deep architectures incorporating our attention mechanism achieve superior classification performance while consistently highlighting important spectral bands during training, enabling the generation of highly compact yet discriminative feature sets.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1017", "problem_id": "10170001", "content": "Time series data presents a unique and escalating challenge in the field of machine learning. As the volume of time series data expands, utilizing deep models that can concurrently learn and classify features may become impractical or yield subpar results. In this study, we introduce a method for feature learning using long short-term memory (LSTM) networks in conjunction with prediction through gradient boosting trees (XGB). Our focus is on the significant context of electronic health record data, where we forecast the likelihood of hypoxemia occurring five minutes in advance by analyzing previous features. We note two key insights: 1) LSTM networks excel at capturing long-range dependencies from a single feature, and 2) gradient boosting trees effectively and efficiently integrate numerous features, including fixed attributes such as height and weight. Based on these insights, we generate features through \"supervised\" representation learning with LSTM networks. Enhancing the original XGB model with these newly generated features results in considerably improved performance compared to using either method alone.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1018", "problem_id": "10180001", "content": "Acquiring interpretable representations that separate the fundamental factors of variation in data generation is widely regarded as crucial for machine learning. Although disentangled representations have proven beneficial for various applications, including abstract reasoning and unbiased classification, their scalability and practical utility remain uncertain. We present a novel high-resolution dataset comprising 1 million synthetic images and more than 1,800 labeled real-world images of the same configuration. Unlike prior datasets, this collection incorporates correlations, a sophisticated underlying framework, and enables assessment of transfer performance in both in-distribution and out-of-distribution scenarios for unseen synthetic and real-world data. We introduce innovative architectures to extend disentangled representation learning to high-resolution environments and perform an extensive empirical analysis of disentangled representations using this dataset. Our findings indicate that disentanglement serves as a reliable indicator for out-of-distribution task performance.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1019", "problem_id": "10190001", "content": "In the context of open set recognition (OSR), current approaches primarily focus on recognizing individual instances, even when they are presented in batches, and rely on a predetermined threshold to either reject or assign them to a known class. The threshold's value is crucial, yet its selection is often based on knowledge of known classes, which can lead to risks due to the lack of information about unknown classes. A more practical OSR system should not only reject unknown instances but also explore the possibility of discovering new classes among the rejected instances, a aspect that existing methods have overlooked. This paper proposes a novel collective decision strategy, aiming to enhance existing OSR methods by considering the relationships between testing instances and facilitating the discovery of new classes. To achieve this, a collective decision-based OSR framework (CD-OSR) is introduced, which modifies the Hierarchical Dirichlet process (HDP) to eliminate the need for a predefined decision threshold, enabling simultaneous open set recognition and new class discovery. The effectiveness of CD-OSR is demonstrated through extensive experiments on benchmark datasets, as shown in Figure A, B, C (References [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1020", "problem_id": "10200001", "content": "Time series classification (TSC) encompasses various algorithmic approaches that leverage distinct discriminatory patterns. Among these, classifiers relying on phase-dependent intervals form a notable category. The time series forest (TSF) method is a prominent interval-based classifier, recognized for its efficiency in training and prediction, along with strong performance. However, newer techniques have surpassed TSF in effectiveness. Originally, TSF employs three basic summary statistics for interval analysis. Recently, the `catch22' feature set, comprising 22 diverse and informative time series descriptors, was introduced to enhance time series analysis. We introduce the Canonical Interval Forest (CIF), a novel classifier that integrates TSF with catch22, along with refined training techniques and multivariate classification support. Our results show substantial accuracy gains over both TSF and catch22, positioning CIF competitively with leading methods from other algorithmic families. Furthermore, replacing TSF with CIF in the hierarchical vote collective of transformation-based ensembles (HIVE-COTE) yields notable improvements. HIVE-COTE with CIF achieves unmatched accuracy on the UCR archive, establishing a new benchmark in TSC performance.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1021", "problem_id": "10210001", "content": "Recurrent neural networks (RNNs) are particularly effective for capturing non-linear relationships in dynamic systems using time series data. Since not all external factors influencing these systems are known beforehand, especially in economic forecasting, Error Correction Neural Networks (ECNNs) were developed to address missing input variables. This is achieved by incorporating the prediction error from the previous time step into the current step. The ECNN, implemented in Python with gradient computations, demonstrates superior performance in stock market forecasting compared to basic RNNs, LSTMs, and hybrid models that include de-noising preprocessing. This advantage arises because de-noising may inadvertently remove relevant information.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1022", "problem_id": "10220001", "content": "Occlusion remains a significant challenge in video-based Re-ID, substantially affecting performance. While attention mechanisms have demonstrated utility in addressing occlusion, current methods often fail to incorporate sufficient discriminative information into final video representations. Existing approaches employing a single attention module also do not fully leverage multi-scale spatial information, and the attention of such a module can be diffused across multiple prominent areas of a person. This paper introduces a Concentrated Multi-grained Multi-Attention Network (CMMANet), featuring two multi-attention modules designed to extract multi-grained information by processing multi-scale intermediate features. Each multi-attention module comprises multiple attention submodules that automatically identify various discriminative regions within video frames. To accomplish this, a diversity loss is incorporated to diversify the submodules within each multi-attention module, alongside a concentration loss to integrate their attention responses, ensuring each submodule concentrates on a specific meaningful part. Experimental results demonstrate that the proposed method significantly surpasses state-of-the-art approaches across multiple public datasets.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1023", "problem_id": "10230001", "content": "The increase in global terrorist attacks, often targeting crowded public spaces for maximum impact and visibility, highlights the need for effective preventative measures. While surveillance cameras are commonly used, their effectiveness is limited by human monitoring constraints and their passive nature. This paper introduces a weapon detection system utilizing an ensemble of semantic Convolutional Neural Networks. This system addresses weapon detection and localization by dividing the task into smaller, component-specific sub-problems. This strategy offers computational and practical benefits, including reduced resource demands, parallel training capabilities, and user-adjustable sensitivity to balance false positives and negatives. Furthermore, ensemble theory suggests that aggregating the outputs of individual networks enhances system robustness and reliability, even with weaker individual models. Simulations were conducted to evaluate the accuracy of both individual networks and the integrated system, with promising results from both synthetic and real-world data. These findings suggest that this approach may offer advantages over monolithic, single deep convolutional neural network architectures.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1024", "problem_id": "10240001", "content": "Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN) are both algorithms that facilitate message passing on graphs, primarily aimed at node classification. LPA achieves this by disseminating node label information through the graph's edges, whereas GCN focuses on the propagation and transformation of node feature data. Despite their conceptual similarities, the theoretical connection between LPA and GCN has not been examined thus far. In this study, we explore their relationship through two dimensions: (1) feature/label smoothing, where we assess the dissemination of a node’s feature/label among its neighbors, and (2) feature/label influence, which quantifies the extent to which a node's initial feature/label affects another node's final feature/label. Building on our theoretical insights, we introduce an end-to-end model that integrates GCN and LPA for node classification. In this unifying model, the edge weights are adjustable, with LPA functioning as a regularization technique to aid GCN in refining edge weights, thereby enhancing classification performance. Additionally, our approach can be interpreted as learning attention weights informed by node labels, presenting a more task-focused alternative to existing feature-based attention frameworks. Through numerous tests on real-world graphs, our model outperforms state-of-the-art GCN-based techniques regarding node classification accuracy.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1025", "problem_id": "10250001", "content": "This study addresses the challenge of concept drift in supervised machine learning. We utilize graphical models to extract the observable structure of the data and draw inferences about shifts in the underlying context. Unlike earlier methods for detecting concept drift, our approach does not rely on a specific supervised machine learning model targeting a particular variable; instead, it aims to examine concept drift as an independent feature of dataset evolution. In particular, we analyze the progression of a graphical model by observing the formation of new connections and the loss of existing ones across various time intervals. The paper proposes a technique that emphasizes these changes and ultimately generates a metric to assess stability over time. We validate the method using real-world data from the Australian Electric market.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1026", "problem_id": "10260001", "content": "This research investigates semantic foggy scene understanding (SFSU), an area underexplored despite significant advancements in image dehazing and semantic scene understanding for clear-weather images. Addressing the challenge of acquiring and annotating foggy images, we create synthetic fog on real-world, clear-weather outdoor images and use these partially synthetic data for SFSU, utilizing advanced convolutional neural networks (CNN). Specifically, we introduce a complete pipeline for adding synthetic fog to clear images using incomplete depth information. Applying this to the Cityscapes dataset, we generate Foggy Cityscapes, comprising 20550 images. We address SFSU through both typical supervised learning and a novel semi-supervised learning approach that integrates supervised learning with unsupervised supervision transfer from clear-weather images to their synthetic foggy versions. Furthermore, we examine the impact of image dehazing on SFSU. For evaluation, we introduce Foggy Driving, a dataset of 101 real-world foggy driving scene images with ground truth annotations for semantic segmentation and object detection. Extensive experiments demonstrate that 1) supervised learning using our synthetic data substantially enhances the performance of state-of-the-art CNNs for SFSU on Foggy Driving; 2) our semi-supervised learning method further improves performance; and 3) image dehazing provides a slight improvement to SFSU when combined with our learning strategy. The datasets, models and code are made publicly available.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1027", "problem_id": "10270001", "content": "Japanese comics, known as manga, have historically been produced in black and white. Recently, alongside traditional monochrome comics, the emergence of full color comics, which offer a more visually appealing format, has been noted. However, the process of manual colorization for these comics is labor-intensive and costly. While there have been advancements in automatic colorization techniques, most have been tailored for illustrations rather than comics. Given that comics consist of a series of sequential images, maintaining a coherent painting style is essential. To address this issue, we introduce a semi-automatic colorization approach utilizing generative adversarial networks (GAN) that learns the artistic style of a specific comic from a limited training dataset. Our method accepts a combination of a screen tone image and a flat colored image as input, producing a colorized result. Experimental results indicate that our approach outperforms existing methods.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1028", "problem_id": "10280001", "content": "Anomaly detection in data analysis, while compelling, remains a difficult research area for real-world applications. As data dimensionality grows, understanding the semantic context of its description becomes crucial for effective anomaly characterization. Current anomaly detection techniques, however, struggle with high-dimensional data like ImageNet, having been primarily evaluated on lower-dimensional, cleaner, and more separable datasets such as MNIST and CIFAR-10. In this work, we explore anomaly detection in scenarios involving complex, high-dimensional normal data. We posit that anomaly data is typically characterized by semantically interpretable features that can also define semantic sub-clusters within normal data. Our hypothesis is that a feature space capable of semantically separating normal data sub-clusters can also effectively distinguish unseen anomalies. We propose a method involving semantic clustering of normal data, followed by training a classifier to learn a discriminative feature space for anomaly detection. Extensive experiments using MNIST, CIFAR-10, and ImageNet, with various normal and anomaly data combinations, demonstrate that our anomaly detection approach surpasses state-of-the-art methods, especially when dealing with high-dimensional, real-world images.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1029", "problem_id": "10290001", "content": "Machine learning techniques for visual emotion recognition hold significant potential; however, current methodologies are limited by training and evaluating models on small datasets encompassing a restricted range of visual emotion concepts. Our investigation reveals a crucial, yet previously disregarded, problem concerning dataset biases within existing visual emotion benchmarks. We develop a set of evaluations to demonstrate and quantify how these biases impede the development of a broadly applicable emotion recognition model. To address this, we introduce a webly supervised method that utilizes a substantial volume of stock image data. Our technique employs a straightforward, but efficient, curriculum-based training strategy to acquire distinctive emotion features. We find that models trained using our extensive stock image dataset demonstrate considerably enhanced generalization capabilities compared to those trained on existing datasets, and this is achieved without any manual labeling. Furthermore, visual representations learned via our approach show considerable promise across various tasks involving diverse image and video datasets.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1030", "problem_id": "10300001", "content": "We introduce a measure called Layer Saturation, which quantifies the fraction of eigenvalues required to account for 99% of the variance in latent representations, enabling the examination of learned features in neural network layers. This metric leverages spectral analysis and can be calculated efficiently, facilitating real-time monitoring of representations throughout training. Additionally, we explore potential uses of this measure by describing its patterns across various neural architectures and tasks. Our findings also demonstrate a connection between saturation levels and the generalization capabilities as well as the predictive accuracy of neural networks.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1031", "problem_id": "10310001", "content": "Stochastic variational inference serves as a well-known method for performing approximate Bayesian inference in deep models. Although numerous effective techniques exist for initializing loss minimization in deep learning, the initialization of stochastic variational inference has received considerably less focus. To bridge this gap, we introduce a new layer-wise initialization approach rooted in Bayesian linear models. Extensive experiments on regression and classification tasks, involving Bayesian DeepNets and ConvNets, demonstrate that our method achieves faster and more reliable convergence than existing initialization strategies derived from loss minimization literature.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1032", "problem_id": "10320001", "content": "The PFDet team introduces an extensive object detection system that supports training with vast datasets utilizing 512 GPUs, addresses classes with sparse verification, and deals with significant class imbalance. Our approach led to us securing the runner-up position in the Google AI Open Images Object Detection Track 2018 on Kaggle.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1033", "problem_id": "10330001", "content": "We examine fundamental principles of convex duality, with an emphasis on the broadly applicable and highly beneficial Fenchel-Rockafellar duality. We outline its applications across multiple reinforcement learning (RL) scenarios, such as policy evaluation or optimization, whether in online or offline contexts, along with considerations for both discounted and undiscounted rewards. The derivations lead to several compelling findings, including the capability to conduct policy evaluation and on-policy policy gradient using behavior-agnostic offline data, as well as strategies for policy learning through maximum-likelihood optimization. While many of these findings have been published in diverse formats before, we offer a cohesive analysis and viewpoint on these outcomes, which we trust will assist researchers in effectively employing the tools of convex duality to enhance advancements in RL.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1034", "problem_id": "10340001", "content": "Salient object detection has seen significant advancements with deep learning techniques; however, challenges persist due to the varying sizes and unidentified categories of salient objects. These challenges are intrinsically linked to the effective use of multi-level and multi-scale features. To address this, we introduce aggregate interaction modules designed to fuse features from neighboring levels, minimizing noise introduction by employing minimal up- or down-sampling rates. Furthermore, self-interaction modules are incorporated into each decoder unit to derive more effective multi-scale features from the integrated features. Recognizing that class imbalance stemming from scale variation diminishes the effectiveness of the binary cross entropy loss and leads to spatial inconsistencies in predictions, we employ a consistency-enhanced loss function. This loss function emphasizes the distinction between foreground and background while maintaining consistency within each class. Empirical evaluations conducted on five benchmark datasets reveal that our proposed method, without any post-processing steps, outperforms 23 state-of-the-art methods. The source code is available at https://github.com/lartpang/MINet.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1035", "problem_id": "10350001", "content": "The rise of artificial intelligence techniques in the biomedical field prompts researchers to focus more on uncertainty quantification (UQ) within machine-assisted medical decision-making. In classification tasks, previous studies on UQ are challenging to compare due to the absence of a standardized quantitative evaluation metric. Acknowledging that effective UQ models should recognize when classification models fail, we propose a novel metric, the area under Confidence-Classification Characteristic curves (AUCCC), to quantitatively assess the performance of UQ models. The AUCCC is designed to be threshold-free, resilient to perturbations, and unaffected by classification performance. We utilize AUCCC to evaluate various UQ methods (e.g., max softmax output) to demonstrate its effectiveness. Additionally, we introduce a straightforward approach called Uncertainty Distillation (UDist), which enhances UQ performance by having a confidence model refine the confidence estimates derived from deep ensembles. The proposed technique is readily implementable and consistently surpasses strong baseline models on both natural and medical image datasets in our experiments.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1036", "problem_id": "10360001", "content": "This paper introduces a hierarchical image segmentation technique. The approach initially constructs a disaffinity graph from the image and applies watershed segmentation to create oversegmented regions. Subsequently, a new graph is generated based on these regions, which are then merged using a single linkage clustering algorithm that incorporates size-dependent modifications. The method's near-linear time complexity enables its application to segmenting large images, as demonstrated through its use in addressing the complex task of segmenting 3D electron microscopic brain images.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1037", "problem_id": "10370001", "content": "In this study, we introduce an innovative method for self-supervised learning aimed at creating visual features that are aware of both global and local attention. Our strategy involves training a model to distinguish between particular transformations of an input image and its patched versions. By employing this technique, our method surpasses the best existing competitor by 1.03% on the Tiny-ImageNet dataset and by 2.32% on the STL-10 dataset. Additionally, our method exceeds the performance of the fully-supervised learning approach on the STL-10 dataset. The experimental findings and visual representations demonstrate the effectiveness of learning visual representations that are attentive to both global and local aspects.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1038", "problem_id": "10380001", "content": "This study emphasizes the importance of representation learning in preventing biased prediction outcomes, particularly in situations where learned representations are utilized by external parties with unspecified goals. To address this issue, we introduce and investigate adversarial representation learning as a means of promoting fairness among these parties. By linking various adversarial objectives to group fairness metrics, including demographic parity, equalized odds, and equal opportunity, we demonstrate the significance of objective selection in achieving fair predictions through both theoretical analysis and experimental validation. Additionally, our research provides the first comprehensive experimental evaluation of fair transfer learning, showcasing that the learned representations can facilitate fair predictions on novel tasks while preserving their utility, thereby fulfilling a fundamental objective of fair representation learning.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1039", "problem_id": "10390001", "content": "The application of deep learning technologies has become ubiquitous in autonomous driving, yielding promising outcomes across various aspects, with object detection being a crucial component in enhancing an autonomous agent's environmental perception and subsequent reaction. Nevertheless, traditional vision-based object detectors have struggled to deliver satisfactory performance in real-time driving scenarios. This paper introduces a real-time streaming perception system, which secured the 2nd Place solution in the Streaming Perception Challenge (Workshop on Autonomous Driving at CVPR 2021) for the detection-only track. Unlike conventional object detection challenges that prioritize absolute performance, streaming perception tasks necessitate a balance between accuracy and latency, a critical consideration for real-time autonomous driving. By leveraging YOLOv5 as the foundational framework and incorporating data augmentation, Bag-of-Freebies, and Transformer, our approach enhances streaming object detection performance while incurring negligible additional inference costs. Our method achieves a streaming AP of 33.2 (verified by the organizer as 34.6) on the Argoverse-HD test set under the specified hardware requirements, substantially outperforming the fixed baseline of 13.6 established by the host team, thereby demonstrating its potential for practical application.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1040", "problem_id": "10400001", "content": "Reinforcement learning (RL) enables the training of agents to accomplish complex objectives in uncertain environments, but a major challenge lies in defining an appropriate reward function for the agent to optimize. Imitation learning has been employed to address this issue, but conventional methods require first-person demonstrations, where the agent is given a sequence of states and corresponding actions, limiting their applicability due to the difficulty of collecting such demonstrations. In contrast, humans can learn from third-person demonstrations, where they observe others performing tasks and infer the task to achieve it themselves. This paper introduces a method for unsupervised third-person imitation learning, where an agent learns to achieve a goal in a simple environment from a demonstration of a teacher achieving the same goal from a different viewpoint, without a correspondence between teacher and student states. Leveraging recent advances in domain confusion, our approach yields domain-agnostic features that are essential for the training process, and we demonstrate its effectiveness through experiments in a pointmass domain, a reacher domain, and an inverted pendulum, as shown in Figure A, B, C (see References [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1041", "problem_id": "10410001", "content": "We investigate the distributed computation of the truncated singular value decomposition problem and introduce an algorithm referred to as \\texttt, which enhances communication efficiency. Our approach involves uniformly distributing the dataset across m nodes and alternating between several (specifically p) local power iterations followed by a global aggregation step. During the aggregation process, we suggest applying weights to each local eigenvector matrix using the orthogonal Procrustes transformation (OPT). As a practical alternative to OPT, we utilize sign-fixing, which involves a diagonal weighting matrix with \\pm 1 entries, demonstrating improved computational complexity and consistency in experiments. Theoretically, we establish that under specific conditions, \\texttt reduces the necessary number of communications by a factor of p to achieve a predetermined level of accuracy. Additionally, we show that a strategy of periodically decreasing p can lead to highly precise solutions. Our experiments validate the effectiveness of the \\texttt algorithm.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1042", "problem_id": "10420001", "content": "Graph neural networks (GNNs) have recently gained prominence as a framework for extending deep learning to graph-structured and relational data. Yet, with industrial datasets growing in scale, the message-passing operations essential for information propagation in GNN layers often become computationally prohibitive. While sampling techniques have been proposed to approximate full-graph training within feasible limits, challenges such as high variance and insufficient theoretical support persist. To mitigate these limitations, we reformulate GNN neighbor sampling as a multi-armed bandit problem, incorporating a novel reward function that introduces controlled bias to reduce variance and prevent unstable, potentially unbounded outcomes. Unlike previous bandit-based GNN approaches, our method achieves near-optimal regret while considering the training dynamics imposed by SGD. Practically, this results in more stable variance estimates and achieves competitive or improved test accuracy across multiple benchmarks.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1043", "problem_id": "10430001", "content": "This paper explores practical deep learning techniques for generating and improving level maps and textures in video games across various platforms, offering game developers and level artists novel creative avenues. Designing detailed, intricate, and realistic game levels is a demanding and resource-intensive process. Recent advances in deep learning, however, provide level designers and visual artists with powerful new tools capable of generating infinite worlds for increased game replayability and adapting educational games to individual player requirements. We introduce seven level map creation methods employing statistical, machine learning, and deep learning techniques, including: Generative Adversarial Networks for creating new images from existing examples (e.g. ProGAN); Super-resolution techniques for upscaling images while preserving crisp detail (e.g. ESRGAN); Neural style transfer for changing visual themes; Image translation - turning semantic maps into images (e.g. GauGAN); Semantic segmentation for turning images into semantic masks (e.g. U-Net); Unsupervised semantic segmentation for extracting semantic features (e.g. Tile2Vec); and Texture synthesis - creating large patterns based on a smaller sample (e.g. InGAN).", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1044", "problem_id": "10440001", "content": "Deep neural networks (DNNs) challenge the traditional bias-variance trade-off, as increasing parameters in a DNN that perfectly fits its training data often enhances its generalization performance. Understanding the rationale behind the advantages of such over-parameterization presents a significant challenge within deep learning theory. In this work, we investigate the final layer representation of different deep architectures, including Wide-ResNets for image classification, and identify a mechanism we refer to as *representation mitosis*: when the last hidden representation is sufficiently wide, its neurons tend to divide into groups that convey identical information, differing only by statistically independent noise. Similar to a mitotic process, the number of these groups, or \"clones,\" increases linearly with the layer's width, provided the width exceeds a critical threshold. We demonstrate that a crucial factor for initiating mitosis is the continuation of the training process until the training error reaches zero. Lastly, we reveal that in one of the learning tasks we examined, a wide model with multiple automatically generated clones significantly outperforms a deep ensemble based on architectures where the last layer matches the size of the clones.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1045", "problem_id": "10450001", "content": "This study proposes a novel monocular visual odometry algorithm that combines the strengths of geometry-based approaches and deep learning techniques. Traditional VO/SLAM systems, which often rely on geometric methods, require careful customization for various application scenarios and can be susceptible to scale-drift issues. While recent deep learning-based methods have attempted to learn VO in an end-to-end manner, their performance still lags behind that of geometry-based approaches. To address this, we re-examine the fundamentals of VO and investigate the optimal integration of deep learning with epipolar geometry and the Perspective-n-Point (PnP) method. Our approach involves training two convolutional neural networks (CNNs) to estimate single-view depths and two-view optical flows, which serve as intermediate outputs. These deep predictions are then utilized to design a robust frame-to-frame VO algorithm, termed DF-VO, which outperforms both pure deep learning-based and geometry-based methods. Notably, our system mitigates the scale-drift issue with the aid of a scale-consistent single-view depth CNN. Comprehensive experiments conducted on the KITTI dataset demonstrate the robustness of our system, and a detailed ablation study reveals the impact of various factors on our approach, as shown in Figure A, B, C (References [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1046", "problem_id": "10460001", "content": "This paper advocates that an explicit 3D shape representation is essential for achieving genuine novel view synthesis, enabling objects to be rendered from arbitrary viewpoints. Our approach reconstructs point clouds to model object geometry, allowing free rotation and projection into sparse images, which are then refined into dense views using an image completion network. The point cloud is derived from a single RGB image by predicting pixel-wise depth maps and incorporating camera intrinsics. Through forward and backward warping between input and target views, the network is trained end-to-end without requiring supervised depth data. Experiments on the 3D ShapeNet benchmark demonstrate the advantages of point clouds as an explicit 3D representation for novel view synthesis. Source code and data will be available at https://lhoangan.github.io/pc4novis/.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1047", "problem_id": "10470001", "content": "The efficacy of deep neural networks in capturing key input features through their representations is a well-documented phenomenon, yet the underlying mechanisms remain poorly comprehended from both theoretical and practical perspectives. A crucial inquiry in supervised learning pertains to the capacity of these representations to discern and retain features pertinent to classification while disregarding non-informative, noisy elements. To formally investigate this issue, we adopt a generative framework wherein each class is linked to a distinct high-dimensional manifold, with each input generated from two latent vectors: a \"manifold identifier\" \\gamma, which distinguishes between classes, and a \"transformation parameter\" \\theta, which introduces intra-class variations such as changes in pose, background, or lighting. For instance, \\gamma could symbolize a prototypical image of a dog, while \\theta could represent variations in its appearance. Our research presents both theoretical and empirical evidence suggesting that neural network representations can be perceived as locality-sensitive hash (LSH)-like functions, which map inputs to embeddings that depend solely on the informative \\gamma and are invariant to \\theta, thereby effectively extracting the manifold identifier \\gamma. A significant implication of this finding is the facilitation of one-shot learning for previously unseen classes, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1048", "problem_id": "10480001", "content": "In a semi-supervised setting, Generative Adversarial Networks (GANs) are capable of learning a mapping from random noise to realistic images, which can be leveraged for semi-supervised image classification, particularly in scenarios where no training data is available for unknown classes. Nevertheless, when the unknown class exhibits similarities with known classes, GANs may generalize and produce images that resemble both classes, thereby compromising classification performance. To address this issue, we introduce the Vanishing Twin GAN, a novel approach that involves training a weak GAN in conjunction with a regular GAN, utilizing the generated output image from the weak GAN in parallel with the regular GAN, ultimately enhancing semi-supervised image classification in cases where image similarity can negatively impact classification tasks.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1049", "problem_id": "10490001", "content": "Inspired by human ability to learn from limited examples, few-shot learning often uses benchmarks where representations are learned from a small base class dataset. This approach neglects the extensive prior knowledge humans possess before encountering new tasks. Moreover, even with a robust representation, some domains may have limited or no base class data. To address this, we investigate a scenario where a classifier pre-trained on a large, distinct dataset provides the representation, without access to its training details. In this scenario, limited base class examples serve to adapt the representation to the current domain, rather than facilitate learning from the beginning. We refine the representation in two steps, initially using the few available base class examples and subsequently using the even more limited data from new tasks. During this process, we extract a spatial attention map from the pre-trained classifier, enabling focus on relevant objects while minimizing background noise. This is crucial because the limited base class data prevent the network from implicitly learning where to focus. Our results demonstrate that a pre-trained network can be efficiently adapted to novel classes without employing meta-learning techniques.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1050", "problem_id": "10500001", "content": "Attention-based neural networks have demonstrated superior performance across numerous tasks, with deterministic attention mechanisms being more prevalent than stochastic approaches, which often face optimization challenges or complex architectures. This study presents Bayesian attention belief networks, employing a decoder network that models unnormalized attention weights through a hierarchical gamma distribution framework, alongside an encoder network built using Weibull distributions arranged in a deterministic-upward-stochastic-downward configuration to approximate the posterior. These auto-encoding networks are optimized via a differentiable variational lower bound. Existing models with deterministic attention, including pretrained ones, can be easily adapted to this Bayesian framework. Evaluations on multiple language understanding tasks reveal that our approach surpasses both deterministic attention and leading stochastic attention methods in accuracy, uncertainty estimation, cross-domain generalization, and resilience to adversarial attacks. Additionally, we validate the broad applicability of our method in neural machine translation and visual question answering, highlighting its potential for diverse attention-based applications.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1051", "problem_id": "10510001", "content": "The application of deep learning to single-image super-resolution (SISR) has yielded impressive results in recent years, but most convolutional neural network-based models are computationally intensive, limiting their practical applications. To address this limitation, this study proposes a lightweight super-resolution network, termed Adaptive Weighted Super-Resolution Network (AWSRN), for SISR. The AWSRN features a novel local fusion block (LFB) that facilitates efficient residual learning through the combination of stacked adaptive weighted residual units (AWRU) and a local residual fusion unit (LRFU). Additionally, an adaptive weighted multi-scale (AWMS) module is introduced to maximize the utilization of features in the reconstruction layer, comprising multiple convolutions of varying scales, with the option to remove redundant scale branches based on the adaptive weights' contribution, resulting in a lightweight network. Experimental evaluations on commonly used datasets demonstrate that the proposed AWSRN outperforms state-of-the-art methods with comparable parameters and computational overhead for scale factors of x2, x3, x4, and x8, with the code available at: https://github.com/ChaofWang/AWSRN.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1052", "problem_id": "10520001", "content": "Proximal Policy Optimization (PPO), a high-performing deep reinforcement learning algorithm particularly effective in continuous control, is limited by its exploration capabilities. While traditional reinforcement learning offers exploration strategies that balance exploration and exploitation, their complexity hinders application in complex environments. This paper examines PPO's Gaussian action exploration mechanism within dense reward continuous control tasks, highlighting the impact of exploration on performance. To address this, an uncertainty-estimation-based exploration enhancement mechanism is introduced and integrated into PPO, resulting in the intrinsic exploration module PPO (IEM-PPO) algorithm suitable for complex environments. Evaluated on MuJoCo, IEM-PPO is compared against Curiosity-driven Exploration (ICM-PPO) and the original PPO. Results indicate that, while requiring longer training, IEM-PPO exhibits improved sample efficiency, cumulative reward, stability, and robustness.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1053", "problem_id": "10530001", "content": "In this study, we demonstrate that optimizing video feature spaces for maximum predictability of temporal cycles enhances action classification. We introduce a new learning method known as Cycle Encoding Prediction (CEP), which effectively captures the high-level spatio-temporal dynamics of unannotated video data. CEP constructs a latent space where the notions of closed temporal loops, both forward-backward and backward-forward, are approximately maintained. Utilizing self-supervision, CEP harnesses the bi-directional temporal coherence present in the video stream and implements loss functions that promote both the closure of temporal cycles and the separation of contrasting features. The network architecture comprises a single feature encoder for all video clips and integrates two predictive modules that learn transitions in both temporal directions. We employ our framework for the pretext training of networks aimed at action recognition tasks, demonstrating substantial performance improvements on the standard datasets UCF101 and HMDB51. Comprehensive ablation studies validate the efficacy of the proposed elements. The source code for the CEP components is provided alongside this publication.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1054", "problem_id": "10540001", "content": "With the advancement of deep neural network (NN) techniques, there is growing interest in implementing NN-based solutions on edge computing devices like mobile phones or embedded controllers. These platforms typically face limitations in energy and computational resources, yet modern NN architectures are rarely optimized for such constraints. Current methods for minimizing NN resource consumption generate fixed models that operate at a single point in the performance-resource tradeoff. This work introduces a method for developing runtime-throttleable NNs capable of dynamically adjusting their performance and resource utilization based on a control input. Such adaptable networks enable efficient resource allocation, such as reducing consumption under simple conditions or low battery levels. We propose a general framework for throttling through block-level gating, implement it across various standard CNN architectures, and show that our method achieves smooth performance scaling across diverse operating conditions in image classification and object detection tasks, with minimal impact on maximum accuracy.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1055", "problem_id": "10550001", "content": "Deep Convolutional Neural Networks (CNNs) have achieved remarkable success in the domains of image classification and object detection. In these areas, the outputs from all layers of CNNs are frequently viewed as a high-dimensional feature vector obtained from an input image, and understanding the relationship between detailed feature vectors and the concepts present in the input image is crucial. Nevertheless, there has been limited research addressing this important aspect. To tackle this, we introduce a novel method that creates an edited version of each original CNN feature vector by utilizing the maximum entropy principle to eliminate specific vectors. These omitted vectors relate to undesirable concepts within each image category. By training a classifier on the combined feature sets, we can significantly enhance the model's generalization across individual categories, especially when the training data is scarce. Experimental results from classification-based object detection on standard datasets like VOC 2007 (60.1%), 2010 (56.4%), and 2012 (56.3%) demonstrate a clear increase in mean average precision (mAP) when using simple linear support vector machines.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1056", "problem_id": "10560001", "content": "Image representations are typically derived from class labels, which serve as a basic approximation of human image comprehension. This study demonstrates that transferable image representations can be acquired without manual annotations by simulating human visual attention. Our analysis uses a distinctive gaze tracking dataset of sonographers conducting routine clinical fetal anomaly screenings. We develop models of sonographer visual attention by training a convolutional neural network (CNN) to predict gaze on ultrasound video frames through visual saliency prediction or gaze-point regression. We assess the transferability of the derived representations for the task of ultrasound standard plane detection in two scenarios. Firstly, we employ transfer learning by fine-tuning the CNN with a limited set of labeled standard plane images. Our findings indicate that fine-tuning the saliency predictor outperforms training from scratch, yielding an average F1-score enhancement of 9.6% overall and 15.3% for cardiac planes. Secondly, we implement a straightforward softmax regression on the feature activations from each CNN layer to evaluate the representations independent of transfer learning hyper-parameters. Our results show that the attention models produce robust representations, nearly matching the accuracy of a fully-supervised baseline model, except for the final layer.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1057", "problem_id": "10570001", "content": "Roof falls resulting from geological factors present significant safety risks in the mining and tunneling sectors, leading to work disruptions, injuries, and even fatalities. Numerous large-opening limestone mines in the Eastern and Midwestern United States experience roof fall issues attributed to elevated horizontal stresses. The conventional strategy for managing such roof fall dangers predominantly relies on visual assessments and the expertise of professionals. This research introduces an artificial intelligence (AI) system designed to detect roof fall hazards stemming from high horizontal stresses. We leverage images of both hazardous and non-hazardous roof conditions to create a convolutional neural network capable of autonomously identifying dangerous roof situations. To address the challenge of limited input data, we adopt a transfer learning technique, whereby a pre-trained network is utilized as a foundation for classification in a related domain. Our findings indicate that this method is effective in distinguishing between hazardous and safe roof conditions, achieving a statistical accuracy of 86%. Nonetheless, accuracy alone does not suffice for a dependable hazard management framework. The reliability and constraints of the system can be enhanced by comprehending the features the network employs. Consequently, we apply an interpretation technique known as integrated gradients to pinpoint the significant geological characteristics in each image used for prediction. The analysis reveals that the system aligns with expert assessment in detecting roof fall hazards. The AI system developed in this study highlights the potential of deep learning to aid in geological hazard management and likely serves as a crucial component of autonomous tunneling operations, especially in scenarios where hazard identification relies significantly on expert input.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1058", "problem_id": "10580001", "content": "Convolutional neural networks (CNNs) achieve remarkable performance in super-resolution (SR) tasks, yet their complex architectures often lead to high memory usage and computational demands, limiting their applicability on devices with constrained resources. This paper introduces a new contrastive self-distillation (CSD) approach designed to both compress and accelerate existing SR models. Specifically, a lightweight student network is derived from a teacher network through channel splitting, followed by a novel contrastive loss that enhances SR image quality and improves PSNR/SSIM metrics by facilitating explicit knowledge transfer. Comprehensive experiments show that the CSD method successfully optimizes well-known SR models, including EDSR, RCAN, and CARN. Code is available at https://github.com/Booooooooooo/CSD.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1059", "problem_id": "10590001", "content": "An iris recognition system faces risks from presentation attacks (PAs), where malicious individuals use replicas like printed eyes, plastic replicas, or cosmetic contact lenses to deceive the system. In this study, we introduce a robust iris PA detection solution named D-NetPAD, built on the DenseNet convolutional neural network framework, which exhibits strong generalization across various PA artifacts, sensors, and datasets. Our experiments on a proprietary dataset alongside a publicly available dataset (LivDet-2017) verify the proposed method's efficacy in distinguishing iris PAs. The method achieves a true detection rate of 98.58% with a false detection rate of 0.2% on the proprietary dataset and exceeds the performance of leading techniques on the LivDet-2017 dataset. To elucidate the operation of D-NetPAD, we employ t-SNE plots and Grad-CAM to visualize intermediate feature distributions and fixation heatmaps, respectively. Additionally, we perform a frequency analysis to characterize the features extracted by the network. The source code and trained model can be accessed at https://github.com/iPRoBe-lab/D-NetPAD.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1060", "problem_id": "10600001", "content": "Wasserstein distributionally robust optimization (WDRO) aims to train models that minimize the worst-case risk within a Wasserstein ball around the empirical data distribution. Although WDRO has emerged as a promising inference tool, its theoretical foundations remain incomplete. Gao et al. (2017) introduced a minimizer using a tractable approximation of the worst-case risk but did not establish risk consistency. This work presents a new minimizer derived from an innovative approximation theorem, along with corresponding risk consistency guarantees. Additionally, we extend WDRO inference to locally perturbed data, encompassing Mixup (Zhang et al., 2017) as a specific instance, demonstrating that our approximation and consistency findings generalize to such scenarios. Empirical evaluations on image classification tasks confirm the method's robustness, with results indicating superior accuracy over baseline models on noisy datasets.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1061", "problem_id": "10610001", "content": "We introduce a multi-object tracking and segmentation (MOTS) technique that circumvents the need for fine-tuning or benchmark-specific hyperparameter optimization. Our approach specifically targets the data association challenge, an area identified by the HOTA metric—which more accurately reflects human visual perception through balanced evaluation of detection and association quality—as requiring further development. Our method constructs tracklets via instance segmentation and optical flow, subsequently employing a space-time memory network (STM), originally designed for one-shot video object segmentation, to refine the association of tracklets across temporal discontinuities. To our knowledge, our method, MeNToS, represents the pioneering application of STM networks for tracking object masks in MOTS. This approach achieved 4th place in the RobMOTS challenge. The project website is available at https://mehdimiah.com/mentos.html.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1062", "problem_id": "10620001", "content": "Edge detection plays a pivotal role in image processing for object identification, making it essential to comprehend edge detection techniques thoroughly. Edges define an object's outline, marking the transition between the object and its background, as well as the borders between overlapping objects. Accurate edge identification enables precise object localization and the measurement of fundamental attributes like area, perimeter, and shape. Given that computer vision relies on recognizing and categorizing objects within images, edge detection serves as a critical component. We evaluated two edge detection algorithms employing distinct methodologies and analyzed their performance across diverse scenarios to ascertain the more suitable detector under varying conditions.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1063", "problem_id": "10630001", "content": "Self-explainable deep learning models are designed to inherently represent latent data concepts, eliminating the need for subsequent explanation methods. Focusing on a model that formulates the classifier function as a linear function, we demonstrate that leveraging probabilistic latent variables and strategically modifying model components enhances both explanation quality and predictive accuracy. Beyond conventional visualization methods, we introduce a novel technique to improve comprehension of latent concepts. Additionally, we present a method employing two distinct self-supervision strategies to extract relevant concepts, leading to a substantial performance improvement. Notably, our approach performs effectively with limited data and attains target accuracy rapidly. Comprehensive results are presented using the CIFAR10, CIFAR100, and AWA2 datasets, illustrating the efficacy of our method on datasets of varying complexity.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1064", "problem_id": "10640001", "content": "Existing research on unsupervised learning has primarily concentrated on developing structural priors to extract meaningful features, yet this work often overlooked the description length of the learned representations, which is a straightforward and impartial indicator of model complexity. In this paper, we first introduce the \\varphi metric, which assesses unsupervised models by examining their reconstruction accuracy and the extent of compression in their internal representations. We then define two baseline activation functions (Identity, ReLU) and propose three sparse activation functions (top-k absolutes, Extrema-Pool indices, Extrema) as potential structures that aim to minimize the \\varphi metric. Additionally, we introduce Sparsely Activated Networks (SANs), which comprise kernels with shared weights that are convolved with the input during encoding and processed through a sparse activation function. In the decoding phase, the same weights are utilized to convolve with the sparse activation map, and the partial reconstructions from each weight are aggregated to recreate the input. We evaluate the SANs using the five previously discussed activation functions across various datasets (Physionet, UCI-epilepsy, MNIST, FMNIST) and demonstrate that models chosen based on the \\varphi metric possess a small description representation length along with interpretable kernels.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1065", "problem_id": "10650001", "content": "This study conducts an experimental examination of various feature engineering pipelines in conjunction with a Convolutional Neural Network (CNN) for distinguishing between eyes-open and eyes-closed states using electroencephalogram (EEG) time-series data from the Bonn dataset. By leveraging the Takens' embedding to create a geometric representation of the time-series, simplicial complexes are constructed from the EEG data, and subsequently, two topological invariants - the \\epsilon-series of Betti-numbers and the \\epsilon-series of graph spectra - are compared to the raw EEG time series, addressing a gap in the literature by providing a benchmark. Drawing inspiration from Topological Data Analysis, these methods are employed for feature engineering to capture the local geometric properties of the time-series. Furthermore, the robustness of these feature pipelines to downsampling and data reduction is assessed, with the aim of establishing a clearer understanding of time-series classification using geometric features and the response of CNNs to time-series data of varying resolutions.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1066", "problem_id": "10660001", "content": "This study presents a theoretical framework for reconstructing three-dimensional scenes from sequential video frames, enabling the visualization of videos in 3D. The approach involves extracting features associated with moving rigid objects in 3D from individual frames and establishing correspondences between them. The vanishing point, calculated in each frame to determine the direction of the moving object, is utilized to position the 3D structure of the object in space. Preliminary experiments have been carried out, with the resulting data made publicly available, as shown in Figure A, B, C (References [1], [2], [3]). These initial findings demonstrate the viability of the proposed method, and the paper concludes by outlining potential avenues for future research, including the incorporation of non-rigid objects and the consideration of camera motion, as discussed in [4] and [5].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1067", "problem_id": "10670001", "content": "We present a set of datasets derived from fundamental physics research—encompassing particle physics, astroparticle physics, and hadron- and nuclear physics—designed for supervised machine learning investigations. To facilitate advancements in interdisciplinary machine learning and transfer learning within fundamental physics, we are releasing datasets comprising hadronic top quarks, cosmic-ray induced air showers, phase transitions in hadronic matter, and generator-level histories. Utilizing these data, we propose a straightforward but versatile graph-based neural network architecture suitable for diverse supervised learning tasks across these fields. Our results demonstrate that this method achieves performance levels comparable to specialized, state-of-the-art techniques across all datasets. To ease adoption for different problems, we offer clear guidelines on creating graph-based data representations relevant to fundamental physics, along with code implementations for several representations. Code implementations are also supplied for our proposed method and all reference algorithms.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1068", "problem_id": "10680001", "content": "The task of video captioning, situated at the confluence of computer vision and natural language processing, poses significant challenges and has numerous practical applications, including video retrieval, surveillance, assistance for the visually impaired, and human-machine interaction. Although recent advances in deep learning have yielded promising outcomes, the performance of video captioning still lags behind that of other computer vision tasks, such as image classification and object detection. A major limitation of existing video captioning approaches is their reliance on the cross-entropy loss function, which does not correlate with the commonly used evaluation metrics, including BLEU, METEOR, CIDER, and ROUGE, thereby making it an inadequate surrogate for the true loss function. To overcome this limitation, this paper proposes a dynamic loss network (DLN) that provides an additional feedback signal directly aligned with the evaluation metrics, and demonstrates superior performance to previous methods on the Microsoft Research Video Description Corpus (MSVD) and MSR-Video to Text (MSRVTT) datasets.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1069", "problem_id": "10690001", "content": "This work investigates the global optimization of a target variable within a causal framework where sequential interventions are possible, a challenge relevant to fields such as biology, operations research, and communications, where optimizing system-level metrics is crucial. The proposed method integrates principles from causal inference, uncertainty quantification, and sequential decision-making, extending Bayesian optimization to settings with causal dependencies among input variables. By leveraging the causal graph structure, the approach enhances decision-making efficiency, reducing optimization costs and avoiding suboptimal outcomes. The introduced Causal Bayesian Optimization (CBO) algorithm dynamically balances the exploration-exploitation trade-off alongside a novel observation-intervention trade-off, which arises when combining empirical interventional data with do-calculus-based effect estimates. The effectiveness of CBO is validated through synthetic experiments and real-world case studies.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1070", "problem_id": "10700001", "content": "Phase retrieval (PR) is a crucial element in contemporary computational imaging systems, with numerous algorithms having emerged over the last fifty years. Recent developments in deep learning present opportunities for more robust and rapid PR solutions. A promising strategy known as deep unfolding establishes a systematic relationship between traditional model-based iterative methods and modern data-driven deep learning. Algorithms that have undergone unfolding, enhanced by data learning, demonstrate significant improvements in both performance and convergence speed compared to their predecessors. However, a major limitation of many existing unfolded algorithms is their restriction to a predetermined number of iterations when utilizing layer-dependent parameters. This paper introduces an innovative framework for deep unfolding that addresses these constraints. While our framework is applicable to a broad range of inverse problems, we focus on PR as an illustrative case. Our approach is grounded in an unfolded generalized expectation-consistent signal recovery (GEC-SR) algorithm, where damping factors are designated for data-driven learning. Specifically, we present a hypernetwork designed to produce the damping factors for GEC-SR. Rather than directly learning a fixed set of optimal damping factors, the hypernetwork adapts by generating optimal factors tailored to specific clinical conditions, thereby enhancing its versatility for various situations. To enable the hypernetwork's adaptability to fluctuating layer counts, we implement a recurrent architecture to create a dynamic hypernetwork that generates a damping factor capable of changing in real-time across layers. Furthermore, we incorporate a self-attention mechanism to bolster the hypernetwork's robustness. Extensive experimental results indicate that our proposed algorithm surpasses existing methods in terms of convergence speed and accuracy, maintaining effectiveness even in challenging conditions where numerous traditional PR algorithms may falter or become unstable.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1071", "problem_id": "10710001", "content": "We introduce an enhancement of sparse Canonical Correlation Analysis (CCA) aimed at uncovering multiple-to-multiple linear correlations within a single variable set. In contrast to CCA, which identifies correlations between two matched data sets with distinct variable groups, our proposed method, Canonical Autocorrelation Analysis (CAA), detects multivariate correlations within a single set of variables. This approach is particularly beneficial for revealing underlying parsimonious structures in data, each pertaining to a limited subset of features. Moreover, the identified correlations are easily interpretable, as they comprise pairs of sparse linear combinations of the original features. We demonstrate the applicability of CAA as an anomaly detection tool, especially when anomalous data deviates from expected correlation structures. We showcase the effectiveness of CAA in two domains where single-class and unsupervised learning of correlation structures are crucial: breast cancer diagnosis and radiation threat detection. In the analysis of the Wisconsin Breast Cancer dataset, single-class CAA shows competitive performance with supervised methods documented in literature. For the radiation threat detection task, unsupervised CAA significantly outperforms a commonly used unsupervised method in the field while offering valuable insights for threat evaluation.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1072", "problem_id": "10720001", "content": "Deep learning has achieved remarkable results in complex perception tasks and is commonly used to power intelligent software applications. Nevertheless, deep neural networks are susceptible to adversarial examples. Various defense mechanisms have been developed to counter these adversarial inputs, but they often lack robustness against novel attacks or are tailored to specific attack types. Furthermore, many existing methods suffer from intricate designs, leading to substantial computational overhead and latency, rendering them impractical for real-world software deployment. To address these limitations, we introduce DAFAR, a feedback-based framework designed to effectively and universally detect and mitigate adversarial examples while minimizing area and time overhead. DAFAR features a straightforward architecture comprising a victim model, a plug-in feedback network, and a detector. The core concept involves integrating high-level features from the victim model's feature extraction layers into the feedback network to reconstruct the input, creating a feedback autoencoder. This approach transforms subtle attacks on the victim model into readily detectable reconstruction-error attacks on the feedback autoencoder for potent adversarial instances. Conversely, weaker attacks are disrupted during the reformation process, compromising their adversarial structure. Experimental results on the MNIST and CIFAR-10 datasets demonstrate DAFAR's efficacy against prevalent and sophisticated attacks without compromising performance on legitimate samples, exhibiting high effectiveness and universality across diverse attack methods and parameter settings.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1073", "problem_id": "10730001", "content": "Research has indicated that individual neurons in convolutional neural networks used for image-level classification tasks can implicitly acquire semantically meaningful concepts, encompassing everything from basic textures and shapes to complete or partial objects, effectively creating a \"dictionary\" of concepts through the learning process. In this study, we present a straightforward and effective zero-shot learning method inspired by this finding. Named Neuron Importance-Aware Weight Transfer (NIWT), our approach establishes a mapping of domain knowledge regarding novel \"unseen\" classes onto this repository of learned concepts and subsequently optimizes the network parameters to proficiently integrate these concepts—essentially constructing classifiers by uncovering and assembling learned semantic concepts within deep networks. Our method demonstrates enhanced performance compared to prior techniques on the CUBirds and AWA2 generalized zero-shot learning benchmarks. We apply our approach to a wide array of semantic inputs as external domain knowledge, such as attributes and natural language captions. Additionally, by learning inverse mappings, NIWT aids in generating visual and textual explanations for the predictions made by the newly developed classifiers and assigns names to the neurons. Our code is accessible at https://github.com/ramprs/neuron-importance-zsl.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1074", "problem_id": "10740001", "content": "The Wasserstein distance and its variants, such as the sliced-Wasserstein (SW) distance, have gained significant interest in machine learning. The SW distance shares key properties with the Wasserstein distance but is computationally more efficient, making it suitable for applications like generative modeling and supervised or unsupervised learning. This study establishes the mathematical relationship between the SW distance and the Radon transform and introduces a novel family of distances for probability measures, termed generalized sliced-Wasserstein (GSW) distances, using the generalized Radon transform. Additionally, we demonstrate that the GSW distance can be expanded into a maximum GSW (max-GSW) distance and specify the conditions under which both GSW and max-GSW qualify as valid distance metrics. Lastly, we evaluate the proposed distances numerically in various generative modeling scenarios, including SW flows and SW auto-encoders.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1075", "problem_id": "10750001", "content": "Parkinson's disease (PD) is a neurodegenerative disorder characterized mainly by symptoms such as muscle rigidity, reduced movement (hypokinesia), slowness of movement (bradykinesia), and tremors. For patients in the advanced stages of PD, Deep Brain Stimulation neurosurgery (DBS) serves as the most effective option when medical therapies no longer elicit a response. This surgical intervention leads to neuronal activity through electrical stimulation, which is quantified as the Volume of Tissue Activated (VTA). Precise localization of the VTA within the brain's volume requires accurate knowledge of the tip position of the DBS electrodes and their spatial orientation. In this study, we present an automatic method for identifying DBS electrodes through a threshold-based segmentation technique in medical imaging, adaptively determining the optimal threshold value. Our approach facilitates the detection of DBS electrodes within Computed Tomography (CT) images, demonstrating significant resilience to noise by employing automatic threshold detection strategies.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1076", "problem_id": "10760001", "content": "While Generative Adversarial Networks (GANs) have achieved remarkable success in image synthesis, the underlying mechanisms of deep generative representations and the role of layer-wise stochasticity in producing photo-realistic images remain poorly understood. This study reveals that advanced GAN architectures, such as StyleGAN and BigGAN, develop a structured semantic hierarchy as variation factors when generating scenes from their learned representations. By analyzing layer-wise activations across diverse semantic levels, we establish a causal relationship between these activations and the resulting image semantics, uncovering interpretable variation factors that GANs utilize for scene composition. Our findings demonstrate that GANs with layer-wise latent codes exhibit specialization in generating hierarchical semantics: early layers influence spatial structure, middle layers govern object categories, and later layers handle scene attributes and color schemes. This discovery of controllable latent factors enables precise semantic manipulation of generated scenes.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1077", "problem_id": "10770001", "content": "We introduce a projection-free metric learning framework for optimizing a convex and differentiable objective function Q(\\M) over a real symmetric matrix \\M, which is used to compute Mahalanobis distances and resides in the positive definite cone. Specifically, we consider \\M to be an element of the space \\cS of generalized graph Laplacian matrices associated with balanced signed graphs, where a matrix \\M in \\cS that is also positive definite is referred to as a graph metric matrix. Notably, \\cS encompasses diagonal-only matrices as a special case, unlike the low-rank metric matrices commonly found in the literature. A key result, known as Gershgorin disc perfect alignment (GDPA), enables the efficient optimization of metric matrices by avoiding full eigen-decomposition. This theorem states that for a matrix \\M in \\cS and a diagonal matrix \\S, where \\S is derived from the first eigenvector of \\M, the Gershgorin discs of the similarity transform \\B = \\S \\M \\S^ are perfectly aligned at the smallest eigenvalue. By leveraging this result, we replace the positive definite cone constraint with tight linear constraints at each iteration, allowing for the efficient optimization of the diagonal and off-diagonal terms of \\M using the Frank-Wolfe method. The first eigenvector is updated using Locally Optimal Block Preconditioned Conjugate Gradient (LOBPCG) with a warm start as the entries of \\M are optimized successively. Our experiments demonstrate that the proposed graph metric optimization approach significantly outperforms cone-projection schemes in terms of speed while achieving competitive binary classification performance, as shown in Figure A, B, C (see References [citation] for details).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1078", "problem_id": "10780001", "content": "Leading approaches for two-sample and independence tests have emerged from both the statistics and machine learning communities, with distance-based tests, or \"energy statistics\", and kernel-based tests, derived from \"kernel mean embeddings\", being prominent methods in their respective fields. A previous fixed-point transformation was introduced to establish a connection between distance and kernel methods in the context of population statistics. This paper presents a novel bijective transformation that maps metrics to kernels, offering a simplified alternative to the fixed-point transformation while retaining similar theoretical properties. This new transformation enables distance methods to yield identical results to kernel methods for sample statistics and p-values, and it better preserves the underlying data structure. By advancing our understanding of distance and kernel-based tests, our findings facilitate a more streamlined implementation of these tests, and enable seamless communication between the vast array of distance-based and kernel-based methodologies, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1079", "problem_id": "10790001", "content": "While Shapley Values (SV) are commonly employed in explainable AI, their interpretation and estimation can be problematic, potentially resulting in misleading conclusions and explanations. We first revisit an invariance principle for SV and establish the proper method for calculating SV for categorical variables, which are highly dependent on their encoding. For tree-based models, we propose two efficient SV estimators that leverage the tree architecture and outperform existing techniques in accuracy. To enhance the interpretation of additive explanations, we suggest filtering out non-influential variables and computing SV only for influential variable groups. This involves utilizing the \"Same Decision Probability\" (SDP) concept, which assesses prediction stability when certain variables are omitted. This preliminary selection process yields sparser additive explanations, facilitating visualization and analysis. Our approach is validated through simulations and comparisons with leading algorithms, demonstrating its practical benefits.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1080", "problem_id": "10800001", "content": "This study explores the use of machine learning for determining pH levels through colorimetric analysis using smartphones. Images of pH test strips served as training data for Least Squares-Support Vector Machine (LS-SVM) classifier algorithms, which accurately categorized different pH values. Variations in image formats had a negligible impact on the machine learning approach's performance. Furthermore, the study examined how lighting conditions affect the perceived color of pH strips, with subsequent experiments evaluating the impact of color variations on the learning model. Testing with JPEG, RAW, and corrected-RAW image formats, acquired under diverse lighting, resulted in perfect classification accuracy, sensitivity, and specificity. These results demonstrate that colorimetric detection utilizing machine learning adapts well to differing experimental conditions, making it suitable for smartphone-based sensing in paper-based colorimetric assays.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1081", "problem_id": "10810001", "content": "We consider molecular optimization to be a problem of translating graphs from one form to another. The objective is to train a system to transform one molecular graph into a different one that exhibits improved properties, based on a dataset of paired molecules. Given that there are various methods to optimize molecules, each input graph can have several feasible translations. Consequently, a significant challenge lies in effectively modeling the diverse outputs of these translations. Our main contributions involve a junction tree encoder-decoder designed for learning varied graph translations, coupled with an innovative adversarial training approach to align molecular distributions. In our model, diverse output distributions are concretely represented through low-dimensional latent vectors that influence the translation process. We assess our model across numerous molecular optimization tasks, demonstrating that it surpasses previously established state-of-the-art benchmarks.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1082", "problem_id": "10820001", "content": "The identification of moving objects is crucial for autonomous driving systems. Following the perception stage, motion planning is generally executed in Bird's Eye View (BEV) space, necessitating the transformation of detected objects from the image plane to the top-view BEV plane. This projection is often susceptible to inaccuracies due to insufficient depth information and noisy mapping in distant regions. Convolutional Neural Networks (CNNs) can utilize the global context of the scene for enhanced projections. In this study, we investigate end-to-end Moving Object Detection (MOD) directly on the BEV map, using monocular images as input. To our knowledge, an appropriate dataset has not previously existed, prompting us to produce an extensive KITTI-raw dataset that includes 12.9k images annotated with moving object masks in BEV space across five categories. This dataset is designed for class-agnostic motion cue-based object detection, with class information provided as meta-data to facilitate improved tuning. We developed a two-stream architecture that fuses RGB images with optical flow, generating motion segmentation directly in BEV space. Our method is compared against inverse perspective mapping of leading motion segmentation predictions from the image plane, revealing a substantial enhancement of 13% in mean Intersection over Union (mIoU) with a basic implementation. This evidences the potential to directly learn motion segmentation outputs in BEV space. Qualitative assessments of our baseline results and dataset annotations can be accessed at https://sites.google.com/view/bev-modnet.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1083", "problem_id": "10830001", "content": "Due to the scarcity of defective samples in industrial quality control, segmentation models often exhibit overfitting when deployed in real-world scenarios. To mitigate this, we introduce a defect simulation method grounded in neural style transfer. This algorithm requires minimal defective sample data for training and can efficiently produce simulated samples for subsequent segmentation. Our approach incorporates a masked histogram matching module to ensure color consistency between simulated defects and genuine defects. To maintain texture coherence with neighboring pixels, we employ a fast style transfer algorithm to seamlessly integrate the generated area into the background. Additionally, we utilize histogram loss to further refine the quality of the generated imagery. Furthermore, we present a novel segmentation network architecture tailored for defect segmentation tasks. We trained the segmentation network using both real and simulated defect samples from button datasets. The findings indicate that the F1 score of the model trained exclusively on simulated data reaches 0.80, surpassing the performance achieved with real samples.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1084", "problem_id": "10840001", "content": "The registration of 3D point clouds is a crucial task in numerous robotics and computer vision applications, with popular methods like iterative closest point and its variants being limited to local optimality. Recent efforts to achieve globally optimal registration have been hindered by poor performance in noisy measurement environments. This study presents a mixed integer programming-based approach that accounts for uncertainty in its optimization, yielding more accurate estimates of globally optimal registration. Additionally, a multi-step optimization strategy is developed, combining fast local methods with the proposed global formulation for practical implementation. The approach is validated through comprehensive simulations and real-world experiments, demonstrating superior performance to state-of-the-art methods across various noise and outlier levels, as well as partial geometric overlap scenarios.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1085", "problem_id": "10850001", "content": "Currently, contemporary Earth Observation systems are continuously gathering vast quantities of satellite data. The extraordinary ability to obtain high-resolution Satellite Image Time Series (SITS) data, which comprises a sequence of images captured over the same geographic region with a high revisit frequency, presents new prospects for observing various elements of the Earth's surface. However, this also introduces new challenges in determining effective methods to analyze and utilize such extensive and intricate image data. A key task related to SITS data analysis is land cover mapping, where satellite imagery is employed through learning techniques to ascertain the status of the Earth’s surface, or the relevant land cover classes. Due to practical limitations, the available labeled information used to train machine learning algorithms is frequently insufficient in quantity and derived at a coarse scale, resulting in imprecise and weak knowledge that can adversely impact the entire process. To address these challenges, we introduce a novel deep learning framework for object-based SITS land cover mapping, referred to as TASSEL (aTtentive weAkly Supervised Satellite image time sEries cLassifier), designed to effectively leverage the weak supervision offered by the coarse labels. Additionally, our framework generates supplementary side-information that enhances model interpretability, aiming to transform the black box into a more transparent gray box. This side-information facilitates the association of spatial interpretations with the model's decisions through visual scrutiny.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1086", "problem_id": "10860001", "content": "This study introduces a novel algorithm designed for recognizing characters in obscured images. The proposed technique demonstrates a possible vulnerability for existing postal systems. The research evaluates the algorithm's effectiveness and proposes defensive strategies to mitigate such security risks.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1087", "problem_id": "10870001", "content": "Identifying a person's focal point provides valuable social information. This paper introduces Gaze360, a substantial gaze-tracking dataset, alongside a technique for accurate 3D gaze estimation within unrestricted images. Comprising data from 238 individuals in diverse indoor and outdoor settings, the dataset features labeled 3D gaze data encompassing a wide spectrum of head orientations and distances. Enabled by an uncomplicated and effective collection process, it is the most extensive publicly accessible resource of its type, considering both the number of participants and the diversity of conditions. The proposed 3D gaze model builds upon prior models by incorporating temporal data and directly generating an assessment of gaze uncertainty. The advantages of the model are highlighted through an ablation study, and its generalization capabilities are validated through cross-dataset evaluation against other recent gaze benchmark datasets. A straightforward self-supervised strategy is also presented to enhance cross-dataset domain adaptation. Finally, the model's utility is showcased through an application that estimates customer attention within a supermarket environment. The dataset and models are available at http://gaze360.csail.mit.edu .", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1088", "problem_id": "10880001", "content": "Human Attribute Recognition (HAR) has garnered significant attention in recent years due to its inherent scientific complexities and vast application potential, with attribute localization being a critical yet underaddressed aspect. This paper introduces a groundbreaking deep learning framework for HAR, termed Distraction-aware HAR (Da-HAR), which augments the feature learning capabilities of deep Convolutional Neural Networks (CNNs) by incorporating a coarse-to-fine attention mechanism to enhance attribute localization. The mechanism operates in two stages: initially, a self-mask block is utilized to coarsely identify and mitigate distractions, followed by a masked attention branch that refines the process by eliminating irrelevant regions at a finer scale. This approach yields more precise feature learning, particularly in scenarios involving substantial occlusions and intricate backgrounds, as evidenced by the state-of-the-art results obtained through comprehensive experiments on the WIDER-Attribute and RAP databases, thereby validating the efficacy of the proposed Da-HAR approach.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1089", "problem_id": "10890001", "content": "The demanding needs of mobile edge computing (MEC) applications necessitate high-capacity MEC hosts densely integrated into future wireless networks. However, running such high-capacity hosts can lead to substantial energy consumption. To address this, a base station (BS) can function as a self-powered unit. This paper investigates an efficient energy dispatch mechanism for self-powered wireless networks with edge computing capabilities. Initially, a two-stage linear stochastic programming problem is established to minimize the system’s total energy consumption cost while meeting energy demands. Next, a semi-distributed, data-driven approach is introduced using a novel multi-agent meta-reinforcement learning (MAMRL) framework to solve the problem. Here, each BS acts as a local agent, analyzing Markovian patterns in energy consumption and generation while transmitting dynamic features to a meta-agent. The meta-agent then optimizes energy dispatch decisions by processing observations from local agents along with their state data. Concurrently, each BS agent refines its energy dispatch policy using parameters learned from the meta-agent. The proposed MAMRL framework is evaluated in deterministic, asymmetric, and stochastic settings, focusing on non-renewable energy use, cost, and accuracy. Results demonstrate that the MAMRL model reduces non-renewable energy consumption by up to 11% and lowers energy costs by 22.4% (with 95.8% prediction accuracy) compared to baseline methods.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1090", "problem_id": "10900001", "content": "Facial editing, a significant task in computer vision and graphics with wide-ranging applications, currently lacks the ability to provide continuous and nuanced editing, such as transitioning from a slight smile to a broad laugh, with intuitive user interaction. To address this, we introduce Talk-to-Edit, an interactive facial editing framework that enables fine-grained attribute manipulation through user-system dialog. The core idea involves modeling a continuous \"semantic field\" within the GAN latent space, where: 1) fine-grained editing is conceptualized as discovering a curved trajectory on the semantic field that aligns with the nuanced attribute landscape, rather than simple linear traversals as in prior methods; 2) the curvature at each step is dynamically adjusted based on the input image and the user's textual instructions; and 3) the system provides language feedback, incorporating both user input and the present state of the semantic field to facilitate meaningful interaction. Additionally, we present CelebA-Dialog, a novel dataset designed to support large-scale research in visual-language facial editing, featuring manually annotated fine-grained attributes and template-based natural language descriptions for each image. Comprehensive quantitative and qualitative evaluations demonstrate that our framework excels in 1) the smoothness of fine-grained edits, 2) maintaining identity and attributes, and 3) generating visually realistic results with fluent dialog. User studies confirm that approximately 80% of participants consistently preferred our system. Our project page is https://www.mmlab-ntu.com/project/talkedit/.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1091", "problem_id": "10910001", "content": "The application of deep learning techniques to hyperspectral image classification has yielded significant accomplishments, but these models often require substantial amounts of labeled data due to their large parameter spaces. Traditional deep learning approaches for HSI classification typically adhere to a patchwise learning framework, but a recently introduced fast patch-free global learning architecture has been proposed to leverage global spatial context information for HSI classification. Nevertheless, this architecture struggles to extract the most distinctive features when confronted with imbalanced sample data. To address the challenges of insufficient and imbalanced HSI classification, this paper presents a spectral-spatial dependent global learning framework, which combines global convolutional long short-term memory and global joint attention mechanism. The proposed framework incorporates a hierarchically balanced sampling strategy and weighted softmax loss to mitigate the issue of imbalanced samples, while the global convolutional long short-term memory module is designed to capture the long-term dependencies of spectral features, and the global joint attention mechanism module is introduced to identify attention areas, thereby learning the most discriminative feature representations. Experimental results on three publicly available HSI datasets demonstrate the superiority of the proposed framework over existing state-of-the-art methods in handling insufficient and imbalanced sample problems, and the code is available at https://github.com/dengweihuan/SSDGL.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1092", "problem_id": "10920001", "content": "The overestimation bias inherent in Q-learning, stemming from its approximation of the maximum action value using the maximum estimated action value, has prompted the development of various algorithms aimed at mitigating this issue. However, a comprehensive understanding of the interplay between bias and performance, as well as the efficacy of existing algorithms in reducing bias, remains elusive. This paper addresses these knowledge gaps by: 1) demonstrating the environment-dependent impact of overestimation bias on learning efficiency; 2) introducing a generalized Q-learning framework, , which incorporates a parameter allowing for flexible bias control; 3) providing theoretical evidence that a specific parameter choice for Maxmin Q-learning yields unbiased estimation with reduced approximation variance compared to Q-learning; and 4) establishing the convergence of the proposed algorithm, along with several preceding Q-learning variants, via a novel Generalized Q-learning framework, as shown in Figure A, B, C (see References [citation]). Empirical evaluations confirm that the proposed algorithm effectively manages estimation bias in simplified environments and achieves enhanced performance in multiple benchmark problems, as discussed in References [citation].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1093", "problem_id": "10930001", "content": "The field of computer vision has witnessed significant activity in facial expression recognition, with diverse applications in areas such as animation, social robotics, and personalized banking. This research investigates the classification of facial expressions in images by leveraging features extracted from pre-trained convolutional neural networks, specifically those trained on the ImageNet database, and subsequently transferring these features to a Linear Support Vector Machine for classification purposes. Experiments were conducted on two publicly available datasets, namely the JAFFE and CK+ databases, Figure A, B, C. The findings indicate that the representations learned from pre-trained networks for object recognition tasks can be effectively transferred and utilized for facial expression recognition, as demonstrated by the achievement of classification accuracies of 92.26% and 92.86% on the CK+ and JAFFE datasets, respectively, with the use of features from earlier layers of the VGG19 network yielding better results for smaller datasets, as reported in References [citation].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1094", "problem_id": "10940001", "content": "Numerous real-world problems involve action spaces that are either high-dimensional, continuous, or both, rendering it impractical to exhaustively enumerate all possible actions. As a result, only limited subsets of actions can be sampled for the purposes of evaluating and refining policies. This paper introduces a comprehensive framework for systematically evaluating and improving policies based on these sampled action subsets. The proposed sample-based policy iteration framework is broadly applicable to any reinforcement learning algorithm that relies on policy iteration. Specifically, we develop Sampled MuZero, a variant of the MuZero algorithm that enables learning in domains with complex action spaces by planning over sampled actions, and validate its effectiveness on the game of Go, as well as the DeepMind Control Suite and Real-World RL Suite continuous control benchmark domains.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1095", "problem_id": "10950001", "content": "Although coordinate-based Multilayer Perceptron (MLP) networks can learn neural implicit representations, they lack the performance required for internal image synthesis; Convolutional Neural Networks (CNNs) are generally preferred for internal generative tasks, albeit with larger model sizes. We introduce Neural Knitwork, a neural implicit representation architecture for learning natural images. This architecture synthesizes images by optimizing image patch distribution adversarially and ensuring consistency across patch predictions. This work represents the first coordinate-based MLP implementation designed for synthesis tasks, including image inpainting, super-resolution, and denoising. The effectiveness of our proposed method is demonstrated through training on these tasks. Results indicate that modeling natural images with patches, instead of pixels, improves fidelity. The resulting model uses 80% fewer parameters than CNN alternatives, achieving similar performance and training duration.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1096", "problem_id": "10960001", "content": "To deploy deep learning models effectively, balancing accuracy with a compact size is crucial for satisfying latency and memory limitations, often leading to the development of networks that are deep for enhanced performance yet thin for computational efficiency. In this paper, we present an efficient training methodology for deep thin networks, supported by theoretical justification. Inspired by model compression techniques, our approach involves a three-stage process: initially, the deep thin network is expanded to a wider configuration and trained to convergence. Subsequently, this trained wide network is used to initialize the original thin network through layerwise imitation, where the thin network is trained to replicate the intermediate outputs of the wide network at each layer. Finally, the initialized thin network undergoes fine-tuning. Using neural mean field analysis, we theoretically validate the superiority of our layerwise imitation method compared to standard backpropagation. Empirical evaluations confirm the effectiveness of our method, demonstrating that ResNet50 trained with our approach can surpass the performance of ResNet101, and BERT Base can achieve results comparable to BERT Large, when ResNet101 and BERT Large are trained using conventional methods.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1097", "problem_id": "10970001", "content": "Deep reinforcement learning has experienced rapid growth in recent years, with many open-source libraries emerging to facilitate research. Despite this, existing frameworks often present significant learning barriers or lack adaptability, hindering rapid prototyping in foundational studies. This work presents Tonic, a Python-based library designed to accelerate innovation by offering: 1) modular, customizable components 2) pre-built implementations of key algorithms including A2C, TRPO, PPO, MPO, DDPG, D4PG, TD3, and SAC 3) compatibility with both TensorFlow 2 and PyTorch 4) integration with continuous-control environments from OpenAI Gym, DeepMind Control Suite, and PyBullet 5) tools for reproducible experiments, visualization, and agent interaction 6) comprehensive benchmarking across 70 continuous-control tasks. The evaluation ensures fairness through consistent random seeds, training procedures, and shared enhancements like observation normalization and non-terminal timeout handling. To illustrate Tonic's experimental efficiency, the paper introduces and assesses a new algorithm named TD4.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1098", "problem_id": "10980001", "content": "Graph neural networks are commonly used to generate node embeddings for predictive analysis in static attributed graphs. Although recent studies have explored learning the temporal relationships between nodes, effectively learning node embeddings to predict changes in node attributes and the dynamic creation or removal of links in real-world dynamic attributed graphs, which exhibit complex co-evolution of node attributes and graph structure, remains a challenge. This paper introduces CoEvoGNN, a new framework designed to model dynamic attributed graph sequences. CoEvoGNN maintains the influence of previous graphs on the current graph by generating embeddings sequentially and employs a temporal self-attention mechanism to capture long-range dependencies in the evolution. Furthermore, the model parameters are jointly optimized for both attribute inference and link prediction tasks across time, enabling the model to learn the co-evolutionary patterns of attribute changes and link formations. This framework is adaptable to various graph neural algorithms, and we present three implementations: CoEvoGCN, CoEvoGAT, and CoEvoSAGE. Experimental results show that the proposed framework and its methods surpass existing baselines in predicting complete, unseen graph snapshots of personal attributes and interpersonal links in dynamic social and financial graphs.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1099", "problem_id": "10990001", "content": "Reinforcement learning has gained significant attention within the machine learning field in recent years. Numerous methods exist for addressing reinforcement learning challenges, with continuous advancements being introduced. However, applying reinforcement learning involves tackling several complex obstacles. Establishing benchmarks is crucial for evaluating novel algorithms and facilitating comparisons with existing approaches, ensuring measurable progress in the domain. Reproducible results are essential for fair assessments of algorithmic enhancements. This paper reviews key contributions to reinforcement learning benchmarking and examines their role in helping researchers overcome current challenges. The discussed contributions represent widely adopted and contemporary methods in the literature, analyzed based on their implementation, task specifications, and included benchmarked algorithms. The survey highlights the diversity of available reinforcement learning benchmarking tasks and advocates for standardized research practices. Furthermore, it serves as a guide for researchers unfamiliar with the various tasks applicable to developing and evaluating new reinforcement learning algorithms.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1100", "problem_id": "11000001", "content": "This research presents TrimTuner, a pioneering system designed to optimize machine learning tasks in cloud environments by utilizing sub-sampling methods, which significantly reduce the optimization cost while adhering to user-defined constraints. TrimTuner simultaneously optimizes cloud and application-specific parameters, distinguishing itself from existing cloud optimization approaches by eliminating the need for retraining the model with the full dataset for each new configuration sampled. By applying sub-sampling techniques to datasets that are up to 60 times smaller than the original, TrimTuner achieves a reduction in optimization costs of up to 50 times. Additionally, TrimTuner accelerates the recommendation process by a factor of 65 compared to state-of-the-art hyper-parameter optimization techniques that employ sub-sampling, attributed to two key factors: a novel domain-specific heuristic that minimizes the number of configurations requiring evaluation of the acquisition function, and the use of an ensemble of decision trees, which further enhances the recommendation speed by an order of magnitude, as shown in Figure A, B, C (References [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1101", "problem_id": "11010001", "content": "As adversarial attacks gain prominence, researchers are exploring ways to deceive trained 2D object detectors. One notable approach involves embedding adversarial patches, such as logos, into images. However, adversarial attacks from 3D perspectives remain underexplored, despite their importance for maintaining attack effectiveness in real-world scenarios. This work introduces a novel 3D adversarial logo attack, where a 2D texture image is transformed into a 3D adversarial logo through a texture mapping process called logo transformation. The resulting 3D adversarial logo serves as a flexible adversarial texture, allowing easy adjustments to its shape and position, thereby enhancing adversarial training for computer-generated imagery. Unlike conventional adversarial patches, this method maps attacks into 3D space and propagates them back to the 2D domain via differentiable rendering. Moreover, our 3D adversarial logo effectively deceives advanced deep object detectors even under model rotations, advancing the feasibility of practical physical-world attacks. Our implementation is accessible at https://github.com/TAMU-VITA/3D_Adversarial_Logo.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1102", "problem_id": "11020001", "content": "Capsule Networks (CapsNets) have emerged as a potential substitute for Convolutional Neural Networks (CNNs) in image-based object recognition, showcasing several benefits over CNNs in existing studies. However, a significant gap exists in understanding how to generate explanations for individual classification decisions made by CapsNets. While prevalent saliency methods, such as Grad-CAM, are designed to elucidate CNN-based classifications by integrating activation values with corresponding gradients to produce saliency maps, their direct application to CapsNets is hindered by the iterative routing mechanism inherent in the CapsNet architecture. Addressing this interpretability challenge requires either the development of novel post-hoc interpretation techniques tailored for CapsNets or the modification of the CapsNet model itself to incorporate intrinsic explanatory capabilities. This paper explores the latter approach, introducing interpretable Graph Capsule Networks (GraCapsNets) that substitute the routing component with a multi-head attention-based Graph Pooling mechanism, enabling the efficient and effective generation of individual classification explanations. Surprisingly, this alteration yields additional advantages, as GraCapsNets exhibit improved classification accuracy, reduced parameter count, and enhanced adversarial robustness compared to traditional CapsNets, while preserving desirable characteristics like disentangled representations and robustness to affine transformations.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1103", "problem_id": "11030001", "content": "Variational autoencoders have proven to be effective generative models across various applications; however, their reliance on standard Gaussian or Gaussian mixture priors restricts their capacity to model the topological and geometric features of data within the latent space. This paper presents an Encoded Prior Sliced Wasserstein AutoEncoder (EPSWAE), incorporating an auxiliary prior-encoder network that learns a flexible prior to align with the encoded data manifold. The autoencoder and prior-encoder networks undergo iterative training using the Sliced Wasserstein Distance (SWD), which provides an efficient measure of dissimilarity between two sampleable distributions, avoiding the constraints of KL divergence and the computational demands of adversarial training. Furthermore, we enhance the traditional SWD by incorporating a nonlinear shearing technique, specifically, averaging across random transformations, to more effectively discern differences between distributions. The prior is further refined to encode the data manifold through a structural consistency term, which promotes isometry between the feature space and the latent space. Finally, interpolating within the latent space representation of the data manifold yields samples that reside on the manifold itself, offering advantages over standard Euclidean interpolation. To this end, we introduce a graph-based algorithm for identifying network-geodesics in latent space from samples of the prior that maximize the density of samples along the path while minimizing total energy. We evaluate our framework on the 3D-spiral, MNIST, and CelebA datasets, demonstrating that its latent representations and interpolations achieve performance comparable to state-of-the-art methods using equivalent architectures.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1104", "problem_id": "11040001", "content": "Model-based reinforcement learning can achieve greater sample efficiency than model-free methods, but current techniques often suffer from model bias, resulting in weaker generalization and asymptotic performance. These methods commonly rely on model predictive control (MPC), which is computationally demanding during decision-making and lacks an explicit policy representation, preventing policy transfer. To address these limitations, we introduce an uncertainty-aware model-based policy optimization framework where the agent jointly learns a dynamics model with uncertainty estimation and optimizes a policy based on these models. The policy gradient is derived using automatic differentiation through the learned models. Our method demonstrates strong sample efficiency on demanding continuous control tasks, achieving competitive asymptotic performance while requiring substantially fewer samples than leading baseline approaches.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1105", "problem_id": "11050001", "content": "The growing availability of publicly accessible graph-structured data has increased the demand for machine learning methods tailored to such data. Traditional machine learning techniques typically rely on tabular data, prompting significant research into embedding algorithms that convert graph data into real-valued vector representations. Current graph embedding methods focus solely on structural properties, neglecting semantic insights from the domain. This paper highlights the value of incorporating semantic information into graph embeddings by introducing a framework that leverages domain-specific interpretations of nodes and edges, while also considering the requirements of downstream machine learning applications. Through experiments in two real-world domains, we demonstrate that our approach produces embeddings that are both easy to implement and achieve comparable or superior performance in machine learning tasks relative to domain-agnostic methods.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1106", "problem_id": "11060001", "content": "HarperValleyBank, a freely available spoken dialog corpus in the public domain, is presented. This corpus comprises approximately 23 hours of audio data extracted from 1,446 human-to-human conversations involving 59 distinct speakers, simulating basic consumer banking interactions. Intentions and utterance templates were chosen to achieve realistic variability, simultaneously managing task complexity and restricting the vocabulary to roughly 700 unique terms. Included are audio data, transcripts, and annotations pertaining to speaker identification, caller intent, dialog actions, and emotional valence. The corpus's size and domain specificity facilitate rapid transcription experiments utilizing contemporary end-to-end neural methodologies. Furthermore, baseline models for representation learning are supplied, modifying current techniques to embed waveforms for subsequent prediction tasks. Experimental results demonstrate that tasks utilizing the provided annotations are influenced by both model selection and corpus size.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1107", "problem_id": "11070001", "content": "Content-based video retrieval focuses on identifying videos within a large database that closely resemble or are near-duplicates of a query video. The performance of such systems heavily relies on video representation techniques and similarity search methods. Traditional approaches often depend on extensive manually labeled training data, which is both resource-intensive and inefficient. Furthermore, many existing systems use frame-level features for similarity comparisons, leading to high storage and computational costs. To overcome these limitations, we introduce SVRTN, a novel retrieval system that leverages self-supervised learning to derive video representations from unannotated data, eliminating the need for costly manual labeling. Additionally, SVRTN employs a transformer architecture to consolidate frame-level features into clip-level representations, minimizing storage requirements and search complexity. This approach captures complementary and discriminative information through frame interactions while maintaining robustness to frame order variations and missing data, enabling more flexible retrieval. Extensive evaluations on the FIVR-200K and SVD datasets demonstrate SVRTN's superior accuracy and efficiency in video retrieval tasks.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1108", "problem_id": "11080001", "content": "Facial expression manipulation involves modifying an image's expression based on a specified condition. Existing approaches typically rely on discrete emotion labels or absolute conditions, such as facial action units, but often alter unrelated regions or lack precision in fine-grained adjustments. Addressing these limitations, we introduce a new method that employs relative action units instead of continuous absolute conditions, ensuring only targeted regions—indicated by non-zero relative AUs—are modified. Additionally, our U-Net-based generator incorporates a Multi-Scale Feature Fusion (MSF) mechanism to enhance expression editing quality. Comprehensive experiments, including both quantitative and qualitative assessments, confirm the superiority of our approach over current state-of-the-art techniques. Code is available at \\url.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1109", "problem_id": "11090001", "content": "The examination of satellite imagery is expected to be an essential resource for advancing sustainable development. Although Convolutional Neural Networks (CNNs) have achieved notable progress in analyzing natural images, their use in the context of multi-spectral satellite images—characterized by numerous input channels—remains largely underexplored. This paper compares various techniques for integrating multi-band information with CNNs, showcasing the effectiveness of each approach on the semantic segmentation task of agricultural vegetation, specifically vineyards. Our findings indicate that the prevalent practice of using bands chosen by a domain expert results in considerably lower test accuracy compared to other methods. In particular, we assess the following approaches: bands selected by an expert, utilizing all accessible bands, learning attention maps across the input bands, and employing Bayesian optimization to guide band selection. Our results reveal that merely utilizing all available bands enhances test performance, and we demonstrate that Bayesian optimization, introduced for band selection in this research, can additionally elevate accuracy.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1110", "problem_id": "11100001", "content": "Humans have the capability to articulate the contents of images with varying levels of detail. In contrast, most existing image captioning models lack an understanding of user intent, limiting their ability to produce a range of descriptions tailored to different user preferences. In this research, we introduce the Abstract Scene Graph (ASG) framework to encapsulate user intent at a detailed level and determine the specificity of the generated descriptions. The ASG is structured as a directed graph that comprises three components: \\textbf (object, attribute, relationship), derived from the image without relying on explicit semantic labels, making it accessible for both manual and automated generation. Building upon the ASG, we propose a new model, ASG2Caption, which can interpret user intent and semantics represented in the graph, thereby producing captions that align with the graph's structure. Our model demonstrates enhanced controllability when conditioned on ASGs compared to meticulously designed baselines across the VisualGenome and MSCOCO datasets, and it significantly enhances caption diversity by enabling the automatic sampling of varied ASGs as control signals.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1111", "problem_id": "11110001", "content": "Tracking-by-detection methodologies generally involve two phases: initially, generating samples near the target object, and subsequently, categorizing each sample as either the target or background. Contemporary trackers employing this framework commonly utilize raw image samples as inputs for deep convolutional networks, leading to increased computational demands and reduced speed. To mitigate this issue, we introduce a novel visual tracking technique that leverages sampling deep convolutional features. Our approach involves inputting a single cropped image centered on the target into a custom deep convolution network, with samples generated on the resulting feature maps via spatial bilinear resampling. Furthermore, we incorporate a generative adversarial network to expand the set of positive samples and enhance tracking accuracy. Empirical evaluations on standard datasets reveal that our method attains performance levels comparable to state-of-the-art trackers, while significantly accelerating tracking-by-detection methods that rely on raw-image samples.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1112", "problem_id": "11120001", "content": "The task of graph classification has garnered significant interest across multiple machine learning disciplines, including kernel methods, sequential modeling, and graph embedding, with each approach demonstrating unique advantages and disadvantages. Despite their promising outcomes, many of these methods are grounded in intricate mathematical formulations and necessitate substantial computational resources to optimize their performance. In response, we introduce a straightforward and efficient algorithm that leverages the spectral decomposition of the graph Laplacian for graph classification, providing a baseline score for a dataset. Our results indicate that this approach yields competitive performance relative to state-of-the-art algorithms, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1113", "problem_id": "11130001", "content": "The creation of autonomous vehicles capable of navigating complex situations and effectively interacting with other drivers relies on the semantic learning and comprehension of driving environments, frequently through the analysis of extensive naturalistic driving datasets. A crucial approach for enabling automated vehicles to learn from human drivers and acquire insights involves understanding the fundamental components of overall traffic flow, known as traffic primitives. However, the exponential growth of data poses a significant challenge in extracting these primitives from high-dimensional, time-series traffic data involving diverse road users. Consequently, automated primitive extraction is becoming an increasingly cost-effective means of assisting autonomous vehicles in understanding and predicting intricate traffic scenarios. Moreover, primitives extracted from raw data should be 1) suitable for automated driving applications and 2) readily usable for generating novel traffic scenarios. Current research, however, lacks a method for automatically learning these primitives from large-scale traffic data. This paper contributes in two key ways: first, we present a novel framework for generating new traffic scenarios from limited traffic data; second, we introduce a nonparametric Bayesian learning method—a sticky hierarchical Dirichlet process hidden Markov model—to automatically extract primitives from multidimensional traffic data without prior knowledge of primitive settings. The developed method is validated using one day of naturalistic driving data, and experimental results demonstrate that the nonparametric Bayesian learning method can extract primitives from traffic scenarios where both binary and continuous events occur.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1114", "problem_id": "11140001", "content": "We introduce a novel approach to efficiently predict accurate depths from monocular images, leveraging wavelet decomposition within a fully differentiable encoder-decoder framework to achieve optimal efficiency. By predicting sparse wavelet coefficients, our method is capable of reconstructing high-fidelity depth maps. Notably, our approach deviates from previous methods as it learns wavelet coefficients without requiring direct supervision, instead relying on supervision of the final reconstructed depth image obtained through inverse wavelet transform. Furthermore, we demonstrate the feasibility of learning wavelet coefficients in a fully self-supervised setting, eliminating the need for ground-truth depth data. Our method is applied to various state-of-the-art monocular depth estimation models, yielding comparable or superior results while reducing the computational requirements of the decoder network by more than half. Code at https://github.com/nianticlabs/wavelet-monodepth", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1115", "problem_id": "11150001", "content": "The objective of video inpainting is to generate coherent content in spatio-temporal regions that have been compromised. To accomplish this, establishing correspondences between adjacent frames is crucial for accurately predicting the missing content. Existing approaches rely on attention mechanisms, flow-based warping, or 3D temporal convolution to achieve this. Nevertheless, flow-based warping can introduce artifacts when optical flow estimates are inaccurate, and temporal convolution can be hindered by spatial misalignment issues. This limitation is addressed by the proposed 'Progressive Temporal Feature Alignment Network', which enhances the features of the current frame by progressively incorporating features warped from neighbouring frames using optical flow, thereby correcting spatial misalignment during temporal feature propagation. This approach yields significant improvements in the visual quality and temporal coherence of the inpainted videos, enabling state-of-the-art performance on the DAVIS and FVI datasets, surpassing other deep learning-based methods, as demonstrated in Figure A, B, C (see References [citation] for details), with the code available at https://github.com/MaureenZOU/TSAM.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1116", "problem_id": "11160001", "content": "The utilization of 3D morphable models for object class shape representation is prevalent in computer vision and graphics applications, with deep 3D morphable models emerging as a promising approach by leveraging deep learning on 3D mesh data with hierarchical structures to extract multi-scale information. Although significant progress has been made in designing convolution operators, the aggregation of vertex features across hierarchical levels remains a crucial aspect that warrants further investigation. Unlike traditional mesh decimation methods, this work introduces an attention-based module that learns mapping matrices to facilitate effective feature aggregation across hierarchical levels, where the mapping matrices are generated through a compatibility function of trainable keys and queries that are shared across all data samples of the same object class and optimized via the target objective. Notably, the proposed module can seamlessly replace existing feature aggregation mechanisms in downsampling and upsampling architectures without requiring modifications to the underlying framework. Experimental results demonstrate that end-to-end training of the mapping matrices yields state-of-the-art performance on various 3D shape datasets, surpassing existing morphable models, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1117", "problem_id": "11170001", "content": "We conduct an empirical investigation into debiasing techniques for classifiers, revealing that these methods frequently struggle to generalize beyond their training samples, and may even exacerbate fairness issues. A comprehensive assessment of the debiasing effect necessitates thorough cross-validation, which is often insufficiently applied. Our findings indicate that this occurrence can be attributed to the bias-variance trade-off, where enforcing a fairness constraint leads to an increase in variance. Subsequent experiments reinforce the theoretical assertion that estimation variance is significantly influenced by the base rates of the protected class. Evaluating the trade-offs between fairness and performance supports the paradoxical idea that partial debiasing might actually produce improved outcomes for out-of-sample data.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1118", "problem_id": "11180001", "content": "Developing machine learning models that can withstand adversarial inputs is a daunting task, prompting a closer examination of the fundamental issue of acquiring robust representations. To this end, we introduce the concept of representation vulnerability, which quantifies the maximum alteration in mutual information between input and output distributions under the most adverse input perturbation. A theorem is subsequently established, providing a lower bound on the minimum adversarial risk attainable by any downstream classifier, based on its representation vulnerability. Furthermore, an unsupervised learning approach is proposed to derive inherently robust representations by maximizing the worst-case mutual information between input and output distributions. The efficacy of this approach is demonstrated through experiments on downstream classification tasks, which validate the robustness of the representations obtained using our unsupervised learning methodology, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1119", "problem_id": "11190001", "content": "The prevailing method for activity recognition involves data-driven techniques that learn optimal representations of visual features such as skeleton frames or RGB videos. Although significant advancements have been made using single-modal approaches with increasingly large datasets, integrating multiple data modalities at the feature level remains underexplored. This paper introduces a multimodal feature fusion model that combines skeleton and RGB data to enhance human activity inference, aiming to boost recognition accuracy by leveraging complementary information across modalities. For skeleton data, a graph convolutional subnetwork is employed to extract representations, while for RGB videos, spatial-temporal regions of interest are used, with skeleton-derived attention features guiding the learning process. The model can be trained individually or jointly via end-to-end backpropagation. Experiments on the NTU-RGB+D and Northwestern-UCLA Multiview datasets demonstrate state-of-the-art performance, showing that the proposed skeleton-driven attention mechanism enhances cross-modal interaction and yields more distinctive features for activity recognition.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1120", "problem_id": "11200001", "content": "Experimental studies indicate that the performance of distributed training using stochastic gradient descent (SGD) is heavily influenced by batch size and, in asynchronous setups, by gradient staleness. Notably, speedup gains plateau when batch sizes exceed a threshold or delays become excessive. We introduce a data-dependent parameter that accounts for this saturation effect in both scenarios. Our extensive theoretical framework, covering strongly convex, convex, and non-convex cases, integrates and extends previous research that typically examined only one of these factors. This method also enables us to achieve better speedup outcomes under common sparsity conditions. The analysis provides theoretically grounded recommendations for adjusting learning rates in practical applications. Our findings are shown to be optimal and are validated through numerical experiments.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1121", "problem_id": "11210001", "content": "Addressing inverse problems remains a significant challenge across various applications such as deblurring, image inpainting, and source separation. Conventional methods typically address these problems by either directly or indirectly estimating the inverse of the model. Techniques that rely on explicit inversion demand detailed knowledge of the measurement process, which is often impractical, and depend on strong analytical regularizers to limit the solution space—approaches that frequently lack generalizability. Alternatively, deep learning-based methods have achieved notable success but require extensive datasets of source-observation pairs, which can be costly to obtain. This paper introduces an unsupervised approach using generative adversarial networks (GANs) to tackle inverse problems. By leveraging a pre-trained GAN in the source signal domain, we demonstrate reliable recovery of solutions for underdetermined problems in a blind manner, without prior knowledge of the measurement process. Our method iteratively refines estimates of both the model and the solution, yielding competitive performance in blind source separation, image deblurring, and edge map reconstruction, outperforming multiple baseline techniques.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1122", "problem_id": "11220001", "content": "This paper introduces novel generator architectures for Boundary Equilibrium Generative Adversarial Networks, inspired by Learning from Simulated and Unsupervised Images through Adversarial Training. The approach removes the dependency on a noise-driven latent space, instead employing the generator primarily as a refinement network to enhance synthetic images into photorealistic outputs. By replacing noise injection with an image-based framework, it addresses the latent space's ambiguous characteristics. The proposed architecture offers a flexible and straightforward design, enabling better control over the balance between constrained refinement and expressive capability. Unlike existing techniques, this method does not rely on paired or unpaired datasets of real and synthetic images during training, requiring only a limited collection of real images.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1123", "problem_id": "11230001", "content": "Prominent strategies in action recognition aim to extract insights from both the spatial and temporal aspects of input videos. Techniques that achieve State of the Art (SotA) precision typically utilize 3D convolutional layers to encapsulate temporal data from video frames. Implementing these convolutions necessitates sampling brief clips from the input video, with each clip containing a sequence of closely aligned frames. As each short clip represents only a minor segment of the entire video, multiple clips must be sampled during inference to encompass the full temporal duration. This requirement results in a higher computational burden, rendering it inefficient for practical usage. We tackle this computational constraint by markedly minimizing the number of frames needed during inference. Our method leverages a temporal transformer that employs global attention across video frames, thereby more effectively utilizing the significant information present in each frame. Consequently, our approach enhances input efficiency and achieves SotA performance (on the Kinetics dataset) using a significantly reduced amount of data (frames per video), computation, and latency. Specifically, on Kinetics-400, we attain 80.5 top-1 accuracy with \\times 30 fewer frames per video and \\times 40 faster inference compared to the leading method. Code is available at: https://github.com/Alibaba-MIIL/STAM.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1124", "problem_id": "11240001", "content": "Adversarial examples refer to deliberately modified inputs designed to mislead machine learning models into making incorrect predictions, with alterations that remain largely imperceptible to humans. While extensive research has focused on adversarial attacks in image processing, audio-based classification tasks have been comparatively understudied. This work explores the presence of universal perturbations in speech command classification, showing that such attacks can be crafted to generalize effectively across diverse models. A new analytical approach is introduced to assess universal perturbations at varying degrees of universality, revealing that higher universality levels reduce the likelihood of successful perturbation generation. Furthermore, a refined framework is presented to quantify perturbation-induced distortions more precisely, highlighting the limitations of conventional evaluation methods in audio-related applications.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1125", "problem_id": "11250001", "content": "This study tackles the challenge of generating a depth map for a scene from a single RGB image. We introduce a fully convolutional framework that incorporates residual learning to capture the complicated relationship between monocular images and depth representations. To enhance the resolution of the output, we propose an innovative technique for effectively learning feature map up-sampling within the network. For the optimization process, we utilize the reverse Huber loss, which is particularly advantageous given the value distributions often found in depth maps. Our model consists of a unified architecture that is trained end-to-end without reliance on post-processing methods like CRFs or other refinement techniques, allowing it to operate in real-time for both images and videos. In our evaluations, we demonstrate that our proposed model has fewer parameters and requires less training data compared to current leading solutions, while also surpassing all existing methods in depth estimation performance. The code and models are accessible to the public.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1126", "problem_id": "11260001", "content": "X-ray security screening plays a crucial role in ensuring aviation and transport safety, driving interest in automated detection systems. This study systematically reviews computational algorithms for X-ray security imaging, categorizing them into traditional machine learning and advanced deep learning techniques. The initial section outlines classical machine learning methods applied in X-ray security imaging, followed by an in-depth analysis of modern deep learning applications. The classification framework further divides deep learning approaches into supervised, semi-supervised, and unsupervised learning, emphasizing tasks such as object classification, detection, segmentation, and anomaly identification. Additionally, the paper evaluates prominent X-ray datasets and establishes performance benchmarks. Considering emerging trends in deep learning, the study concludes with insights and future research directions for X-ray security imaging.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1127", "problem_id": "11270001", "content": "This study demonstrates the capacity of an image classifier network to generate high-resolution, photorealistic, and diverse images on a large scale through a novel methodology termed Synthesize-It-Classifier (STIC). Unlike traditional approaches, STIC leverages the classifier's understanding of the class boundary to perform gradient ascent with respect to class logits, and subsequently synthesizes images using the Gram Matrix Metropolis Adjusted Langevin Algorithm (GRMALA) on a blank canvas, eliminating the need for an explicit generator network. The classifier refines its class boundary estimates by iteratively incorporating synthesized images as fake samples during training, leading to enhanced classification accuracy and synthetic image quality. Notably, the combination of hard and soft fake samples, generated through one-hot class conditioning and mixup of classes, respectively, improves class interpolation. The effectiveness of STIC is showcased through an Attentive-STIC network on the ImageNet dataset, and further improved results are achieved by introducing a class conditional score classifier, Score-STIC, on various real-world datasets, including ImageNet, LSUN, and CIFAR 10.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1128", "problem_id": "11280001", "content": "Cervical spondylosis (CS) is a prevalent chronic condition affecting a substantial proportion of the population, resulting in considerable individual and societal burdens. Early detection is crucial for enhancing treatment outcomes and minimizing costs, but the complex pathology and mild symptoms, particularly in the initial stages, pose significant diagnostic challenges. Furthermore, the lengthy and expensive nature of hospital-based medical services often detracts from the attention devoted to CS diagnosis. Consequently, there is a pressing need for a convenient, low-cost, and intelligent CS identification method. This paper introduces a deep learning-based approach for identifying CS using surface electromyography (sEMG) signals, addressing the complexities, high dimensionality, and limited usability of sEMG data through the development of a multi-channel EasiCSDeep algorithm, which incorporates feature extraction, spatial relationship representation, and classification. As the first initiative to leverage deep learning and sEMG data for CS identification, the proposed EasiCSDeep algorithm demonstrates substantial improvements over existing state-of-the-art algorithms, as shown in Figure A, B, C (References [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1129", "problem_id": "11290001", "content": "Multi-objective optimization is commonly found in engineering applications, yet the techniques for solutions and parameter choices tend to be specific to individual problems. In this research, we develop a reinforcement learning hyper-heuristic framework and introduce four low-level heuristics that synergize with the single point search algorithm MOSA/R (Multi-Objective Simulated Annealing Algorithm based on Re-seed) to address general multi-objective optimization challenges. By leveraging metrics such as domination amount, crowding distance, and hypervolume measurements, the suggested hyper-heuristic approach can adaptively and autonomously fulfill diverse optimization needs. This method not only demonstrates enhanced and more reliable performance against AMOSA, NSGA-II, and MOEA/D in benchmark scenarios, but it also yields encouraging results in a general structural fault identification task. The findings of this study can be applied to a wide range of optimization tasks in design and manufacturing.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1130", "problem_id": "11300001", "content": "Connected and Automated Vehicles (CAVs) are anticipated to revolutionize both the industrial and personal transportation sectors. Given the intricacy of these systems, it is crucial to conduct functional verification and validation of safety components prior to their integration into public use. Recently, a scenario-driven methodology has gained traction for CAVs, highlighting the necessity of a robust scenario data foundation. The large-scale research facility Test Bed Lower Saxony (TFNDS) provides extensive information for a scenario database focused on motorways. To facilitate this, it is essential to identify and classify the relevant scenarios within the gathered trajectory data. This study tackles this challenge and introduces a framework for identifying on-ramp scenarios, which also allows for scenario categorization and evaluation. The effectiveness of the framework is demonstrated using a dataset obtained from the TFNDS.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1131", "problem_id": "11310001", "content": "In the field of AI, machine learning stands out as the predominant tool utilized in both research and industry. A key algorithm in machine learning is the Gradient Boosting Decision Tree (GBDT), which requires substantial computational resources and time for training. To reduce the training duration of GBDT, various studies have attempted to implement GBDT on Parameter Server. However, these implementations typically rely on synchronous parallel algorithms, which do not fully exploit the capabilities of Parameter Server. This paper explores the potential of employing asynchronous parallel methods for training the GBDT model, introducing an algorithm termed asynch-SGBDT (asynchronous parallel stochastic gradient boosting decision tree). Our theoretical and experimental findings suggest that the scalability of asynch-SGBDT is affected by factors such as dataset sample diversity, sampling rate, step length, and GBDT tree configuration. Furthermore, experimental outcomes reveal that the training process of asynch-SGBDT achieves a linear speedup in an asynchronous parallel environment, provided that the datasets and GBDT trees satisfy high scalability criteria.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1132", "problem_id": "11320001", "content": "Multiview super-resolution image reconstruction (SRIR) typically involves resampling and inverting the camera point spread function (PSF) by combining non-redundant data from multiple low-resolution (LR) images onto a high-resolution (HR) grid. However, this approach is complicated by the nonlinearity and ill-posedness of resampling from nonuniform samples and inverting the PSF, which are often addressed through linearization, regularization, and iterative optimization, ultimately compromising the recovery of high-frequency information. In contrast, this work presents a novel perspective on multiview SRIR, where explicit expressions are derived to relate the high-frequency spectra of the unknown HR image to the spectra of the LR images, allowing for the reconstruction of the super-resolution image by focusing solely on the high-frequency spectra. By selecting one LR image as a reference to represent the low-frequency spectra of the HR image, the proposed method leverages information from all other views to reconstruct the high-frequency spectra, differing from single-image methods that rely on prior constraints. The use of explicit closed-form expressions enables the relationship between local high-frequency information in the reference HR image and local low-frequency information in the sequence of views to be defined, leading to a superior solution, as demonstrated by results and comparisons with recently published state-of-the-art methods (Figure A, B, C) [References: citation].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1133", "problem_id": "11330001", "content": "Recognizing and locating image patches or collections of image features is a crucial aspect of various computer vision applications. Historically, this has been achieved through template matching. However, this approach is often fragile when confronted with changes in appearance due to factors such as viewpoint variations, partial occlusions, and non-rigid deformations. This paper evaluates an enhanced template matching technique that demonstrates greater resilience to such appearance changes, thus enabling more precise identification of image patches. In conventional template matching, the comparison between the template and the image operates independently of other templates. In contrast, the method presented here incorporates the evidence from the image regarding the template at each position, as well as considering the extensive array of alternative interpretations associated with that template in different locations and other templates. Specifically, the proposed template matching technique utilizes a type of probabilistic inference known as \"explaining away.\" The algorithm for implementing this method has been previously employed to mimic various neurobiological processes and has been applied in tasks such as image contour detection and pattern recognition. This study marks its inaugural use in the context of image patch matching, demonstrating markedly improved outcomes compared to existing leading techniques.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1134", "problem_id": "11340001", "content": "We introduce a new approach to enhance cross-person gaze estimation using only eye and face images by explicitly addressing individual variations. Our method begins with initial gaze predictions from an existing model (InitNet), followed by three key components: the Validity Module (VM), Self-Calibration (SC), and Person-specific Transform (PT) Module. The VM assesses the reliability of input images, filtering out invalid samples such as those with eye blinks to minimize their impact. The SC and PT modules then refine predictions for valid samples—SC adjusts for deviations by aligning initial outputs with the dataset’s overall distribution, while PT learns broader person-specific transformations by leveraging prior predictions from the same individual. Evaluated on EVE, XGaze, and MPIIGaze datasets, our method achieves substantial improvements over state-of-the-art techniques, with relative gains of 21.7%, 36.0%, and 32.9%, respectively. It also secured first place in the GAZE 2021 Competition on the EVE dataset. The implementation is available at https://github.com/bjj9/EVE_SCPT.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1135", "problem_id": "11350001", "content": "A novel approach for providing both local and global explanations for predictions made by machine learning black-box models using tabular data is introduced. This method is realized through a system known as AFEX (Attention-like Feature EXplanation), which is composed of two primary components. The first component consists of one-feature neural subnetworks designed to create a specific representation for each feature utilizing a basis of shape functions. These subnetworks incorporate shortcut connections with adjustable parameters to enhance their performance. The second component of AFEX generates shape functions for the features as a weighted summation of the basis shape functions, with the weights determined through an attention-like mechanism. AFEX also detects pairwise interactions among features by analyzing the pairwise products of shape functions associated with different features. An enhancement of AFEX that includes an additional surrogate model, intended to approximate the black-box model, is also proposed. Importantly, AFEX is trained in a single integrated procedure on the entire dataset, which eliminates the need to retrain the neural networks during the explanation phase. Numerical tests involving both synthetic and real-world data demonstrate the effectiveness of AFEX.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1136", "problem_id": "11360001", "content": "In this study, we examine a category of multi-state autoregressive processes suitable for modeling non-stationary time-series data of interest. To effectively capture the various autoregressive (AR) states present in an observed time series, it is essential to determine the optimal number of states. We introduce a novel model selection method utilizing the Gap statistics, which employs a null reference distribution derived from stable AR filters to assess whether the inclusion of an additional AR state significantly enhances model performance. To achieve this, we establish a new distance metric between AR filters based on mean squared prediction error (MSPE) and present an efficient strategy to create random stable filters that are uniformly distributed across the coefficient space. We also provide numerical results to assess the effectiveness of the proposed methodology.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1137", "problem_id": "11370001", "content": "Unsupervised (or self-supervised) learning of graph representations is crucial for enabling a range of graph data mining tasks in the absence of external supervision. The primary challenge lies in efficiently encoding the graph's structure and the attributes of its nodes and edges into a lower-dimensional space. Many current unsupervised techniques focus on generating similar representations for nodes that are topologically adjacent. Recent findings indicate that incorporating additional graph-level information, such as shared details among all nodes, helps develop representations that consider the global characteristics of the graph, thereby significantly enhancing their quality. Nonetheless, many graphs contain additional structural insights, such as nodes frequently forming multiple clusters that represent structurally comparable nodes. Based on this realization, we introduce a graph representation learning approach named Graph InfoClust (GIC), which aims to capture cluster-level information content as well. These clusters are generated using a differentiable K-means method and are optimized by maximizing the mutual information within nodes in the same clusters. This optimization process allows node representations to encompass more comprehensive information and interactions, thereby improving their quality. Experimental results demonstrate that GIC surpasses state-of-the-art methods across various downstream tasks (node classification, link prediction, and node clustering), achieving an average improvement of 0.9% to 6.1% over the leading competing method.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1138", "problem_id": "11380001", "content": "Depth estimation from a single viewpoint faces challenges, as a model trained on images from a specific camera does not effectively transfer to images captured by an alternative camera model. As a result, transitioning to a different camera necessitates the creation of a completely new training dataset. In this study, we introduce an innovative convolution technique that incorporates camera parameters, enabling neural networks to learn patterns that are sensitive to calibration. Experimental results demonstrate a significant enhancement in the generalization performance of depth prediction networks, clearly surpassing the current leading methods when training and testing images are obtained from different cameras.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1139", "problem_id": "11390001", "content": "Model compression and knowledge distillation have demonstrated effectiveness in transfer learning across different architectures and domains. A crucial condition for this approach is that the training examples must be aligned between the domains. We illustrate that in numerous significant scenarios, such corresponding data can be artificially produced using computer graphics methods, facilitating domain adaptation via distillation. This methodology is employed to develop models that can identify low-resolution images from labeled high-resolution images, recognize non-localized objects from labeled localized objects, and distinguish line drawings from labeled color images, among others. Tests conducted on a range of fine-grained recognition datasets show that this method enhances recognition accuracy for lower-quality data and surpasses strong benchmarks for domain adaptation. Lastly, we provide insights into the technique's functionality through visual representations and connections to existing research.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1140", "problem_id": "11400001", "content": "Video super-resolution (VSR) seeks to reconstruct a photo-realistic high-resolution (HR) video frame from its associated low-resolution (LR) frame (the reference frame) in conjunction with several adjacent frames (the supporting frames). The misalignment between the reference frame and each supporting frame, caused by differing movements of cameras or objects, presents a significant challenge in achieving temporal alignment, which is crucial for VSR. Traditionally, VSR techniques have relied on optical flow to align these frames for temporal coherence; however, the efficacy of such image-level wrapping models is heavily contingent on the precision of optical flow predictions, with inaccuracies potentially introducing artifacts into the wrapped supporting frames, subsequently affecting the quality of the reconstructed HR video frame. To address this issue, we introduce a temporal deformable alignment network (TDAN) in this paper, which allows for adaptive alignment of the reference frame and each supporting frame at the feature level without requiring optical flow computations. The TDAN predicts dynamic offsets for sampling convolution kernels based on features from both the reference and supporting frames. By applying the appropriate kernels, TDAN aligns the supporting frames with the reference frame. A reconstruction network is then employed to generate the HR video frame using the aligned frames along with the reference frame. Experimental results confirm the effectiveness of the proposed TDAN-based VSR model.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1141", "problem_id": "11410001", "content": "Creating a 3D game environment can be a labor-intensive process that often demands considerable effort. This typically entails the synthesis, coloring, and positioning of 3D models within the scene. To alleviate this burden, machine learning can be employed to automate certain facets of game development. Previous studies have focused on automating the generation of game scene backgrounds using machine learning, yet the automatic coloring of models remains a less explored area. The task of automatically coloring a 3D model is complex, particularly when it involves the digital representation of a vibrant, multi-part object, as it necessitates a comprehensive understanding of the object's structure and the color scheme for each part. Existing single-stage methods have limitations, including the requirement for object segmentation or the generation of separate components that must be assembled to create the final model. In response to these challenges, we introduce a two-stage training methodology to create auto-colored 3D models. In the initial stage, a 3D point cloud is generated to represent the object, while the subsequent stage involves assigning colors to the points within this cloud. Utilizing the triangulation technique, we produce a 3D mesh where the surfaces are colored by interpolating the colors of the points that correspond to the vertices of each triangle in the mesh. This method enables the creation of a seamless coloring scheme. Our experimental results indicate that this two-stage approach outperforms traditional single-stage methods in both shape reconstruction and coloring.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1142", "problem_id": "11420001", "content": "Generative Zero-shot learning (ZSL) aims to learn from observed classes, apply this acquired knowledge, and produce samples for unobserved classes based on their descriptions. To enhance ZSL accuracy, it is essential for models to better comprehend the descriptions pertaining to unseen classes. We propose a new type of regularization that prompts generative ZSL models to focus more on the description of each category. Our experimental findings indicate that there are significant improvements in the performance of several leading models in the areas of generalized zero-shot recognition and classification when trained on datasets that are based on textual descriptions, such as CUB and NABirds, as well as on attribute-based datasets like AWA2, aPY, and SUN.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1143", "problem_id": "11430001", "content": "This paper addresses the problem of learning a distribution over intricate and realistic indoor environments by proposing Generative Scene Networks (GSN), a novel approach that decomposes scenes into multiple local radiance fields, enabling rendering from arbitrary camera viewpoints. The GSN model can serve as a prior for generating new scenes or completing incomplete scenes based on sparse 2D observations. While recent studies have demonstrated the effectiveness of generative radiance field models in capturing properties like multi-view consistency and view-dependent lighting, these models are limited to single objects with constrained viewpoints, such as cars or faces, and lack the capacity to represent complex indoor scenes. In contrast, the proposed decomposition scheme in GSN allows for the representation of larger and more complex scenes while maintaining details and diversity, and the learned prior facilitates high-quality rendering from novel viewpoints that differ significantly from the observed ones, outperforming existing models in terms of rendering quality across various scene datasets, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1144", "problem_id": "11440001", "content": "Point sets serve as a versatile and efficient representation commonly employed in 3D deep learning, yet their discrete structure limits their ability to capture continuous and detailed geometries, creating challenges for shape generation tasks. To address this, we transform discrete point sets into smooth surfaces using the implicit moving least-squares (IMLS) method, which inherently defines local implicit functions over point clouds. By integrating IMLS surface generation into deep neural networks, our approach combines the adaptability of point sets with the precision of implicit surfaces. IMLSNet predicts an octree structure to guide the placement of MLS points and leverages learned local priors to model shape geometry. Additionally, the implicit function evaluation operates independently of the neural network after MLS point prediction, ensuring rapid runtime performance. Experimental results on 3D reconstruction show that IMLSNets surpass leading learning-based techniques in both reconstruction accuracy and efficiency, while comprehensive ablation studies confirm the effectiveness of our network architecture and loss functions.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1145", "problem_id": "11450001", "content": "The growing accessibility of electronic health records (EHR) has facilitated research into various medical inquiries. One of the primary considerations for EHR analysis is the selection of cohorts for the hypothesis being examined. When dealing with rare diseases, the cohorts derived from EHRs often consist of a very limited number of records, which undermines the reliability of any analysis. Data augmentation techniques have been effectively utilized in other fields to mitigate this problem, primarily through the creation of simulated records. In this study, we introduce ODVICE, a data augmentation framework that utilizes the medical concept ontology to systematically enhance records via an innovative ontologically driven Monte-Carlo graph spanning algorithm. This tool enables end users to define a small set of interactive controls to influence the augmentation process. We assess the significance of ODVICE by performing experiments on the MIMIC-III dataset across two learning tasks. Our findings indicate that cohorts augmented by ODVICE exhibit improved predictive performance, with an approximately 30% increase in area under the curve (AUC) compared to the non-augmented dataset and other data augmentation methods.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1146", "problem_id": "11460001", "content": "Computer vision has made significant advancements through (a) encoding images as uniformly structured pixel grids and (b) applying localized feature convolutions. Yet, convolutions process all pixels uniformly without prioritizing importance, rigidly model all concepts regardless of image content, and fail to effectively connect spatially distant elements. This study rethinks this approach by (a) representing images as semantic visual tokens and (b) employing transformers to comprehensively model token interactions. Notably, our Visual Transformer operates in a semantic token space, dynamically focusing on relevant image regions based on contextual relevance—unlike pixel-space transformers, which demand substantially higher computational resources. With an optimized training strategy, our VTs surpass convolutional models, improving ResNet's ImageNet top-1 accuracy by 4.6 to 7 points while using fewer FLOPs and parameters. In semantic segmentation tasks on LIP and COCO-stuff, VT-enhanced feature pyramid networks (FPN) achieve a 0.35-point mIoU gain while reducing FPN FLOPs by 6.5x.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1147", "problem_id": "11470001", "content": "Denoising Score Matching with Annealed Langevin Sampling (DSM-ALS) has emerged as a successful technique in generative modeling, wherein a neural network is initially trained to approximate the score of a distribution. Subsequently, Langevin dynamics are employed to generate samples from the data distribution presumed by the score network. Although the generated samples exhibit compelling visual quality, this method seemingly underperforms Generative Adversarial Networks (GANs) when evaluated using the Fr\\'echet Inception Distance, a conventional benchmark for generative models. However, we demonstrate that this discrepancy diminishes upon denoising the final Langevin samples via the score network. Furthermore, we introduce two enhancements to DSM-ALS: 1) Consistent Annealed Sampling, presented as a more robust alternative to Annealed Langevin Sampling, and 2) a hybrid training paradigm integrating both Denoising Score Matching and adversarial objectives. Through the integration of these advancements and the investigation of diverse network architectures, we significantly improve score matching methodologies, achieving image generation results on CIFAR-10 that rival state-of-the-art techniques.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1148", "problem_id": "11480001", "content": "This study introduces deformable templates as a method for segmenting and locating biological structures within medical images. This approach represents structures using a prototype template that is modified by a parametric warp mapping, which deforms the template's original shape. The localization process employs a multi-stage, multi-resolution algorithm to minimize computational demands and processing time. Initially, the algorithm pinpoints image regions with a high probability of containing the target objects, followed by an examination of these regions at incrementally higher resolutions. The algorithm concludes by warping the prototype template to align with the localized objects. The proposed algorithm is demonstrated with four application examples utilizing MRI, x-ray, and ultrasound images.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1149", "problem_id": "11490001", "content": "Currently, machine learning models are increasingly being utilized across various applications. Companies offer pre-trained models as application programming interfaces (APIs) that developers can integrate with third-party components alongside their own models and data to develop intricate data products aimed at addressing specific challenges. The intricacy of these products, coupled with a lack of oversight and understanding of the internal workings of each component, leads to significant issues, including diminished transparency, challenges with auditability, and the potential for unmanaged risks, rendering them effectively as black-boxes. Ensuring accountability for these solutions poses difficulties for both auditors and the machine learning field. In this paper, we introduce a wrapper that enhances the output predictions of a black-box model by incorporating a measure of uncertainty. This wrapper enables the black-box to be audited for accuracy risks—stemming from low-quality or uncertain decisions—and offers a proactive strategy to mitigate these risks through decision rejection; specifically, we can opt not to provide predictions when there is a substantial risk or uncertainty associated with a decision. Leveraging the uncertainty measure, we propose a rejection system that prioritizes the more confident predictions while discarding those with greater uncertainty, thereby increasing the trustworthiness of the system. We illustrate the proposed technique and methodology in a practical context using a simulated sentiment analysis API grounded in natural language processing across various domains. The results highlight the effectiveness of the uncertainty calculated by the wrapper and its strong correlation with poor-quality predictions and misclassifications.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1150", "problem_id": "11500001", "content": "In multimodal tasks, an effective feature extraction network must capture as much information as possible while ensuring a strong mutual understanding between the extracted feature embeddings and those from other modalities, which is often more crucial for feature fusion. Consequently, identifying the best configuration for the feature extraction network is a significant subproblem in multimodal tasks. Many current studies overlook this issue or employ a trial-and-error approach. This paper formulates the problem as an optimization challenge and introduces a novel method that transforms the optimization into a matter of comparative upper bounds, drawing on the mathematical principles of extreme value transformation. This approach, compared to traditional methods, minimizes time expenditure. Additionally, to address the frequent misalignment between feature similarity and semantic similarity in multimodal time-series contexts, we leverage the concept of contrastive learning and propose a multimodal time-series contrastive loss (MTSC). We demonstrate the practicality of our method in the audio-visual video parsing task, with extensive analysis confirming that our techniques enhance the integration of diverse modal features.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1151", "problem_id": "11510001", "content": "This study presents the , a nonconvex but analytically manageable optimization framework designed to enhance understanding of deeply trained neural networks. The model is constructed by decoupling the top layer from the rest of the network and applying distinct constraints to each component. Despite its simplicity, the Layer-Peeled Model captures key behaviors of well-trained networks, serving as a valuable tool for interpreting and anticipating common deep learning training patterns. For class-balanced datasets, we establish that solutions to this model yield a simplex equiangular tight frame, providing insight into the neural collapse phenomenon \\cite. Notably, in imbalanced scenarios, our analysis uncovers a previously unrecognized effect termed , which significantly constrains model performance on minority classes. Furthermore, the Layer-Peeled Model offers strategies to address Minority Collapse, with computational experiments later validating predictions initially derived from this theoretical framework.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1152", "problem_id": "11520001", "content": "Conventional techniques for video frame interpolation or extrapolation rely on precise pixel-level correspondences, such as those obtained through optical flow. The quality of these methods heavily depends on accurate flow estimation, often producing noticeable artifacts when the estimation fails. While recent auto-encoder-based approaches have demonstrated significant improvements, they are typically designed for specific interpolation or extrapolation tasks, limiting their adaptability. To address these constraints, we present a unified network that parameterizes the target frame position, enabling both interpolation and extrapolation within a single framework. Our method incorporates a transitive consistency loss for improved network regularization and employs a multi-scale architecture to facilitate parameter sharing across layers. By bypassing the computational overhead of global optical flow optimization, our approach offers an efficient and versatile solution for video frame synthesis. Experimental evaluations confirm that our method outperforms existing state-of-the-art techniques.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1153", "problem_id": "11530001", "content": "Detecting defects in solar cell electroluminescence (EL) images presents a complex yet significant challenge. While numerous approaches have been developed for EL defect identification, existing techniques often fall short due to variations in defect characteristics and background conditions. This study introduces an innovative approach leveraging generative adversarial networks (GANs) for defect segmentation. The proposed method initially employs a GAN to eliminate defect areas from input images while preserving the background, producing defect-free counterparts. A subtracted image is then derived by computing the difference between the original defective image and the generated defect-free version, followed by thresholding to isolate defect regions. To ensure background consistency during image generation, we introduce a novel strong identity GAN (SIGAN) incorporating a specialized strong identity loss. Beyond defect segmentation, SIGAN also facilitates dataset augmentation for limited defective samples. Additionally, we present EL-2019, a new dataset containing solar cell EL images categorized into crack, finger interruption, and defect-free types. Evaluations on EL-2019 demonstrate the method’s effectiveness, achieving a 90.34% F-score and surpassing several state-of-the-art techniques in solar cell defect segmentation performance.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1154", "problem_id": "11540001", "content": "This study investigates the benefits of incorporating multiple behavioural types into recommendation systems, recognizing that users typically engage in implicit actions, such as clicking, before making explicit decisions, like purchases. Prior research has highlighted the distinct roles of implicit and explicit feedback in generating useful recommendations, but often considers these behaviours separately or overlooks the sequential interactions between users and items. Building on the hypothesis that a user's preference is shaped by both long-term and short-term interests, we propose several Deep Learning architectures, including Implicit to Explicit (ITE), as well as BERT-ITE and BERT-ITE-Si, which leverage Bidirectional Encoder Representations from Transformers to combine users' long- and short-term preferences, with and without side information, to enhance user representation. Our experimental results, conducted on two large-scale datasets, demonstrate the superiority of our models over existing state-of-the-art approaches, validating the effectiveness of capturing the implicit to explicit order and integrating long- and short-term preferences, as shown in Figure A, B, C (References [1], [2], and [3]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1155", "problem_id": "11550001", "content": "Pedestrian detection is a significant area of research due to its critical role in various applications, particularly in automotive, surveillance, and robotics sectors. Although notable advancements have been made, pedestrian detection remains a challenging issue that necessitates increasingly precise algorithms. Recent years have seen deep learning, particularly convolutional neural networks, rise to prominence as the benchmark for accuracy in several computer vision applications, including image classification, object detection, and segmentation, often significantly surpassing former leading methods. In this paper, we introduce a deep learning-based pedestrian detection system by tailoring a general-purpose convolutional network to address this specific task. Through comprehensive analysis and optimization of each phase within the detection pipeline, we present an architecture that surpasses conventional techniques, achieving a level of accuracy comparable to that of state-of-the-art methods while maintaining low computational demands. Ultimately, we evaluated the system on an NVIDIA Jetson TK1, a 192-core platform anticipated to serve as a pioneering computational engine for future autonomous vehicles.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1156", "problem_id": "11560001", "content": "This paper introduces a novel spatially supervised recurrent convolutional neural network architecture designed for visual object tracking. The proposed recurrent convolutional network leverages both the historical sequence of object locations and the salient visual features extracted by deep neural networks. Drawing inspiration from recent bounding box regression techniques in object detection, we investigate the regression potential of Long Short-Term Memory (LSTM) networks for temporal processing. We suggest integrating high-level visual features derived from convolutional networks with region-specific data. Differing from current deep learning trackers that employ binary classification for region proposals, our method utilizes regression to directly predict tracking locations at both the convolutional layer and the recurrent unit. Comprehensive experimental evaluations and comparisons against state-of-the-art tracking methods using challenging benchmark video tracking datasets demonstrate the enhanced accuracy and robustness of our tracker, coupled with a low computational overhead. Empirical results indicate that the proposed method achieves superior tracking performance across the majority of evaluated video sequences, frequently surpassing the next best method by a significant degree.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1157", "problem_id": "11570001", "content": "Answering intricate logical queries on vast, incomplete knowledge graphs (KGs) poses a significant challenge. A recent, promising approach involves embedding both KG entities and the query itself into a vector space, positioning entities that satisfy the query in close proximity to it. Nevertheless, existing research represents queries as solitary points in this space, which is problematic since complex queries can encompass a substantial set of answer entities, making it unclear how to accurately represent such a set as a single point. Moreover, previous work is limited to handling queries that utilize conjunctions (\\wedge) and existential quantifiers (\\exists), while queries involving logical disjunctions (\\vee) remain unresolved. This study introduces query2box, a novel embedding-based framework designed to reason over arbitrary queries incorporating \\wedge, \\vee, and \\exists operators within massive and incomplete KGs. The core idea is to embed queries as boxes, or hyper-rectangles, where the points within a box correspond to the set of answer entities for the query. It is demonstrated that conjunctions can be naturally represented as box intersections, and a negative result is proven, indicating that handling disjunctions would necessitate embeddings with dimensions proportional to the number of KG entities. However, by converting queries into Disjunctive Normal Form, query2box can efficiently handle arbitrary logical queries with \\wedge, \\vee, \\exists. The effectiveness of query2box is validated on three large KGs, yielding up to a 25% relative improvement over the current state of the art, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1158", "problem_id": "11580001", "content": "In environments characterized by continuous state and action spaces, advanced actor-critic reinforcement learning algorithms are capable of addressing highly complex challenges, yet they can also encounter failures in seemingly simple environments, with the reasons for such failures remaining inadequately understood. This paper offers a formal explanation for these failures, specifically in sparse reward and deterministic settings. Initially, we demonstrate through a basic control problem that the learning process can become trapped at a fixed point that corresponds to a suboptimal solution. Subsequently, by generalizing from this example, we present a comprehensive analysis of the fundamental mechanisms at play, leading to a new insight into one of the convergence behaviors of these algorithms. This newly gained perspective sheds fresh light on existing solutions to the issues we identified and proposes additional potential methods.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1159", "problem_id": "11590001", "content": "Glycemic regulation is crucial in critical care settings; however, it poses substantial challenges due to the absence of research on personalized optimal strategies. This study seeks to establish personalized glycemic trajectories for severely ill septic patients by employing data-driven policies to determine ideal blood glucose targets to guide clinicians. We utilized a sparse autoencoder to represent patient states and implemented a reinforcement learning approach through policy iteration to derive the optimal policy from the data. Additionally, we assessed the expected return based on the policy derived from the observed glycemic trajectories, resulting in a function that illustrates the correlation between actual blood glucose levels and 90-day mortality rates. This indicates that the proposed optimal policy could potentially decrease the projected 90-day mortality rate for patients by 6.3%, reducing it from 31% to 24.7%. The findings illustrate that reinforcement learning, paired with effective patient state representation, may offer optimal glycemic trajectories and enable healthcare providers to create personalized glycemic control strategies for septic patients.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1160", "problem_id": "11600001", "content": "Recent studies have focused on deep neural networks (DNNs) in the infinite width/channel limit, as this perspective offers a transparent analytical approach to deep learning through connections to Gaussian Processes (GPs). However, this viewpoint neglects a vital component of deep learning in finite DNNs, namely feature learning, which is essential to their effectiveness. This work addresses this limitation by developing a self-consistent Gaussian Process theory that incorporates the effects of strong finite-DNNs and feature learning, using DNNs trained with noisy gradient descent on a large dataset. The application of this theory to a simple two-layer linear convolutional neural network (CNN) model yields results that align well with experimental findings, and both analytical and numerical analyses reveal a distinct transition between a feature learning regime and a lazy learning regime. Furthermore, significant finite-DNN effects are also observed in a non-linear two-layer fully connected network, demonstrating the versatility of the proposed self-consistent theory in examining feature learning and other non-lazy effects in finite DNNs.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1161", "problem_id": "11610001", "content": "This paper introduces a high-performance, scalable algorithm for deriving non-monotonic logic programs from statistical learning models. The core idea involves transforming the search for optimal clauses into High-Utility Itemset Mining (HUIM) problems, where feature values and their corresponding importance are regarded as transactions and utilities, respectively. To efficiently identify locally significant features and their weights from ensemble tree models, we employ TreeExplainer, a fast and scalable implementation of the SHAP Explainable AI method. Empirical evaluations using standard UCI datasets demonstrate that our approach significantly enhances both classification performance and training efficiency compared to ALEPH, a leading Inductive Logic Programming (ILP) system.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1162", "problem_id": "11620001", "content": "The process of human visual recognition of activities or external agents is characterized by the interplay between high-level plan recognition and low-level perception, raising the question of whether low-level perception can be enhanced by high-level plan recognition. To investigate this, we propose a framework that utilizes recognized plans to generate improved top-down attention maps, referred to as plan-recognition-driven attention maps, with the goal of enhancing perception performance, as discussed in \\cite. Our approach introduces the Pixel Dynamics Network, a specialized observation model that predicts the next states of object points at each pixel location based on pixel observations and pixel-level action features, effectively learning a pixel-level dynamics model internally. As a variant of Convolutional Neural Network (ConvNet) with a custom-designed architecture, the Pixel Dynamics Network leverages the parallel computation capabilities of ConvNets while learning the dynamics model. Furthermore, we establish the equivalence between the Pixel Dynamics Network as an observation model and the belief update in the partially observable Markov decision process (POMDP) framework. The effectiveness of our Pixel Dynamics Network is evaluated in the context of event recognition tasks, where it is integrated as a subroutine into an event recognition system, ER-PRN, which recognizes events based on observations augmented by plan-recognition-driven attention.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1163", "problem_id": "11630001", "content": "Deep learning models excel at radiology image classification, but their reliance on extensive labeled datasets hinders their real-world usability. Semi-supervised learning (SSL) methods, which utilize small labeled datasets in conjunction with larger unlabeled datasets, present a solution for lowering annotation expenses. This paper introduces NoTeacher, a new consistency-based SSL framework that integrates probabilistic graphical models. Differing from Mean Teacher, which relies on a teacher network updated through temporal ensembling, NoTeacher uses two separate networks, thus removing the necessity for a teacher network. We illustrate how NoTeacher can be tailored to address various difficulties in radiology image classification, detailing modifications for 2D and 3D inputs, single and multi-label classification, and discrepancies in class distribution between labeled and unlabeled training data. Empirical assessments on three publicly available benchmark datasets, encompassing common radiology modalities (X-Ray, CT, MRI), demonstrate that NoTeacher achieves AUROC scores exceeding 90-95% of fully supervised models with only 5-15% of the data labeled. Furthermore, NoTeacher surpasses existing SSL techniques with limited hyperparameter optimization, suggesting its potential as a reliable and useful semi-supervised learning approach for radiology applications.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1164", "problem_id": "11640001", "content": "This paper presents Point-MVSNet, a pioneering deep learning framework for multi-view stereo (MVS) that operates directly on point clouds, diverging from traditional cost volume-based approaches. The proposed method employs a coarse-to-fine strategy, initiating with the generation of a coarse depth map that is subsequently converted into a point cloud and iteratively refined through residual estimation between current and ground truth depths. By integrating 3D geometric priors and 2D texture information into a feature-enriched point cloud, the network effectively estimates the 3D flow for each point, yielding a point-based architecture that surpasses cost-volume-based counterparts in terms of accuracy, computational efficiency, and flexibility. As demonstrated by experimental results on the DTU and Tanks and Temples datasets, Point-MVSNet achieves substantial improvements in reconstruction quality over state-of-the-art methods, with the source code and trained models available at https://github.com/callmeray/PointMVSNet.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1165", "problem_id": "11650001", "content": "The task of Semantic Scene Completion (SSC) involves simultaneously inferring the 3D semantic segmentation of a scene and completing its 3D shapes. To address this challenge, we introduce PALNet, a novel hybrid network that operates on single depth input and leverages a two-stream architecture to extract 2D and 3D features from multiple stages, utilizing fine-grained depth information to capture contextual and geometric scene cues effectively. Unlike existing SSC methods that treat all scene parts equally, thereby allocating unnecessary attention to object interiors, our approach incorporates Position Aware Loss (PA-Loss), a training loss function that accounts for position importance. Specifically, PA-Loss considers Local Geometric Anisotropy to determine the significance of different scene positions, which is particularly beneficial for recovering crucial details such as object boundaries and scene corners. The efficacy and superior performance of our proposed method are demonstrated through comprehensive experiments on two benchmark datasets, with additional resources available at https://github.com/UniLauX/PALNet, including models and a video demo, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1166", "problem_id": "11660001", "content": "The emergence of neuromorphic hardware positions spiking neural networks as a promising energy-efficient alternative to traditional artificial neural networks. Nonetheless, their application in computer vision is still relatively limited, primarily addressing basic tasks like digit recognition. More complex problems, such as segmentation and object detection, pose challenges due to the scarce research on deep spiking neural networks addressing these issues. This paper aims to take a significant initial step toward advancing computer vision through supervised spiking neural networks. We introduce a deep convolutional spiking neural network designed for the localization of a single object within a grayscale image, utilizing DECOLLE, a spiking model that facilitates local surrogate gradient-based learning. The positive results obtained on the Oxford-IIIT-Pet dataset validate the potential of leveraging spiking neural networks with a supervised learning framework for tackling more sophisticated vision tasks in the future.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1167", "problem_id": "11670001", "content": "In recent times, representation learning techniques have significantly impacted various multimedia computing applications. Specifically, deep convolutional neural networks (CNNs) have achieved performance levels comparable to humans in certain restricted image classification tasks. However, the process of training CNNs from the ground up for new tasks or datasets is often complicated and requires considerable time. In this context, transfer learning has emerged as a valuable strategy, allowing for the adaptation of pre-trained CNNs to new datasets and categories by re-training only the final classification layer. This paper aims to enhance this process to facilitate better knowledge transfer among CNN architectures, thereby accelerating training during fine-tuning for image classification. This improvement is realized by integrating and transferring additional weights, informed by similarity between source and target classes. The research includes an evaluation of semantic versus content-based similarities, demonstrating improved initial performance and training efficiency, as well as superior long-term results when training data is scarce.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1168", "problem_id": "11680001", "content": "Creating realistic images of intricate visual scenes poses difficulties when attempting to regulate their structural composition. While earlier techniques demonstrated control over scenes with limited elements using scene graphs, this method falters as graph complexity—measured by object and edge count—grows. This study identifies a key constraint in existing approaches: their failure to recognize semantic equivalence in graphs. We introduce a new model that overcomes these challenges by deriving canonical graph representations from data, leading to enhanced image synthesis for complex scenes. Our approach exhibits superior empirical results on extensive scene graphs, resilience to input graph noise, and adaptability to semantically equivalent structures. Additionally, we validate the model’s enhanced performance across three benchmarks: Visual Genome, COCO, and CLEVR.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1169", "problem_id": "11690001", "content": "Graph convolutional networks (GCNs), which represent human body skeletons through nodes and edges, have shown exceptional results in skeleton-based action recognition. Yet, existing state-of-the-art approaches typically assume fully observed skeletons, which may not hold in practical settings due to potential incompleteness or noise in captured data. To address this, we introduce a robust skeleton-based action recognition method that effectively handles noisy skeleton features. Our approach leverages predictive coding to maximize mutual information between clean and corrupted skeletons during training. Extensive experiments on NTU-RGB+D and Kinetics-Skeleton datasets with imperfect skeleton data confirm that our method outperforms current leading techniques under noisy conditions.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1170", "problem_id": "11700001", "content": "The challenge of efficient exploration in Reinforcement Learning has yet to be fully resolved, with existing approaches often relying on rewarding agents for inadvertently discovering new situations. To address this, we propose Model-Based Active eXploration (MAX), an active exploration algorithm that leverages an ensemble of forward models to proactively plan the observation of novel events. By optimizing agent behavior based on a novelty measure derived from a Bayesian exploration perspective, estimated through the disagreement among predicted futures by ensemble members, MAX achieves efficient exploration. Empirical results demonstrate that MAX outperforms strong baselines by at least an order of magnitude in semi-random discrete environments, where directed exploration is crucial for progress, and also scales to high-dimensional continuous environments, where it constructs task-agnostic models applicable to various downstream tasks.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1171", "problem_id": "11710001", "content": "Generative Adversarial Networks (GANs) and their variants demonstrate strong performance in producing synthetic data, yet they face critical challenges: (i) training often centers on memorizing input samples, risking privacy breaches when handling sensitive or personally identifiable data, and (ii) limited control over the specificity of generated outputs due to inherent randomness. To mitigate these concerns, we introduce imdpGAN, an information-maximizing differentially private GAN framework that ensures privacy while learning meaningful latent representations. Evaluations on the MNIST dataset confirm that imdpGAN safeguards individual data privacy and leverages latent codes to regulate output specificity. Binary classification tests on digit pairs reveal a trade-off between utility and privacy, with accuracy declining as privacy constraints tighten. Although imdpGAN maintains stable training, it incurs a 10-fold increase in runtime compared to conventional GANs. Additionally, we validate the framework's scalability on the CelebA dataset, demonstrating its effectiveness in balancing privacy and controlled output generation.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1172", "problem_id": "11720001", "content": "This paper addresses the issue of limited sample availability in human parsing tasks by employing a self-learning approach, which involves generating pseudo-labels for unlabeled data to fine-tune the model. However, the use of noisy pseudo-labels can lead to error propagation, which is mitigated by proposing a trainable graph reasoning method that leverages the topological structure of the human body to correct two common errors in pseudo-labels: global structural errors and local consistency errors. To rectify global errors, a high-level graph model is constructed from category-wise features, which is then decoupled to reconstruct category features that better capture the human body's topology. Additionally, local errors are reduced by enlarging the receptive field of features, achieved by projecting feature pixels into a local graph model to capture pixel-wise relations, and then reversing the relation information back to the pixels. The integration of global structural and local consistency modules enables the generation of confident pseudo-labels for retraining, as evidenced by extensive experiments on the LIP and ATR datasets, which demonstrate the superiority of our approach over state-of-the-art methods in supervised human parsing tasks, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1173", "problem_id": "11730001", "content": "This study introduces a Reinforcement Learning (RL) approach to circumvent Google reCAPTCHA v3, framing the challenge as a grid world problem where an agent learns to navigate and interact with the reCAPTCHA button to attain a high score. The research investigates how the agent's performance is affected by varying the grid world's cell size, revealing a decline in performance when the agent takes larger steps towards its objective. To overcome the reCAPTCHA system regardless of grid resolution, a divide and conquer strategy is employed. The proposed methodology yields a success rate of 97.4% on a 100x100 grid and 96.7% on a 1000x1000 screen resolution, demonstrating its effectiveness.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1174", "problem_id": "11740001", "content": "Recent progress in graph representation learning, particularly through graph convolutional networks, has significantly enhanced performance across various graph-based benchmark tasks. Although new methods for generating node embeddings excel in node classification and link prediction, their use in graph classification—assigning a single label to an entire graph—remains largely basic, often relying on simple global pooling or predefined heuristics for hierarchical graph reduction. A key advancement in addressing this limitation is differentiable graph coarsening, which enables adaptive, data-driven graph size reduction within a graph neural network framework, similar to downsampling in CNNs. However, earlier pooling methods suffer from quadratic memory demands during training, limiting scalability for large graphs. By integrating recent innovations in graph neural network architecture, we show that hierarchical graph classification can achieve strong performance while maintaining sparsity. Our findings, validated on multiple standard benchmarks, underscore a promising direction for future research in graph neural networks.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1175", "problem_id": "11750001", "content": "This study tackles the challenge of identifying key training samples that significantly impact the performance of tree ensemble-based models, such as Random Forest (RF) and Gradient Boosted Decision Trees (GBDT). To formalize this problem, we examine the effect of removing each individual training sample on the model's predictions, which can be achieved through leave-one-out retraining. Although recent research has developed efficient methods for conducting this analysis on parametric models, we extend this framework to accommodate non-parametric GBDT ensembles, assuming fixed tree structures. Additionally, we present a generalized approach to derive approximations that balance the trade-off between accuracy and computational complexity. Our methods are evaluated across various experimental settings and use-case scenarios, demonstrating their effectiveness in identifying influential training samples and computational efficiency compared to baseline approaches, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1176", "problem_id": "11760001", "content": "Multimodal image super-resolution (SR) involves enhancing a low-resolution image using complementary information from a different image modality. Current deep learning approaches for this task often overlook fundamental SR principles, prompting us to develop a multimodal network architecture that incorporates coupled sparse priors to efficiently merge cross-modal data during reconstruction. The design of our interpretable network is derived from an innovative iterative technique for coupled convolutional sparse coding. We demonstrate the effectiveness of our approach by applying it to RGB-guided near-infrared image super-resolution, where our model achieves superior performance compared to existing state-of-the-art techniques.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1177", "problem_id": "11770001", "content": "This study introduces a method based on convolutional neural networks to determine the relative pose between two cameras. The designed network receives RGB images from both cameras as input and directly outputs the relative rotation and translation. The training process follows an end-to-end approach, leveraging transfer learning from a large classification dataset. The proposed method is contrasted with commonly utilized local feature-based techniques (SURF, ORB), revealing a significant enhancement over the baseline results. Furthermore, a version of the proposed architecture that incorporates a spatial pyramid pooling (SPP) layer is assessed and demonstrates additional improvements in performance.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1178", "problem_id": "11780001", "content": "This paper introduces `GridMask,' an innovative data augmentation technique that employs information removal to achieve top performance across multiple computer vision applications. We examine the necessity of information elimination, highlight constraints in current algorithms, and present our structured approach—a straightforward yet highly efficient method involving the selective deletion of image regions. Comprehensive experiments demonstrate that our technique surpasses AutoAugment, which relies on computationally intensive reinforcement learning for policy optimization. Significant performance gains are observed on ImageNet for classification, COCO2017 for object detection, and Cityscapes for semantic segmentation, validating the method's efficacy and broad applicability.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1179", "problem_id": "11790001", "content": "The opioid overdose crisis, commonly referred to as the \"opioid epidemic,\" is a significant public health issue in the U.S., leading to adverse effects such as declining health, rising crime, and familial disruptions. To enhance overdose monitoring and pinpoint regions requiring intervention, this study explores opioid overdose prediction through real-time crime patterns. Prior research has established connections between opioid misuse and criminal behavior, driven by factors like financial incentives and shared underlying causes. Building on these findings, we introduce a new spatio-temporal model for overdose forecasting that utilizes crime incident patterns. The model employs multi-head attentional networks to capture diverse feature representations, forming a \"community-attentive\" deep learning framework that optimizes predictions by integrating regional group dynamics. Furthermore, our approach enables the interpretation of influential features and community contributions to local incident forecasting. Evaluations on two real-world overdose datasets demonstrate that our model outperforms existing methods while offering valuable insights into the spatio-temporal interplay between crime trends and opioid overdose occurrences.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1180", "problem_id": "11800001", "content": "Recent research in adversarial machine learning has shifted focus toward visual perception in autonomous vehicles, particularly examining adversarial examples (AEs) in object detection models. However, the visual perception pipeline also involves tracking detected objects through Multiple Object Tracking (MOT) to map the trajectories of surrounding obstacles. Since MOT is inherently resilient to object detection errors, existing attack methods that indiscriminately target detection face a significant challenge: they require a success rate exceeding 98% to impact tracking outcomes, a threshold no current technique achieves. This paper pioneers the investigation of adversarial attacks on the entire visual perception system in autonomous driving, introducing a novel approach called tracker hijacking, which effectively deceives MOT by exploiting AEs in object detection. Our method demonstrates that successful AEs on just one frame can manipulate an object's position relative to the vehicle’s path, creating potential safety risks. Evaluations on the Berkeley Deep Drive dataset reveal that, when targeting three frames, our attack achieves nearly 100% success, whereas conventional detection-focused attacks reach only 25%.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1181", "problem_id": "11810001", "content": "A basic convolutional neural network achieved success in the ISISPA color constancy challenge. The results could have been further improved by partially replicating the neural network structure proposed by (Bianco, 2017) under these conditions.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1182", "problem_id": "11820001", "content": "Understanding the properties of neural networks can be achieved by characterizing the associated function spaces. This paper explores how the framework of reproducing kernel Banach spaces can be applied to address this issue. Specifically, we establish a representer theorem applicable to a broad category of reproducing kernel Banach spaces that permit an appropriate integral representation and encompass one hidden layer neural networks that may have infinite width. Additionally, we demonstrate that for a defined category of ReLU activation functions, the norm in the corresponding reproducing kernel Banach space can be described through the inverse Radon transform of a bounded real measure, with the norm expressed as the total variation norm of that measure. Our findings provide simplifications and extensions to recent results found in [34,29,30].", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1183", "problem_id": "11830001", "content": "A crucial challenge in detecting salient objects lies in the effective modeling of their semantic properties using a data-driven approach. To address this, we introduce a deep saliency model that leverages a fully convolutional neural network (FCNN) with whole images as input and whole saliency maps as output, facilitating a data-driven encoding of underlying saliency prior knowledge. Our model employs a multi-task learning framework to explore the inherent relationships between saliency detection and semantic image segmentation, enabling collaborative feature learning and the production of effective features for object perception through shared fully convolutional layers. This architecture allows for the capture of semantic information about salient objects at various levels, while reducing feature redundancy by exploiting the feature-sharing properties of salient object detection. Additionally, a graph Laplacian regularized nonlinear regression model is proposed for refining saliency estimates. As evidenced by experimental results, our approach outperforms state-of-the-art methods, demonstrating its efficacy in salient object detection, as shown in Figure A, B, C (References [1], [2], and [3] provide further details).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1184", "problem_id": "11840001", "content": "This paper addresses the challenge of reducing the number of measurements required for digital image acquisition and reconstruction while maintaining a specified level of accuracy. It outlines fundamental concepts from sampling theory to demonstrate that the minimum required signal sampling rate for accurate reconstruction corresponds to the spectrum sparsity of the signal's sparse approximation. The study reveals that the compressed sensing method, proposed as a means to minimize the sampling rate, does not achieve the theoretical lower bound for sampling rates. A straightforward and intuitive model is used to clarify the strengths and weaknesses of compressed sensing. Additionally, the paper introduces a technique called Arbitrary Sampling and Bounded Spectrum Reconstruction (ASBSR-method), which approaches the theoretical minimum sampling rate for images. The authors also present and discuss experimental results validating the ASBSR-method and explore its potential extensions for resolving various underdetermined inverse problems, including color image demosaicing, inpainting, reconstruction from sparsely sampled or decimated projections, recovery from the modulus of the Fourier spectrum, and reconstruction from sparse samples in the Fourier domain.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1185", "problem_id": "11850001", "content": "Deep Learning (DL) is being applied to increasingly complex domains, offering transformative potential while introducing new concerns about its dependability in critical roles. This paper introduces a model-independent approach for evaluating the reliability of DL classifiers, combining robustness analysis with the operational profile (OP) of specific applications. The method involves dividing the input space into fine-grained segments and aggregating their robustness relative to ground truth, guided by the OP, with estimators provided for both robustness and operational profiles. This enables the calculation of misclassification probabilities (pmi) along with confidence intervals. A prototype tool is illustrated through simplified examples, while model assumptions and practical implementation challenges are examined. Although the approach highlights inherent obstacles in DL reliability assessment—such as limited ground truth data and scalability constraints—it proposes interim solutions to facilitate progress in this research area.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1186", "problem_id": "11860001", "content": "Traditional methods for causal discovery require the formulation of a new model whenever they are presented with data from a different underlying causal structure. Nonetheless, these datasets frequently contain common information—such as the dynamics that describe causal relationships—which is disregarded with this methodology. We introduce Amortized Causal Discovery, an innovative framework that capitalizes on these shared dynamics to deduce causal relationships from time-series data. This approach allows for the development of a single, amortized model capable of inferring causal relationships across samples derived from various causal graphs, effectively harnessing the shared information. Our experimental results indicate that this method, realized as a variational model, significantly enhances causal discovery performance and demonstrate its potential for successful application in scenarios with hidden confounding.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1187", "problem_id": "11870001", "content": "What role do visual priors about the environment (such as the understanding that the world is three-dimensional) play in the acquisition of subsequent motor skills (like navigating through complex settings)? Additionally, what are the effects of failing to employ these visual priors during learning? We explore these inquiries by embedding a versatile perceptual skill set (including components like a distance estimator and an edge detector) within a reinforcement learning paradigm (refer to Fig. 1). This skill set, referred to as \"mid-level vision,\" equips the policy with a more refined perception of the world compared to unprocessed images. Our extensive research indicates that leveraging mid-level vision leads to policies that learn at an accelerated rate, generalize more effectively, and achieve superior overall performance, relative to starting from scratch or utilizing advanced visual and non-visual representation learning techniques. We demonstrate that traditional computer vision objectives are particularly advantageous in this context and can be seamlessly incorporated into reinforcement learning systems. Ultimately, we discovered that no single visual representation proved beneficial across all downstream tasks; therefore, we computationally derive a task-agnostic collection of representations optimized for various downstream objectives.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1188", "problem_id": "11880001", "content": "The automated examination of digitized historical documents involves numerous image processing challenges, often complicated by insufficient labeled training data for machine learning. Deep neural networks offer a potential solution by enabling pre-training on unrelated image domains followed by domain-specific fine-tuning. A common approach involves leveraging models initially trained on ImageNet for object detection, though the effectiveness of this strategy for historical documents—which exhibit distinct visual characteristics—remains uncertain. This study conducts an extensive empirical evaluation of ImageNet pre-training’s impact on various tasks such as character recognition, style classification, manuscript dating, semantic segmentation, and content-based retrieval. Although semantic segmentation yields inconsistent outcomes, the findings consistently demonstrate that ImageNet pre-training enhances performance in classification and retrieval tasks across multiple network architectures.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1189", "problem_id": "11890001", "content": "In environments where agents have restricted visibility or resources, they cannot comprehensively perceive a scene before attempting to interpret it, rendering traditional semantic segmentation architectures impractical. This paper presents a novel approach to incrementally segment a scene based on a series of partial observations. The core concept involves an agent refining its environmental understanding by focusing on areas of highest uncertainty. Our proposed method incorporates a self-supervised attention mechanism and a specialized architecture that utilizes and maintains spatial memory maps to fill in unobserved areas, enabling the agent to select and attend to specific areas while using cues from previously visited areas to infer unseen parts. With only 18% of image pixels processed (via 10 retina-like glimpses), we achieve mean pixel-wise accuracy of 78.1%, 80.9%, and 76.5% on the CityScapes, CamVid, and Kitti datasets, respectively. An ablation study examining the impact of glimpse quantity, input image size, and retina-like glimpse effectiveness is conducted, and comparisons with several baselines demonstrate that optimal results are obtained when the agent has access to a very low-resolution view of the scene at the initial timestep.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1190", "problem_id": "11900001", "content": "Foraminifera are unicellular marine organisms that can either live in the water column or on the ocean floor. They create shells made up of one or more chambers throughout their lifespan, and these shells eventually become fossils within marine sediments. The classification and quantification of these fossils have become essential tools in fields such as oceanography and climatology. At present, the identification and counting of microfossils are done manually with a microscope, which is a labor-intensive process. Consequently, the development of automated methods for this task is deemed crucial across various research areas. This paper presents initial steps towards creating a deep learning model capable of detecting and classifying microscopic foraminifera. The proposed model utilizes a VGG16 architecture that has been pretrained on the ImageNet dataset, which is then fine-tuned for foraminifera classification through transfer learning. Moreover, a new image dataset comprising microscopic foraminifera and sediment samples from the Barents Sea region is introduced.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1191", "problem_id": "11910001", "content": "A crucial aspect of ensuring the safe deployment of machine learning (ML) systems, particularly in critical applications, is the systematic identification and mitigation of vulnerabilities in neural networks. One prevalent type of safety risk arises from learned shortcuts, which refer to spurious correlations that a network exploits to make decisions, yet lack semantic relevance to the task at hand, potentially leading to poor generalization to novel inputs. While explainability methods can help uncover such vulnerabilities, their applicability is limited in black-box setups where access to the network is restricted, a common scenario when utilizing third-party ML components. To overcome this limitation, we propose an approach that utilizes an interpretable-by-design network as a proxy to the black-box model, enabling the detection of learned shortcuts. By leveraging the proxy's introspective capabilities, we automatically identify candidate shortcuts and systematically validate their transferability to the black-box model, demonstrating the effectiveness of this approach using a BagNet as the proxy model on the A2D2 autonomous driving dataset, where extracted patch shortcuts are shown to significantly impact the black-box model, ultimately contributing to the development of safer ML models.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1192", "problem_id": "11920001", "content": "This research explores the application of reinforcement learning in informing the decisions of a general-purpose cache manager, which plays a crucial role in determining the overall performance of computer systems by controlling the caching of objects, their retention duration, and eviction when the cache is full, all of which affect the cache hit rate and required storage size. An effective cache manager should minimize unnecessary operations, maximize the cache hit rate to reduce round trips to slower backend storage, and optimize storage size for a high hit rate. To achieve this, the project designs three distinct agents for each cache management task and examines two advanced reinforcement learning architectures - a single multi-task agent and a multi-agent system - for tackling multi-decision problems. Additionally, a framework is introduced to facilitate the modeling of computer system problems as reinforcement learning tasks, handling delayed experiences, observations, and reward assignments while allowing scalability to multiple agents. Simulation results, based on an established database benchmark system, demonstrate that reinforcement learning agents can outperform heuristic-driven algorithms by achieving higher cache hit rates while minimizing required space, adapting to changing workloads, and dynamically adjusting caching strategies, with the proposed model being applicable to various cache types, such as file system caches, and representing the first instance of modeling cache manager decisions as a multi-task control problem, as per our knowledge.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1193", "problem_id": "11930001", "content": "In recent years, convolutional neural networks (CNNs) have achieved significant success in semantic segmentation, a crucial task in applications like autonomous driving. However, training CNNs demands substantial data, which is both hard to acquire and time-consuming to annotate. Modern computer graphics techniques enable training CNNs using realistic synthetic images with automatically generated annotations. Nevertheless, performance is hindered by the discrepancy between real and synthetic data. To address this, we introduce a curriculum-based learning strategy designed to reduce the domain gap in semantic segmentation of urban environments. This curriculum domain adaptation method initially tackles simpler tasks to deduce essential characteristics of the target domain. Specifically, the initial task involves learning global label distributions across images and local distributions across prominent superpixels. These distributions are relatively easy to estimate due to the inherent regularities present in urban scene images (e.g., the dimensions and spatial arrangements of buildings, roads, vehicles, etc.). Subsequently, a segmentation network is trained, with its predictions in the target domain regularized to align with the previously inferred properties. Experimental results demonstrate that our approach surpasses baseline methods on two datasets and with two backbone networks. Furthermore, we present comprehensive ablation studies to evaluate our method.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1194", "problem_id": "11940001", "content": "Object detection is a crucial initial step in automated video analysis for numerous computer vision applications. Conventional methods for object detection in video typically involve object detectors, which often necessitate manually annotated training data for binary classifier training, or background subtraction techniques, which require a training sequence devoid of objects to establish a background model. To streamline analysis, object detection without a distinct training phase is essential. Prior efforts have explored motion information for this purpose; however, existing motion-based methods frequently struggle with complex scenarios, including non-rigid motion and dynamic backgrounds. This paper introduces DEtecting Contiguous Outliers in the LOw-rank Representation (DECOLOR), a unified framework designed to overcome these challenges. DECOLOR integrates object detection and background learning into a singular optimization process, efficiently solvable via an alternating algorithm. We elucidate the connections between DECOLOR and alternative sparsity-based methodologies. Empirical evaluations using both synthetic and real-world sequences confirm that DECOLOR surpasses state-of-the-art techniques and exhibits efficacy across diverse and intricate scenarios.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1195", "problem_id": "11950001", "content": "Estimating the soft foreground from a natural image without auxiliary information, known as automatic image matting (AIM), is a valuable tool for image editing applications. Existing methods have primarily focused on learning semantic features to facilitate matting for images featuring prominent opaque foreground objects, such as humans and animals, but struggle when applied to images with transparent or intricate foregrounds, or those with non-salient foregrounds. This paper explores the challenges associated with extending current methods to these more complex image types and proposes a novel end-to-end matting network capable of generating a unified semantic representation in the form of a generalized trimap for diverse images. The network utilizes learned semantic features and an attention mechanism to concentrate on transition areas, enhancing matting accuracy. To evaluate the generalization capabilities of AIM models, a new test set, AIM-500, comprising 500 natural images with manually labeled alpha mattes, has been created. Experimental results show that the proposed network, trained on available composite matting datasets, surpasses existing methods in both objective and subjective evaluations, as demonstrated in Figure A, B, C (see References [citation] for details), and the source code and dataset are available at https://github.com/JizhiziLi/AIM.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1196", "problem_id": "11960001", "content": "Existing hypergraph expansion techniques operate exclusively on either vertices or hyperedges, neglecting the inherent symmetry in data co-occurrence and leading to information depletion. To mitigate this, we introduce a novel hypergraph formulation that considers vertices and hyperedges as equivalent entities for hypergraph learning. This new expansion method establishes a bijective mapping, transforming the hypergraph into a homogeneous structure by defining vertex-hyperedge pairs as \"line nodes.\" By converting the hypergraph into a simple graph, our proposed approach ensures compatibility between existing graph learning algorithms and higher-order structures, serving as a unifying framework for diverse hypergraph expansion methods. Empirical evaluation on five hypergraph datasets demonstrates that our method significantly outperforms state-of-the-art (SOTA) baselines.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1197", "problem_id": "11970001", "content": "The capacity to effectively search for images within an indexed database is fundamental to various user experiences. By integrating user feedback through multi-modal inputs, we can enhance flexibility and interaction to meet specific requirements more precisely. Our focus is on text feedback, particularly through descriptive natural language queries. Given a reference image along with textual user input, our objective is to retrieve images that comply with the constraints outlined by both input types. This task presents challenges, as it necessitates comprehending the textual semantics from the feedback and translating these insights into visual representations. To tackle these issues, we introduce a novel architecture called TRACE, which features a hierarchical feature aggregation module designed to learn composite visio-linguistic representations. TRACE demonstrates state-of-the-art performance across three benchmark datasets: FashionIQ, Shoes, and Birds-to-Words, yielding average improvements of approximately ~5.7%, ~3%, and ~5% respectively in the R@K metric. Our comprehensive experiments and ablation studies indicate that TRACE consistently surpasses existing methods by substantial margins in both quantitative and qualitative measures.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1198", "problem_id": "11980001", "content": "We investigate model-based offline Reinforcement Learning utilizing general function approximation. We introduce an algorithm called Constrained Pessimistic Policy Optimization (CPPO), which incorporates a general function class and employs a constraint to represent pessimism. Assuming that the true model resides within our function class, CPPO is capable of learning using offline data that offers only partial coverage; specifically, it can devise a policy that competes against any policy represented by the offline data, achieving polynomial sample complexity relative to the statistical complexity of the function class. Furthermore, we illustrate that this algorithmic framework can be implemented across various specialized Markov Decision Processes, where additional structural assumptions can enhance the understanding of partial coverage. A prominent illustration is the low-rank MDP with representation learning, where partial coverage is characterized by the relative condition number associated with the underlying unknown ground truth feature representation. Lastly, we explore the Bayesian framework within offline RL. The primary advantage of Bayesian offline RL is that we do not have to explicitly formulate pessimism or reward penalties, which can be challenging in scenarios that exceed linear model structures. We propose a posterior sampling-based incremental policy optimization algorithm (PS-PO), which iteratively samples a model from the posterior distribution and executes one-step incremental policy optimization within the sampled model. Theoretically, PS-PO can learn a nearly optimal policy under partial coverage with polynomial sample complexity, on average regarding the prior distribution.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1199", "problem_id": "11990001", "content": "The growing adoption of Artificial Intelligence as a Service has underscored the need to safeguard well-trained models as intellectual property, with two primary protection strategies emerging: ownership verification and usage authorization. This paper introduces Non-Transferable Learning (NTL), a groundbreaking approach that embeds exclusive data representation in the learned model, thereby limiting its generalization capacity to specific domains, and offering effective solutions for both model verification and authorization. Unlike conventional watermarking techniques, which are susceptible to advanced watermark removal methods, our NTL-based model verification method demonstrates robust resistance to state-of-the-art removal techniques, as evidenced by comprehensive experiments conducted on four such methods using the digits, CIFAR10 & STL10, and VisDA datasets. Furthermore, whereas existing usage authorization solutions focus on granting access to specific users, our NTL-based approach provides data-centric protection by substantially degrading model performance on unauthorized data, with its efficacy validated through experiments on diverse datasets, Figure A, B, C, as reported in References [citation].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1200", "problem_id": "12000001", "content": "Objective: Image classification serves as a foundational application in AI-driven imaging, yet manual labeling remains labor-intensive. Previous work showed that reinforcement learning (RL) effectively classifies 2D MRI brain slices. This study advances the field by automating label extraction from clinical reports and extending classification to 3D volumes. The approach consists of two phases: Part 1 employs SBERT-based natural language processing to derive labels from reports, while Part 2 applies these labels to train a Deep-Q Network (DQN) for 3D volume classification via RL. Methodology: In Part 1, SBERT was trained on 90 radiology report impressions to predict class labels. Part 2 implemented multi-step classification using 3D convolutions and TD(0) Q-learning, trained on 90 images and tested on 61, with labels generated by SBERT. A supervised deep learning model was also evaluated on the same dataset for comparison. Findings: Part 1 demonstrated perfect SBERT accuracy (100%) in distinguishing normal and metastatic scans. In Part 2, while supervised learning overfit training data (66% test accuracy), RL achieved 92% accuracy, with statistical significance (p=3.1×10^-5).", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1201", "problem_id": "12010001", "content": "This paper frames the adaptive learning problem, which involves creating a personalized learning plan, or policy, that selects the most suitable learning materials based on a learner's underlying characteristics, as a Markov decision process (MDP). Assuming these latent traits are continuous and have an unknown transition model, we utilize a model-free deep reinforcement learning approach, specifically the deep Q-learning algorithm, to derive the optimal learning policy from learner data without requiring knowledge of the actual transition model. To maximize the use of available data, a neural network-based transition model estimator is developed to mimic the learner's learning process, which can be integrated into the deep Q-learning algorithm to enhance the discovery of the optimal policy. As demonstrated by numerical simulation studies, the proposed algorithm is highly effective in identifying a suitable learning policy, and with the assistance of the transition model estimator, it can determine the optimal policy using a relatively small number of learners, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1202", "problem_id": "12020001", "content": "The registration of 3D point clouds remains a formidable challenge, particularly when dealing with partial correspondences and the absence of initial estimation information, which complicates the determination of the rigid transformation between two point clouds. To address this issue, this paper proposes a comprehensive deep-learning-based approach for point cloud registration. The methodology involves the utilization of a revised LPD-Net for feature extraction and aggregation via a graph network, followed by the application of self-attention and cross-attention mechanisms to enhance structural and corresponding information between the two input point clouds. The approach then generates virtual corresponding points using a soft pointer-based method and ultimately resolves the registration problem through the implementation of the SVD method. The efficacy of the proposed approach is demonstrated through comparative results on the ModelNet40 dataset, which indicate state-of-the-art performance in point cloud registration tasks, and experimental results on the KITTI dataset, which validate its effectiveness in real-world applications, with the source code available at \\url.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1203", "problem_id": "12030001", "content": "Many researchers employ classification-based approaches for vehicle re-identification, necessitating periodic updates to incorporate new vehicle models. This paper introduces two vehicle re-identification techniques. The first, a conventional approach, relies on an image of the target vehicle and is trained using datasets like VRIC and VehicleID. The study details how a classification-trained network can enhance this method's performance. The second technique utilizes a representative image of a similar vehicle—matching make, model, year, and color—when direct images of the target are unavailable, generating shape and color features for database matching. To ensure robustness, a fine-grained classification system was developed, categorizing vehicles based on four hierarchical attributes: manufacturer (e.g., Mercedes-Benz), model (e.g., C Class), model year, and perspective. A separate color classification model was also trained. The paper presents re-identification results and demonstrates the method's application on video footage and controlled datasets using a custom tool. This research received partial funding from a grant.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1204", "problem_id": "12040001", "content": "Domain generalization addresses the challenge in machine learning where the data used for training differs from that used for testing, originating from distinct data domains. We propose a straightforward theoretical framework for learning to generalize across these domains, characterized by a meta-distribution that encompasses various data distributions, which may possess different supports. In our framework, the training data supplied to the learning algorithm comprises several datasets, each sourced from a specific domain drawn from the meta-distribution. We explore this framework across three distinct scenarios: a multi-domain Massart noise context, a decision tree multi-dataset environment, and a feature selection context, and we discover that domain generalization can be achieved efficiently with polynomial samples in each case. Our experiments indicate that the feature selection algorithm effectively disregards irrelevant correlations, thereby enhancing generalization.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1205", "problem_id": "12050001", "content": "This paper aims to demonstrate that the newly introduced graphical model, Conditional Random Fields (CRF), serves as a framework for incorporating micro-level data regarding biological entities into a mathematical model to analyze their macro-level behavior. Specifically, we will utilize the CRF model to address a significant classification challenge in protein science: the prediction of protein secondary structure based on the observed primary structure. A comparative analysis using benchmark datasets against twenty-eight other methods reveals that the CRF model not only achieves highly accurate predictions but also, due to its modular design and capability to integrate various, overlapping, and non-independent information sources, positions itself as a highly adaptable tool for addressing a wide range of issues in bioinformatics.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1206", "problem_id": "12060001", "content": "We propose a novel approach for real-time rendering of Neural Radiance Fields (NeRFs) utilizing PlenOctrees, a 3D octree-based representation that accommodates view-dependent effects. By pre-tabulating NeRFs into PlenOctrees, our method achieves rendering speeds of over 150 FPS for 800x800 images, exceeding the performance of conventional NeRFs by a factor of 3000, without compromising quality or the capability for free-viewpoint rendering of complex scenes with arbitrary geometry and view-dependent effects. To preserve view-dependent effects like specularities, we employ a factorization of appearance using closed-form spherical basis functions, demonstrating that NeRFs can be trained to predict radiance in spherical harmonic representation, thereby eliminating the need for viewing direction as a neural network input. Additionally, direct optimization of PlenOctrees minimizes reconstruction loss, resulting in comparable or superior quality to existing methods, while also reducing training time by eliminating the need for full NeRF training convergence. This real-time neural rendering technique has the potential to enable innovative applications, including 6-DOF industrial and product visualizations, as well as next-generation AR/VR systems, with PlenOctrees also supporting in-browser rendering, as showcased in the interactive online demo, video, and code available at https://alexyu.net/plenoctrees.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1207", "problem_id": "12070001", "content": "This paper investigates the nonconvex optimization landscapes associated with learning overcomplete representations, specifically focusing on (i) sparsely used overcomplete dictionaries and (ii) convolutional dictionaries, both relevant to unsupervised learning in high-dimensional data analysis. Addressing the gap between the practical effectiveness and theoretical understanding of simple nonconvex algorithms, we demonstrate that these problems can be expressed as \\ell^4-norm optimization problems subject to a spherical constraint and analyze the geometric characteristics of their nonconvex optimization landscapes. We establish that, for both problem types, the nonconvex objectives possess favorable global geometric structures, where each local minimizer approximates a target solution and all saddle points display negative curvature. This characteristic facilitates the creation of globally convergent optimization methods using straightforward initialization strategies. We further demonstrate that the benign geometric structures—proximity of local minimizers to target solutions and negative curvature at saddle points—exist either throughout the entire space or within a sufficiently large region, guaranteeing that local search algorithms, such as Riemannian gradient descent, initialized simply, can effectively approximate the target solutions. The validity of our theoretical findings is supported by numerical experiments.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1208", "problem_id": "12080001", "content": "Controllable scene synthesis, which aims to generate 3D data adhering to specific abstract specifications designed for straightforward user interaction and detailed control, can be effectively achieved using scene graphs. Scene graphs, comprising objects (nodes) and their relationships (edges), facilitate semantic control over the generated content. Addressing the limitations of prior research that relies on synthetic data and object mesh retrieval, this paper introduces a novel end-to-end approach for directly generating shapes from a scene graph. Furthermore, the proposed model supports scene modification through the scene graph interface. By employing Graph Convolutional Networks (GCN), a variational Auto-Encoder is trained on object and edge categories, alongside 3D shapes and scene layouts, enabling the sampling of novel scenes and shapes.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1209", "problem_id": "12090001", "content": "Accurate identification of dependencies among multiple time series observations is essential for various applications, including anomaly detection, financial risk management, causal analysis, and demand forecasting. Nevertheless, existing methods are often hindered by the computational and numerical challenges associated with estimating high-dimensional and time-varying covariance matrices, typically limiting their capacity to handle a few hundred dimensions or relying on strong assumptions regarding inter-series dependencies. To address this limitation, we introduce a novel approach that integrates a recurrent neural network (RNN)-based time series model with a Gaussian copula process output model, characterized by a low-rank covariance structure, enabling the reduction of computational complexity and accommodation of non-Gaussian marginal distributions. By significantly decreasing the number of parameters, our method facilitates the modeling of time-varying correlations across thousands of time series, as demonstrated through substantial accuracy improvements over state-of-the-art baselines on several real-world datasets, and further supported by an ablation study examining the contributions of our model's components.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1210", "problem_id": "12100001", "content": "Navigating roads safely necessitates interaction with and anticipation of the actions of other road users. This paper introduces FIERY, a probabilistic model that forecasts future events from a bird's-eye perspective using monocular camera inputs. By predicting the segmentation and motion of dynamic objects, FIERY generates non-parametric trajectories that anticipate the future behaviour of road agents. Unlike traditional autonomous driving systems, our approach integrates perception, sensor fusion, and prediction to directly estimate bird's-eye-view predictions from RGB camera data, thereby eliminating the need for high-definition maps. Through end-to-end learning from camera data, FIERY effectively captures the uncertainty of future events and predicts multiple possible trajectories. As demonstrated by experiments on the NuScenes and Lyft datasets, our model surpasses existing prediction benchmarks, with the code and trained models available at https://github.com/wayveai/fiery.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1211", "problem_id": "12110001", "content": "The increasing prevalence of Antimicrobial Multidrug Resistance (AMR) in Intensive Care Unit (ICU) patients represents a significant worldwide challenge. This research investigates multivariate time series (MTS) data from 3476 patients admitted to the ICU of University Hospital of Fuenlabrada (Madrid) between 2004 and 2020, revealing that 18\\% of these patients developed AMR during their ICU stay. The primary objective is to predict early AMR development by employing the time-series cluster kernel (TCK) to discern similarities within the MTS data. The TCK's performance as a kernel is assessed through dimensionality reduction techniques for visualization and classification. Empirical findings demonstrate that TCK effectively identifies patients who acquire AMR within the initial 48 hours of ICU admission and exhibits strong classification performance.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1212", "problem_id": "12120001", "content": "Neural architecture search (NAS) allows for the automated exploration of diverse design spaces to enhance neural network efficiency, which is particularly crucial for on-device applications where accuracy gains must be weighed against computational costs. However, evaluating model performance metrics is often resource-intensive. Prior approaches rely on proxies, such as operation counts or layer-wise assessments, to approximate hardware performance, but their inaccuracies reduce NAS effectiveness. To overcome this limitation, we introduce BRP-NAS, a hardware-aware NAS method leveraging a precise performance predictor built on graph convolutional networks (GCN). Additionally, we analyze predictor accuracy across various metrics and demonstrate that incorporating binary model relations and an iterative data selection strategy enhances sample efficiency. Our approach surpasses existing methods on NAS-Bench-101 and NAS-Bench-201, with the predictor effectively extracting meaningful features from the DARTS search space, outperforming second-order baselines. To highlight the challenges in latency estimation, we also release LatBench, a dataset containing latency measurements for NAS-Bench-201 models across multiple devices.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1213", "problem_id": "12130001", "content": "Recent advancements in neural architecture search (NAS) have explored diverse strategies, ranging from computationally intensive discrete sampling methods to more efficient differentiable techniques, though the latter often restrict the search space. An intermediate solution involves optimizing architectures through learned embedding spaces, such as those generated by graph neural network-based variational autoencoders, combining benefits from both approaches. These methods have demonstrated strong results across multiple benchmarks, yet their reliability and accuracy hinge on their ability to faithfully reconstruct networks from embeddings. This paper introduces a two-sided variational graph autoencoder that efficiently encodes and precisely reconstructs neural architectures from different search spaces. Our evaluation covers architectures from ENAS, NAS-Bench-101, and NAS-Bench-201, demonstrating that the smooth embedding space enables performance prediction for architectures beyond the training domain (e.g., those with additional operations). Consequently, this approach eliminates the need for costly Bayesian optimization or reinforcement learning while still identifying high-performing network designs.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1214", "problem_id": "12140001", "content": "This paper presents a variational approach for multi-view shape-from-shading using natural lighting conditions. The core concept involves linking PDE-based solutions for single-image shape-from-shading across multiple images and color channels through a variational framework. Instead of sequentially addressing individual SFS problems and enforcing inter-image consistency—an approach known to yield inferior outcomes—we develop an efficient ADMM-based solution for the coupled problem. Extensive testing on both synthetic and real-world images shows that combining multi-view reconstruction with shape-from-shading produces precise dense reconstructions without requiring dense correspondence computation. The variational integration across multiple views enables shape-from-shading methods to tackle complex real-world reconstruction tasks, generating intricate geometric details even in regions with gradual brightness changes and minimal texture.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1215", "problem_id": "12150001", "content": "A multitude of compression schemes have been devised to address the issue of Deep Neural Networks' (DNNs) excessive size, including the teacher-student approach, which involves transferring knowledge from a complex teacher network to a simpler student network. This paper introduces a novel methodology, termed the teacher-class network, wherein a single teacher network conveys knowledge to multiple student networks, collectively referred to as a class of students. Unlike traditional methods, where knowledge is transferred to a single student for problem-specific logits, the proposed approach enables each student to learn a subset of the teacher's knowledge, represented as a dense representation, allowing the collective knowledge of the student class to be applied to various problems. The student networks are designed to operate within a specified budget, with their combined parameters comparable to, or fewer than, those of a single student in the conventional teacher-student framework, as demonstrated by maintaining the collective parameters of all students less than or equivalent to that of a single student in the teacher-student approach. Furthermore, these compact student networks can be trained independently, facilitating deployment on devices with limited memory and parallel processing systems, such as data centers. The efficacy of the proposed teacher-class architecture is evaluated on several benchmark datasets, including MNIST, FashionMNIST, IMDB Movie Reviews, and CAMVid, across multiple tasks, including classification, sentiment classification, and segmentation, with results indicating that the approach surpasses the state-of-the-art single student method in terms of accuracy and computational cost, often achieving accuracy comparable to the teacher network with 10-30 times fewer parameters.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1216", "problem_id": "12160001", "content": "Model predictive control (MPC) offers robust control for robotic systems, notably autonomous aerial vehicles like quadcopters. Nevertheless, MPC's computational intensity and reliance on system state estimation pose challenges, especially in intricate, unstructured settings. Reinforcement learning offers a potential alternative by learning policies that directly map sensor data to actions, bypassing explicit state estimation. However, its application to unstable systems is problematic due to potential catastrophic failures during the policy learning phase. Our approach integrates MPC with reinforcement learning within a guided policy search framework. MPC generates training data using complete state observations from an instrumented environment. This data trains a deep neural network policy, which uses only raw sensor data from the vehicle. Post-training, this policy effectively controls the robot without full state knowledge and with reduced computational demands compared to MPC. We validate our method by training a simulated quadrotor to avoid obstacles, employing simulated onboard sensors and eliminating explicit state estimation during testing.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1217", "problem_id": "12170001", "content": "Deep learning has emerged as a prominent classification platform, garnering significant attention from researchers and achieving successful applications across various domains. However, in fields such as bioinformatics and robotics, the construction of large-scale, well-annotated datasets is often hindered by the high costs associated with data acquisition and annotation, thereby limiting the development of deep learning techniques. To address the issue of insufficient training data, transfer learning offers a viable solution by relaxing the assumption that training and test data must be independent and identically distributed (i.i.d.), thus motivating its application in this context. This survey provides a comprehensive review of current research in transfer learning using deep neural networks and their applications, with a focus on defining deep transfer learning, categorizing its techniques, and examining recent research endeavors that utilize these methods, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1218", "problem_id": "12180001", "content": "This paper addresses the challenge of efficiently exploring unknown environments, a critical issue in artificial intelligence. We introduce a \"learning to explore\" paradigm, training a policy across a range of environments. During evaluation, the trained policy is applied to a new, unseen environment drawn from the same distribution, with the objective of maximizing the number of unique states visited within a fixed number of steps. We specifically investigate graph-structured state-spaces, prevalent in applications such as software testing and map construction. This exploration task is framed as a reinforcement learning problem, where the agent receives rewards for discovering previously unvisited states, and a graph-structured memory is used to record the agent's path. Empirical evaluations show that our method excels at exploring spatial maps and, when applied to complex coverage-guided software testing of specialized programs and real mobile applications, surpasses the performance of manually designed, expert-level techniques.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1219", "problem_id": "12190001", "content": "Imaging systems in practical settings are susceptible to noise, optical distortions, and other flaws, which complicate image processing for both human interpretation and advanced analytical tasks. Traditional cameras typically isolate image acquisition from subsequent processing steps. This conventional approach processes raw sensor data through a sequential pipeline including demosaicking, denoising, deblurring, tone-mapping, and compression, with the goal of producing visually appealing images. Conversely, higher-level processing encompasses feature extraction, classification, tracking, and data fusion. This segregated design, while facilitating efficient development, results in isolated performance evaluations that do not consider the camera system's ultimate objective. For instance, current demosaicking and denoising algorithms are developed using perceptual image quality metrics without accounting for domain-specific tasks like object detection. We introduce a fully differentiable end-to-end architecture that simultaneously performs demosaicking, denoising, deblurring, tone-mapping, and classification. This architecture learns processing pipelines that deviate from those of existing Image Signal Processors (ISPs) optimized for perceptual quality, maintaining finer details even if it means allowing more noise and artifacts. Using both captured and simulated data, we show that our model significantly enhances perception in low-light and other difficult conditions, which is critical for real-world applications. Furthermore, we observed that the proposed model achieves state-of-the-art accuracy when optimized for image reconstruction in low-light scenarios, confirming the architecture's potential as a versatile network for reconstruction and analysis tasks beyond those specifically explored in this study.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1220", "problem_id": "12200001", "content": "The identification and characterization of patient blood samples are crucial for diagnosing blood-related diseases, making automated blood cell subtype detection and classification methods highly valuable in medical applications. Medical image processing and analysis provide robust tools for medical diagnosis. This study addresses white blood cell classification by analyzing the morphological features of their outer contours and color. We investigate various preprocessing and segmentation techniques, including color-based segmentation, morphological processing, and contouring, alongside feature extraction methods such as corner detection algorithms and Histograms of Gradients (HOG). Dimensionality reduction is performed using Principal Component Analysis (PCA). The resulting features are then used to classify white blood cells into Eosinophil, Lymphocyte, Monocyte, and Neutrophil categories using both unsupervised (k-nearest neighbors) and supervised (Support Vector Machine, Decision Trees, Linear Discriminant Analysis, Quadratic Discriminant Analysis, Naive Bayes) algorithms. Furthermore, we explore several Deep Convolutional Neural Network architectures (SqueezeNet, MobileNetV1, MobileNetV2, InceptionNet, etc.) with and without preprocessing/segmentation. Our aim is to identify a robust algorithm that exhibits minimal time complexity and resource requirements. The findings of this research can guide the selection of appropriate algorithms for automated blood cell classification based on specific application needs.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1221", "problem_id": "12210001", "content": "Classification models used in applications like fraud detection, credit risk assessment, and medical diagnosis must balance accuracy with interpretability. While linear approaches such as logistic regression provide a reasonable trade-off, they struggle with high-cardinality categorical predictors or non-linear data patterns. Common preprocessing techniques like weight-of-evidence transformation address these limitations, but their underlying binning processes often lack systematic research and rely on heuristic or expert-driven methods. This paper introduces a structured, data-driven approach that discretizes continuous variables using spline-based binning to capture non-linear relationships while maintaining interpretability through discrete predictors. Additionally, we enhance the weight-of-evidence method by incorporating shrinkage estimators for proportion estimation. These improvements enable better handling of non-linear and categorical predictors, boosting classification accuracy without sacrificing model interpretability or increasing overfitting risks. Experimental results in fraud detection demonstrate the approach's efficacy, with the dataset and implementation code made available to ensure reproducibility and adoption.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1222", "problem_id": "12220001", "content": "This paper presents PowerGym, an open-source reinforcement learning platform designed for Volt-Var control optimization in power distribution systems, adhering to the OpenAI Gym APIs. The primary objective of PowerGym is to reduce power loss and voltage violations while adhering to the physical constraints of networked systems. It includes four IEEE benchmark distribution systems - 13Bus, 34Bus, 123Bus, and 8500Node - along with modified designs that offer varying levels of control complexity. To facilitate broader applicability and customization, PowerGym provides a comprehensive guide, enabling users to adapt the environment to their specific distribution systems. The effectiveness of PowerGym is demonstrated through the implementation and evaluation of state-of-the-art reinforcement learning algorithms, as well as an analysis of controller performance, with the repository accessible at \\url.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1223", "problem_id": "12230001", "content": "Recently, semi-supervised learning (SSL) has achieved remarkable success in utilizing unlabeled data to enhance the performance of deep learning models, thereby significantly diminishing the need for extensive labeled datasets. Numerous SSL methods have been developed, demonstrating encouraging results on well-known benchmarks like ImageNet and CIFAR-10. Nevertheless, some existing techniques, particularly those based on data augmentation, have proven to be unsuitable for practical industrial applications. Consequently, this study introduces the pseudo-representation labeling, a straightforward and adaptable framework that employs pseudo-labeling techniques to iteratively annotate a limited volume of unlabeled data for use in training. Moreover, our framework incorporates self-supervised representation learning, allowing the classifier to benefit from the representation learning derived from both labeled and unlabeled datasets. This approach is versatile and can be integrated with various model structures, serving as a general method to enhance existing models. In comparison to current techniques, pseudo-representation labeling is more intuitive and effectively addresses real-world challenges. Empirical results indicate that it surpasses existing state-of-the-art semi-supervised learning approaches in industrial classification tasks, such as those encountered with the WM-811K wafer map and the MIT-BIH Arrhythmia dataset.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1224", "problem_id": "12240001", "content": "We investigate the application of Evolution Strategies (ES), a type of black box optimization algorithm, as an alternative to widely-used MDP-based reinforcement learning methods such as Q-learning and Policy Gradients. Our experiments conducted on MuJoCo and Atari demonstrate that ES serves as an effective solution approach that scales exceptionally well with the number of available CPUs: utilizing a novel communication method reliant on common random numbers, our ES implementation requires only scalar communication, enabling scalability to over a thousand parallel workers. This capability allows us to achieve 3D humanoid walking within 10 minutes and attain competitive performance on most Atari games after just one hour of training. Furthermore, we underscore several benefits of ES as a black box optimization method: it remains invariant to action frequency and delayed rewards, is resilient to very long horizons, and does not require temporal discounting or value function approximation.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1225", "problem_id": "12250001", "content": "For resource-constrained systems like robotics and embedded devices, extracting depth from stereo images must be both efficient and precise. Current convolutional neural network-based stereo matching techniques demand substantial GPU processing, making them challenging to implement on embedded systems. This paper introduces MTStereo 2.0, a stereo matching technique designed for systems with limited resources that need accurate and efficient depth estimation. It uses a Max-tree hierarchical representation of stereo image pairs to find matching regions along scan-lines. The approach incorporates a cost function that assesses the similarity of regional contextual data using Max-trees, along with a cost aggregation strategy that preserves disparity borders. MTStereo 2.0 builds upon MTStereo 1.0 by a) employing a more resilient cost function, b) enhancing the detection of erroneous matches, and c) calculating disparity maps with pixel-level accuracy rather than node-level precision. MTStereo offers precise sparse and semi-dense depth estimation without the intensive GPU requirements of CNN-based methods, making it suitable for low-power embedded and robotic devices. The proposed method was evaluated on benchmark datasets including KITTI 2015, Driving, FlyingThings3D, Middlebury 2014, Monkaa, and TrimBot2020, demonstrating competitive accuracy and efficiency. The code is available at https://github.com/rbrandt1/MaxTreeS.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1226", "problem_id": "12260001", "content": "This work tackles the challenge of generating a high-resolution image from a set of lower-resolution images taken from slightly varying spatial and temporal perspectives. The main obstacles to overcome in this endeavor are achieving precise alignment of the input images, processing raw and noisy data to maintain fidelity to the original camera captures, and developing a suitable image prior that can effectively regularize the reconstruction process. By leveraging the concept introduced by Wronski et al. that aliasing can be beneficial in this context, we propose a hybrid approach that integrates learnable parameters with the interpretability of traditional inverse problem-solving methods. Our method's efficacy is validated through experiments on both synthetic and real-world image bursts, surpassing existing state-of-the-art results on several benchmarks and yielding outstanding qualitative outcomes on real raw image bursts captured using smartphones and prosumer cameras.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1227", "problem_id": "12270001", "content": "This study introduces an approach for unsupervised video object segmentation by leveraging knowledge from image-based instance embedding networks. These networks generate embedding vectors for individual pixels, facilitating the grouping of pixels that belong to the same object. Despite being trained on static images, the embeddings remain consistent across successive video frames, enabling temporal object association. By integrating these embeddings with objectness and optical flow features—without requiring model retraining or online adjustments—the method achieves superior performance compared to existing unsupervised segmentation techniques on the DAVIS and FBMS datasets.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1228", "problem_id": "12280001", "content": "To achieve significant acceleration in point-feature embeddings during testing, we introduce a novel approach employing dual multi-layer perceptrons (MLPs) and a lookup table (LUT) to convert point-coordinate inputs into high-dimensional features. Unlike PointNet's MLP-based feature embedding, which involves millions of dot products, our method eliminates matrix-vector multiplications at test time, instead retrieving nearest entries from a precomputed MLP stored in a LUT and performing interpolation on an irregularly structured 3D lattice. This framework, termed LUTI-MLP (LUT Interpolation MLP), enables end-to-end training of an irregularly tabulated MLP integrated with a LUT without requiring approximations during inference. Additionally, LUTI-MLP substantially accelerates Jacobian computation for the embedding function relative to global pose coordinates on Lie algebra \\(\\mathfrak{3}\\), beneficial for point-set registration tasks. Extensive experiments on ModelNet40 demonstrate that LUTI-MLP, even with a compact (e.g., \\(4^3\\)) lattice, matches MLP performance while delivering remarkable speedups: \\(100\\times\\) for embeddings, \\(12\\times\\) for approximate Jacobians, and \\(860\\times\\) for canonical Jacobians.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1229", "problem_id": "12290001", "content": "Current self-supervised approaches for video representation primarily emphasize modeling temporal characteristics, yet the distinction between stationary and non-stationary attributes remains understudied. Stationary features, consistent across frames, facilitate video-level action classification, whereas non-stationary features, which vary over time, are more suitable for fine-grained temporal tasks like action segmentation. We contend that a unified representation for both feature types is ineffective and instead introduce a contrastive learning framework that separates the representation space into stationary and non-stationary components by leveraging long and short video segments. Stationary features are preserved across both segment lengths, while non-stationary features are derived by aggregating short segments to align with their corresponding long sequences. Experimental validation shows that stationary features excel in action recognition, while non-stationary features enhance action segmentation performance. Additionally, our analysis reveals that stationary features encode stable, time-invariant attributes, whereas non-stationary features capture dynamic, temporally evolving aspects.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1230", "problem_id": "12300001", "content": "Document layout analysis (DLA) involves segmenting a document image into distinct regions, serving as a critical component in systems for document comprehension and data extraction. Developing techniques that require minimal training data while maintaining effectiveness can advance DLA progress. We introduce a Human-in-the-loop (HITL) collaborative intelligence framework for DLA, motivated by the idea that HITL enhances model learning by incorporating limited data informed by domain expertise. While HITL typically identifies key samples using confidence metrics, this approach proves inadequate for DLA tasks. To address this, we present the Key Samples Selection (KSS) method, which improves sample selection accuracy in high-level tasks like semantic segmentation through agent collaboration, thereby lowering operational expenses. Selected samples undergo human annotation before being used to refine the model. Drawing inspiration from reinforcement learning, we redesign the learning system with a sample-based agent update strategy, enhancing the agent’s adaptability to new data. This approach yields notable performance gains on two benchmarks (DSSE-200 (from 77.1% to 86.3%) and CS-150 (from 88.0% to 95.6%)) while utilizing only 10% of labeled data.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1231", "problem_id": "12310001", "content": "of strong baseline methods. The Transformer's attention mechanism faces scalability challenges for long sequences due to its quadratic computational and memory requirements. This paper introduces Luna, a linear unified nested attention approach that approximates softmax attention through two nested linear attention functions, achieving linear time and space complexity. Luna first compresses the input sequence into a fixed-length representation using the initial attention function, then reconstructs it with the second function. Unlike conventional attention mechanisms, Luna incorporates an additional fixed-length sequence as both input and output, enabling linear attention operations while preserving sufficient contextual details. Comprehensive experiments on long-context sequence modeling, neural machine translation, and large-scale pretraining with masked language modeling demonstrate Luna's competitive or superior performance against multiple strong baselines, confirming its effectiveness and efficiency.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1232", "problem_id": "12320001", "content": "Spatio-temporal data, comprising multiple time series from various spatial locations, are pervasive in dynamic systems, such as air quality monitoring, and pose significant forecasting challenges due to complex spatial, temporal, and causal factors. To address this, we introduce HyperST-Net, a general framework leveraging hypernetworks for deep spatio-temporal models, which comprises three primary components: spatial, temporal, and deduction modules. The deduction module generates parameter weights for the temporal module based on spatial characteristics extracted by the spatial module. We also propose a generalized HyperST layer and adapt it for basic neural network layers, including HyperST-Dense and HyperST-Conv. Experimental results on three real-world tasks show that models incorporating our framework exhibit substantial predictive improvements, surpassing state-of-the-art baselines, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1233", "problem_id": "12330001", "content": "Domain Randomization (DR) typically necessitates extensive training data to achieve satisfactory results. This requirement stems from DR's methodology of producing randomized data via a uniform distribution across simulation parameters, which often leads to the creation of uninformative samples. In this study, we present a theoretical analysis of DR utilizing concepts from multi-source domain adaptation. Leveraging our analysis, we introduce Adversarial Domain Randomization (ADR) as an optimized version of DR that generates adversarial samples relevant to the learner during training. ADR is implemented as a policy that operates within the quantized simulation parameter space. During each iteration, the policy's action yields labeled data, and the reward is defined as the negative of the learner's loss on this data. Consequently, ADR frequently produces previously unseen samples for the learner, such as truncated and occluded objects for object detection and challenging classes for image classification. Evaluations conducted on datasets including CLEVR, Syn2Real, and VIRAT for various tasks demonstrate that ADR achieves superior performance compared to DR by requiring fewer data samples.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1234", "problem_id": "12340001", "content": "Dynamic dispatching seeks to efficiently assign appropriate resources to optimal locations at optimal times. It represents a fundamental challenge for operational efficiency in the mining sector. While deep reinforcement learning (RL) theoretically presents an ideal approach to tackling this issue, the industry frequently depends on heuristics or human judgment, which tend to be short-term and less optimal. This paper examines the primary obstacles associated with applying deep RL to the dynamic dispatching challenge within the mining industry.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1235", "problem_id": "12350001", "content": "Current dehazing techniques typically rely on hand-engineered features or CNN-based approaches with pixel-wise MSE loss to produce visually enhanced images. While these methods improve image quality, they do not consistently enhance performance in high-level vision applications like image classification. This study explores an alternative approach by prioritizing not only pixel-based metrics such as PSNR but also ensuring that dehazed images maintain or improve classification accuracy. We introduce an integrated CNN framework comprising three components: a dehazing sub-network (DNet), a classification-driven conditional GAN (CCGAN), and a classification sub-network (CNet) tailored for image recognition, achieving superior results in both visual quality and classification tasks. Extensive evaluations on the CUB-200-2011 and Caltech-256 datasets confirm that our method surpasses existing state-of-the-art dehazing techniques in both dehazing metrics and classification performance.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1236", "problem_id": "12360001", "content": "Eddy current testing (ECT) serves as a reliable method for assessing the depth of surface defects in metals, yet current practices heavily depend on operator expertise and manual inspection. This study tackles the automation of defect depth assessment by leveraging cutting-edge deep learning (DL) approaches. The work presents three key contributions: first, the development of a compact, high-performance ECT device utilizing a Zynq-7020 system-on-chip for rapid data collection and in-phase/quadrature demodulation; second, the creation and public release of the MDDECT dataset, comprising 48,000 scans from 18 defects with varying depths and lift-offs; and third, the framing of depth evaluation as a time series classification task, with multiple advanced 1D residual convolutional neural networks tested on the MDDECT dataset. A 38-layer 1D ResNeXt model achieves 93.58% accuracy in classifying defects in stainless steel sheets, with depths ranging from 0.3 mm to 2.0 mm at 0.1 mm intervals. Furthermore, the trained ResNeXt1D-38 model demonstrates robustness against lift-off interference.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1237", "problem_id": "12370001", "content": "The integration of modular subsystems is increasingly being utilized to facilitate sensing, reasoning, and decision-making processes in critical domains such as transportation, healthcare, and industrial automation, where timely and accurate performance is paramount. To optimize the overall efficacy of computing systems, this work explores the application of reinforcement learning to dynamically configure the constellation of interacting modules that comprise these systems. However, system-wide optimization poses a complex combinatorial problem, as localized enhancements to individual modules can have far-reaching, detrimental effects on the overall system performance due to the resultant shifts in input distributions to subsequent modules. To address this challenge, we introduce metareasoning approaches that leverage a nuanced representation of inputs, continuously monitor the system's state, and adaptively adjust module configurations in real-time to maximize operational utility. Our results demonstrate substantial improvements in the performance of both real-world and synthetic pipelines using various reinforcement learning techniques, as shown in Figure A, B, C (References [1], [2], and [3] provide further details).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1238", "problem_id": "12380001", "content": "A new attention gate model is introduced for medical imaging applications, capable of automatically identifying and focusing on target structures with diverse shapes and sizes. By incorporating attention gates into convolutional neural networks, the model can inherently distinguish between relevant and irrelevant image regions, emphasizing key features pertinent to the task at hand and thereby eliminating the need for separate tissue or organ localization modules. The proposed Attention U-Net architecture, which seamlessly integrates attention gates into the standard U-Net framework with negligible computational overhead, demonstrates enhanced sensitivity and predictive accuracy. Evaluation of this architecture on two extensive CT abdominal datasets for multi-class image segmentation reveals consistent improvements in predictive performance across various datasets and training sizes, while maintaining computational efficiency, as shown in Figure A, B, C (References [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1239", "problem_id": "12390001", "content": "We introduce the Cosmology and Astrophysics with MachinE Learning Simulations (CAMELS) Multifield Dataset (CMD), a comprehensive compilation of hundreds of thousands of 2D maps and 3D grids capturing diverse properties of cosmic gas, dark matter, and stars across 2,000 unique simulated universes at multiple cosmic epochs. These maps and grids, spanning regions of approximately 100 million light years, were derived from thousands of advanced hydrodynamic and gravity-only N-body simulations within the CAMELS project. Tailored for machine learning applications, CMD stands as the largest dataset of its kind, exceeding 70 Terabytes in size. This paper provides a detailed description of CMD and highlights several potential uses, with particular emphasis on parameter inference, framing the associated challenges as an open invitation to the research community. All data and additional technical specifications are available at https://camels-multifield-dataset.readthedocs.io.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1240", "problem_id": "12400001", "content": "Graphs have become essential in signal processing and machine learning, serving as a means to represent relationships between entities. They are applied in diverse applications, including vertex clustering, semi-supervised vertex classification, supervised classification of graph signals, and signal denoising. However, graphs are often not directly accessible and must be inferred from data. Validating these inferred graphs is inherently challenging, as performance depends on the intended downstream task, making algorithm comparisons difficult. To address this, we present publicly available, user-friendly benchmarks designed to evaluate the strengths and weaknesses of graph inference techniques. Additionally, we compare several leading methods from the literature.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1241", "problem_id": "12410001", "content": "Entropy regularization is a frequently employed technique in reinforcement learning to refine policy optimization, purportedly by promoting the adoption of more random policies. This study scrutinizes this assertion through novel visualizations of the optimization terrain, achieved via random perturbations of the loss function. Initially, it is demonstrated that policy optimization poses challenges, even with precise gradient information, due to the objective function's inherent geometry. Subsequently, it is qualitatively illustrated that, within certain environments, elevated policy entropy can lead to a smoother optimization landscape, thus bridging local optima and facilitating the implementation of increased learning rates. This paper introduces innovative instruments for interpreting the optimization landscape, validates that policy entropy acts as a regularizer, and emphasizes the complexity of formulating universally applicable policy optimization algorithms.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1242", "problem_id": "12420001", "content": "Existing Domain Adaptation (DA) models often rely on category-relevant losses constructed from hard or soft labels. However, hard labels tend to be overconfident due to challenging samples, while soft labels can be ambiguous due to numerous small, noisy probabilities, both potentially leading to negative transfer. Furthermore, category-irrelevant losses effective in Closed-Set DA (CSDA) are not applicable in Open-Set DA (OSDA) and need to be category-relevant due to the division of target data into shared and private classes. To address these limitations, we introduce a unified DA framework called Importance Filtered Cross-Domain Adaptation (IFCDA). This framework employs an importance filtered mechanism to generate refined soft labels, thereby reducing negative transfer. Specifically, soft labels are categorized into confident and ambiguous labels; confident labels retain only the maximum probability, while ambiguous labels are truncated using a threshold to preserve only prominent probabilities. Additionally, a generalized graph-based label propagation method is designed to obtain soft labels in both CSDA and OSDA, incorporating an extra component in the label vector for novel class detection in the target domain. The category-relevant losses in both scenarios are then reformulated using these filtered soft labels, and the category-irrelevant MMD loss in CSDA is transformed into a class-wise MMD form utilizing the newly designed importance filtered soft labels. Notably, setting the extra components to 0 makes CSDA a special case, enabling the approach to function effectively in both CSDA and OSDA. Extensive experiments on standard cross-domain object recognition datasets demonstrate that our proposed method surpasses several state-of-the-art techniques in both scenarios.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1243", "problem_id": "12430001", "content": "This paper introduces DistSTN, a new framework designed for automatic target recognition (ATR) in synthetic aperture radar (SAR) imagery. Unlike traditional SAR ATR methods, DistSTN tackles the more complex real-world situation of non-cooperative targets where training data has incomplete and restricted aspect angle coverage, while testing data has unrestricted aspect angles. Rather than learning pose-invariant features, DistSTN employs a sophisticated feature disentangling model to isolate pose-related factors from identity-specific features within SAR target representations, allowing for independent manipulation of the target image representation. A pose discrepancy spatial transformer module is integrated into DistSTN to disentangle these pose factors, explicitly modeling the geometric transformation between factors of different targets. Furthermore, DistSTN incorporates an amortized inference approach, utilizing an encoder-decoder structure for efficient feature extraction and recognition. Experiments conducted using the moving and stationary target acquisition and recognition (MSTAR) dataset validate the efficacy of the proposed methodology, demonstrating that DistSTN attains superior recognition accuracy compared to existing ATR algorithms.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1244", "problem_id": "12440001", "content": "Perception systems for Autonomous Driving are currently flourishing, largely driven by progress in Deep Learning. However, most approaches predominantly utilize the rich semantic data from RGB images, while Deep Learning applications for other common autonomous vehicle sensors, such as lidars or radars, remain underexplored. This paper introduces an innovative method to analyze the motion dynamics of surrounding vehicles using only lidar data. A key challenge lies in distinguishing the ego-motion of the sensing vehicle from the movement of other observed vehicles. To address this, we design a CNN architecture that processes pairs of consecutive lidar scans during inference. During training, we incorporate auxiliary pretext tasks that utilize image data, including semantic vehicle detection and a novel lidar-flow feature integrating traditional image-based optical flow with lidar measurements. Our results demonstrate significant promise, showing that incorporating distilled image knowledge during training enhances the network's performance during inference, even without image data at test time.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1245", "problem_id": "12450001", "content": "Graphical models are a potent means of identifying complex patterns in multivariate data, and are widely utilized in both Bayesian statistics and machine learning. This paper presents the R package BDgraph, which enables Bayesian structure learning for general undirected graphical models, encompassing both decomposable and non-decomposable models with continuous, discrete, and mixed variables. By incorporating recent advancements in the Bayesian literature, such as those proposed by Mohammadi and Wit (2015) and Dobra and Mohammadi (2018), the package offers efficient implementation of these methods. To enhance computational efficiency, the package leverages C++ for intensive tasks and is interfaced with R, while also supporting parallel computing capabilities. Additionally, the package includes a range of functions for simulation, visualization, and multivariate datasets from the literature, which demonstrate its capabilities. The paper provides a concise overview of the statistical methodologies implemented in the package, followed by a detailed explanation of its usage, and culminates in illustrative examples using both real and artificial datasets to showcase the package's functionality.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1246", "problem_id": "12460001", "content": "In this research, we investigate the development of a two-stage framework that allows users to directly control high-level features of a natural scene. Central to our method is a deep generative network capable of creating images that depict a scene as though captured during different seasons (e.g. winter), weather conditions (e.g. a cloudy day), or times of day (e.g. sunset). After generating the scene with the specified attributes, the resulting appearance is applied to the original image while maintaining the integrity of the semantic details, resulting in a photorealistic manipulation outcome. Since our framework generates an imagined version of the scene, it does not depend on any reference style image, which is a common requirement in many appearance or style transfer techniques. Additionally, it can concurrently adjust a scene based on a varied range of transient attributes using a single model, thereby removing the necessity of training multiple networks for each translation task. Our extensive set of qualitative and quantitative findings illustrates the superiority of our approach in comparison to existing methods.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1247", "problem_id": "12470001", "content": "We introduce an innovative approach for simultaneously learning a 3D face parametric model and performing 3D face reconstruction using multiple data sources. Conventional techniques typically train 3D face models using a single type of input, such as scanned data or unconstrained images. While 3D scans provide precise facial geometry, they require costly capture systems and often have limited subject diversity. Conversely, in-the-wild images are abundant and easily accessible but lack explicit geometric details. Our proposed method integrates these diverse sources, including scanned data, facial images, and a substantial collection of iPhone X-captured RGB-D images, which help bridge the disparity between the two primary sources. Experiments show that incorporating varied training data enhances the model's robustness and performance.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1248", "problem_id": "12480001", "content": "To ensure efficient software development and dependable user experiences, effective software testing is crucial. Due to the limitations of human testing regarding efficiency and cost, automated software testing methodologies are necessary. This paper introduces DRIFT, a Reinforcement Learning (RL) framework designed for functional software testing. DRIFT utilizes the symbolic representation of the user interface and employs Q-learning via Batch-RL, modeling the state-action value function using a Graph Neural Network. We evaluate DRIFT by testing the Windows 10 operating system, demonstrating its capacity to consistently activate targeted software functionalities in a fully automated fashion. Our experiments assess the ability to execute individual and combined tasks across various applications, thus proving the capacity of our framework to efficiently test software across a broad spectrum of testing objectives.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1249", "problem_id": "12490001", "content": "Transformers have recently gained significant attention not only in natural language processing but also in computer vision applications. This study explores the potential of Transformers for face recognition and compares their effectiveness with conventional CNNs. To address potential limitations of the original Transformer architecture, which may overlook inter-patch relationships, we enhance the patch generation method by implementing overlapping sliding patches for token creation. Our experiments utilize the CASIA-WebFace and MS-Celeb-1M datasets for training, with evaluation conducted across multiple standard benchmarks such as LFW, SLLFW, CALFW, CPLFW, TALFW, CFP-FP, AGEDB, and IJB-C. Results indicate that Face Transformer models trained on the large-scale MS-Celeb-1M dataset perform comparably to CNNs with equivalent parameter counts and computational complexity (MACs). For research transparency and advancement, we have made our Face Transformer models and implementation code publicly accessible at https://github.com/zhongyy/Face-Transformer.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1250", "problem_id": "12500001", "content": "Recent advancements in style transfer models have shown impressive artistic outcomes, yet current approaches struggle with spatial distortions and unnatural artifacts when using photographs as style references. We present a theoretically grounded modification to the network architecture that significantly improves photorealism and ensures accurate style transfer. Our method leverages wavelet transforms, which integrate seamlessly with deep networks, to develop a wavelet-corrected transfer approach based on whitening and coloring transforms (WCT^2). This technique maintains structural details and the statistical characteristics of VGG feature space during stylization. To our knowledge, this is the first end-to-end framework capable of processing a 1024×1024 resolution image in 4.7 seconds while delivering photorealistic results without post-processing. Additionally, our model enables stable video stylization without temporal constraints. The code, generated images, and pre-trained models are accessible at https://github.com/ClovaAI/WCT2.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1251", "problem_id": "12510001", "content": "Transfer learning utilizing pre-trained Convolutional Neural Networks (CNNs) has proven effective for various image classification tasks. This paper introduces a novel approach for recognizing pain expressions in neonates through transfer learning. We specifically utilize a pre-trained CNN originally designed for face recognition (VGG Face) and other CNNs pre-trained on different datasets for image classification (iVGG F, M, and S) to derive deep features from neonates' facial images. In the final phase, multiple supervised machine learning classifiers are employed to differentiate between facial expressions indicative of pain and those indicating no pain. The proposed method yielded an area under the curve (AUC) of 0.841 and an accuracy of 90.34 on a test dataset, representing an approximate 7-point increase over the accuracy achieved with traditional handcrafted features. Additionally, we suggest merging deep features with traditional features, positing that this combination would enhance pain classification performance. The integration of deep and traditional features resulted in an accuracy of 92.71 and an AUC of 0.948. These findings indicate that transfer learning serves as a more efficient alternative to training CNNs from scratch for extracting relevant features for pain expression recognition in neonates. Furthermore, they demonstrate that the fusion of deep and traditional handcrafted features can effectively enhance pain expression recognition performance, potentially benefiting similar applications.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1252", "problem_id": "12520001", "content": "Graph neural networks (GNNs) have proven effective across various structured data applications, including molecular property prediction and social network analysis. Inspired by their versatility, we introduce RankGNNs, a novel framework integrating neural Learning to Rank (LtR) techniques with GNNs. These models are trained using pairwise graph preferences, indicating that one graph is favored over another. This approach is particularly useful in drug discovery, where experts aim to identify the most viable molecules from extensive candidate pools. Our experimental results show that the pairwise RankGNN method either substantially surpasses or performs comparably to the conventional point-wise baseline, which addresses LtR through GNN-based graph regression.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1253", "problem_id": "12530001", "content": "We introduce and justify the feature selective anchor-free (FSAF) module, a straightforward and efficient component for single-shot object detectors. This module can be integrated into single-shot detectors utilizing a feature pyramid architecture. The FSAF module effectively addresses two shortcomings associated with traditional anchor-based detection: 1) heuristic-driven feature selection; and 2) overlap-dependent anchor sampling. The core idea of the FSAF module involves online feature selection linked to the training of multi-level anchor-free branches. In particular, each level of the feature pyramid is connected to an anchor-free branch, enabling box encoding and decoding without anchors at any level. During the training phase, we dynamically allocate each instance to the most appropriate feature level. At inference, the FSAF module can operate concurrently with anchor-based branches by generating predictions simultaneously. We demonstrate this concept through straightforward implementations of anchor-free branches and an online feature selection approach. Experimental evaluations on the COCO detection track indicate that our FSAF module surpasses anchor-based methods in performance while also being more efficient. When combined with anchor-based branches, the FSAF module significantly enhances the baseline RetinaNet across various configurations, incurring almost negligible additional inference overhead. Consequently, the optimized model achieves an impressive state-of-the-art mAP of 44.6%, surpassing all existing single-shot detectors on COCO.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1254", "problem_id": "12540001", "content": "Fields like manufacturing and healthcare require ongoing monitoring and analysis of their processes, particularly through the use of time series generated by sensors. Time series data can be utilized to clarify and forecast concept drifts during operation. Typically, a sufficient volume of data is necessary to produce valuable analysis outcomes. Nonetheless, dependable data sets are frequently absent, as may occur when event streams and time series data are gathered separately, during the initiation of a new process, or when acquiring adequate data volume is prohibitively expensive. There are also additional hurdles related to preparing time series data from various event sources, inconsistencies in data collection frequency, and concept drift. This paper introduces the GENLOG methodology, designed to create reliable event and time series data that aligns with the distribution of the foundational input data set. GENLOG utilizes data resampling techniques and allows users to select specific segments of the log data to manage the training of a recurrent neural network for data stream generation. The generated data is then resampled to match its original frequency and incorporated into the corresponding log data file. In summary, GENLOG has the potential to enhance small data sets and, in turn, facilitate the practice of online process mining.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1255", "problem_id": "12550001", "content": "Local approximation techniques are widely used to enhance the scalability of Gaussian processes (GPs) for large datasets. These methods lower computational complexity by partitioning the data into subsets and training individual local experts on each. Predictions from these experts are then combined, either under the assumption of conditional independence (CI) or dependence among them. While CI-based aggregation is computationally efficient, it often results in unreliable uncertainty estimates. Conversely, modeling dependent experts yields accurate predictions and better uncertainty quantification but incurs prohibitively high computational overhead. To address this, we introduce a theory-driven expert selection mechanism that discards less influential experts, significantly reducing the computational burden of dependent expert aggregation while maintaining well-calibrated uncertainty estimates. Our approach employs methods from undirected graphical models, utilizing sparse precision matrices to encode conditional dependencies among experts, thereby identifying the most relevant ones. Additionally, we...", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1256", "problem_id": "12560001", "content": "Recent advancements in building change detection through deep learning have shown significant progress, yet challenges persist with insufficiently discriminative features leading to fragmented regions and uneven boundaries. To address this, we present a dual task constrained deep Siamese convolutional network (DTCDSCN), incorporating three sub-networks: a change detection network and two semantic segmentation networks. This architecture simultaneously performs change detection and semantic segmentation, enhancing object-level feature discrimination and producing more coherent change maps. Additionally, a dual attention module (DAM) is introduced to leverage channel and spatial dependencies, refining feature representation. The focal loss function is also optimized to mitigate sample imbalance. Evaluations on the WHU building dataset demonstrate the method's effectiveness, achieving state-of-the-art results in precision, recall, F1-score, and intersection over union.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1257", "problem_id": "12570001", "content": "Computer vision plays a critical role in enabling autonomous navigation for diverse robotic systems, including self-driving vehicles, military robots, and space exploration rovers. However, current methodologies predominantly rely on analyzing images acquired within the visible light spectrum. This paper addresses the segmentation of off-road environments using infrared imagery, employing salient features to overcome challenges posed by variations in scale, brightness, and viewing angle. Specifically, Speeded-Up Robust Features (SURF) are proposed as a foundational element, justified by several factors detailed in the paper, and two SURF implementations are compared. SURF features are extracted from images representing varied terrain types, and a terrain class membership function is estimated for each feature using either a multi-layer perceptron or nearest neighbors approach. These membership values, along with feature spatial positions, are then used to estimate class membership values for all image pixels. To mitigate segmentation flickering caused by abrupt transitions between terrain types and to accelerate the segmentation process, camera position tracking and feature position prediction are implemented. A comparative analysis of the multi-layer perceptron and nearest neighbor classifiers is presented. The terrain segmentation error rate achieved with nearest neighbors on the testing set is 16.6+-9.17%.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1258", "problem_id": "12580001", "content": "The exceptional performance of generic object detection algorithms in recent years has not been fully replicated in underwater datasets, which pose unique challenges due to color shift, low contrast, and sediment-induced blurring. Furthermore, the tendency of underwater creatures to appear in close proximity to each other in images complicates detection tasks. To mitigate these issues, this study explores the development of augmentation policies that can effectively simulate overlapping, occluded, and blurred objects, with the goal of creating a more robust model. A novel augmentation technique, termed RoIMix, is proposed, which facilitates interactions among multiple images by combining proposals extracted from different images. Unlike conventional data augmentation methods that operate on individual images, RoIMix generates enhanced training samples by mixing proposals from multiple images, leading to improved performance of region-based object detectors, as demonstrated by experiments on the Pascal VOC and URPC datasets.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1259", "problem_id": "12590001", "content": "We introduce a BlackBox specifically designed for applications in medical imaging. Traditional methods, such as saliency maps, fail to clarify how variations in specific anatomical regions relate to outcomes, which is vital for fostering transparency in healthcare decision-making. Our framework elucidates the outcome by gradually modifying the semantic impact of the associated outcome label. When a query is presented to a classifier, Generative Adversarial Networks generate a series of perturbations that progressively transition the posterior probability from the original class to its opposite. We formulate a loss function aimed at preserving crucial and potentially relevant elements, such as support devices, in the counterfactually generated images. We conduct a thorough evaluation across various classification tasks involving chest X-ray images. Our findings reveal that the counterfactually generated visual explanations align with clinically relevant measurements of the disease, both in quantitative and qualitative terms.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1260", "problem_id": "12600001", "content": "The accuracy of 3D Multi-object tracking (MOT) is essential for autonomous systems, and recent approaches have relied on a tracking-by-detection pipeline, which involves independent feature extraction for each object to construct an affinity matrix, followed by the Hungarian algorithm for data association. A critical component of this pipeline is learning distinctive features for individual objects to minimize confusion during data association. This work presents two novel techniques to enhance discriminative feature learning for MOT: (1) a feature interaction mechanism utilizing Graph Neural Networks, allowing the feature of one object to be informed by the features of other objects, thereby enabling features to converge towards similar objects (likely with the same ID) and diverge from dissimilar objects (likely with different IDs), resulting in more distinctive features; (2) a joint feature extractor that simultaneously learns appearance and motion features from both 2D and 3D spaces, leveraging complementary information from different modalities to produce more discriminative features. To prevent the joint feature extractor from relying heavily on a single modality, an ensemble training paradigm is also proposed. Extensive evaluation demonstrates that the proposed method achieves state-of-the-art performance on the KITTI and nuScenes 3D MOT benchmarks, as shown in Figure A, B, C (see References [citation] for details), and the code is available at https://github.com/xinshuoweng/GNN3DMOT.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1261", "problem_id": "12610001", "content": "This paper addresses the challenge of egocentric action anticipation, which involves forecasting the actions that a camera user is likely to undertake soon, as well as identifying the objects they will engage with. We introduce the Rolling-Unrolling LSTM, a learning framework designed to predict actions from egocentric video footage. This approach is structured around three key elements: 1) a dual LSTM architecture that separates the tasks of summarizing previous actions and predicting future ones, 2) a Sequence Completion Pre-Training strategy that enhances the LSTMs' focus on distinct sub-tasks, and 3) a Modality ATTention (MATT) mechanism that effectively integrates multi-modal predictions derived from RGB frames, optical flow fields, and object-based features. We validate our proposed method on the EPIC-Kitchens, EGTEA Gaze+, and ActivityNet datasets. Experimental results indicate that our architecture represents the state-of-the-art in the field of egocentric videos, attaining leading results in the 2019 EPIC-Kitchens egocentric action anticipation challenge. Additionally, our method demonstrates strong performance on ActivityNet compared to unsupervised pre-training approaches and shows versatility across early action recognition and action recognition tasks. To foster further exploration in this complex area, we have made our code, trained models, and pre-extracted features accessible at our website: http://iplab.dmi.unict.it/rulstm.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1262", "problem_id": "12620001", "content": "We introduce a new classifier network named STEP, designed to identify human emotions based on gait through a Spatial Temporal Graph Convolutional Network (ST-GCN) framework. Our approach leverages the distinctive characteristics of gait from an RGB video of a person walking, enabling the classification of their emotional state into one of four categories: happy, sad, angry, or neutral. To train our model, we utilize hundreds of annotated real-world gait videos and enhance the dataset with thousands of synthetic gait samples produced by an innovative generative network, STEP-Gen, which is built upon an ST-GCN based Conditional Variational Autoencoder (CVAE). Furthermore, we introduce a unique push-pull regularization loss within the CVAE framework of STEP-Gen to create realistic gaits, thereby improving the classification performance of STEP. Additionally, we present a new dataset named E-Gait, comprising 2,177 human gaits annotated with perceived emotions, alongside thousands of synthetic gaits. In application, STEP effectively learns affective features and achieves a classification accuracy of 89% on the E-Gait dataset, surpassing previous methods by 14-30% in accuracy.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1263", "problem_id": "12630001", "content": "Probabilistic graphical models are essential for various machine learning applications. A core challenge in statistical inference is computing the partition function, or normalizing constant, which is often computationally infeasible, thus motivating research into approximation techniques. Iterative variational methods are a widely used and effective class of such techniques; however, even the most advanced variational methods can yield unsatisfactory results or struggle to converge in complex scenarios. This work explores an alternative approach by calculating the partition function through sequential variable summation. By integrating concepts from mini-bucket elimination with tensor network and renormalization group techniques borrowed from statistical physics, we introduce reliable approximation algorithms. These \"convergence-free\" methods demonstrate promising empirical performance on both synthetic and real-world benchmark models, even in challenging cases.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1264", "problem_id": "12640001", "content": "This study addresses the challenge of predicting remaining useful life (RUL) of systems or machines using sensor data. Existing RUL estimation methods often depend on predefined degradation patterns, while real-world sensor data is typically noisy and contains missing values. We introduce Embed-RUL, a new approach that eliminates the need for degradation assumptions, handles noise effectively, and manages missing data. The method employs a sequence-to-sequence Recurrent Neural Network (RNN) model to create embeddings for multivariate time series segments, where embeddings from normal and degraded machines exhibit distinct patterns useful for RUL prediction. These embeddings effectively capture underlying time series trends while suppressing noise, ensuring proximity for machines with similar operational behavior despite varying noise levels in sensor readings. Evaluations on a public turbofan engine dataset and a proprietary real-world dataset confirm that Embed-RUL surpasses current state-of-the-art methods across multiple performance metrics.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1265", "problem_id": "12650001", "content": "One of the most significant yet difficult challenges faced by autonomous vehicles in city settings is forecasting the future actions of nearby pedestrians, particularly at crossing locations. This prediction relies on numerous social and environmental factors, especially the interactions among road users. To effectively capture such interactions, a comprehensive perspective of the scene and the movement of road users in three-dimensional space is necessary. However, this critical information is absent from the current benchmark datasets on pedestrian behavior. To address these issues, we propose 1) an innovative graph-based model aimed at predicting pedestrian crossing behavior. Our approach encapsulates the interactions of pedestrians with surrounding road users through clustering and the application of relative importance weighting based on features derived from a bird's-eye view. 2) We present a new dataset that includes 3D bounding boxes and annotations of pedestrian behaviors, enhancing the existing nuScenes dataset. Our method demonstrates state-of-the-art performance on this new data, showing improvements exceeding 15% across various metrics when compared to existing approaches. The dataset is accessible at https://github.com/huawei-noah/datasets/PePScenes.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1266", "problem_id": "12660001", "content": "We introduce an innovative approach for estimating multi-view depth from a single video, which is essential for numerous applications including perception, reconstruction, and robot navigation. While prior learning-based techniques have achieved impressive outcomes, most focus on estimating the depth of individual video frames in isolation, neglecting the robust geometric and temporal relationships between frames. Additionally, existing state-of-the-art (SOTA) models primarily utilize fully 3D convolution networks for cost regularization, resulting in high computational demands that hinder their implementation in practical scenarios. Our method provides temporally consistent depth estimation by employing a unique Epipolar Spatio-Temporal (EST) transformer to directly link geometric and temporal correlations across multiple depth maps. To lower the computational expenses, we propose a compact hybrid network, inspired by recent Mixture-of-Experts models, which includes a 2D context-aware network and a 3D matching network that independently learn 2D contextual information and 3D disparity cues. Extensive experiments indicate that our method surpasses SOTA techniques in both depth estimation accuracy and computational efficiency.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1267", "problem_id": "12670001", "content": "Although extensive object detection datasets like MS-COCO strive for precise ground truth bounding box definitions, labeling ambiguities persist. To address this, we introduce a new bounding box regression loss function designed to simultaneously learn bounding box transformations and localization variance. This loss function substantially enhances localization accuracy across different architectures with minimal computational overhead. Furthermore, the learned localization variance enables the merging of adjacent bounding boxes during non-maximum suppression (NMS), leading to additional improvements in localization performance. Empirical results on MS-COCO demonstrate a significant increase in Average Precision (AP) for VGG-16 Faster R-CNN, rising from 23.6% to 29.1%. Notably, for ResNet-50-FPN Mask R-CNN, our approach yields improvements of 1.8% and 6.2% in AP and AP90, respectively, surpassing the performance of existing state-of-the-art bounding box refinement techniques. Our code and models are available at: github.com/yihui-he/KL-Loss.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1268", "problem_id": "12680001", "content": "The paradigm of online learning with expert advice has gained widespread adoption in numerous machine learning applications, wherein a learner selects an expert to consult and subsequently makes a decision. Often, the experts are interconnected, allowing the learner to observe the losses incurred by a subset of experts related to the chosen one. This interplay among experts can be represented by a feedback graph, which can inform the learner's decision-making process. Nevertheless, in real-world scenarios, the nominal feedback graph is frequently beset by uncertainties, obscuring the true relationships between experts. This study investigates various scenarios of potential uncertainties and devises innovative online learning algorithms that can effectively utilize the uncertain feedback graph while mitigating its uncertainties. The proposed algorithms are shown to achieve sublinear regret under relatively mild conditions, as demonstrated through experiments conducted on real-world datasets, Figure A, B, C, (References [1], [2], [3]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1269", "problem_id": "12690001", "content": "This paper introduces an efficient algorithm for solving the generalized sparse coding (SC) inference problem. The proposed method is applicable to both single dictionary scenarios, where each data point is represented sparsely using a single dictionary, and multiple dictionary scenarios, such as those found in morphological component analysis (MCA), where a signal is decomposed into additive components, each sparsely represented in a distinct dictionary. Both the SC task and its MCA generalization are formulated as \\ell_1-regularized least-squares optimization problems. To expedite sparse code acquisition, a deep learning architecture is presented, representing a trainable, time-unfolded version of the Split Augmented Lagrangian Shrinkage Algorithm (SALSA), a specific instance of the Alternating Direction Method of Multipliers (ADMM). The algorithm's performance, denoted as LSALSA (learned-SALSA), is empirically validated on image vision tasks, demonstrating significant improvements in inference time, sparse code quality, and visual clarity for both classic SC and MCA problems. A theoretical analysis of the LSALSA network reveals that it accurately implements a truncated ADMM on a learned cost function with curvature modified by a learned parameterized matrix. By extending a recent Stochastic Alternating Optimization framework, it is shown that a gradient descent step on this learned loss landscape is equivalent to a modified gradient descent step on the original loss landscape, suggesting that LSALSA's acceleration arises from its ability to learn a gradient correction for steeper descent.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1270", "problem_id": "12700001", "content": "Graph kernels, a common technique for assessing graph similarity, are widely used in graph classification tasks. Despite this, kernel methods remain underdeveloped for node classification, a problem related to graph representation learning, with existing solutions often relying on heuristic approaches. This paper introduces a new theoretical kernel-based framework designed for node classification, aiming to connect these two graph representation learning challenges. Inspired by graph kernel techniques, our method is designed to learn node representations that effectively capture the structural properties of a graph. We provide theoretical evidence that our formulation achieves a level of performance comparable to any positive semidefinite kernel. To facilitate efficient kernel learning, we introduce a new node feature aggregation method and a data-driven similarity metric for use during training. Significantly, our framework offers flexibility and can enhance other graph-based deep learning models, such as Graph Convolutional Networks (GCNs). Empirical evaluations on standard node classification benchmarks demonstrate that our model achieves state-of-the-art results.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1271", "problem_id": "12710001", "content": "As machine learning models are increasingly deployed in critical applications within law enforcement, medicine, education, and employment, it is crucial to define their appropriate use cases and prevent their application in unsuitable contexts. To this end, we advocate for the release of models accompanied by documentation outlining their performance characteristics. This paper introduces \"model cards,\" a framework designed to promote transparent model reporting. Model cards are concise documents that accompany trained machine learning models, presenting benchmarked evaluations across diverse conditions, including cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) relevant to the intended application domains. These cards also specify the intended usage context, performance evaluation methodologies, and other pertinent information. While our primary focus is on human-centered machine learning models within computer vision and natural language processing, this framework is adaptable for documenting any trained machine learning model. To illustrate the concept, we provide model cards for two supervised models: one for detecting smiling faces in images and another for identifying toxic comments in text. We propose model cards as a move toward the responsible democratization of machine learning and AI, fostering greater transparency in AI technology's functionality. We anticipate that this work will inspire developers to accompany their trained machine learning models with comprehensive evaluation metrics and relevant documentation.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1272", "problem_id": "12720001", "content": "Assessing the performance of generative adversarial networks (GANs) presents significant difficulties. This study re-examines several established sample-based evaluation metrics for GANs, focusing on the challenge of evaluating the metrics themselves. The analysis begins by establishing essential criteria for metrics to yield informative scores, including the ability to differentiate between real and generated data, detect mode dropping and mode collapsing, and identify overfitting. Through a series of meticulously designed experiments, the research thoroughly investigates existing sample-based metrics, pinpointing their advantages and disadvantages in real-world scenarios. The findings indicate that kernel Maximum Mean Discrepancy (MMD) and the 1-Nearest-Neighbor (1-NN) two-sample test appear to fulfill most of the desired characteristics, assuming that sample distances are calculated within an appropriate feature space. Furthermore, the experiments reveal notable insights into the behavior of several widely used GAN models, such as their tendency to memorize training data and their proximity to learning the intended distribution.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1273", "problem_id": "12730001", "content": "Face anti-spoofing (FAS) is crucial for securing face recognition systems against presentation attacks. Recent FAS research has utilized domain generalization to enhance the ability to detect diverse and novel attacks. Prior work in this area has typically defined each domain as an anti-spoofing dataset, emphasizing the development of learning methodologies. In contrast, this paper introduces a method that allows the network to autonomously determine its domain using clustered convolutional feature statistics from intermediate layers, eliminating the need to predefine domains as datasets. Pseudo-domain labels are generated using both the network's feature extraction capabilities and depth estimators, which were previously employed only as auxiliary components in FAS. The proposed method's effectiveness is demonstrated through four sets of experiments, where the network is trained on three datasets and evaluated on the remaining dataset.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1274", "problem_id": "12740001", "content": "The vision-and-language navigation (VLN) task involves training an agent to follow natural language instructions to reach a goal location in unfamiliar, realistic environments. Two primary challenges exist in VLN: first, the agent must dynamically align relevant segments of the instruction with changing visual scenes; second, during training, the agent often mimics the shortest path to the target, leading to suboptimal performance due to differences in action selection between training and inference. By sampling actions from its predicted distribution during training, the agent can explore varied trajectories, improving success rates. However, without exposure to optimal paths, the agent may take unnecessarily long routes. To address these issues, we introduce a cross-modal grounding module with dual attention mechanisms to enhance alignment between language and visual inputs. Additionally, we propose alternating between imitation and exploration learning strategies to reduce the training-inference gap, further optimizing performance through adversarial learning. Experiments on the R2R dataset show that our approach generalizes well and complements existing methods, achieving competitive results in both effectiveness and efficiency compared to state-of-the-art techniques.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1275", "problem_id": "12750001", "content": "Detecting moving objects represents a fundamental challenge in computer vision. Event-based cameras, inspired by biological vision systems, offer advantages such as decreased latency, high dynamic range (HDR), reduced motion blur, and low power consumption compared to traditional frame-based cameras. Despite these benefits, event cameras are susceptible to noise and have limited resolution. Detecting moving objects using these sensors poses a challenge because they only record binary brightness changes, omitting essential visual cues like texture and color. This paper explores the use of k-means clustering for moving object detection in event-based data. Experimental evaluations on public datasets demonstrate that k-means achieves a notable performance gain compared to existing state-of-the-art approaches.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1276", "problem_id": "12760001", "content": "A recent strand of technical artificial intelligence research, known as Explainable AI, has sought to elucidate the opaque nature of complex models by providing insights into their decision-making processes. This study introduces an innovative approach to visually explaining deep learning networks, utilizing saliency maps to accurately identify and localize entire object regions. Unlike existing state-of-the-art methods, the proposed approach yields more intuitive and transparent visual explanations, thereby enhancing the trust of human experts in the model's outputs. The efficacy of the proposed method is validated through a comprehensive evaluation framework, incorporating both quantitative and qualitative assessments on diverse datasets, including general and clinical data sets, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1277", "problem_id": "12770001", "content": "Crowdsourcing is now a common technique in supervised learning when labeled training data is limited or hard to acquire. Existing crowdsourcing models typically assume labelers can answer complete questions, requiring them to distinguish between all possible classes in classification tasks, which can be challenging in practice. To address this, we introduce a comprehensive probabilistic model designed for simpler, binary (\"yes\" or \"no\") queries. Our model estimates the joint posterior distribution of labeler confusion matrices alongside the posterior probability of each object's class. We employ Monte Carlo Sampling and Black Box Variational Inference to develop an approximate inference method, deriving the necessary gradients. We evaluated our model using two realistic crowdsourcing scenarios: the classification of irregular astronomical time-series and the image classification of animals. The results demonstrate comparable performance to traditional full query crowdsourcing methods. Furthermore, we highlight the importance of modeling labeler errors in accurately estimating true classes. We also provide the research community with two datasets collected from our experiments. The code is publicly available.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1278", "problem_id": "12780001", "content": "Graph neural networks (GNN) and label propagation algorithms (LPA) are both methods for message passing, demonstrating exceptional effectiveness in semi-supervised classification tasks. GNN utilizes a neural network to facilitate feature propagation for prediction purposes, whereas LPA relies on label propagation through the graph's adjacency matrix to derive outcomes. Despite their individual successes, an effective approach to integrating these two types of algorithms has yet to be established. To tackle this challenge, we introduce a novel Unified Message Passing Model (UniMP), designed to blend feature and label propagation during both training and inference phases. Initially, UniMP employs a Graph Transformer network that uses feature embeddings and label embeddings as input data for propagation. Furthermore, to prevent overfitting from self-loop input label information during training, UniMP implements a masked label prediction strategy, where a random percentage of input label data is obscured and subsequently predicted. Conceptually, UniMP merges feature propagation with label propagation and has demonstrated strong empirical performance, achieving state-of-the-art results in semi-supervised classification on the Open Graph Benchmark (OGB).", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1279", "problem_id": "12790001", "content": "Numerous features purportedly enhance the accuracy of Convolutional Neural Networks (CNNs), necessitating empirical validation of feature combinations on extensive datasets alongside theoretical substantiation. While certain features are restricted to specific models, problems, or small datasets, others, like batch normalization and residual connections, exhibit broad applicability. This study posits that Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT), and Mish-activation belong to this class of universally beneficial features. By integrating novel features—WRC, CSP, CmBN, SAT, Mish activation, Mosaic data augmentation, CmBN, DropBlock regularization, and CIoU loss—and strategically combining them, state-of-the-art performance is achieved: 43.5% AP (65.7% AP50) on the MS COCO dataset, maintaining a real-time processing speed of approximately 65 FPS on a Tesla V100. The source code is available at https://github.com/AlexeyAB/darknet.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1280", "problem_id": "12800001", "content": "We present an unsupervised approach for detecting and segmenting elements within images of dynamic scenes that, at certain moments, appear to move as unified entities, which we term objects. Our approach begins by segmenting the motion field through the reduction of mutual information among segments. Subsequently, it utilizes these segments to develop object models applicable for detection in static images. Both static and dynamic models are formulated using deep neural networks that are trained cohesively through a bootstrapping method, facilitating adaptation to previously unseen objects. Although the training demands motion data, the resulting object segmentation network is applicable to both static images and videos during inference. As the quantity of observed videos increases, the detection of more moving objects is initiated, acting as a regularizer for novel objects, thereby transforming our method into an unsupervised continual learning framework for object segmentation. We evaluate our models against leading methods in video object segmentation and salient object detection. Across six benchmark datasets, our models demonstrate advantageous performance even compared to those that utilize pixel-level supervision, despite not requiring any manual labeling.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1281", "problem_id": "12810001", "content": "Style transfer typically involves transferring the color palette and textural qualities of a style image onto a content image, maintaining the content image's structural integrity. This work addresses the broader challenge of semantic style transfer, which focuses on learning a correspondence between the overarching style of two distinct, unpaired image datasets, while retaining shared semantic information across these domains. We present XGAN (\"Cross-GAN\"), a dual adversarial autoencoder, designed to acquire a shared representation of the semantic content common to both domains in an unsupervised fashion, concurrently learning image translations between the domains in both directions. Drawing upon concepts from domain adaptation, we introduce a semantic consistency loss to promote semantic preservation within the learned embedding space. We demonstrate encouraging qualitative outcomes in the task of transforming faces into cartoons. Furthermore, the CartoonSet dataset, compiled for this purpose, is released publicly at google.github.io/cartoonset/ as a novel benchmark for semantic style transfer.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1282", "problem_id": "12820001", "content": "Traditional object detection methods relying on deep learning necessitate extensive bounding box annotations for training, incurring significant costs for acquiring such high-quality labeled data. Few-shot object detection, designed to adapt to new classes using only a limited number of annotated instances, presents a considerable challenge due to the potential oversight of fine-grained features in novel objects when data is scarce. In this paper, we introduce Dense Relation Distillation with Context-aware Aggregation (DCNet) to address the few-shot detection problem by maximizing the utilization of annotated novel object features and capturing fine-grained details of query objects. Leveraging a meta-learning framework, the Dense Relation Distillation module comprehensively exploits support features through dense matching of support and query features across all spatial locations in a feed-forward manner. This extensive use of guidance information equips the model to manage common challenges like variations in appearance and occlusions. Furthermore, the Context-aware Aggregation module adaptively integrates features from different scales to enhance the representation of scale-aware features. Experimental results demonstrate that our method achieves state-of-the-art performance on the PASCAL VOC and MS COCO datasets. Code will be made available at https://github.com/hzhupku/DCNet.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1283", "problem_id": "12830001", "content": "This document serves as a concise overview of our submission to the Recognizing Families In the Wild Data Challenge (4th Edition), associated with the FG 2020 Forum. The field of automatic kinship recognition has garnered significant interest among researchers due to its potential applications, yet it remains a challenging endeavor due to the limited data available for assessing whether two faces are related by blood. In this study, we reviewed existing techniques and introduced our own approach. We explored various methods, including those based on deep metric learning, to generate deep embedding features for each image, subsequently evaluating whether they are blood relatives using Euclidean distance or class-based methods. Ultimately, we identified optimization strategies, such as increasing the number of negative samples and utilizing high-resolution images, which enhanced our performance. Additionally, we developed a symmetric network utilizing a binary classification method, which yielded our highest scores across all tasks.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1284", "problem_id": "12840001", "content": "Graph neural networks (GNNs) utilize graph operations, including neural network training, to perform various tasks related to graphs. Although recent studies have applied GNNs to functional magnetic resonance imaging (fMRI) data, a major limitation remains: the difficulty in providing neuroscientifically interpretable explanations for classification results. To address this, we propose a framework for analyzing fMRI data using the Graph Isomorphism Network (GIN), a recently introduced powerful GNN for graph classification. A key contribution of this work is the recognition that GIN can be viewed as a dual representation of convolutional neural networks (CNNs) in graph space, where shift operations are defined using the adjacency matrix. This insight allows us to adapt CNN-based saliency map techniques to GNNs, specifically tailoring them to the proposed GIN with one-hot encoding, to visualize crucial brain regions. Our framework is validated using large-scale resting-state fMRI (rs-fMRI) data to classify subjects by sex based on brain graph structure, and the resulting saliency maps exhibit high consistency with prior neuroimaging evidence on sex differences, as expected (Figure A, B, C) (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1285", "problem_id": "12850001", "content": "The field of computer vision-based analysis of ancient coins has garnered significant attention in recent years, yet the current state of the art in published research yields subpar results, falling short of practical applicability. This paper introduces a series of key contributions aimed at advancing the field, starting with a critical examination of the prevailing approach to visual coin matching, which is deemed impractical due to the vast disparity between the number of existing ancient coin types and those that have been digitally or otherwise documented. Instead, we propose a shift in focus towards deciphering the semantic content of coins, and present a novel methodology that leverages multimodal real-world input to extract and associate semantic concepts with corresponding coin images, utilizing a bespoke convolutional neural network to learn the visual representation of these concepts. Our approach is validated through empirical experiments on the largest and most comprehensive dataset of ancient coins to date, yielding highly encouraging outcomes, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1286", "problem_id": "12860001", "content": "This research investigates a crucial, yet relatively unexplored issue in online reinforcement learning, namely the rapid and secure enhancement of policies. To address this challenge, a new exploration approach, termed diverse exploration (DE), is introduced, which involves learning and deploying a varied set of safe policies to interact with the environment. Theoretical foundations of DE are established, demonstrating how diversity in behavioral policies facilitates efficient exploration while maintaining exploitation. Empirical results demonstrate that an online policy improvement framework incorporating the DE strategy can concurrently achieve swift policy enhancement and ensure safe performance in online settings, as shown in Figure A, B, C (References [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1287", "problem_id": "12870001", "content": "Modern CNN-based object detection models achieve high accuracy but demand powerful GPUs for real-time operation, making them too resource-intensive for memory-constrained embedded systems. Autonomous systems rely on embedded processors, necessitating highly compressed detection networks that maintain accuracy. While existing lightweight models exist, their precision is insufficient for safe driving applications. This paper introduces YOffleNet, an optimized object detection model that achieves significant compression with minimal accuracy loss, tailored for real-time autonomous driving. Building on YOLOv4's architecture, YOffleNet replaces computationally intensive CSP DenseNet with ShuffleNet's lighter modules, drastically reducing complexity. Evaluations on the KITTI dataset demonstrate that YOffleNet is 4.7 times smaller than YOLOv4-s while achieving 46 FPS on an NVIDIA Jetson AGX Xavier embedded GPU. Despite the high compression, accuracy remains strong at 85.8% mAP, only 2.6% below YOLOv4-s, proving its suitability for embedded autonomous systems requiring fast and precise object detection.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1288", "problem_id": "12880001", "content": "Currently, Knowledge Graphs have become a standard method for representing relational information within large and diverse datasets; however, they often include significant levels of imputed noise when generated automatically. To tackle this issue, several error detection techniques have been suggested, primarily concentrating on path ranking and representation learning. This study introduces several leading methods and proposes a hybrid, modular approach for the task. We evaluate various techniques across two benchmarks and one actual biomedical publications dataset, highlighting the effectiveness of our method and offering insights into graph embeddings in the context of noisy Knowledge Graphs.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1289", "problem_id": "12890001", "content": "Current object detection techniques typically need extensive labelled training data for each category and are trained offline in batch mode. These constraints significantly impede their ability to adapt to new categories with limited labelled data. This paper addresses these limitations by investigating the Incremental Few-Shot Detection (iFSD) problem, where new classes are added incrementally with only a few examples, without revisiting previously learned base classes. We introduce OpeN-ended Centre nEt (ONCE), a detector specifically designed for incrementally learning to detect new objects with limited examples. This is accomplished through an adaptation of the CentreNet detector for few-shot learning and by meta-learning a class-specific code generator for registering new classes. ONCE adheres to the incremental learning paradigm, requiring only a single forward pass of few-shot training samples for new class registration, and no access to base classes, making it suitable for embedded devices. Comprehensive experiments on standard object detection and fashion landmark detection tasks demonstrate the viability of iFSD, highlighting a promising new area of research.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1290", "problem_id": "12900001", "content": "This paper introduces a flexible variational framework for fair clustering that incorporates a novel Kullback-Leibler (KL) divergence-based fairness constraint alongside diverse clustering objectives, such as prototype or graph-based approaches. Distinct from existing combinatorial and spectral methods, this variational multi-objective approach allows for tunable balancing between fairness and clustering performance. We establish a rigorous general upper bound, leveraging a concave-convex decomposition of the fairness term, its Lipschitz-gradient characteristics, and Pinsker's inequality. This tight upper bound, optimized in conjunction with various clustering objectives, provides a scalable and convergent solution. Notably, the method independently updates each assignment variable at each iteration, facilitating distributed computation for large datasets, which is critical for exploring different fairness-accuracy trade-offs. In contrast to spectral relaxation techniques, our formulation avoids eigenvalue decomposition. Extensive experimental evaluations and comparisons against state-of-the-art methods on standard fair clustering datasets demonstrate the highly competitive performance of our variational formulation in achieving both fairness and clustering goals.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1291", "problem_id": "12910001", "content": "Detecting color in fashion attributes poses a significant challenge due to the subjective nature of its perception, which cannot be adequately addressed by existing classification methods that rely on predefined discrete color names. We contend that color detection is more accurately framed as a regression problem, and to this end, we introduce a novel two-stage architecture that incorporates attention modules. The initial stage of our approach involves correcting image illumination and identifying the primary discrete color, while the second stage employs a dual-attention mechanism, combining colorname-attention (which adapts to the detected color) and object-attention (which adapts to the clothing category), to inform a spatial pooling operation over the RGB values of image pixels. Furthermore, our method is extended to accommodate garments with multiple colors, and we compile a dataset where each fashion item is associated with a continuous color palette, demonstrating the empirical advantages of our proposed approach, as shown in Figure A, B, C (References [1], [2], and [3] provide additional context).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1292", "problem_id": "12920001", "content": "Cutting-edge neural network architectures are expanding in scale and yielding remarkable generalization outcomes, though this progress is accompanied by a reduction in interpretability. A particularly pressing issue is identifying the appropriate moment to cease model training, as this decision substantially influences generalization capabilities. Convolutional neural networks (ConvNets) consist of complex feature spaces generated by the integration of various channels, making it difficult to analyze intermediate data representations and the model's progression due to the curse of dimensionality. We introduce channel-wise DeepNNK (CW-DeepNNK), an innovative channel-specific generalization estimate utilizing non-negative kernel regression (NNK) graphs, which enables local polytope interpolation on lower-dimensional channels. This technique enhances instance-based interpretability concerning both the acquired data representations and inter-channel relationships. Drawing from our findings, we apply CW-DeepNNK to formulate a new early stopping criterion that (i) does not depend on a validation set, (ii) relies on a task performance metric, and (iii) permits stopping at varying stages for each channel. Our experimental results indicate that this approach outperforms the conventional criterion associated with validation set performance.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1293", "problem_id": "12930001", "content": "In this study, we introduce an innovative layer that utilizes the fast Walsh-Hadamard transform (WHT) combined with smooth-thresholding as a substitute for 1×1 convolution layers in deep neural networks. Within the WHT framework, we enhance the denoising of transform domain coefficients via a new smooth-thresholding non-linearity, which is a refined version of the traditional soft-thresholding operator. Additionally, we present a set of operators that do not require multiplication, derived from the fundamental 2×2 Hadamard transform, to facilitate the implementation of 3×3 depthwise separable convolution layers. By incorporating these two layer types, we substitute the bottleneck layers in MobileNet-V2, achieving a reduction in the network's parameters while only slightly compromising accuracy. Specifically, by altering the last set of bottleneck layers, we lower the parameter count from 2.270M to 540K, resulting in an accuracy drop from 95.21% to 92.98% on the CIFAR-10 dataset. Our method notably enhances data processing speed, as the fast Walsh-Hadamard transform operates with a computational complexity of O(m log_2 m), making it more efficient than traditional 1×1 convolution layers. Indeed, the fast Walsh-Hadamard layer processes a tensor in \\mathbb^ approximately twice as fast as the 1×1 convolution layer on the NVIDIA Jetson Nano board.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1294", "problem_id": "12940001", "content": "Reconstructing faces from monocular images in varied lighting conditions poses significant challenges. The integration of deep neural network encoders with differentiable rendering has paved the way for rapid monocular reconstruction of geometry, lighting, and reflectance, and can be trained in a self-supervised manner to enhance robustness and generalization. Nevertheless, these methods are limited by their reliance on differentiable rasterization-based image formation models and scene parameterization, restricting them to Lambertian face reflectance and resulting in poor shape details. Although ray tracing has been introduced in optimization-based frameworks for monocular face reconstruction, yielding state-of-the-art results, such approaches are inherently slow and lack robustness. Building upon existing methods, this paper proposes a novel approach that substantially enhances reconstruction quality and robustness in general scenes by combining a CNN encoder with a differentiable ray tracer, allowing for personalized diffuse and specular albedos, advanced illumination models, and realistic self-shadows, thereby significantly improving the reconstruction of shape, appearance, and lighting, even in scenes with complex illumination, and enabling practical applications such as relighting and self-shadows removal, with results demonstrating improved accuracy and validity compared to state-of-the-art methods.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1295", "problem_id": "12950001", "content": "Vanilla object detection and instance segmentation models exhibit a significant bias towards identifying common objects, particularly in long-tailed scenarios. Current strategies predominantly tackle this challenge during the training phase, utilizing methods like re-sampling or re-weighting. This paper explores a largely neglected strategy—post-processing calibration of confidence scores. We introduce NorCal, or Normalized Calibration for long-tailed object detection and instance segmentation, as a straightforward method that adjusts the predicted scores of each class based on its training sample size. Our findings indicate that managing the background class separately and normalizing scores across classes for each proposal are essential for achieving enhanced performance. On the LVIS dataset, NorCal demonstrates a significant improvement across nearly all baseline models, benefiting not only rare classes but also common and frequently occurring ones. Additionally, we perform comprehensive analyses and ablation studies to shed light on various modeling decisions and mechanisms inherent to our approach.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1296", "problem_id": "12960001", "content": "This paper aims to enhance the precision of copy-move forgery detection (CMFD) in image forensics through the introduction of an innovative scheme. The suggested approach combines both block-based and keypoint-based methods for detecting forgery. Initially, the speed-up robust feature (SURF) descriptor in log-polar space and the scale invariant feature transform (SIFT) descriptor are extracted from the entire tampered image. Subsequently, the generalized 2 nearest neighbor (g2NN) technique is utilized to generate numerous matching pairs. Following this, the random sample consensus (RANSAC) algorithm is applied to eliminate mismatched pairs, facilitating a preliminary localization of the fraudulent regions. To present these forged areas with greater accuracy, we propose an effective algorithm dubbed evolving circular domains coverage (ECDC), which seeks to identify suitable threshold areas by extracting block features from gradually evolving circular domains centered on the matched pairs. Finally, morphological operations are employed to enhance the delineation of the identified forgery regions. Experimental findings demonstrate that the proposed CMFD framework outperforms other leading CMFD techniques in terms of detection efficacy across a range of attacks.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1297", "problem_id": "12970001", "content": "In this study, we develop dynamic models for parameterized groups of dynamical systems that exhibit different characteristics. These models are represented as stochastic processes that depend on a latent context variable, which we deduce from the observed changes in the system. The probabilistic approach enables us to determine a sequence of actions that, with a limited number of interactions with the environment, effectively explores the system within the specified parameterized family. This is accomplished by guiding the system through transitions that provide the most information about the context variable. We illustrate the efficacy of our exploration method through a non-linear toy problem and two established reinforcement learning environments.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1298", "problem_id": "12980001", "content": "In electronic trading markets, the only directly observable data often comprises price and volume time series generated by the interactions of various market participants. To evaluate trading strategies prior to their implementation in real-time trading, calibrated multi-agent market environments are frequently employed to simulate time series that closely mirror historical data. Comprehensive testing requires that these trading strategies be assessed across a range of market contexts, encompassing typical days as well as periods of market stress, such as those witnessed at the onset of the COVID pandemic. This paper tackles the challenge of calibrating parameters in multi-agent simulators to effectively replicate characteristics of different market regimes. We introduce an innovative two-step approach that involves training a discriminator capable of differentiating between authentic and simulated price and volume time series as part of a GAN with self-attention. Subsequently, this discriminator is integrated into an optimization framework to fine-tune the parameters of a simulator model based on predefined agent archetypes for specific market scenarios. Our findings demonstrate the efficacy of our proposed method through experimental results.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1299", "problem_id": "12990001", "content": "In the age of big data, enhancing memory and computational efficiency is essential for effectively implementing technologies. Hashing stands out as one of the most efficient strategies for addressing the computational challenges associated with large datasets. A natural approach to this challenge is spectral hashing, which integrates affinity to derive binary codes. However, the binary constraints render the optimization problem difficult to solve. To address this issue, various relaxation techniques have been suggested to lessen the computational demands of generating binary codes while still achieving satisfactory outcomes. The drawback of all existing relaxation methods lies in their reliance on one or more supplementary auxiliary variables to yield high-quality binary codes during the relaxation process. The inclusion of these auxiliary variables necessitates a coordinate descent approach, which escalates computational complexity. We contend that the introduction of these variables is unwarranted. Accordingly, we present a new relaxed formulation for spectral hashing that does not require any additional variables. Moreover, rather than addressing the problem in the original space where the number of variables matches that of the data points, we tackle it within a significantly smaller space, retrieving the binary codes from this solution. This strategy simultaneously alleviates memory and computational complexity. To obtain the solution, we employ two optimization methods: projected gradient and optimization on manifold. Through extensive experiments conducted on four public datasets, we demonstrate that our proposed efficient spectral hashing (ESH) algorithm achieves remarkable retrieval performance comparable to state-of-the-art methods while maintaining low complexity.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1300", "problem_id": "13000001", "content": "Primate visual cognition surpasses artificial neural networks in its capacity to 'imagine' an object, including newly encountered ones, across various attributes such as pose, position, color, and texture. To enhance neural networks' ability to envision objects with diverse attributes, we introduce a set of objective functions, formulated over groups of examples, as a novel learning framework called Group-Supervised Learning (GSL). GSL enables the decomposition of inputs into disentangled representations with interchangeable components, which can be reassembled to generate new samples. For example, images of red boats and blue cars can be decomposed and recombined to create novel images of red cars. We present an auto-encoder-based implementation, termed group-supervised zero-shot synthesis network (GZS-Net), trained with our framework, capable of generating high-quality red cars even without exposure to such examples during training. Our model and framework are evaluated on existing benchmarks and a newly released dataset, with qualitative and quantitative results showing that GZS-Net trained with GSL surpasses state-of-the-art approaches.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1301", "problem_id": "13010001", "content": "We introduce a novel color photometric stereo (CPS) technique capable of reconstructing highly detailed 3D facial geometry from a single image. The approach employs three uncalibrated near point lights with distinct colors and a single camera. To ensure reliable self-calibration of the light sources, we leverage a 3D morphable model (3DMM) and semantic segmentation of facial regions. Spectral ambiguity is resolved through a unified framework integrating albedo consensus, albedo similarity, and proxy prior. Rather than relying on albedo spatial constancy, we propose a new similarity metric based on the albedo norm profile. Experimental results demonstrate that our method achieves cutting-edge performance, producing high-fidelity geometry with fine details like wrinkles from a single input image.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1302", "problem_id": "13020001", "content": "Sparse signal representations created through linear combinations of learned atoms have achieved cutting-edge performance in various practical signal processing tasks. To effectively process high-dimensional signals, approximation techniques are essential due to the NP-hard nature of determining optimal atoms for sparse coding. This study investigates greedy algorithms aimed at the unsupervised learning of dictionaries comprised of shift-invariant atoms and introduces a novel approach where each atom is chosen with an average equal probability, mirroring the homeostatic regulation found in recurrent convolutional neural networks. This equiprobable selection can be integrated into multiple greedy data learning algorithms to ensure that all atoms undergo adaptation during training, preventing any single atom from being favored in the linear combination. Through simulation experiments, we show that dictionary learning utilizing equiprobable selection yields increased entropy in the sparse representation and reduced reconstruction and denoising errors, applicable to both ordinary matching pursuit and orthogonal matching pursuit with shift-invariant dictionaries. Additionally, we demonstrate that employing equiprobable selection lowers the computational costs associated with matching pursuits, enabling quicker and more precise dictionary learning algorithms.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1303", "problem_id": "13030001", "content": "The development of various methods to interpret complex predictive models has led to high expectations for post-hoc model explainability, but it has been discovered that these explanations often lack robustness and trustworthiness, making them susceptible to manipulation. This study focuses on attacking Partial Dependence plots, profiles, and PDP, which are widely used techniques for explaining predictive models trained on tabular data, and demonstrates that they can be adversarially manipulated, raising concerns particularly in high-stakes applications such as finance and medicine where model auditability is crucial. By poisoning the data using genetic and gradient algorithms, it is possible to alter and shift explanations in a desired direction, highlighting the vulnerability of these methods. Notably, this research is the first to launch attacks on variable dependence explanations, and its innovative approach utilizing a genetic algorithm is highly versatile, as it can be applied in a model-agnostic and explanation-agnostic manner, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1304", "problem_id": "13040001", "content": "The utilization of numerical rewards in reinforcement learning is often accompanied by limitations and challenges, despite their desirable properties. A promising alternative is the employment of ordinal rewards, which has garnered increased attention in recent years. This paper proposes a generalized framework for adapting reinforcement learning to accommodate ordinal rewards, demonstrating its applicability through the modification of established algorithms, such as Q-learning, and the introduction of Ordinal Deep Q-Networks, which enable the use of ordinal rewards in deep reinforcement learning. Evaluations conducted using the OpenAI Gym framework reveal that the ordinal variants exhibit performance comparable to their numerical counterparts in several problems, and preliminary evidence suggests that they can yield superior results in scenarios with less complex and more straightforward reward signals, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1305", "problem_id": "13050001", "content": "The embedding space, where semantically similar objects are clustered together and dissimilar ones are dispersed, is fundamental to various computer vision applications. However, conventional methods typically employ a single metric to represent all data points in this space, which can exhibit complex, non-uniform distributions with diverse notions of similarity, such as appearance, shape, color, or semantic meaning. These single-metric approaches often struggle to capture the multitude of relationships between objects and fail to generalize effectively. This paper presents a novel, straightforward divide-and-conquer approach to deep metric learning, which achieves significant improvements over the current state-of-the-art. By partitioning both the data and embedding space into K smaller sub-problems, our method learns K distinct distance metrics in non-overlapping subspaces defined by neural network embedding layer neuron groups. This approach enhances convergence speed and generalization by reducing the complexity of each sub-problem relative to the original one, as demonstrated by its substantial outperformance of state-of-the-art methods on CUB200-2011, CARS196, Stanford Online Products, In-shop Clothes, and PKU VehicleID datasets in retrieval, clustering, and re-identification tasks.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1306", "problem_id": "13060001", "content": "Yuru-chara, mascot characters designed by local entities to promote regional information and products, incur significant creation costs, suggesting the potential benefits of employing machine learning techniques like generative adversarial networks (GANs). Recent studies indicate that incorporating class conditions into GAN training datasets enhances learning stability and image quality. However, applying class conditional GANs is challenging when dealing with limited original data or ambiguous class definitions, as often encountered with yuru-chara images. This paper introduces a class conditional GAN framework that leverages clustering and data augmentation. Initially, K-means++ clustering is applied to a yuru-chara image dataset to establish class conditional categories. Subsequently, data augmentation is performed, quintupling the dataset size. Furthermore, a model integrating ResBlock and self-attention mechanisms into a class conditional GAN-based network is developed and trained using the generated class conditional yuru-chara dataset. Evaluation of the generated images revealed the influence of varying clustering methods on the resulting image characteristics.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1307", "problem_id": "13070001", "content": "Conventional control techniques fall short in numerous scenarios involving the management of Cyber-Physical Systems (CPS). In these environments, CPS controllers are required to function and react to unpredictable interactions, circumstances, or failure instances. Addressing this unpredictability necessitates the application of executive and cognitive control capabilities that facilitate planning and reasoning. Inspired by drone racing, this dissertation tackles these issues in advanced flight control by exploring the application of deep neural networks to incorporate crucial aspects of higher-level cognition in the development of low-level flight controllers. This thesis presents the creation and distribution of an open-source, comprehensive solution stack for constructing neuro-flight controllers. The stack includes a methodology for developing a multicopter digital twin to synthesize a flight controller tailored to a specific aircraft, a tuning framework for establishing training environments (GymFC), and firmware for the world’s first neural network-enabled flight controller (Neuroflight). GymFC’s innovative strategy combines the digital twinning paradigm for flight control training, enabling a smooth transition to hardware implementation. Furthermore, this thesis investigates alternative reward function systems and modifications to the software environment to help integrate simulation with real-world deployment settings. The findings reported in this thesis illustrate that reinforcement learning can be utilized to train neural network controllers that are proficient in maintaining stable flight and executing precise aerobatic maneuvers in real-world contexts. Consequently, this research lays the groundwork for the advancement of next-generation flight control systems.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1308", "problem_id": "13080001", "content": "Recent advancements have led to the proliferation of accessible and effective facial manipulation techniques for videos, such as FaceSwap and deepfakes. While offering utility across various domains, the ease with which these technologies can generate highly realistic edits raises concerns about their potential for misuse, including the propagation of misinformation and cyber harassment. Consequently, the objective identification of manipulated faces in video sequences has become critically important. This paper addresses the challenge of detecting face manipulation in videos, focusing on contemporary techniques. Specifically, we investigate the ensemble of diverse Convolutional Neural Network (CNN) models. Our approach involves creating multiple models from a base network (EfficientNetB4) by integrating: (i) attention layers and (ii) siamese training methodologies. The results demonstrate that the proposed ensemble method achieves promising face manipulation detection performance on two publicly available datasets comprising over 119000 videos.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1309", "problem_id": "13090001", "content": "The assessment of video inpainting methods has become increasingly quantitative in recent studies, yet the video and mask content utilized for evaluation purposes has been somewhat overlooked. The inherent attributes of camera and background scene motion significantly impact the complexity of the task and influence different methods to varying degrees, but current evaluation frameworks neglect to account for these factors, resulting in limited understanding of the modes of failure in inpainting. To bridge this knowledge gap, we introduce the Diagnostic Evaluation of Video Inpainting on Landscapes (DEVIL) benchmark, comprising two primary components: (i) a novel dataset of videos and masks annotated with respect to key inpainting failure modes, and (ii) an evaluation framework that assesses performance on subsets of the dataset defined by specific content attributes, with scoring based on reconstruction quality, realism, and temporal consistency. By exposing systematic performance variations induced by distinct input content characteristics, the DEVIL benchmark facilitates a more nuanced analysis of video inpainting methods, serving as a valuable diagnostic tool for the field, with our code available at https://github.com/MichiganCOG/devil.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1310", "problem_id": "13100001", "content": "The application of deep generative models (DGMs) to detect out-of-distribution (OOD) inputs appears intuitive, yet these models often assign higher probabilities or densities to OOD images than to those from the training distribution, a phenomenon that can be attributed to model misestimation. This study demonstrates that without assumptions about the relevant out-distributions, no method can achieve performance better than random chance. Furthermore, an examination of the typical set hypothesis reveals that assuming support overlap between in- and out-distributions has significant implications, and that the typical set is an arbitrary choice for defining OOD detection. The findings indicate that estimation error is a more likely cause of OOD detection failures than the mismatch between likelihood-based detection methods and the out-distributions of interest, and illustrate how even slight estimation errors can lead to such failures, with important implications for future research in deep generative modeling and OOD detection.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1311", "problem_id": "13110001", "content": "Deep learning has shown remarkable success in camera localization; however, contemporary single-image methods often lack robustness, resulting in significant outliers. This issue has been partially addressed by using sequential (multi-image) or geometric constraint techniques, which enable the system to disregard dynamic objects and varying lighting conditions for enhanced outcomes. In this study, we demonstrate that employing attention mechanisms can direct the network to concentrate on more geometrically stable objects and features, thereby achieving state-of-the-art results on standard benchmarks, even when only a single image is utilized as input. We provide extensive experimental validation using public indoor and outdoor datasets. By visualizing the saliency maps, we illustrate how the network effectively learns to dismiss dynamic objects, leading to improved performance in global camera pose regression. The source code is available at https://github.com/BingCS/AtLoc.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1312", "problem_id": "13120001", "content": "Automated segmentation in medical imaging presents significant challenges due to the need for extensive manually labeled datasets, which are time-consuming to produce. Traditional learning-based methods often struggle with precise boundary delineation due to insufficient geometric constraints. Recent advances in contrastive learning, a subset of self-supervised learning, have shown promise across various applications. This study introduces Contrastive Voxel-wise Representation Learning (CVRL), a novel approach incorporating geometric constraints to enhance global-local feature learning for volumetric medical image segmentation with sparse annotations. Our framework efficiently captures 3D spatial context and anatomical details by employing a voxel-to-volume contrastive strategy for global feature extraction and a voxel-to-voxel contrastive mechanism to leverage local spatial cues. Additionally, we incorporate an elastic interaction-based active contour model as a geometric regularization term to facilitate accurate and efficient object delineation in an end-to-end learning framework. Experimental evaluations on the Atrial Segmentation Challenge dataset highlight the effectiveness of our method, particularly in scenarios with minimal annotated data.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1313", "problem_id": "13130001", "content": "Many current deep reinforcement learning (RL) methods for session-based recommendations either depend on expensive real-time interactions with actual users or utilize potentially biased models based on rules or user behavior data for learning. This study shifts the focus to the development of recommendation policies within a purely batch or offline framework, which means learning solely from historical interaction logs or batch data derived from an unknown and sub-optimal behavior policy, without additional access to real-world data or user behavior models. We introduce BCD4Rec: Batch-Constrained Distributional RL for Session-based Recommendations. BCD4Rec takes advantage of the latest developments in batch (offline) RL and distributional RL to learn from offline logs while addressing the inherently stochastic nature of user rewards, which arise from diverse latent interest preferences (environments). Our results show that BCD4Rec significantly enhances performance over the behavior policy and surpasses strong RL and non-RL baselines in the batch context, assessed with standard performance metrics such as Click Through Rates and Buy Rates. Additionally, BCD4Rec exhibits beneficial characteristics: i. it recommends items from the appropriate latent categories, leading to improved value estimates despite a large action space (comparable to the total number of items), and ii. it effectively mitigates popularity bias common in clicked or purchased items found in offline logs.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1314", "problem_id": "13140001", "content": "Depth estimation methods based on unsupervised learning have gained increasing attention due to their independence from the need for extensive amounts of densely labeled training data, which is difficult to obtain. In this study, we introduce a new unsupervised monocular video depth estimation technique for natural environments, leveraging the cutting-edge approach of Zhou et al. that simultaneously estimates depth and camera motion. Our method improves upon the baseline in three key ways: 1) we introduce an extra supervisory signal by utilizing the left-right binocular image reconstruction loss derived from the computed disparities, allowing the left frame to be reconstructed using the temporal frames and right frames of stereo vision; 2) we train the network using two types of view synthesis loss along with left-right disparity consistency regularization to estimate depth and pose concurrently; 3) we implement edge-aware smooth L2 regularization to smooth the depth map, while maintaining the contours of the target. Comprehensive experiments on the KITTI autonomous driving dataset and the Make3D dataset demonstrate the effectiveness of our algorithm in terms of training efficiency. We achieve results comparable to the baseline with only 3/5 of the training data. Furthermore, the experimental findings indicate that our method exceeds the performance of traditional supervised approaches that rely on either ground truth depth or predetermined pose for training.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1315", "problem_id": "13150001", "content": "Space-time video super-resolution (STVSR) enhances both the spatial resolution and frame rate of videos that are initially of low quality in both aspects. While deformable convolution techniques have recently shown potential in STVSR, their application is limited to interpolating frames that were specifically defined during training. Furthermore, these techniques have not fully utilized the short-term motion information present between consecutive frames. Addressing these limitations, this paper introduces a Temporal Modulation Network (TMNet) designed to generate arbitrary intermediate frames while ensuring precise high-resolution reconstruction. Our approach involves a Temporal Modulation Block (TMB) to adjust deformable convolution kernels, enabling controlled feature interpolation. To effectively leverage temporal information, we also present a Locally-temporal Feature Comparison (LFC) module, combined with a Bi-directional Deformable ConvLSTM, to capture both short-term and long-term motion patterns within videos. Empirical evaluations conducted on three standard datasets demonstrate that TMNet surpasses existing STVSR methods. The implementation code can be accessed at https://github.com/CS-GangXu/TMNet.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1316", "problem_id": "13160001", "content": "Deep neural networks (DNNs) are now widely employed in face recognition (FR) applications, yet research indicates they remain susceptible to adversarial attacks that can compromise real-world FR systems. Current attack methods either produce perturbations effective only in digital settings or require specialized hardware, lacking adaptability to diverse physical conditions. This paper introduces FaceAdv, a physical-world attack that creates adversarial stickers to fool FR systems. The approach includes a sticker generator for producing varied sticker designs and a transformer to digitally place these stickers on faces, refining their effectiveness through feedback. Extensive testing on three prominent FR systems (i.e., ArcFace, CosFace, and FaceNet) demonstrates that FaceAdv outperforms existing attacks, achieving higher success rates for both dodging and impersonation attempts. Additional evaluations confirm the robustness of the proposed method.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1317", "problem_id": "13170001", "content": "A learned generative model typically generates biased statistics in relation to the true data distribution. To address this bias, a common approach is importance sampling, which involves weighting model samples by the likelihood ratio of the model and true distributions. In instances where the likelihood ratio cannot be determined, we can estimate it by training a probabilistic classifier to differentiate between samples from the two distributions. We apply this likelihood-free importance weighting technique to mitigate the bias present in generative models. Our findings indicate that this method consistently enhances standard goodness-of-fit metrics used to assess the sample quality of advanced deep generative models, implying a reduction in bias. Furthermore, we illustrate its effectiveness in relevant applications such as a) data augmentation for classification utilizing generative adversarial networks, and b) model-based policy evaluation with off-policy data.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1318", "problem_id": "13180001", "content": "The pel-recursive method for computing 2-D optical flow has been widely explored in computer vision for motion estimation in image sequences, yet it continues to present challenges, including handling outliers, motion discontinuities, and occlusion. This approach depends on spatio-temporal intensity changes caused by motion. Our introduced adaptive regularized method addresses these challenges within a unified framework, employing a data-driven Mixed Norm (MN) technique to determine the optimal motion vector for each pixel. The model accommodates diverse noise types, reflecting various error sources. Motion vector estimation incorporates local image characteristics and is derived by minimizing a mixed norm functional with a regularization parameter based on kurtosis. This parameter balances the influence of the fourth norm, ensuring convexity of the functional. A key benefit of our method is that it does not require prior knowledge of the noise distribution. Experimental results demonstrate that this technique yields reliable optical flow estimates.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1319", "problem_id": "13190001", "content": "We introduce RON, a high-performance framework designed for general object detection that strategically combines the strengths of region-based approaches (e.g., Faster R-CNN) and region-free methods (e.g., SSD). Built on a fully convolutional architecture, RON tackles two key challenges: (a) multi-scale object localization and (b) negative sample mining. For (a), we implement a reverse connection mechanism that allows multi-level detection across CNN layers, while for (b), we introduce an objectness prior to efficiently narrow down potential object regions. These components—reverse connection, objectness prior, and detector—are jointly optimized through a multi-task loss function, enabling RON to generate final detections directly from diverse feature map locations. Evaluations on PASCAL VOC 2007, PASCAL VOC 2012, and MS COCO datasets show RON's strong performance, achieving 81.3% mAP on PASCAL VOC 2007 and 80.7% mAP on PASCAL VOC 2012 using VGG-16 with 384×384 input resolution. The framework demonstrates even greater advantages on larger, more complex datasets like MS COCO while maintaining efficiency—running at 15 FPS with 1.5G GPU memory during testing, three times faster than Faster R-CNN.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1320", "problem_id": "13200001", "content": "Object proposal generation techniques are prevalent in various computer vision applications. However, their effectiveness in video-related tasks is often hampered by issues like motion blur, poor contrast, and object deformation. To address these limitations, this paper introduces a target-specific object proposal generation (TOPG) method that leverages contextual video information to improve accuracy. Specifically, TOPG integrates color and edge information, two complementary objectness cues, to generate more robust object proposals across diverse and challenging environments, leading to a significant improvement in recall. Furthermore, an object proposal ranking strategy is introduced to enhance the rank accuracy of the generated proposals. Empirical evaluations on challenging visual tracking datasets demonstrate that the proposed TOPG method achieves a substantial recall increase (approximately 20%-60%) compared to several state-of-the-art methods. The TOPG method is then applied to visual tracking, resulting in a TOPG-based tracker (TOPGT) that utilizes TOPG as a sample selection strategy to identify high-quality target candidates. The comprehensive nature of TOPG-generated object proposals, encompassing both hard negative and positive samples, facilitates the training of effective classifiers and provides robust target candidates for visual tracking. Experimental results highlight the superior performance of TOPGT in visual tracking, surpassing other state-of-the-art trackers (by approximately 3%-11% in distance precision compared to the VOT2015 challenge winner).", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1321", "problem_id": "13210001", "content": "Recent advancements in computer vision techniques have garnered significant attention in the realm of precision agriculture. These techniques primarily aim to identify objects of interest, such as crops and weeds, while distinguishing them from their background. Weeds, defined as undesirable plants that grow alongside crops, compete for essential resources like nutrients, water, and sunlight, leading to decreased crop yields. Consequently, accurate weed detection and mapping are vital for localized weed management, as they help minimize labor costs and the reliance on herbicides. This study explores the application of color and texture features to differentiate between soybean crops and weeds. Various feature extraction methods, including two color spaces (RGB and HSV), gray level co-occurrence matrix (GLCM), and local binary pattern (LBP), are applied to train a Support Vector Machine (SVM) classifier. The experimental phase utilized a publicly available image dataset of soybean crops acquired through an unmanned aerial vehicle (UAV). Findings from the study indicated that the highest classification accuracy (over 96%) was achieved by integrating color and LBP features.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1322", "problem_id": "13220001", "content": "The pursuit of developing meaningful representations that preserve essential content for a specific task while eliminating harmful variations is a significant challenge in machine learning. This study addresses the issue of creating representations that are unaffected by a particular data factor or characteristic. We model the representation learning process as an adversarial minimax game. Our analysis of the optimal equilibrium in this context reveals that it involves enhancing the uncertainty in predicting the adverse factor based on the representation while simultaneously increasing the accuracy of task-specific predictions. We demonstrate the effectiveness of our proposed framework across three benchmark tasks: equitable and bias-free classification, language-agnostic generation, and illumination-invariant image classification. Results indicate that our approach produces an invariant representation and improves generalization, reflected in enhanced performance outcomes.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1323", "problem_id": "13230001", "content": "Recent advances in video anomaly detection (VAD) have focused on training deep autoencoders to capture normal patterns with minimal reconstruction errors, identifying anomalies as inputs with large errors at test time. Nevertheless, the robust generalization capabilities of deep autoencoders can sometimes lead to accurate reconstructions of abnormal inputs, hindering their detection. To overcome this limitation, we propose a novel anomaly detection approach that leverages discriminative prototypes of normal data to guide the reconstruction of video frames, thereby favoring normal events and distorting abnormal ones. Our method employs a prototype-guided memory module to perform discriminative latent embedding, introducing a new criterion and corresponding loss function that encourage memory items to store representative embeddings of normal data, or prototypes. Additionally, we design a two-branch autoencoder consisting of a future frame prediction network and an RGB difference generation network sharing the same encoder, allowing our model to learn temporal regularity from stacked RGB differences that capture motion information similar to optical flow. Experimental results on three benchmark datasets demonstrate the superiority of our method over state-of-the-art approaches, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1324", "problem_id": "13240001", "content": "Change Captioning involves articulating the differences between images using natural language. Many current approaches approach this issue as a difference evaluation without considering the presence of distractions such as changes in viewpoint. However, such viewpoint alterations frequently occur in reality and can obscure the semantic differences that need to be conveyed. In this study, we introduce an innovative visual encoder designed to clearly differentiate between viewpoint and semantic changes within the change captioning task. Additionally, we simulate human attention preferences and present a novel reinforcement learning framework that directly refines attention using language evaluation rewards. Comprehensive experimental results indicate that our approach significantly surpasses the leading methods in both the Spot-the-Diff and CLEVR-Change datasets.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1325", "problem_id": "13250001", "content": "Neural networks face vulnerabilities when exposed to adversarial examples, which raises concerns regarding their deployment in security-critical systems. In response, we introduce a Denoiser and UPsampler Network (DUP-Net) architecture aimed at enhancing defenses for 3D adversarial point cloud classification, wherein the two components effectively reconstruct surface smoothness by either removing or adding points. In our approach, statistical outlier removal (SOR) serves as the denoiser, while a data-driven upsampling network functions as the upsampler. DUP-Net offers three key advantages over baseline defenses. Firstly, utilizing DUP-Net enhances the target model's resilience against white-box adversarial attacks. Secondly, the use of statistical outlier removal contributes additional robustness due to its non-differentiable nature as a denoising technique. Lastly, the upsampling network can be trained on limited datasets and demonstrates commendable defense capabilities against adversarial attacks generated from diverse point cloud datasets. We perform extensive experiments to confirm the practical effectiveness of DUP-Net as a defense solution. Our most effective defense measures lead to an elimination of 83.8% of attacks based on C&W and l_2 loss (point shifting), 50.0% of attacks based on C&W and Hausdorff distance loss (point adding), and 9.0% of attacks stemming from saliency map techniques (point dropping) with 200 points dropped on PointNet.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1326", "problem_id": "13260001", "content": "This paper addresses the challenge of learning concise, low-dimensional representations for sequential data by capturing inherent spatio-temporal features. To effectively extract these informative cues, we frame the problem using contrastive representation learning, introducing a novel objective function based on optimal transport. Our approach aims to identify a low-dimensional subspace representation that simultaneously (i) maximizes the Wasserstein distance between the data (embedded within the subspace) and an adversarial data distribution, (ii) preserves the temporal ordering of the data, and (iii) minimizes data distortion. We introduce a novel framework that integrates Wasserstein GANs with a classifier to generate the adversarial distribution, providing a systematic approach for creating effective negative distributions for contrastive learning, a known difficult area. The complete objective is formulated as a subspace learning problem on the Grassmann manifold and solved using Riemannian optimization techniques. We validate our approach through experiments on human action recognition in video sequences, demonstrating competitive performance compared to established baselines.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1327", "problem_id": "13270001", "content": "The bokeh effect, commonly employed in photography, produces images with sharp foreground objects against a blurred background, typically achieved using Single Lens Reflex cameras and shallow depth-of-field. While contemporary smartphones often utilize dual rear cameras or advanced auto-focus systems for capturing bokeh images, single-rear camera smartphones lacking robust auto-focus depend on software-based solutions, which are also valuable for post-capture bokeh effect generation. This paper introduces a comprehensive deep learning framework designed to create high-quality bokeh effects in images by merging the original image with various smoothed versions, facilitated by a monocular depth estimation network. The presented method is benchmarked against a saliency detection baseline and several techniques from the AIM 2019 Challenge on Bokeh Effect Synthesis. Extensive experiments are conducted to evaluate the distinct components of the proposed algorithm. The network's efficiency allows it to process an HD image in 0.03 seconds, and it secured second place in the Perceptual Track of the AIM 2019 Bokeh effect challenge.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1328", "problem_id": "13280001", "content": "Unsupervised node embedding techniques are valuable for various downstream tasks like node classification and link prediction. A universal node embedding is intended to be versatile and advantageous for diverse downstream applications. This paper presents PanRep, a graph neural network (GNN) model designed for unsupervised learning of universal node representations in heterogenous graphs. PanRep employs a GNN encoder to generate node embeddings and incorporates four decoders, each dedicated to capturing distinct topological and node feature characteristics. Utilizing these characteristics, the innovative unsupervised framework learns universal embeddings suitable for different downstream tasks. PanRep can be further fine-tuned to accommodate scenarios with limited labeled data, positioning it as a pretrained model for extracting node embeddings from heterogenous graph data. Experimental results demonstrate that PanRep surpasses both unsupervised and certain supervised methods in node classification and link prediction, particularly when the labeled data for the supervised methods is scarce. Moreover, PanRep-FT (with fine-tuning) outperforms all other supervised approaches, highlighting the benefits of pretraining models. As a practical application, PanRep-FT is used to identify potential new drugs for Covid-19, demonstrating the utility of universal embeddings in drug repurposing and successfully identifying several drugs currently undergoing clinical trials as promising candidates.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1329", "problem_id": "13290001", "content": "The potential of graph neural networks (GNNs) to be applied in various fields that utilize graph data has been explored, yet a standardized framework for training and comparison is lacking, making it challenging to fairly evaluate new methods, including diverse model architectures and data augmentation techniques. To address this, a reproducible and standardized benchmark is proposed, allowing for consistent training settings to be applied to node classification tasks. This benchmark comprises 9 datasets from diverse fields, ranging from small to medium scale, and 7 distinct models, with a k-fold assessment strategy for small datasets and a standardized training procedure for all datasets, thereby establishing a uniform experimental pipeline for GNNs to facilitate fair comparisons of model architectures. The impact of input features on model performance is investigated through data augmentation using node2vec and Laplacian eigenvectors, revealing the significance of topological information in node classification tasks. The results show that increasing model layers does not necessarily enhance performance, except in cases where the graphs are disconnected, such as the PATTERN and CLUSTER datasets. Furthermore, data augmentation proves to be highly beneficial, particularly when using node2vec, leading to a substantial improvement in baseline performance, as seen in Figure A (Reference [1], Figure B [2], and Figure C [3]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1330", "problem_id": "13300001", "content": "Conditional generative adversarial networks (cGAN) have significantly advanced conditional image generation, a core challenge in computer vision. While most research has prioritized performance gains, limited attention has been given to enhancing cGAN's resilience to noise. The generator's regression can produce substantial output errors, reducing cGAN's reliability for practical use. This paper presents RoCGAN, a new cGAN framework that utilizes target space structure to mitigate this problem. By incorporating an unsupervised pathway into the generator, RoCGAN ensures outputs remain aligned with the target manifold despite heavy noise. We demonstrate that RoCGAN retains key theoretical properties of GANs and show through experiments that it substantially surpasses current leading cGAN models across multiple domains, including natural scenes and facial images.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1331", "problem_id": "13310001", "content": "We introduce a pair of unsupervised clustering algorithms designed to analyze data presumed to originate from a uniform distribution within a metric space X. These algorithms generate a data clustering by establishing a topological model for the connected components of X. Both algorithms operate by choosing a graph from a natural one-parameter family of graphs defined on the data samples, employing either a geometric or an information-theoretic criterion for graph selection. The kernel of the graph Laplacian is then used to estimate the connected components of X, enabling the algorithm to function autonomously, without needing prior knowledge of the number of clusters or other supplementary information.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1332", "problem_id": "13320001", "content": "Three-dimensional object detection utilizing point clouds presents a significant computer vision challenge with broad applicability in 3D scene understanding. Recent studies have extensively explored end-to-end trainable Hough voting for generating object proposals. However, existing voting mechanisms often suffer from incomplete object surface votes and substantial outlier votes originating from complex backgrounds, thereby limiting the effective use of input point cloud data. Drawing inspiration from back-tracing techniques used in traditional Hough voting, we propose a novel 3D object detection approach termed Back-tracing Representative Points Network (BRNet). BRNet employs a generative process to back-trace representative points from vote centers and subsequently revisits complementary seed points in the vicinity. This process enhances the capture of detailed local structural features around potential objects derived from the raw point clouds. Consequently, BRNet's combined bottom-up and top-down strategy promotes mutual consistency between predicted vote centers and raw surface points, leading to improved object localization and class prediction. Empirical results demonstrate that BRNet achieves state-of-the-art performance on the ScanNet V2 (+7.5% mAP@0.50) and SUN RGB-D (+4.7% mAP@0.50) datasets, while maintaining computational efficiency. The source code is available at https://github.com/cheng052/BRNet.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1333", "problem_id": "13330001", "content": "Forecasting seasonal time series presents significant challenges because of the long-term dependencies introduced by seasonal patterns. This study introduces a two-phase approach for predicting univariate seasonal time series. The initial phase focuses on learning the extended temporal patterns within a window that extends past the prediction horizon. Leveraging these learned patterns, the second phase improves forecast accuracy within the target horizon. Both phases combine auto-regressive modeling with neural networks to account for linear and non-linear time series features. Our method demonstrates superior performance on the M4 Competition Hourly datasets. Additionally, we illustrate that integrating intermediate outputs from the first phase into existing forecasting models can substantially boost their predictive capabilities.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1334", "problem_id": "13340001", "content": "Predicting pedestrian movement in complex urban settings remains a significant obstacle for autonomous vehicle development. While recurrent neural networks are commonly employed for this task due to their ability to model temporal dependencies, they often struggle with extended sequential data. To address this limitation, we propose a transformer network-based framework, leveraging its demonstrated efficiency and superior performance over RNNs in sequence modeling. Our framework integrates historical positional data, agent interactions, and scene context to achieve robust pedestrian trajectory predictions. Evaluations on two real-world datasets demonstrate that our framework surpasses existing baseline methods in both short-term and long-term prediction accuracy.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1335", "problem_id": "13350001", "content": "The construction of an image relies on two fundamental elements: color and structure, with the latter being effectively conveyed through a diverse range of colors, enabling neural networks to recognize objects within the image. Nevertheless, when the color space is severely limited, the structural information tends to degrade, leading to potential failures in image understanding by neural networks. This interplay between color and structure prompts an investigation into the problem of identifying and preserving the most informative structural elements in images while restricting the color space to a minimal number of bits, thereby facilitating accurate image recognition. To address this challenge, a novel color quantization network, termed ColorCNN, is proposed, which learns to extract structural information from images in an end-to-end manner using classification loss. ColorCNN operates by generating a color index map and an RGB color palette for a given color space size, quantizing the colors in the original image, and subsequently evaluating the performance of the color-quantized image using a pre-trained task network. The experimental results demonstrate that, even with an extremely limited 1-bit color space, the proposed network achieves a top-1 accuracy of 82.1% on the CIFAR10 dataset, significantly outperforming traditional color quantization methods, and exhibits superiority over other image compression methods in the low bit-rate regime when encoded with PNG, with the code available at https://github.com/hou-yz/color_distillation.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1336", "problem_id": "13360001", "content": "Given the increasing need to combat COVID-19, surveillance cameras often capture low-resolution face images with masks. Existing face super-resolution methods typically fail to address both resolution enhancement and mask removal in a single framework. This study approaches mask occlusion as image noise and introduces JDSR-GAN, a joint learning network designed for masked face super-resolution. The generator, consisting of a denoising module and a super-resolution module, processes low-quality masked faces to produce high-resolution, high-quality outputs. A discriminator with carefully formulated loss functions ensures the fidelity of the reconstructed images. Additionally, identity preservation and attention mechanisms are integrated to enhance feature representation and learning. By simultaneously performing denoising and super-resolution, these tasks mutually reinforce each other, leading to improved results. Comprehensive experiments demonstrate that JDSR-GAN outperforms existing methods that handle the two tasks independently.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1337", "problem_id": "13370001", "content": "Goal-oriented dialogue systems have gained significant interest in artificial intelligence due to their wide-ranging applications. In such tasks, a questioner poses an action-driven query, and the answerer provides a response aimed at guiding the questioner toward an appropriate action. Recent advancements employ deep learning and reinforcement learning to formulate effective questions, yet these methods face challenges in developing proficient recurrent neural questioners because of the difficulty in modeling sequential sentence generation. Inspired by theory of mind, we introduce \"Answerer in Questioner's Mind\" (AQM), an innovative information-theoretic approach for goal-oriented dialogue. AQM enables the questioner to pose inquiries and draw conclusions using an estimated probabilistic representation of the answerer. By explicitly computing the information gain of potential intentions and responses for each question, the questioner deduces the answerer's intent and selects the most suitable query. Our framework is evaluated on two goal-oriented visual dialogue tasks: \"MNIST Counting Dialog\" and \"GuessWhat?!\", where AQM demonstrates substantial performance improvements over competing methods.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1338", "problem_id": "13380001", "content": "The problem of determining an optimal correspondence between two object sets is a pervasive issue that emerges in various applications, such as natural language processing and computer vision, where it is used to match 'bag-of-words' representations. Typically, solving this assignment problem entails a cubic time complexity, and the pairwise computation can be prohibitively expensive for large datasets. This paper presents an algorithm that achieves linear time complexity for finding an optimal assignment when the cost function between objects is defined by a tree-based distance metric. By leveraging this approach, we approximate the edit distance between two graphs in linear time by matching their vertices. To accomplish this, two novel tree distances are introduced: one captures discrete and structural differences between vertices, while the other enables comparison of continuous labels. The efficacy and efficiency of our proposed methods are validated using a combination of synthetic and real-world datasets, as shown in Figure A, B, C (References [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1339", "problem_id": "13390001", "content": "Recent advancements in deep learning have significantly improved face recognition capabilities; however, cross-pose face recognition continues to pose a considerable challenge. Deep learning algorithms often struggle to mitigate performance disparities resulting from variations in pose, primarily due to intra-class differences among face images with different poses and pose imbalances within training datasets. A promising and efficient approach involves learning pose-invariant features by mapping to the feature space of frontal faces. In this paper, we introduce a method for progressively transforming profile face representations towards a canonical pose, utilizing an attentive pair-wise loss. Initially, to simplify the transformation of profile face features into a frontal pose, we propose learning the feature residual between the source pose and a similar pose incrementally, thereby traversing to the feature space of a closer pose by incorporating the learned residual. Subsequently, we introduce an attentive pair-wise loss function to guide feature transformation in the most effective direction. The proposed progressive module and attentive pair-wise loss are lightweight and easy to implement, increasing the parameter count by approximately 7.5%. Experimental results on the CFP and CPLFW datasets validate the effectiveness of our proposed method. Code is available at https://github.com/hjy1312/AGPM.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1340", "problem_id": "13400001", "content": "The emergence of adversarial examples has posed a significant threat to various computer vision tasks, including object detection, yet existing attack methods are hindered by two major limitations: limited transferability, resulting in a low success rate when targeting diverse detection methods, and substantial computational costs, leading to prolonged generation times that render them impractical for video data. To overcome these limitations, this study employs a generative approach to produce adversarial images and videos, thereby reducing processing times. Furthermore, to enhance transferability, the feature maps extracted from the feature network, which typically underlie object detectors, are targeted for disruption. By leveraging the Generative Adversarial Network (GAN) framework, the proposed method combines high-level class loss and low-level feature loss to jointly train the adversarial example generator, as demonstrated in experiments on PASCAL VOC and ImageNet VID datasets, which show that the method can efficiently generate adversarial examples for images and videos with improved transferability, enabling simultaneous attacks on two representative object detection models, namely proposal-based models like Faster-RCNN and regression-based models like SSD.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1341", "problem_id": "13410001", "content": "Deep reinforcement learning (RL) has demonstrated effectiveness in addressing complex environments, though it typically demands extensive sampling and computational resources. Progress in RL also necessitates adaptable tools that facilitate rapid prototyping of novel approaches while maintaining efficient experimentation cycles. To address these needs, we introduce PyTorchRL, a modular RL library built on PyTorch, enabling the assembly of agents from reusable and extensible components. The framework also supports flexible distributed training architectures, decoupled from agent-specific elements. Together, these capabilities streamline the implementation and evaluation of new concepts, enhancing research efficiency and enabling the exploration of more demanding RL tasks. We illustrate PyTorchRL’s utility through diverse applications and demonstrate its effectiveness by achieving state-of-the-art test performance in the Obstacle Tower Unity3D challenge.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1342", "problem_id": "13420001", "content": "Boosted decision trees generally provide strong accuracy, precision, and ROC area. Nonetheless, the posterior probabilities generated from boosting are often poorly calibrated, leading to suboptimal squared error and cross-entropy measures. We empirically illustrate the reasons behind AdaBoost's tendency to produce biased probabilities and evaluate three calibration techniques to address this issue: Platt Scaling, Isotonic Regression, and Logistic Correction. Additionally, we test the approach of utilizing log-loss for boosting instead of the conventional exponential loss. Our findings indicate that Logistic Correction and boosting with log-loss are effective for enhancing weak models like decision stumps, yet they perform inadequately with more complex models such as full decision trees. In contrast, Platt Scaling and Isotonic Regression substantially enhance the accuracy of the predicted probabilities.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1343", "problem_id": "13430001", "content": "Effective image inpainting necessitates the realistic reconstruction of damaged image areas. Current methods typically address this by either replicating image segments or creating semantically consistent patches based on the surrounding context, often overlooking the crucial need for both visual and semantic credibility. This paper introduces a Pyramid-context ENcoder Network (PEN-Net), a deep generative model for image inpainting. PEN-Net utilizes a U-Net architecture to reconstruct images by encoding contextual semantics from full-resolution input and decoding the acquired semantic features back into image form. A key component is the pyramid-context encoder, which iteratively learns region affinity through attention mechanisms applied to a high-level semantic feature map, subsequently transferring this learned attention to preceding low-level feature maps. By filling in missing content through attention transfer in a pyramid-like manner, the approach ensures both visual and semantic coherence in the inpainted image. Furthermore, the paper presents a multi-scale decoder incorporating deeply-supervised pyramid losses and an adversarial loss, facilitating rapid training convergence and yielding more realistic results during testing. Comprehensive experiments across multiple datasets demonstrate the proposed network's superior performance.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1344", "problem_id": "13440001", "content": "The ubiquitous presence of glass-like objects, such as windows, bottles, and mirrors, in real-world environments presents a significant challenge for sensing and segmentation tasks, which have numerous applications in areas like robot navigation and grasping. The complexity of arbitrary scenes behind these objects exacerbates the difficulty of this task. To address the problem of segmenting glass-like objects, this paper proposes an enhanced boundary learning approach. A novel refined differential module is introduced to generate more precise boundary cues, while an edge-aware point-based graph convolution network module is designed to capture the global shape representation along the boundary. These lightweight and effective modules can be integrated into various segmentation models to improve their performance. By leveraging these modules, a decoder is designed to produce accurate segmentation results, particularly at the boundary. The efficacy of this approach is demonstrated through extensive experiments on three glass-like object segmentation datasets, namely Trans10k, MSD, and GDD, where it achieves state-of-the-art performance. Furthermore, the generality and superiority of this approach are validated against recent methods on three general segmentation datasets, including Cityscapes, BDD, and COCO Stuff, with code and models available at (\\url).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1345", "problem_id": "13450001", "content": "This study explores the challenge of optimal signal detection, focusing on large-scale multiple-input multiple-output (MIMO) systems. Given the NP-hard nature of this problem, an optimal solution necessitates identifying the shortest path within a decision tree. However, current optimal search algorithms often exhibit excessive computational demands, rendering them impractical for extensive MIMO systems. To mitigate this, we introduce a hyperaccelerated tree search (HATS) algorithm, a novel heuristic search method. This algorithm leverages a deep neural network (DNN) to approximate the optimal heuristic, which is then used to accelerate the underlying memory-constrained search algorithm. This approach is rooted in the principle that the underlying heuristic search algorithm achieves peak efficiency when paired with the optimal heuristic function. Simulation results demonstrate that the proposed algorithm achieves bit error rate (BER) performance comparable to optimal levels in large-scale systems, while maintaining bounded memory usage. Simultaneously, it explores a minimal number of tree nodes, suggesting near-optimal efficiency in practical applications, thus making it suitable for large-scale systems. The implementation of this study is accessible at https://github.com/skypitcher/hats.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1346", "problem_id": "13460001", "content": "This study introduces a deep neural network designed for model-free forecasting of chaotic dynamical systems using noisy data. The deep learning approach focuses on estimating the conditional probability distribution of state variables, utilizing a Long Short-Term Memory (LSTM) network to capture nonlinear dynamics and a softmax layer to represent probability distributions. Training involves minimizing a regularized cross-entropy loss function. Evaluations on delay-time chaotic systems, including the Mackey-Glass and Ikeda equations, demonstrate the LSTM's effectiveness in predicting nonlinear dynamics while efficiently suppressing noise. The results reveal that the prediction uncertainty in multi-step forecasts does not follow a monotonic trend over time; instead, the standard deviation of predictions exhibits dynamic fluctuations, increasing or decreasing unpredictably.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1347", "problem_id": "13470001", "content": "This study presents a unified perspective on various machine learning problems by formulating them as the minimization of functionals defined over the space of probability measures, thereby encompassing a broad range of applications. Notably, our framework provides a common foundation for understanding generative adversarial networks, variational inference, and actor-critic methods in reinforcement learning, demonstrating their interconnectedness. Furthermore, we introduce a generalized optimization technique, termed probability functional descent (PFD), which is applicable to our formulation, and illustrate how this approach can recover existing methods that were previously developed in isolation within the aforementioned contexts.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1348", "problem_id": "13480001", "content": "Dynamic textures, image sequences displaying temporal regularities, find widespread use in computer vision and graphics. A significant challenge lies in generating high-quality, perceptually similar samples from a given dynamic texture exemplar. While many methods have addressed this issue, none perform uniformly well across all dynamic texture types. This paper explores the synthesizability of dynamic texture samples. To achieve this, we introduce a method that learns regression models to correlate dynamic texture samples with synthesizability scores, leveraging a dataset annotated for synthesizability. Specifically, we define dynamic texture sample synthesizability and characterize samples using spatiotemporal features. Utilizing these features and the annotated dataset, we train regression models to predict synthesizability scores and learn classifiers to select the most appropriate EDTS methods. A hierarchical scheme is then used to refine the selection, partition, and synthesizability prediction of dynamic texture samples. Finally, the learned synthesizability is applied to detect synthesizable regions in videos. Experimental results validate the effectiveness of our method in learning and predicting dynamic texture sample synthesizability.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1349", "problem_id": "13490001", "content": "The efficient completion of design cycles for intricate systems, such as consumer electronics and hypersonic vehicles, is contingent upon swift simulation-based prototyping, which often entails navigating complex, high-dimensional spaces characterized by correlated control variables and quantities of interest that exhibit non-Gaussian and potentially multimodal distributions. To address this challenge, we propose a global sensitivity analysis (GSA) framework that is both model-agnostic and moment-independent, leveraging differential mutual information to assess the impact of control variables on quantities of interest. By integrating a deep neural network surrogate to replace computationally intensive components of the physics-based model, we satisfy the data requirements of this information-theoretic approach to GSA, enabling the explanation of network predictions and the deployment of the surrogate to facilitate design loop closure. This framework, which can be viewed as an uncertainty quantification method for interrogating the surrogate, is compatible with a diverse range of black-box models, as demonstrated by its application to energy storage, where it yields meaningful and discernible rankings, ultimately providing an \"outer loop\" for accelerated product design by identifying sensitive input directions and performing optimization over reduced parameter subspaces.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1350", "problem_id": "13500001", "content": "This study contributes to the existing body of research on data-driven detection of bid-rigging cartels by introducing a novel deep learning-based approach, which identifies cartel participants by analyzing their pairwise bidding interactions with other firms. Specifically, the method leverages a convolutional neural network, typically used for image recognition, in conjunction with graphs that plot the normalized bid values of a reference firm against those of other participating firms in the same tenders. Using procurement data from Japan and Switzerland, graphs are constructed for both collusive and competitive episodes, with a subset used to train the neural network to distinguish between collusive and competitive bidding patterns, and the remaining graphs used to evaluate its out-of-sample performance. The results show a high average accuracy of around 90% when applying the method to data from either country or a combined dataset, although predictive performance decreases when training on data from one country and testing on the other, likely due to differences in procurement procedures. Nevertheless, the high accuracy achieved with a relatively small sample of graphs highlights the significant potential of deep learning approaches in detecting and combating bid-rigging cartels, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1351", "problem_id": "13510001", "content": "Visual attention mechanisms play a crucial role in neural network architectures for computer vision, enabling models to concentrate on specific objects or regions within an image to extract meaningful features and enhance representation learning. Recent advancements have introduced continuous-domain attention models that leverage the inherent continuity of images, though these often rely on unimodal density functions like Gaussians, limiting their effectiveness for complex or disjoint regions of interest. This paper presents a novel continuous attention approach that generates multimodal densities using Gaussian mixtures, employing the EM algorithm to cluster relevant image regions and a description length penalty to determine the optimal number of mixture components. The resulting densities are expressed as linear combinations of unimodal attention mechanisms, facilitating efficient Jacobian computation during backpropagation. Evaluations on the VQA-v2 dataset demonstrate competitive performance in visual question answering, with attention patterns that align more closely with human focus in VQA-HAT. Additionally, our model produces more interpretable multimodal attention maps, effectively distinguishing objects from backgrounds in intricate scenes.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1352", "problem_id": "13520001", "content": "Numerous classification tasks involve structured data, often represented as graphs, which can also evolve dynamically with changes in vertices and edges over time. This study aims to leverage both structural and temporal information by employing a neural network-based approach, a combination not previously explored in existing architectures. We introduce two innovative methods that integrate Long Short-Term Memory networks and Graph Convolutional Networks to capture long-term dependencies alongside graph topology. The effectiveness of our techniques is demonstrated by their encouraging experimental outcomes.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1353", "problem_id": "13530001", "content": "Normalizing Flows (NFs) effectively model complex distributions p(y) characterized by significant inter-dimensional correlations and high multimodality by leveraging an invertible neural network to transform a simple base density p(z) according to the change of variables formula. This characteristic is advantageous in multivariate structured prediction, where traditional per-pixel loss methods fail to adequately represent strong output dimension correlations. We investigate conditional normalizing flows (CNFs), a subset of NFs that model conditional densities p(y|x) by conditioning the base density to output space mapping on an input x. CNFs offer efficient sampling and inference, can be trained using a likelihood-based objective, and, as generative flows, are immune to mode collapse and training instability. We introduce a practical approach for training continuous CNFs for binary problems and demonstrate their application in super-resolution and vessel segmentation, achieving competitive results on standard benchmark datasets based on both likelihood and conventional metrics.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1354", "problem_id": "13540001", "content": "Fueled by advancements in sequence modeling, particularly the revitalization of Long-Short Term Memory networks (LSTMs) and the emergence of Gated Recurrent Units (GRUs), video captioning and summarization have gained considerable traction. Current methodologies typically employ CNNs to extract spatio-temporal features and leverage GRUs or LSTMs, complemented by soft attention mechanisms, to capture dependencies. While these attention layers enhance the focus on salient features and improve recurrent unit performance, the underlying limitations of the recurrent units persist. The advent of the Transformer model has significantly reshaped the sequence modeling landscape. In this study, we develop a Transformer-based model for video captioning, employing 3D CNN architectures such as C3D and Two-stream I3D for video feature extraction. We also implement dimensionality reduction strategies to manage the overall model size. The results of our experiments on the MSVD and ActivityNet datasets for Single and Dense video captioning tasks are presented.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1355", "problem_id": "13550001", "content": "This paper details the vision-guided robotic picking system engineered by Team Applied Robotics for the Amazon Picking Challenge 2016. The competition required the development of a robotic system capable of picking diverse products from shelves or totes. Our discussion encompasses design choices, strategic approaches, the high-resolution 3D vision system implemented, the integration of texture and shape-based object detection algorithms, robot path planning methodologies, and the object manipulators that were created.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1356", "problem_id": "13560001", "content": "The capability of generative adversarial networks (GANs) to learn and represent various distributions is still not fully understood, despite being touted as \"universal distribution learners\". Heavy-tailed distributions, which are commonly found in fields such as financial risk assessment, physics, and epidemiology, pose a particular challenge. Current GAN architectures struggle to replicate the asymptotic behavior of these distributions, which can be attributed to their inherent design. Furthermore, the infinite moments and large distances between outlier points characteristic of heavy-tailed distributions lead to unstable or near-zero gradients when using conventional loss functions. To overcome these limitations, we introduce the Pareto GAN, which utilizes extreme value theory and the functional properties of neural networks to learn distributions that match the asymptotic behavior of marginal feature distributions. We also identify issues with standard loss functions and propose alternative metric spaces to facilitate stable and efficient learning, with our approach being evaluated on various heavy-tailed datasets, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1357", "problem_id": "13570001", "content": "Various pooling filters for multiple instance learning (MIL) are utilized in MIL models. This study examines how different MIL pooling filters influence the effectiveness of MIL models in practical MIL scenarios. We developed a neural network-based MIL framework featuring five distinct MIL pooling filters: `max', `mean', `attention', `distribution', and `distribution with attention'. Additionally, we created five MIL tasks based on a real-world dataset of lymph node metastases. Our findings indicate that the performance of our framework varies with different pooling filters for each task and also differs across tasks. Therefore, choosing the appropriate MIL pooling filter for each specific task is essential for improved performance. Notably, models employing `distribution' and `distribution with attention' pooling filters generally achieved superior results across nearly all tasks, a trend we attribute to the comprehensive information captured by the `distribution' based filters. In contrast, point estimate based filters like `max' and `mean' yield only point estimates of distributions while `distribution' based filters encapsulate full distributional information. Finally, we compared the performance of our neural network model with the `distribution' pooling filter against the best existing MIL methods on classical MIL datasets, with our model outperforming them.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1358", "problem_id": "13580001", "content": "Visual Analytics (VA) tools and methodologies have proven to be vital in assisting users in enhancing classification models, interpreting model outcomes, and auditing results. Recently, VA has been employed to convert classification models from predictive to descriptive tools. The goal is to utilize these models as proxies for data patterns, enabling visualization to grasp the phenomena represented by the data. While existing approaches have been beneficial and motivating, they often rely on low-complexity classification models to facilitate easier interpretation, which limits their ability to capture complex data patterns. This paper introduces VAX (multiVariate dAta eXplanation), a novel VA technique designed to aid in recognizing and visually interpreting patterns within multivariate datasets. In contrast to current similar methods, VAX leverages the concept of Jumping Emerging Patterns to pinpoint and merge various patterns, generating explanations through logical combinations of data variables. The effectiveness of VAX in interpreting intricate multivariate datasets is illustrated through case studies utilizing two real-world datasets that encompass diverse scenarios.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1359", "problem_id": "13590001", "content": "Corporate credit ratings indicate the level of creditworthiness of organizations and are vital for effective financial risk management. However, credit rating data in practice often exhibits long-tail distributions, which pose significant challenges due to class imbalance in the corporate credit rating system. To address this issue, we introduce a new framework called Contrastive Pre-training for Corporate Credit Rating (CP4CCR), inspired by recent advancements in pre-training methods within self-supervised representation learning, aimed at overcoming class imbalance. In the initial phase, we implement contrastive self-supervised pre-training without label data to achieve a robust class-agnostic initialization. This phase includes the development of two self-supervised tasks within CP4CCR: (i) Feature Masking (FM) and (ii) Feature Swapping (FS). In the subsequent phase, we can train any conventional corporate credit rating model using the pre-trained network as a starting point. Comprehensive experiments on the dataset of publicly listed Chinese corporations demonstrate that CP4CCR enhances the performance of standard corporate credit rating models, particularly for classes with limited samples.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1360", "problem_id": "13600001", "content": "The advantages of employing the natural gradient are widely recognized across various optimization tasks. Nonetheless, its practical use in training standard neural networks is constrained by the associated rise in computational complexity. Helmholtz Machines, a specific class of generative models, consist of two Sigmoid Belief Networks (SBNs) functioning as an encoder and a decoder, typically trained with the Wake-Sleep (WS) algorithm and its refined variant, RWS. Research has demonstrated that the localized connections in SBNs' graphical architecture create sparsity in the Fisher information matrix. This sparsity leads to a block diagonal structure, which can be leveraged to efficiently lower the computational cost of inverting the Fisher matrix, enabling exact natural gradient computation without approximations. We propose geometric modifications to established methods, developing the Natural Wake-Sleep (NWS) and Natural Reweighted Wake-Sleep (NRWS) algorithms. Our experimental evaluation of these new geometric approaches focuses on convergence speed and log-likelihood values, considering both iteration count and time complexity, and shows enhancements over their non-geometric counterparts.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1361", "problem_id": "13610001", "content": "Contemporary time-series datasets often encompass numerous output response variables measured over extended durations. For instance, in neuroscience, the activity of hundreds or thousands of neurons is recorded during specific behaviors and in response to sensory inputs. Multi-output Gaussian process models utilize the nonparametric properties of Gaussian processes to model the relationships between multiple outputs. Nevertheless, these models commonly assume that the correlations among output response variables remain constant across the input space. Stochastic linear mixing models (SLMM) offer greater flexibility by allowing mixture coefficients to vary with input, enabling them to effectively capture intricate output dependencies. However, current inference methods for SLMMs are computationally prohibitive for large datasets, limiting their applicability to many modern time-series challenges. This paper introduces a novel regression framework, the orthogonal stochastic linear mixing model (OSLMM), which incorporates an orthogonal constraint among the mixing coefficients. This constraint alleviates the computational demands of inference while preserving the capacity to model complex output dependencies. We present Markov chain Monte Carlo inference procedures for both SLMM and OSLMM and empirically demonstrate the enhanced scalability and reduced prediction error of OSLMM compared to state-of-the-art techniques across several real-world applications. Utilizing neurophysiological recordings, we employ the inferred latent functions for concise visualization of population responses to auditory stimuli, achieving superior results compared to a competing method (GPFA). These findings collectively suggest that OSLMM will be valuable for analyzing diverse, large-scale time-series datasets.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1362", "problem_id": "13620001", "content": "The healthcare industry is distinct from other sectors, being a high-priority area where individuals demand the utmost quality of care and services, irrespective of costs. Despite consuming a significant portion of the budget, it has not met societal expectations. Typically, medical data interpretation is carried out by healthcare professionals; however, this process is limited because of subjectivity, the complex nature of images, considerable variability among different interpreters, and fatigue. Following the success of deep learning in various real-world applications, it has emerged as a promising approach offering accurate solutions for medical imaging, potentially revolutionizing the future of healthcare practices. This chapter explores the advanced deep learning architectures and their enhancements utilized for medical image segmentation and classification. In the final section, we address the challenges faced by deep learning techniques in medical imaging and highlight ongoing research issues.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1363", "problem_id": "13630001", "content": "Deep neural networks have attained leading performance levels across various applications, including computer vision, speech processing, and language translation. Nevertheless, memory and computational constraints limit the deployment of these advanced models on many devices. To overcome this challenge, we introduce SQuantizer, an innovative training technique that simultaneously optimizes neural networks for sparsity and reduced precision while preserving accuracy and achieving significant compression. By integrating sparsification and low-bit quantization in a single training phase, our method follows an order proven to be most effective. Experiments show that SQuantizer maintains top-tier accuracy with 4-bit and 2-bit precision for ResNet18, MobileNet-v2, and ResNet50, even under substantial sparsity. The approach yields compression ratios of 18x for ResNet18, 17x for ResNet50, and 9x for MobileNet-v2, with minimal accuracy drops of 1% and 2% for ResNets and MobileNet-v2, respectively. When applied to object detection, our method also maintains strong performance on YOLO-v3. Furthermore, the single-pass training enables efficient prototyping and neural architecture search. Our findings also provide valuable insights into the trade-offs between sparsity and quantization.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1364", "problem_id": "13640001", "content": "This research investigates generalization bounds for metric learning, where data embeddings are generated by neural networks. We establish uniform generalization guarantees for two scenarios: a sparse regime and a non-sparse regime we refer to as . The sparse regime applies when the parameters exhibit small \\ell_1-type norms, achievable through suitable regularization, analogous to classification tasks. In contrast, unregularized SGD optimization of metric learning losses generally yields non-sparse solutions. However, we demonstrate that despite this absence of sparsity, novel properties of these solutions enable dimension-free generalization guarantees, offering explanations for real-world non-sparse experimental settings. The observed phenomena are validated using the MNIST and 20newsgroups datasets.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1365", "problem_id": "13650001", "content": "Achieving high sample efficiency is crucial when tuning policy parameters for robotic controllers. This study compares the performance of two leading policy optimization techniques: Deep Deterministic Policy Gradient (DDPG), a contemporary deep reinforcement learning actor-critic method known for its effectiveness across diverse control tasks, and Covariance Matrix Adaptation Evolution Strategy (CMA-ES), a direct policy search and black-box optimization algorithm commonly employed in robot learning. We assess these algorithms on a continuous variant of the mountain car benchmark to analyze and compare their sample complexities. Initial analysis leads us to anticipate that DDPG will exhibit greater sample efficiency compared to CMA-ES, a prediction supported by our experimental findings.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1366", "problem_id": "13660001", "content": "In recent years, deep reinforcement learning has achieved notable success in complex single-agent environments, with growing applications in multi-agent settings. This paper introduces MAGNet, a new multi-agent reinforcement learning method that employs a self-attention mechanism to construct a relevance graph of the environment and incorporates a message-generation strategy. When tested on the synthetic predator-prey scenario and the Pommerman game, MAGNet demonstrates superior performance compared to leading MARL techniques such as Multi-agent Deep Q-Networks (MADQN), Multi-agent Deep Deterministic Policy Gradient (MADDPG), and QMIX.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1367", "problem_id": "13670001", "content": "The utilization of mobile and augmented reality platforms to present Wikipedia content can foster innovative modes of interaction within urban-centric user communities, thereby enhancing learning, communication, and knowledge sharing. To achieve this goal, this study develops a mobile application capable of identifying notable sites listed on Wikipedia, leveraging a deep neural network trained on crowd-sourced images of points of interest, including buildings, statues, museums, and other physical entities visible in urban settings. An end-to-end pipeline is outlined, encompassing data collection, model training, and application evaluation in both online and real-world contexts, as seen in Figure A, B, C. The site recognition task poses several challenges, stemming from visual similarities between classified sites and noise introduced by the surrounding urban environment, as discussed in References [citation]. However, it is shown that incorporating mobile contextual information, such as user location, orientation, and attention patterns, can substantially mitigate these challenges. Furthermore, an unsupervised learning technique is introduced to de-noise crowd-sourced imagery, resulting in improved classification performance, as demonstrated in the evaluation, which is supported by citations.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1368", "problem_id": "13680001", "content": "Recent advancements in electrocardiogram (ECG) diagnosis systems, driven by deep neural networks (DNNs), have shown potential in automating laborious tasks traditionally performed by cardiologists. Nevertheless, a thorough examination of their susceptibility to adversarial attacks remains insufficient. Existing attack methods developed for image data are not directly transferable due to the unique visual and dynamic characteristics of ECGs. In this study, we conduct a comprehensive investigation into adversarial attacks targeting DNN-based ECG diagnosis systems. We leverage the specific characteristics of ECGs to develop effective attack strategies within two distinct attack models. The findings reveal vulnerabilities in DNN-powered diagnosis systems when subjected to adversarial attacks, emphasizing the necessity for appropriate defensive measures.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1369", "problem_id": "13690001", "content": "The processing of vast amounts of data is a fundamental requirement in contemporary computer vision, applicable to both the training phase and the inference stage after model deployment. The increasing prevalence of scenarios where image capture and processing occur in distinct physical locations, such as in autonomous vehicles and cloud computing, underscores the need for efficient data management. Furthermore, devices often face constraints related to data storage and transmission, including limited storage capacity and channel bandwidth. In such contexts, lossy image compression emerges as a vital strategy to enhance data collection within these constraints. However, this compression method inherently leads to data degradation, potentially compromising the performance of subsequent analysis tasks due to the loss of crucial semantic information. Additionally, discrepancies between training and inference conditions, such as using compressed images during training but original images during inference, can result in covariate shift, further impacting model performance. This study investigates this phenomenon, with a particular emphasis on autonomous driving as a representative scenario, and observes that both semantic information loss and covariate shift contribute to performance degradation, which is influenced by the compression rate. To mitigate this issue, we introduce a dataset restoration approach leveraging image restoration techniques based on generative adversarial networks (GANs), which is adaptable to various image compression methods and downstream tasks without incurring additional costs, making it suitable for resource-constrained devices. Our experiments, focusing on semantic segmentation as a challenging application, demonstrate the effectiveness of our method in alleviating the adverse effects of compression on downstream visual tasks across a range of compression rates and diverse datasets, as shown in Figure A, B, C (see References [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1370", "problem_id": "13700001", "content": "Accurate analysis of customer flow in retail environments is crucial for retailers to gain insights into customer behavior and inform decision-making processes. Despite its importance, the development of innovative technologies for automated people counting has received relatively little attention. To address this, we propose LRCN-RetailNet, a recurrent neural network architecture that learns non-linear regression models to predict people counts from video footage captured by low-cost surveillance cameras, utilizing the RGBP image format which combines color and foreground information. Our architecture effectively considers both spatial features extracted from RGBP images via convolutional layers and temporal coherence through recurrent layers. Through supervised learning, our trained models achieve high accuracy in predicting people counts, and we demonstrate that a simple modification can exclude salespeople from the count. Extensive experiments validate and evaluate the proposed architecture, showing that LRCN-RetailNet significantly outperforms the previous RetailNet architecture and a state-of-the-art object detection neural network, with computational performance experiments confirming the methodology's ability to estimate people counts in real-time, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1371", "problem_id": "13710001", "content": "We develop a framework that initiates visual representation learning through a basic visual grouping mechanism. This grouping process is implemented using a contour detector to divide images into segments, which are then combined into a hierarchical tree structure. Training this foundational grouping method requires only a limited labeled dataset. On a vast collection of unlabeled images, we employ this learned mechanism to autonomously generate hierarchical region predictions. These predictions guide a self-supervised contrastive learning approach, where a deep network learns pixel-level embeddings that align with the predicted region hierarchy. Experimental results show that our method achieves cutting-edge performance as a general pre-training technique, enhancing various downstream applications. We also investigate its utility in semantic region retrieval and object tracking in videos.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1372", "problem_id": "13720001", "content": "Contemporary Bayesian inference applications often employ highly complex models, making their posterior distributions analytically intractable and necessitating approximation techniques. While Markov chain Monte Carlo remains the predominant approach, its computational demands for large datasets or intricate models have spurred growing interest in more efficient variational alternatives. Conventional variational techniques, which minimize the Kullback-Leibler divergence between the posterior and a simplified parametric distribution, offer precise estimates of posterior means but frequently fail to capture higher-order moments and face constraints regarding applicable model types. This work introduces variational approximations through Fisher divergence minimization, accompanied by an effective computational framework that accommodates diverse models without requiring conjugacy or restrictive mean-field assumptions. The method's enhanced performance is illustrated through logistic regression benchmark tests.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1373", "problem_id": "13730001", "content": "This paper presents a novel approach to generating realistic talking face videos, wherein an input audio signal and a brief reference video clip are utilized to synthesize a photorealistic video of the target face, complete with natural lip movements, head poses, and eye blinks that are synchronized with the audio input. The proposed method accounts for both explicit face attributes, such as lip motions that are highly correlated with speech, and implicit attributes, including head poses and eye blinks that exhibit weaker correlations with the input audio. To capture the complex relationships between these face attributes and the audio input, a FACe Implicit Attribute Learning Generative Adversarial Network (FACIAL-GAN) is introduced, which incorporates phonetics-aware, context-aware, and identity-aware information to produce 3D face animations with realistic lip, head, and eye movements. The rendered face images and eye blink attention maps are then processed by the Rendering-to-Video network to generate photorealistic video frames. As demonstrated by experimental results and user studies, the proposed method yields high-quality talking face videos with synchronized lip motions, natural head movements, and eye blinks, outperforming state-of-the-art methods in terms of output quality, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1374", "problem_id": "13740001", "content": "Generative models and inferential autoencoders typically employ the \\ell_2 norm in their optimization objectives. This brief study explores the theoretical application of the Structural Similarity Index (SSIM) to enhance perceptual image quality in these models. We begin by examining SSIM, its associated distance metrics, and the SSIM kernel, demonstrating its universality and suitability for unconditional and conditional moment matching networks. Additionally, we outline the integration of SSIM distance into variational and adversarial autoencoders, as well as unconditional and conditional Generative Adversarial Networks (GANs). Lastly, we advocate replacing the \\ell_2 norm with SSIM distance in least squares GANs for improved performance.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1375", "problem_id": "13750001", "content": "This study introduces a innovative generative framework for synthesizing fluid simulations utilizing a condensed set of parameters, leveraging a convolutional neural network trained on a dataset of discrete, parameterizable fluid simulation velocity fields. The deep learning architecture's ability to extract representative features from the data enables the model to accurately replicate the training dataset while generating realistic intermediate simulations. A custom-designed loss function ensures the model produces divergence-free velocity fields at all times, optimized for fluid dynamics. Furthermore, the proposed approach can effectively manage complex parameterizations in reduced dimensional spaces and advance simulations temporally by integrating a secondary network in the latent space. The resulting model can capture a broad range of fluid behaviors, facilitating various applications including rapid simulation construction, interpolation of fluids with diverse parameters, time re-sampling, latent space simulations, and compression of fluid simulation data, with reconstructed velocity fields generated up to 700x faster than re-simulating the data with the underlying CPU solver, achieving compression rates of up to 1300x.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1376", "problem_id": "13760001", "content": "Extracting meaningful graphs from data is crucial for numerous data mining and machine learning applications, including data representation and analysis, dimensionality reduction, clustering, and visualization. In this study, we introduce GRASPEL, a highly scalable spectral methodology for learning large graphs from data for the first time. By constraining the precision matrix to function as a graph Laplacian, our method focuses on estimating ultra-sparse (tree-like) weighted undirected graphs, establishing a clear link to the previously established graphical Lasso technique. By integrating the most recent high-performance nearly-linear time spectral methods for graph sparsification, coarsening, and embedding, we can learn ultra-sparse yet spectrally robust graphs by identifying and incorporating the most spectrally significant edges. When compared to previous leading graph learning methods, GRASPEL demonstrates enhanced scalability, significantly boosting computational efficiency and the quality of solutions across various data mining and machine learning tasks, including spectral clustering (SC) and t-Distributed Stochastic Neighbor Embedding (t-SNE). Notably, in comparisons with graphs generated by existing techniques, GRASPEL demonstrated superior efficiency and accuracy in spectral clustering.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1377", "problem_id": "13770001", "content": "In recent years, kernel-based sparse coding (K-SRC) has garnered significant interest for its effective representation of nonlinear data structures within the feature space. However, current K-SRC techniques are hindered by inconsistencies between their training and testing optimization frameworks. In this paper, we introduce a novel confident K-SRC and dictionary learning algorithm (CKSC), which emphasizes the discriminative reconstruction of data based on its representation within the kernel space. CKSC reconstructs each data sample through weighted contributions that are confident in its respective class. We integrate novel discriminative elements to apply this approach to both training and testing frameworks within our algorithm. This particular design enhances the consistency of the optimization processes and boosts the discriminative performance during the recall phase. Furthermore, CKSC utilizes supervised information within its dictionary learning framework to improve the discriminative properties of the dictionary. For empirical validation, we apply our CKSC algorithm to multivariate time-series benchmarks such as DynTex++ and UTKinect. Our assertions regarding the algorithm's superior performance are supported by comparisons of its classification outcomes with those of leading K-SRC algorithms.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1378", "problem_id": "13780001", "content": "This study presents a novel approach to nonlinear dictionary learning for histograms within the probability simplex, utilizing principles from optimal transport theory. The technique reconstructs histograms through displacement interpolations (or Wasserstein barycenters) between dictionary atoms, which are synthetic histograms in the same space. The method jointly optimizes these atoms and the weight vectors that best reconstruct each datapoint as a barycenter of the atoms. Computational efficiency is achieved via entropic regularization of the optimal transport problem, resulting in a differentiable, parallelizable, and efficient approximation scheme. Both atoms and weights are learned through gradient-based optimization, with gradients computed using automatic differentiation of generalized Sinkhorn iterations for entropically smoothed barycenters. By employing Wasserstein barycenters instead of conventional matrix products, the method enables nonlinear interactions between atoms and input data reconstructions. Its effectiveness is demonstrated across various image processing applications.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1379", "problem_id": "13790001", "content": "Link prediction is a fundamental task in graph analysis. Drawing inspiration from conventional path-based techniques, this paper introduces a versatile representation learning framework rooted in paths for link prediction. We specifically characterize the representation of a node pair as the generalized sum of all path representations, where each path representation is derived as the generalized product of the edge representations along that path. Guided by the Bellman-Ford algorithm, which addresses the shortest path problem, we demonstrate that the proposed path formulation can be efficiently resolved using the generalized Bellman-Ford algorithm. To enhance the path formulation's effectiveness, we introduce the Neural Bellman-Ford Network (NBFNet), a comprehensive graph neural network framework that tackles this formulation by utilizing learned operators within the generalized Bellman-Ford algorithm. The NBFNet incorporates three neural components—INDICATOR, MESSAGE, and AGGREGATE functions—that correspond to the boundary condition, multiplication operator, and summation operator, respectively. Highly adaptable, the NBFNet encompasses various traditional path-based methods and is applicable to both homogeneous and multi-relational graphs (such as knowledge graphs) in transductive and inductive contexts. Experimental results conducted on both homogeneous and knowledge graphs indicate that the NBFNet significantly outperforms contemporary methods in both transductive and inductive settings, achieving state-of-the-art performance.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1380", "problem_id": "13800001", "content": "This paper introduces an automated face verification system modeled after principles observed in biological systems. The presented algorithm employs a Fourier-Bessel Transform (FBT) to convert the entire image from the spatial domain to the polar frequency domain. A comparison is made between using the entire image versus focusing on localized face image regions (local analysis). The generated representations are then positioned within a dissimilarity space, where each image is characterized by its distance from all other images, and a Pseudo-Fisher discriminator is constructed. Verification tests conducted on the FERET database demonstrated that the local-based algorithm achieved superior performance compared to the global-FBT version. Furthermore, the local-FBT algorithm exhibited state-of-the-art performance under various testing conditions, suggesting its robustness to variations in expression, age, and illumination. The system's resilience to significant occlusion was also assessed, revealing high robustness even with up to 50% face occlusion. The verification system was fully automated through the implementation of face and eye detection algorithms. Under these conditions, the local approach showed only a marginal improvement over the global approach.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1381", "problem_id": "13810001", "content": "The increasing prevalence of embedded devices has spurred significant interest in deep learning inference on these platforms. However, realizing the potential of this area requires addressing substantial obstacles, primarily due to the severe resource limitations of embedded processors, which offer significantly less compute power, memory, and power compared to mobile processors. Consequently, machine learning models and inference frameworks must be highly efficient and operate within extremely small memory footprints. Furthermore, the diverse embedded ecosystem lacks uniformity, with vendors often omitting features like dynamic memory allocation and virtual memory that are standard in mainstream systems, hindering cross-platform compatibility, and resulting in a variety of hardware configurations. To address these challenges, we present TensorFlow Lite Micro (TF Micro), an open-source machine learning inference framework designed for executing deep-learning models on embedded systems. TF Micro confronts both the efficiency demands imposed by resource constraints and the fragmentation issues that impede cross-platform interoperability. The framework employs a distinctive interpreter-based strategy to achieve flexibility while overcoming these limitations. This paper details the design choices and implementation aspects of TF Micro. Finally, an evaluation is presented to highlight its low resource demands and minimal runtime performance overhead.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1382", "problem_id": "13820001", "content": "The process of exploration plays a vital role in reinforcement learning algorithms, as it enables agents to acquire knowledge and develop control over complex, often stochastic environments. Effective learning in reinforcement learning relies heavily on the agent's ability to explore its environment, as insufficient information can significantly impede the learning process. This article presents a comprehensive overview of contemporary exploration techniques in the context of (Sequential) reinforcement learning, accompanied by a systematic classification of these methods.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1383", "problem_id": "13830001", "content": "Facial recognition is a primary method for identifying individuals; however, the aging process introduces numerous variables, including temporal effects, attributes, environmental conditions, and individual-specific changes. The influence of these factors has not been thoroughly investigated in face aging research. This paper introduces a novel comprehensive model, \"Face Age progression With Attribute Manipulation (FAWAM),\" designed to generate facial images across different ages while concurrently modifying attributes and other individual-specific features. Our approach tackles this challenge through two submodules: face age progression and face attribute manipulation. For age progression, we employ an attribute-conscious face aging model using a pyramidal generative adversarial network, enabling the modeling of age-related facial changes while preserving inherent individual characteristics. For attribute manipulation, we utilize an attribute generative adversarial network architecture to modify the age-processed facial image with desired attributes while keeping other details constant. Extensive evaluations on standard large scale datasets demonstrate that our model achieves significant quantitative and qualitative improvements.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1384", "problem_id": "13840001", "content": "We introduce a novel approach for relocalization in large-scale point clouds, combining global place recognition and local 6DoF pose refinement into a unified framework. This is achieved through a Siamese network that simultaneously learns to detect and describe 3D local features directly from raw 3D point data, leveraging FlexConv and Squeeze-and-Excitation (SE) to capture both multi-level geometric information and channel-wise relationships. The network predicts the discriminativeness of local descriptors in an unsupervised manner for 3D keypoint detection, and generates a global descriptor by aggregating local descriptors using an effective attention mechanism, allowing for the inference of both local and global 3D descriptors in a single forward pass. Our method yields competitive results in experiments on various benchmarks for both global point cloud retrieval and local point cloud registration, outperforming state-of-the-art approaches, as shown in Figure A, B, C (References [1], [2], [3]). Furthermore, we demonstrate the generalizability and robustness of our 3D keypoints by achieving favorable performance on the registration of point clouds generated by a visual SLAM system without fine-tuning, and provide code and related materials at https://vision.in.tum.de/research/vslam/dh3d (citations [4], [5]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1385", "problem_id": "13850001", "content": "Sequence labeling is frequently employed for intricate textual information extraction, utilizing probabilistic inference within a graphical model to ensure the consistency of locally extracted fields via constrained transitions. Current methods commonly use recurrent neural networks (RNNs), like LSTM, to generate detailed local features for these models, while enforcing output consistency through a simplified linear-chain model that captures Markovian dependencies. Nevertheless, this straightforward graphical model often fails to represent the complex, non-local relationships between output labels, such as restrictions on field occurrences or co-occurrences. Although RNNs have enhanced context-aware local feature extraction for sequence tagging, their integration with a comparably expressive global graphical model for the output distribution remains a challenge. Our model extends beyond the linear chain CRF by incorporating multiple hidden states per output label, and uses low-rank log-potential scoring matrices to efficiently parametrize transitions, thus learning an embedding space for hidden states. This expanded latent space of inference variables enhances the RNN's feature representation, enabling precise global inference that adheres to intricate, learned non-local output constraints. Experimental results across multiple datasets demonstrate that our model surpasses baseline CRF+RNN models when global output constraints are crucial during inference, and we also analyze the interpretable latent structure.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1386", "problem_id": "13860001", "content": "Hierarchical reinforcement learning (HRL) has emerged as a viable means of enhancing traditional reinforcement learning (RL) methods to tackle more intricate tasks. However, most existing HRL approaches necessitate meticulous, task-specific design and on-policy training, rendering them impractical for real-world applications. This paper explores the development of HRL algorithms that are both general, in that they do not impose additional assumptions beyond those of standard RL algorithms, and efficient, enabling their use with a modest number of interaction samples, thereby making them suitable for real-world problems like robotic control. To achieve generality, a scheme is proposed wherein lower-level controllers are supervised with goals that are automatically learned and proposed by higher-level controllers. For efficiency, the use of off-policy experience for both higher and lower-level training is proposed, which poses a significant challenge due to the dynamic nature of the action space for the higher-level policy as lower-level behaviors change. An off-policy correction is introduced to address this challenge, allowing the leveraging of recent advances in off-policy model-free RL to learn both higher and lower-level policies with substantially fewer environment interactions than on-policy algorithms. The resulting HRL agent, termed HIRO, exhibits general applicability and high sample efficiency. Experimental results demonstrate HIRO's capability to learn complex behaviors for simulated robots, such as manipulating objects to reach target locations, from only a few million samples, equivalent to a few days of real-time interaction, and substantially outperforming prior state-of-the-art HRL methods.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1387", "problem_id": "13870001", "content": "PointNet has transformed the way point clouds are represented, setting benchmarks in classification and segmentation tasks through its innovative approach and later developments. However, its application to point cloud registration has not yet achieved similar success. This paper proposes that PointNet can be viewed as a trainable \"imaging\" function, enabling the use of traditional image alignment techniques like the Lucas & Kanade (LK) algorithm. Key contributions include adapting the LK method for PointNet’s imaging function and integrating both into a unified, trainable recurrent deep neural network. We detail the architecture and evaluate its performance against leading methods in standard registration scenarios, demonstrating advantages such as cross-category generalization and computational efficiency—paving new avenues for deep learning in point cloud registration. Code and videos are available at https://github.com/hmgoforth/PointNetLK.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1388", "problem_id": "13880001", "content": "Understanding social dynamics is crucial for accurately predicting human trajectories. This study introduces a new perspective on modeling social interactions through group-based relationships among pedestrians. By iteratively deriving social representations guided by group annotations, we construct a social behavior framework termed the Recursive Social Behavior Graph, which significantly enhances representation capabilities. A Graph Convolutional Neural Network is employed to disseminate interaction data within this structure. Leveraging the Recursive Social Behavior Graph, our approach outperforms existing methods on the ETH and UCY datasets, achieving average improvements of 11.1% in ADE and 10.8% in FDE, while effectively forecasting intricate social behaviors.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1389", "problem_id": "13890001", "content": "Multi-agent reinforcement learning techniques have demonstrated significant promise in addressing intricate multi-agent challenges, yet they often lack formal theoretical support. Recently, mean field control and mean field games have been identified as feasible solutions for extensive multi-agent scenarios involving numerous agents. In this study, inspired by a motivating scheduling issue, we examine a discrete-time mean field control framework characterized by shared environmental states. We rigorously establish that approximate optimality is attained as the number of agents increases in the finite agent context and discover that a dynamic programming principle is applicable, leading to the formulation of an optimal stationary policy. Given the complexities of deriving exact solutions due to the continuous action space arising from the limiting mean field Markov decision process, we utilize established deep reinforcement learning techniques to address the corresponding mean field control issue. The effectiveness of the derived mean field control policy is assessed in comparison to conventional multi-agent reinforcement learning methods and is shown to converge to mean field performance with a sufficiently large number of agents, thereby corroborating the theoretical findings and achieving competitive outcomes.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1390", "problem_id": "13900001", "content": "Over the past few decades, the increasing availability of facial expression databases has spurred significant interest in Facial Expression Recognition (FER). However, the diverse origins of these datasets introduce several challenges for facial recognition, which are typically tackled using Convolutional Neural Network (CNN) architectures. Unlike CNNs, recent advancements have introduced Transformer models based on attention mechanisms for vision tasks. A key limitation of Transformers is their reliance on extensive training data, whereas most FER datasets remain relatively small compared to other vision applications. To address this, we present a novel approach that combines a vision Transformer with a Squeeze and Excitation (SE) block for FER. Our method is tested on multiple public datasets, including CK+, JAFFE, RAF-DB, and SFEW, demonstrating superior performance over state-of-the-art techniques on CK+ and SFEW, while achieving competitive results on JAFFE and RAF-DB.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1391", "problem_id": "13910001", "content": "We investigate the user association challenge, specifically determining the ideal allocation of user equipment to base stations to meet desired network performance levels. This paper emphasizes the transferability of association policies, as conventional user association methods are frequently tailored to specific scenarios or deployments, necessitating policy redesign or re-learning when user counts or locations shift. In contrast, transferability enables the application of a single user association policy, created for one scenario, to various user deployments without extensive re-learning or redesign, thus significantly alleviating computational and management burdens. To facilitate transferability, we first frame user association as a multi-agent reinforcement learning problem. Subsequently, we introduce a novel distributed policy network architecture leveraging a neural attention mechanism we developed for this situation, which demonstrates transferability among users with zero-shot generalization capability, meaning it does not require further training. Numerical outcomes illustrate our approach's effectiveness in enhancing overall network communication rates, surpassing centralized benchmarks even when the user count doubles from the original training scenario.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1392", "problem_id": "13920001", "content": "This paper introduces a neural network operating in the frequency domain for image super-resolution. Leveraging the convolution theorem, the network transforms spatial convolutions into frequency-domain multiplications. Additionally, the non-linearity typically provided by rectifier units is implemented as a frequency-domain convolution. This approach results in a computationally efficient network during inference while enabling end-to-end parameter learning. The model is trained via backpropagation and avoids complex numbers by utilizing the Hartley transform instead of the Fourier transform. Beyond super-resolution, the network holds promise for various computer vision and image processing tasks commonly addressed in the frequency domain. Experimental results demonstrate superior speed, outperforming existing methods by one to two orders of magnitude with negligible performance degradation.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1393", "problem_id": "13930001", "content": "The field of machine learning is experiencing rapid growth, yielding numerous theoretical advancements and diverse applications across various disciplines. As a crucial component of machine learning, optimization has garnered significant attention from researchers. However, the increasing complexity of models and the exponential expansion of data volumes pose substantial challenges to optimization methods in machine learning. In response, a plethora of studies have been conducted to address optimization problems and enhance optimization techniques in machine learning. A comprehensive review and analysis of optimization methods from a machine learning perspective are essential, as they can provide valuable insights and guidance for the development of both optimization and machine learning research. This paper provides an overview of optimization problems in machine learning, discusses the fundamental principles and progress of commonly employed optimization methods, summarizes their applications and developments in prominent machine learning fields, and ultimately identifies key challenges and open problems in machine learning optimization, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1394", "problem_id": "13940001", "content": "Autonomous systems frequently employ vision-based depth estimation, which typically utilizes a single camera or multiple independent cameras. However, in monocular setups, obtaining dense depth information often requires either supplementary input from expensive LiDARs, such as those with 64 beams, or camera-only methods that are prone to scale ambiguity and infinite-depth issues. This paper presents an alternative approach, proposing the combination of a monocular camera with a lightweight LiDAR, featuring only 4 beams, commonly found in mass-produced automotive-grade laser scanners. Building upon recent self-supervised methods, we introduce LiDARTouch, a novel framework that estimates dense depth maps from monocular images using sparse LiDAR \"touches\" without relying on dense ground-truth depth. The minimal LiDAR input contributes to our framework in three distinct ways: as an additional model input, in a self-supervised LiDAR reconstruction objective function, and in estimating pose changes, a crucial component of self-supervised depth estimation architectures. Our LiDARTouch framework achieves state-of-the-art results in self-supervised depth estimation on the KITTI dataset, validating our approach of integrating the sparse LiDAR signal with visual features. Furthermore, we demonstrate that utilizing a few-beam LiDAR mitigates scale ambiguity and infinite-depth problems inherent to camera-only methods, and that fully-supervised depth-completion methods can be adapted to a self-supervised regime with minimal LiDAR input, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1395", "problem_id": "13950001", "content": "Recent years have witnessed significant advancements in single image super resolution (SR), yet current approaches fall short in enabling the exploration of numerous plausible high-resolution (HR) reconstructions that could have yielded the observed low-resolution (LR) image. The diverse possible interpretations of an LR image can exhibit substantial variations in texture and fine details, often conveying distinct semantic information. This paper presents a new task, termed explorable super resolution, which is addressed through a proposed framework that combines a graphical user interface with a neural network backend, facilitating the editing of SR outputs to uncover the multitude of plausible HR explanations for a given LR input. A key component of our approach is a novel module that can be integrated with any existing SR network, providing an analytical guarantee that the resulting SR outputs will exactly match the LR input when downsampled, while also reducing the reconstruction error of the wrapped SR network and allowing it to adapt to different blur kernels, as demonstrated in various applications, including medical imaging, forensics, and graphics.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1396", "problem_id": "13960001", "content": "The rapid advancement of computational capabilities in modern workstations has brought real-time physically-based rendering within grasp, sparking widespread interest across diverse fields, including medicine, where it facilitates intuitive visualization of 3D data. The integration of embedded devices, such as optical see-through head-mounted displays (OST HMDs), has become a notable trend in medical augmented reality. Nevertheless, harnessing the benefits of physically-based rendering on these devices is hindered by constraints related to computational power, memory usage, and power consumption. To address this trade-off between device limitations and image quality, we propose a novel light field that enables real-time sampling on embedded devices, yielding reasonable rendering outcomes. Our approach is demonstrated in medical applications, and its limitations are discussed. The project's open-source version, accessible at https://github.com/lorafib/LumiPath, offers comprehensive implementation details and exemplary demonstration materials, as seen in Figure A, B, C, and cited in References [citation].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1397", "problem_id": "13970001", "content": "Altering the hue of color images is a simple process, yet it can be misused to skew viewer perceptions. Because hue modification leaves shapes, edges, and textures unaltered, identifying and pinpointing this type of manipulation is challenging. Given that small image regions share identical Color Filter Array (CFA) configurations and demosaicing processes, distortions resulting from localized hue changes can be identified through patch matching within the image itself. In this paper, we introduce a Siamese neural network-based approach for localizing hue modifications, optimized for comparing paired inputs. By carefully structuring the network's outputs, we generate a heatmap that can effectively highlight manipulated areas. Our technique performs well on both uncompressed images and those subjected to JPEG compression, an operation that typically complicates the utilization of CFA and demosaicing artifacts. Empirical results confirm the efficacy of our proposed method.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1398", "problem_id": "13980001", "content": "Language provides a succinct representation of reality, enabling the description of an infinite array of situations and goals through compositionality. Although these descriptions can facilitate the training, conditioning, or organization of interactive agent behavior, establishing a proper connection between language comprehension and reinforcement learning—even in straightforward instruction-following contexts—remains an unresolved issue. This challenge of joint learning can be mitigated through the use of expert demonstrations, auxiliary losses, or neural inductive biases. In this paper, we introduce a novel method called Hindsight Generation for Experience Replay (HIGhER), which expands the Hindsight Experience Replay (HER) technique to settings involving language-conditioned policies. When the agent fails to execute its instruction, HIGhER generates a new directive that aligns with the agent's trajectory and relabels the episode with a positive reward. To accomplish this, HIGhER learns to associate a state with an instruction by referencing past successful trajectories, eliminating the necessity for external expert intervention to relabel episodes, as required in traditional HER. We demonstrate the effectiveness of our method within the BabyAI environment and illustrate how it enhances other instruction-following strategies.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1399", "problem_id": "13990001", "content": "Exogenous state variables and rewards may hinder reinforcement learning by introducing uncontrolled fluctuations in the reward signal. We provide a formal definition of exogenous state variables and rewards, along with conditions that allow for the decomposition of a Markov Decision Process (MDP) containing exogenous states into an exogenous Markov Reward Process, which focuses solely on the exogenous state and reward, and an endogenous Markov Decision Process that is based only on the endogenous rewards. Furthermore, we establish a variance-covariance condition under which Monte Carlo policy evaluation within the endogenous MDP is expedited in comparison to utilizing the complete MDP. Such enhancements in efficiency are likely to extend to a variety of reinforcement learning algorithms. We introduce two methods for identifying the exogenous variables and evaluate their performance across different MDPs. The findings demonstrate that these methods are effective and can substantially accelerate the reinforcement learning process.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1400", "problem_id": "14000001", "content": "The goal of video summarization is to identify key frames that preserve essential information, typically achieved by predicting segment importance scores using a softmax function. Nevertheless, the softmax function is limited in its ability to retain complex visual or sequential information, a challenge known as the Softmax Bottleneck problem. To address this issue, this paper presents a novel framework, the Dual Mixture Attention (DMASum) model, which incorporates Meta Learning to enhance video summarization. The DMASum model utilizes a Mixture of Attention layer (MoA) that increases model capacity by applying self-query attention twice, enabling the capture of second-order changes in addition to initial query-key attention. A new Single Frame Meta Learning rule is also introduced to improve generalization on small datasets with limited training data. The DMASum model effectively leverages both visual and sequential attention, connecting local key-frame attention and global attention in a cumulative manner. Evaluations on the SumMe and TVSum datasets demonstrate significant improvements over state-of-the-art methods, as evidenced by both qualitative and quantitative experiments.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1401", "problem_id": "14010001", "content": "This study revisits Graph Convolutional Neural Networks by unifying spectral and spatial approaches to graph convolution design. We establish theoretical equivalence between spatial and spectral domain convolution processes, enabling a generalized framework for analyzing popular ConvGNNs, revealing their strengths and limitations. Additionally, this framework facilitates the creation of new spectral convolutions with tailored frequency profiles, implemented spatially. We further introduce a depthwise separable convolution extension for graph networks, reducing trainable parameters without compromising model capacity—an innovation previously unexplored in GNN literature. Evaluations on transductive and inductive graph learning tasks demonstrate the method's effectiveness, offering early empirical proof of spectral filter coefficient transferability across graphs. Source code is available at: https://github.com/balcilar/Spectral-Designed-Graph-Convolutions.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1402", "problem_id": "14020001", "content": "We introduce a Deep Differentiable Simplex Layer (DDSL) designed for neural networks in geometric deep learning. This layer serves as a differentiable interface between simplex-based geometric representations—such as point clouds, line meshes, triangular meshes, and tetrahedral meshes—and raster images like 2D/3D grids. Leveraging Non-Uniform Fourier Transform (NUFT), the DDSL enables efficient, anti-aliased, and differentiable rasterization of simplex-based signals. Our work includes a comprehensive theoretical framework and an optimized backpropagation algorithm. Unlike prior differentiable renderers and rasterizers, the DDSL supports arbitrary simplex dimensions and degrees. We demonstrate its utility in 2D shape applications, focusing on two use cases: (1) neural network-driven mesh editing and optimization, and (2) employing DDSL as a differentiable rasterization loss for end-to-end training of polygon generators. Experimental results confirm the efficacy of gradient-based shape optimization in airfoil design, while the differentiable rasterization loss achieves state-of-the-art performance in polygonal image segmentation when provided with ground-truth bounding boxes.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1403", "problem_id": "14030001", "content": "This study introduces an enhanced version of the model-free anomaly detection algorithm, Isolation Forest, termed Extended Isolation Forest (EIF), which addresses the issue of accurately assigning anomaly scores to data points. The problem is illustrated using heat maps, which reveal artifacts caused by the binary tree's branching criteria. A detailed explanation and visual demonstration of this issue are provided, followed by the proposal of two solutions to mitigate it. The first approach involves random data transformation prior to tree creation, which helps to average out biases, while the second, preferred method, utilizes hyperplanes with random slopes to slice the data, effectively eliminating the artifacts in anomaly score heat maps. The improved robustness of the algorithm is demonstrated by analyzing the variance of scores for data points along constant level sets, and its performance is evaluated using synthetic and real-world benchmark datasets, with results showing AUROC and AUPRC, and no significant difference in convergence rate or computation time compared to the standard Isolation Forest.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1404", "problem_id": "14040001", "content": "Common challenges in segmentation, such as overlapping colors and cluttered or weak edges, necessitate additional regularization techniques. One approach to addressing these issues is the use of star-convexity, which has gained popularity in interactive single object segmentation due to its simplicity and compatibility with exact graph cut optimization. This paper introduces a novel method for multiobject segmentation, where objects are constrained to distinct \"hedgehog\" shapes, with each shape's surface normals governed by a vector field, such as the gradients of a distance transform derived from user scribbles. By adjusting the tightness of the constraint, the shape prior can enforce skeleton consistency with the scribbles, ranging from extreme cases to more relaxed hedgehog shapes. Notably, our approach can be reduced to star-convexity with a single click and normal orientation constraints of +/-90 degrees, and when applied to multiple clicks, it defines a multi-star prior, offering significantly broader applications than traditional one-star segmentation, including the separation of multiple non-star organs with similar appearances and weak or noisy edges in medical data, as seen in Figure A (Reference [1], Figure B [2], and Citation [3]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1405", "problem_id": "14050001", "content": "We introduce Sparse R-CNN, a fully sparse approach for image-based object detection. Unlike conventional methods that depend on dense candidate generation, such as placing k anchor boxes across an H×W feature map, our approach utilizes a fixed set of N learned object proposals for classification and localization. This eliminates the need for designing HWk (often numbering in the hundreds of thousands) predefined candidates and complex label assignment strategies, reducing the process to N (e.g., 100) trainable proposals. Notably, the model produces final predictions directly without requiring non-maximum suppression. On the COCO benchmark, Sparse R-CNN matches established detectors in accuracy, speed (22 fps with ResNet-50 FPN), and training efficiency, achieving 45.0 AP under standard 3× training. Our findings encourage reconsidering the reliance on dense priors in object detection frameworks. The implementation is accessible at: https://github.com/PeizeSun/SparseR-CNN.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1406", "problem_id": "14060001", "content": "Data representations that remain unchanged in response to particular factors are beneficial for numerous applications, such as mitigating biases in prediction tasks, managing covariate influences, and isolating significant variation factors. However, it is difficult to develop representations that maintain invariance to arbitrary nuisance factors while still being applicable to other objectives. Current methods frame the balance between performance on tasks and invariance adversarially, employing a minimax optimization process. In our work, we argue that adversarial training is not only unnecessary but can also hinder progress; instead, we propose formulating invariant representation learning as a singular information-theoretic objective that can be optimized directly. Our findings indicate that this method either matches or surpasses the effectiveness of leading adversarial techniques in the context of creating fair representations and generative modeling with controllable transformations.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1407", "problem_id": "14070001", "content": "We present a novel uncertainty metric for classification tasks, termed logit uncertainty, which is derived from neural network logit outputs and demonstrates robustness across various applications. Our analysis reveals that this measure surpasses existing uncertainty quantification methods in performance, particularly for out-of-distribution detection and error identification. We investigate the theoretical underpinnings of this metric and examine its connection to high-density regions. Additionally, we illustrate its application in assessing uncertainty through intermediate outputs during generative adversarial network training. The paper further suggests two practical implementations of logit-based uncertainty and validates its superior effectiveness in real-world scenarios.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1408", "problem_id": "14080001", "content": "Reinforcement learning has proven effective for addressing optimal power flow (OPF) challenges in electric distribution systems. However, relying solely on model-free reinforcement learning techniques, which disregard the physics-based modeling of power grids, can limit optimizer performance and hinder scalability. This study introduces a novel method that integrates physics-based models with learning-based algorithms through imitation learning to tackle distribution-level OPF issues. We enhance deep reinforcement learning (DRL) techniques with imitation learning to optimize battery storage dispatch in distribution systems. The proposed approach leverages near-optimal solutions from a linearized model-based OPF solver to initialize the DRL policy, thereby enhancing training efficiency. The method's efficacy is validated using IEEE 34-bus and 123-bus distribution networks with multiple battery storage units.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1409", "problem_id": "14090001", "content": "Video-based person re-identification (reID), which seeks to match individuals across different video segments, presents significant challenges due to factors such as frame redundancy, evolving appearances, occlusions, and motion blur. To address these issues, we introduce Multi-Granularity Reference-aided Attentive Feature Aggregation (MG-RAFA), an attentive feature aggregation module designed to integrate spatio-temporal features into a highly discriminative video-level representation. To ascertain the significance of each spatio-temporal feature node, our approach involves learning attention from a comprehensive perspective using convolutional operations. Specifically, we infer attention by combining each feature with its relationships—defined as pairwise correlations with a representative set of reference feature nodes (S-RFNs) that encapsulate global video information. Furthermore, to leverage semantic information at various levels, we propose learning multi-granularity attentions based on relationships captured at different granularities. Comprehensive ablation studies confirm the efficacy of our attentive feature aggregation module, MG-RAFA, and our framework achieves state-of-the-art results on three benchmark datasets.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1410", "problem_id": "14100001", "content": "We introduce an efficient method for generating images of atmospheric clouds by leveraging a combination of Monte Carlo integration and neural networks. The complex scattering effects of Lorenz-Mie theory and the high reflectivity of cloud-forming aerosols pose significant challenges for traditional rendering methods, which struggle to accurately capture the distinctive silver lining and bright interior of clouds. Our approach addresses this issue by pre-training a neural network on the spatial and directional distribution of radiant flux from a set of cloud examples, rather than simulating the entire light transport process during rendering. To render a new scene, we extract a hierarchical 3D descriptor of the cloud geometry for each visible point, which is then used as input to a deep neural network that predicts the radiance function for each shading configuration. By progressively feeding the hierarchical descriptor into the network and utilizing a block design with residual connections, we enhance the network's learning efficiency and prediction accuracy while minimizing the number of coefficients required. Our GPU-based implementation enables the interactive synthesis of highly realistic cloud images, virtually indistinguishable from reference solutions, in a matter of seconds. This technique offers a promising solution for applications such as cloud design and the production of high-quality animated content, where temporal stability is crucial.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1411", "problem_id": "14110001", "content": "The application of deep learning in monocular image depth estimation has become increasingly prevalent, yielding promising outcomes. While supervised learning approaches, which rely on ground truth depth as labels, currently represent the most effective method for monocular depth estimation, they require a substantial amount of costly ground truth depth data. As a result, researchers have shifted their focus towards developing unsupervised depth estimation methods, despite their accuracy being lower than that of supervised methods, as they offer a promising avenue for research. This paper proposes an unsupervised monocular vision stereo matching approach, motivated by the finding that stereo matching models outperform monocular depth estimation models under the same unsupervised depth estimation framework, as shown in Figure A (see References [1] and [2] for details). To facilitate monocular stereo matching, two deep convolutional neural network models are constructed: one for reconstructing the right view from the left view, and another for estimating the depth map using the reconstructed right view and the original left view, as illustrated in Figure B. During the testing phase, these two models are combined, resulting in output that surpasses the performance of current state-of-the-art unsupervised depth estimation methods on the challenging KITTI dataset, as demonstrated in Figure C (cited in [3]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1412", "problem_id": "14120001", "content": "Universal machine translation seeks to develop models capable of translating between any language pair, leveraging a corpus of aligned, translated documents for all language combinations. Although massively multilingual models have demonstrated significant empirical success and attracted increasing attention, theoretical analysis of their translation errors remains underdeveloped. This paper provides formal proofs of fundamental limitations in achieving this goal in general, while also presenting positive results under additional, yet realistic, data structure assumptions. Specifically, we establish a lower bound on translation error in the many-to-many translation scenario, demonstrating that any algorithm designed to learn shared sentence representations across multiple language pairs will inevitably produce substantial translation errors on at least one task, without assumptions regarding language structure. Conversely, we demonstrate that if the paired documents in the corpus originate from a natural generative process, a form of generalization can be achieved: a linear number of language pairs, rather than a quadratic number, is sufficient for learning effective representations. Furthermore, our theoretical framework elucidates the impact of different connection graphs between language pairs, revealing that longer paths correlate with increased sample complexity, measured by the total number of documents required per language pair. We anticipate that our theoretical findings and their implications will inform the future algorithmic design of universal machine translation systems.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1413", "problem_id": "14130001", "content": "Over the past two decades, the field of topology optimization has experienced rapid growth, with various methods emerging that leverage shape and topological derivatives or evolutionary algorithms, often formulated on diverse geometric representations and parametrizations. However, a major obstacle common to these approaches is the substantial computational expense incurred by 3D topology optimization problems. To address this challenge, we propose a novel transfer learning approach utilizing a convolutional neural network, which offers several key advantages: it can efficiently handle complex 3D design domains with varied shapes and topologies; facilitate real-time exploration of the design space in response to changes in domain and boundary conditions; require significantly fewer high-resolution examples to enhance learning for new tasks compared to traditional deep learning networks; and demonstrate multiple orders of magnitude improvement in efficiency over established gradient-based methods, such as SIMP, as shown in Figure A, B, C (References [citation]). Our methodology is validated through numerous 2D and 3D examples, showcasing its effectiveness, accuracy, and generalization capabilities, including its ability to handle unseen design domains and achieve an average binary accuracy of approximately 95% at real-time prediction rates, as discussed in References [citation], Figure A, B, C. The proposed transfer learning-based approach exhibits great potential as a practical framework for enabling real-time 3D design exploration based on topology optimization.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1414", "problem_id": "14140001", "content": "Deep reinforcement learning algorithms have demonstrated significant promise in tackling complex real-world problems, such as the Go game and robotic applications, but their success often relies on a meticulously crafted reward function to direct training at each time step. However, in practical scenarios, designing such a reward function can be challenging, and the only available feedback is typically the episodic reward or return, which is received at the end of a trajectory. To address this limitation, this study proposes a novel algorithm for temporal credit assignment, leveraging deep neural networks to decompose the episodic return into a reward signal for each time step in the trajectory. By utilizing this learned reward signal, the efficiency of episodic reinforcement learning can be substantially enhanced. Notably, the use of expressive language models, such as the Transformer, is found to be effective in learning the importance and dependency of states in the trajectory, yielding high-quality and interpretable learned reward signals. The efficacy of the proposed algorithm is validated through extensive experiments on a range of MuJoCo continuous locomotive control tasks, where only episodic returns are available, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1415", "problem_id": "14150001", "content": "Recent studies have utilized Generative Adversarial Networks (GAN) to predict medical images from a single modality, such as generating FLAIR MRI from T1 MRI. Nevertheless, these frameworks are mainly tailored for image-based applications, which restricts their applicability to non-Euclidean data like brain graphs. Although connectomic research has highlighted the potential of incorporating brain graphs in diagnosing neurological disorders, existing geometric deep learning approaches have not been designed to predict multiple target brain graphs from a single source graph. The field of graph generation has gained significant traction in the past two years, but current methods have two major limitations: they typically require learning a separate model for each target domain, limiting their ability to jointly predict multiple domains, and they only consider the global graph structure while neglecting local topological features, such as node centrality. To address these challenges, we propose the MultiGraphGAN architecture, which can predict multiple brain graphs from a single graph while preserving their topological structure. The key contributions of our approach include: (i) developing a graph adversarial auto-encoder for joint prediction of brain graphs, (ii) mitigating the mode collapse issue in GAN by clustering encoded source graphs and using a cluster-specific decoder, and (iii) introducing a topological loss function to ensure the generation of topologically coherent target brain graphs, as shown in Figure A, B, C (References: [citation]). Our MultiGraphGAN demonstrates superior performance compared to its variants, showcasing its potential in generating multi-view brain graphs from a single graph.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1416", "problem_id": "14160001", "content": "Reinforcement Learning (RL) offers a promising solution for controlling virtual character animations. Although RL has been successfully used to simulate physics-based movements, designing reward functions for social behaviors remains difficult due to their minimal physical interaction with the environment. Meanwhile, existing data-driven methods for these behaviors rely on supervised learning, which demands large datasets and has limited generalizability. To overcome these challenges, we introduce RLAnimate, a new deep RL framework that integrates RL’s adaptability with motion data learning. We establish a mathematical training structure by redefining key components like agents, environments, states, and actions, aligning them with character animation principles and model-based RL. Our method enables agents to learn diverse animation dynamics through iterative RL training, guided by motion-capture-derived behavioral representations. Experiments with pointing and waving behaviors show that RLAnimate reduces training time and sample episodes compared to physics-based RL techniques. Additionally, unlike supervised learning approaches, it requires only a small motion clip dataset to learn valid behavior representations.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1417", "problem_id": "14170001", "content": "This study tackles the problem of forecasting non-stationary time series data, specifically focusing on predicting multiple future steps. To overcome this complex challenge, we present DILATE (DIstortion Loss including shApe and TimE), a novel objective function designed for training deep neural networks. DILATE is engineered to precisely forecast abrupt transitions by integrating two components that facilitate accurate shape and temporal change identification. We propose a differentiable loss function compatible with deep neural network training and offer a specialized back-propagation implementation to accelerate optimization. Additionally, we introduce a DILATE variant that offers a seamless generalization of temporally-constrained Dynamic Time Warping (DTW). Empirical evaluations on diverse non-stationary datasets demonstrate DILATE's superior performance compared to models trained using the conventional Mean Squared Error (MSE) loss function, as well as DTW and its derivatives. DILATE's adaptability extends to various model architectures; we emphasize its advantages in training both fully connected networks and specialized recurrent architectures, highlighting its potential to enhance state-of-the-art trajectory forecasting methods.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1418", "problem_id": "14180001", "content": "This study introduces a residual non-local attention network designed for advanced image restoration. Traditional approaches often overlook the uneven information distribution in degraded images, limiting their effectiveness due to reliance on local convolutions and uniform handling of spatial and channel features. To overcome these limitations, we develop local and non-local attention blocks that identify long-range pixel dependencies and prioritize more complex regions. Each block consists of a trunk branch for hierarchical feature extraction and local or non-local mask branches that dynamically adjust these features using mixed attention mechanisms. The local mask branch focuses on structural details through convolutional operations, whereas the non-local branch emphasizes broader dependencies across the feature map. Additionally, we implement residual local and non-local attention learning to optimize deep network training, improving overall feature representation. The proposed framework is versatile, applicable to tasks like image denoising, demosaicing, artifact reduction, and super-resolution. Experimental results show that our method achieves performance on par with or superior to state-of-the-art techniques in both quantitative metrics and visual quality.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1419", "problem_id": "14190001", "content": "The performance of person re-identification (re-ID) systems has significantly improved recently, largely due to advances in deep convolutional neural networks (CNNs). While most deep re-ID research emphasizes the design of novel CNN architectures, less focus has been directed toward exploring loss functions. Verification loss and identification loss, two commonly used loss functions for training deep re-ID models, each possess inherent drawbacks. Verification loss aims to create feature embeddings with reduced intra-class variance and increased inter-class variance, but training with this loss can suffer from slow convergence and instability when dealing with large datasets. Conversely, identification loss offers strong separation and scalability, but its failure to explicitly minimize intra-class variance restricts its re-ID performance, as the same person can exhibit considerable appearance variations across different camera views. To overcome these limitations, we introduce a new support neighbor (SN) loss. Unlike losses derived from data sample pairs or triplets, SN loss is computed using positive and negative support neighbor sets for each anchor sample, incorporating more valuable contextual information and neighborhood structure, which promotes more stable performance. To ensure scalability and separability, a softmax-like function is employed to separate the positive and negative support sets. Furthermore, to minimize intra-class variance, the distance between the anchor's nearest positive neighbor and furthest positive sample is penalized. Integrating SN loss with Resnet50 yields state-of-the-art re-ID results on several widely used datasets.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1420", "problem_id": "14200001", "content": "Generative adversarial networks (GANs) offer a robust algorithmic framework for creating generative models, characterized by several desirable properties, including the ability to operate without a predefined likelihood function, instead relying on a generating procedure, and producing sharp and compelling samples, while also leveraging the strengths of highly accurate neural network classifiers. This work aims to deepen our understanding of GANs, situating them within the broader context of implicit generative models, which generate data through stochastic procedures, and exploring connections to related fields, such as econometrics and approximate Bayesian computation, as seen in Figure A. By developing likelihood-free inference methods, we highlight the importance of hypothesis testing in learning implicit generative models, allowing us to derive the GAN objective function, as well as other related objectives, and directing our attention to the general problem of density ratio estimation, which can be approached through four distinct methods, including classifier-based solutions, divergence minimisation, and moment matching, as discussed in References [1] and [2], and we synthesise these perspectives to reveal relationships between them and the wider literature, identifying avenues for future research and cross-disciplinary collaboration, as illustrated in Figure B and Figure C, and further explored in citations [3] and [4].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1421", "problem_id": "14210001", "content": "Pedestrian trajectory representation learning converts timestamped coordinate sequences of varying lengths into fixed-size vectors, effectively capturing the spatiotemporal features of movement patterns. This is essential for integrating feature-based data mining techniques with trajectory data. A significant challenge in trajectory representation lies in the need to accurately model both environmental limitations, such as physical barriers, and temporal dynamics specific to pedestrian behavior. Conventional sequence-to-sequence autoencoders, which rely on maximum log-likelihood estimation, typically require comprehensive datasets that encompass all possible spatiotemporal variations, an unrealistic requirement in practical scenarios. To address these limitations, we introduce TREP, a pedestrian trajectory representation learning algorithm designed to capture environmental constraints and pedestrian dynamics without relying on extensive training data. TREP employs a sequence-to-sequence autoencoder framework, enhanced with a spatially informed objective function within an actor-critic reinforcement learning paradigm, to intelligently encode trajectory spatiotemporal characteristics and accommodate a wide range of trajectory patterns. Empirical evaluations conducted on both synthetic and real-world datasets confirm TREP's ability to faithfully represent trajectories.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1422", "problem_id": "14220001", "content": "Recent advances in scene text recognition have largely been driven by algorithms that treat the problem as a sequence prediction task, analogous to speech recognition, achieving impressive results but overlooking the inherent two-dimensional nature of text in images, which contrasts with the one-dimensional essence of speech. This oversight can lead to loss of valuable information and introduction of unnecessary noise when compressing text features into a one-dimensional format. This paper presents a novel approach to scene text recognition, adopting a two-dimensional perspective through the Character Attention Fully Convolutional Network (CA-FCN) model, which effectively recognizes text of arbitrary shapes by leveraging a semantic segmentation network with a character attention mechanism and a word formation module, enabling simultaneous script recognition and character position prediction. The proposed algorithm is shown to surpass previous methods on both regular and irregular text datasets, as demonstrated by experiments, and exhibits greater robustness to imprecise localizations during the text detection phase, a common occurrence in practical applications, as seen in Figure A, B, C (References [1], [2], [3]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1423", "problem_id": "14230001", "content": "Skeleton-based action recognition is a significant endeavor that necessitates a comprehensive understanding of the movement characteristics inherent in human actions based on a given skeleton sequence. Recent research has underscored the importance of examining both spatial and temporal features of the skeleton sequence for this purpose. However, effectively extracting distinct spatial and temporal features remains a formidable challenge. In this study, we introduce a novel Attention Enhanced Graph Convolutional LSTM Network (AGC-LSTM) designed for human action recognition utilizing skeleton data. The AGC-LSTM not only captures key features related to spatial layouts and temporal dynamics but also investigates the co-occurrence between spatial and temporal aspects. Additionally, we propose a temporal hierarchical architecture aimed at expanding the temporal receptive fields of the leading AGC-LSTM layer, which enhances its capability to learn high-level semantic representations while significantly lowering computational costs. To further refine the selection of discriminative spatial information, an attention mechanism is integrated to accentuate the significance of crucial joints within each AGC-LSTM layer. We present experimental results utilizing two datasets: the NTU RGB+D dataset and the Northwestern-UCLA dataset. The comparative findings indicate the effectiveness of our method, demonstrating its superiority over existing state-of-the-art techniques on both datasets.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1424", "problem_id": "14240001", "content": "We present a technique for separating controllable and uncontrollable variables by engaging with the environment. This disentanglement results in effective representations and is crucial for the implementation of deep neural networks (DNNs) in areas where interpretability is essential. This research seeks to enhance a current reinforcement learning (RL) strategy for distinguishing controllable from uncontrollable variations, as the existing method lacks a means to represent uncontrollable challenges. To tackle this issue, we concurrently train two DNNs: one focused on the controllable element and the other on the uncontrollable obstacles. To ensure stable training, we employed a pretraining strategy with a model that is resilient to uncontrollable barriers. Experimental simulations indicate that the suggested model can effectively separate independently controllable and uncontrollable factors without the need for labeled data.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1425", "problem_id": "14250001", "content": "The foundation of artificial intelligence relies on learning data representations that can be applied to a wide range of downstream tasks. Unlike conventional methods that focus on evaluating representations based on their performance in tasks such as classification or image generation quality, this study proposes an alternative approach by assessing representations based on their utility in control tasks like object manipulation. Through an extensive evaluation of over 10,000 reinforcement learning policies, we investigate how various representation properties influence generalization to out-of-distribution (OOD) scenarios. Our research culminates in demonstrating the zero-shot transferability of these policies from simulated environments to real-world settings, achieving this without relying on domain randomization or fine-tuning, and ultimately seeks to provide the first comprehensive analysis of the effectiveness of learned representations in real-world OOD downstream tasks.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1426", "problem_id": "14260001", "content": "Current leading methods for LiDAR point cloud semantic labeling largely depend on deep Convolutional Neural Networks (CNNs). A major problem is the difficulty of applying network architectures to different LiDAR sensor types, primarily because of sensor-specific design considerations related to network architecture and data representation. This paper introduces a new CNN architecture for point-wise semantic labeling of LiDAR data that achieves state-of-the-art results and enhances portability across sensor types. This is particularly beneficial considering the rapid advancements in LiDAR hardware technology. We present a comprehensive quantitative cross-sensor analysis of semantic labeling performance, comparing it to a state-of-the-art reference method. The evaluation demonstrates that the proposed architecture exhibits high portability, resulting in a 10 percentage point increase in the Intersection-over-Union (IoU) score compared to the reference approach. Furthermore, the results suggest that the proposed network architecture offers an effective method for automatically generating large-scale training data for new LiDAR sensor types, eliminating the need for extensive manual annotation or multi-modal label transfer.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1427", "problem_id": "14270001", "content": "We introduce CodeReef, an open platform designed to facilitate the sharing of all essential components for cross-platform MLOps (MLSysOps), aimed at automating the deployment of machine learning models across various systems in an optimal manner. Additionally, we present the CodeReef solution, which allows for the packaging and sharing of models in non-virtualized, portable, customizable, and reproducible archive files. These machine learning packages consist of a JSON meta description of the models, encompassing all dependencies, Python APIs, CLI actions, and portable workflows required to automatically build, benchmark, test, and customize models across a range of platforms, AI frameworks, libraries, compilers, and datasets. We provide examples of several CodeReef solutions that facilitate the automatic building, execution, and evaluation of object detection using SSD-Mobilenets, TensorFlow, and the COCO dataset from the latest MLPerf inference benchmarks across a broad spectrum of platforms, including Raspberry Pi, Android devices, IoT devices, and data centers. Our long-term objective is to assist researchers in sharing their innovative techniques as production-ready packages alongside research publications, thereby enabling collaborative and reproducible benchmarking, comparing various ML/software/hardware configurations, and identifying the most efficient options on a Pareto frontier through online CodeReef dashboards.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1428", "problem_id": "14280001", "content": "Despite extensive research on object detection and recognition, the development of reliable fire and flame detection techniques remains limited. This paper introduces an empirical investigation aimed at establishing a comprehensive and reliable method for rapid fire and flame detection in video, suitable for video surveillance and event retrieval applications. The proposed system employs a three-stage cascaded process: (1) identifying potential regions using a background model, (2) classifying fire regions based on color-texture characteristics and a visual word dictionary, and (3) performing temporal verification. Each stage undergoes experimental evaluation and analysis. We anticipate that this work will be beneficial for both academic research and practical applications. Furthermore, we are releasing the proposed system's software, complete with source code, along with a public benchmark and dataset comprising 64 video clips from diverse indoor and outdoor environments under varying conditions. Our system achieves a Recall of 82% and a Precision of 93% on this dataset, demonstrating a significant improvement over existing state-of-the-art methods.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1429", "problem_id": "14290001", "content": "A physical modeling approach, characterized by the simulation and visualization of physical principles, is applied to the shape extraction of active contours, aiming to overcome several major challenges in the application of Active Contours. To achieve this, a technique is developed to enable topological changes in Parametric Active Contours (Snakes), which involves mimicking the process of an expanding balloon filling a closed space with multiple objects, allowing for object identification through the remaining balloon surfaces after removal of touched surfaces. The evolution of Snakes is guided by a burned region, which provides a stopping criterion, and upon termination, ignoring this criterion enables the formation of a connected area through continued evolution and region burning, yielding contours that represent the child snake of each object. Additionally, a novel scheme is proposed to address the issues of contour leakage and segmentation errors in Geometric Active Contours (GAC), which involves a two-stage processing approach, including the simulation of wave propagation in an isotropic substance to enhance the image force effect in GAC based on Level Set, providing satisfactory solutions to these problems. Furthermore, a general image force field is introduced, created on a template plane over the image plane, which is more adaptable to noisy images with complex geometric shapes, supporting the physical models for active contours.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1430", "problem_id": "14300001", "content": "Advances in biomedical applications of deep learning are often limited by the opaque nature of these models. This paper examines the RETAIN architecture for predicting future glucose levels in diabetic patients. With its dual-level attention mechanism, RETAIN maintains high performance comparable to conventional neural networks while offering interpretability. We assess the model using data from real-world type-2 diabetes patients, benchmarking it against a random forest approach and an LSTM-based recurrent neural network. The findings demonstrate that RETAIN surpasses the random forest and matches the LSTM network in both standard accuracy measures and clinical relevance metrics, establishing its validity for glucose prediction. Additionally, we introduce methods to leverage RETAIN's interpretability, which provides valuable insights for both patients and healthcare providers, facilitating better comprehension of model predictions and informing the development of future glucose forecasting systems.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1431", "problem_id": "14310001", "content": "Synthesizing face photos from basic line drawings represents a one-to-many task, as these drawings only capture the contours of a human face. Traditional exemplar-based approaches often rely heavily on specific datasets and struggle to generalize in complex natural environments. Recently, some studies have employed deep neural networks to enhance generalization, yet these methods still face challenges in user controllability. In this paper, we introduce a deep generative model aimed at generating face photos from simple line drawings, guided by face attributes such as hair color and skin tone. To enhance the control over these attributes, we first present an attribute-disentangled variational auto-encoder (AD-VAE) that learns latent representations separated by specified attributes. Subsequently, we perform photo synthesis from the line drawings using AD-VAE. Our experiments demonstrate that the model effectively disentangles attribute variations from other changes in face photos and generates highly detailed, photorealistic images that align with the specified attributes. By treating background and lighting as style components, and the human face as content, we can also produce face photos styled according to a reference style image.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1432", "problem_id": "14320001", "content": "Artificial environments often feature densely arranged objects, many of which are identical and closely positioned. Our research demonstrates that even advanced object detectors struggle with accurate detection in such complex scenarios. We introduce a new deep learning approach specifically tailored for these demanding conditions, featuring three key innovations: (1) a dedicated layer for calculating the Jaccard index as a detection confidence measure; (2) an EM-based merging module that utilizes these confidence scores to address overlapping detection conflicts; and (3) the SKU-110K dataset, a comprehensive annotated collection of retail shelf images designed for training and evaluation in high-density settings. Evaluations on SKU-110K for detection and CARPK/PUCPR+ for counting reveal significant performance improvements over current leading methods. The implementation and dataset will be accessible via \\url.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1433", "problem_id": "14330001", "content": "The optimization of deep learning algorithms typically involves solving complex, non-linear, and non-convex problems, with large-scale machine learning methods, including deep learning and deep reinforcement learning, often relying on first-order algorithms like stochastic gradient descent (SGD). Although SGD is computationally efficient, it suffers from slow convergence rates and requires extensive hyperparameter tuning. In contrast, incorporating second-order curvature information can lead to more robust convergence, but computing Hessian matrices for large-scale problems is impractical. Quasi-Newton methods, such as the limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) approach, offer an attractive alternative, constructing approximate Hessian matrices to inform search directions and achieving superlinear convergence using only first-order gradient information. This work proposes novel optimization methods based on L-BFGS quasi-Newton methods, leveraging line search and trust-region strategies to bridge the gap between first- and second-order methods by utilizing gradient information to compute low-rank updates to Hessian approximations. Theoretical convergence analysis and empirical results on deep learning applications, including image classification tasks and deep reinforcement learning on ATARI 2600 video games, demonstrate robust convergence, favorable generalization properties, and rapid training times, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1434", "problem_id": "14340001", "content": "Identifying key areas within video frames captures the primary semantic content of each scene. This knowledge benefits various applications, from entertainment purposes like automated commentary generation and travel guidance to robotic-assisted procedures such as laparoscopic surgery. Yet, pinpointing these meaningful regions in videos remains a challenge. Our study tackles this issue by employing an RNN-based visual attention model guided by eye fixation data. Experiments indicate that this method shows promise in detecting semantic regions, though its effectiveness depends significantly on the accuracy of the eye fixation annotations.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1435", "problem_id": "14350001", "content": "Current methods for image-text matching generally assess the relevance of an image-text pair by evaluating and consolidating the similarities between the text and each distinct object within the image. However, they overlook the relationships among semantically connected objects, which might collectively influence the match between the image and the text. To tackle this issue, we introduce a Dual Path Recurrent Neural Network (DP-RNN) that symmetrically processes images and sentences using recurrent neural networks (RNN). Specifically, for a given image-text pair, our model rearranges the image objects according to the positions of their most relevant words in the text. Analogous to how hidden features are extracted from word embeddings, the model employs RNN to derive high-level object features from the reordered object inputs. We demonstrate that these high-level object features encapsulate valuable joint information about semantically related objects, which is advantageous for the retrieval task. To evaluate the similarity between the image and text, we integrate a Multi-attention Cross Matching Model into the DP-RNN framework. This model aggregates the relationships between objects and words using cross-modality guided attention and self-attention. Our approach achieves state-of-the-art results on the Flickr30K dataset and competitive outcomes on the MS-COCO dataset. Comprehensive experiments confirm the efficacy of our model.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1436", "problem_id": "14360001", "content": "Lifelong Learning (LL) involves continuously acquiring new knowledge and addressing emerging problems by integrating incremental information over time without losing previously learned information. While Supervised Lifelong Learning (SLL) with labeled data streams has received significant recent attention, this work addresses the challenges of Unsupervised Lifelong Learning (ULL) with unlabeled streaming data, where both data distribution and unknown class labels evolve dynamically. A Bayesian approach is inherently suited for integrating prior knowledge and sequentially updating beliefs with new observations. We propose a comprehensive Bayesian inference framework for ULL, introducing an end-to-end Deep Bayesian Unsupervised Lifelong Learning (DBULL) algorithm that incrementally identifies new clusters while preserving past knowledge using unlabeled data and learning latent representations. To retain prior knowledge effectively, we introduce a novel preservation mechanism based on sufficient statistics of latent representations derived from raw data. Additionally, we incorporate an on-the-fly cluster detection and redundancy elimination strategy, inspired by Nonparametric Bayesian statistics, to dynamically identify emerging clusters. The efficacy of our method is validated through experiments on benchmark image and text datasets in both lifelong and batch learning scenarios.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1437", "problem_id": "14370001", "content": "The considerable increase in the accessibility of observational data across various scientific and technological fields is enhancing the exploration of causal inference. Nevertheless, the estimation of treatment effects from such data encounters two primary obstacles: the absence of counterfactual outcomes and treatment selection bias. Among the commonly utilized and essential techniques for estimating treatment effects are matching methods; however, these existing methods tend to underperform when confronted with data characterized by high dimensionality and complex variables. We introduce a feature selection representation matching (FSRM) approach that utilizes deep representation learning and matching techniques, transforming the original covariate space into a selective, nonlinear, and balanced representation space, where matching is then executed. FSRM employs deep feature selection to reduce the impact of irrelevant variables on treatment effect estimation and integrates a regularization term based on the Wasserstein distance to acquire balanced representations. We assess the effectiveness of our FSRM method across three datasets, and the findings indicate that it outperforms current state-of-the-art methods.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1438", "problem_id": "14380001", "content": "Neural Architecture Search (NAS) enables the automated design of high-performing Deep Neural Network (DNN) architectures for specific tasks. A major challenge in NAS, however, is the high computational expense, primarily caused by costly performance evaluations. Neural predictors offer a solution by directly estimating DNN performance without requiring training, attracting growing research interest. Despite their advantages, these predictors face a critical drawback: the scarcity of labeled DNN architectures needed for effective training. To address this limitation, we introduce Homogeneous Architecture Augmentation for Neural Predictor (HAAP), which employs a homogeneous architecture augmentation algorithm to produce ample training data using homogeneous representations. Additionally, HAAP incorporates a one-hot encoding strategy to enhance the representation of DNN architectures. Evaluations on NAS-Benchmark-101 and NAS-Bench-201 datasets show that HAAP surpasses existing methods while requiring significantly less training data. Ablation studies further confirm the broad applicability of homogeneous architecture augmentation across both benchmarks.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1439", "problem_id": "14390001", "content": "The process of message passing in graph neural networks (GNNs) remains largely unclear. Unlike convolutional neural networks, there has been no theoretical foundation established for GNNs. Interestingly, we find that message passing can be most effectively interpreted through the lens of power iteration. By either completely or partially eliminating activation functions and layer weights from GNNs, we introduce subspace power iteration clustering (SPIC) models that learn iteratively with a single aggregator. Experimental results indicate that our models enhance GNNs and their ability to handle networks with random features. Furthermore, we highlight the design redundancies present in several advanced GNNs and establish a baseline for evaluating models using a random aggregator for message passing. Our results advance the theoretical insights into neural networks.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1440", "problem_id": "14400001", "content": "Text-based person re-identification (Re-id) plays a crucial role in video surveillance by retrieving an individual's image based on a textual description from a vast collection of images. The challenge lies in the difficulty of directly aligning visual content with textual descriptions due to the differences in modality. On one hand, the embeddings derived from text lack sufficient discriminative capability, largely because of the abstract nature of the descriptions. On the other hand, traditional Global Average Pooling (GAP) methods tend to produce more generalized or smoothed features while overlooking significant local features that are essential for effective cross-modal matching. To address these issues, we introduce a novel Dual-path CNN with Max Gated block (DCMG) designed to capture more discriminative word embeddings, enhancing the visual-textual connections by emphasizing important characteristics from both modalities. This framework leverages two deep residual CNNs that are optimized collaboratively using cross-modal projection matching (CMPM) loss and cross-modal projection classification (CMPC) loss to unify the two modalities into a common feature space. Initially, we integrate the pre-trained language model BERT with a convolutional neural network (CNN) to enhance the quality of word embeddings in the text-to-image matching context. Subsequently, we apply a global Max pooling (GMP) layer to ensure that visual-textual features are more attentive to prominent components. Additionally, we introduce a gated block (GB) to create an attention map that highlights the most significant features from both modalities, effectively reducing noise in the max-pooled output. Finally, extensive evaluations on the benchmark dataset, CUHK-PEDES, demonstrate that our approach achieves a rank-1 score of 55.81%, surpassing the best existing method by 1.3%.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1441", "problem_id": "14410001", "content": "Recent developments in integrating attention mechanisms into vision systems have prompted researchers to reevaluate the dominance of convolutional layers as the fundamental component. While attention aids CNNs in managing long-range dependencies, Ramachandran et al. (2019) demonstrated that attention can entirely supplant convolution while delivering top-tier results in vision tasks. This prompts the inquiry: do learned attention layers function akin to convolutional layers? Our findings indicate that attention layers are capable of executing convolution and frequently adopt this behavior in practice. We establish that a multi-head self-attention layer with an adequate number of heads is at least as expressive as any convolutional layer. Empirical results further reveal that self-attention layers focus on pixel-grid patterns in a manner comparable to CNN layers, supporting our theoretical conclusions. Our code is publicly available.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1442", "problem_id": "14420001", "content": "Affective recognition plays a crucial role in human-computer interaction; however, current methods utilizing real-world data lack the accuracy required for practical application. This paper presents an affective recognition technique, developed for the Affective Behavior Analysis in-the-wild (ABAW) 2021 Contest, that centers on facial expression (EXP) and valence-arousal calculation. Our approach is predicated on the assumption that facial expressions in videos are discerned not only from universal features but also from individual-specific temporal changes. Consequently, we developed facial expression and valence-arousal estimation models by initially learning common features across frames and subsequently integrating these with standardized, video-specific features within a time-series framework. Furthermore, this learning process incorporated multi-modal data, including image features, AU, Head pose, and Gaze. The model achieved a facial expression score of 0.546 on the validation set, demonstrating the efficacy of the proposed framework in enhancing both estimation accuracy and robustness.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1443", "problem_id": "14430001", "content": "Monocular 3D object detection is a crucial task in autonomous driving, offering a cost-effective solution, but it poses significant challenges due to the lack of depth information, making it inherently more complex than 2D detection. Despite recent advancements in 2D detection, adapting these detectors to 3D tasks is non-trivial. This paper addresses this challenge by proposing a general framework, FCOS3D, built upon a fully convolutional single-stage detector. The approach involves transforming 7-DoF 3D targets into the image domain, decomposing them into 2D and 3D attributes, and assigning objects to different feature levels based on their 2D scales and projected 3D centers. A redefined center-ness metric, based on a 2D Gaussian distribution centered at the 3D center, is also introduced to align with the 3D target formulation. This framework is simple, effective, and free from 2D detection or 2D-3D correspondence priors, achieving top performance among vision-only methods in the nuScenes 3D detection challenge of NeurIPS 2020, with code and models available at https://github.com/open-mmlab/mmdetection3d.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1444", "problem_id": "14440001", "content": "Reinforcement learning offers a robust framework for acquiring behaviour through environmental interactions, yet it typically yields reactive behaviours where actions are chosen based on immediate observations. This reactive approach can lead to inefficient learning, particularly in complex environments requiring nuanced control. To overcome this limitation, we introduce a proactive paradigm where the agent determines not only the action to take in a given state, but also the duration for which to maintain that action. Our proposed TempoRL method incorporates skip connections between states and learns a skip-policy that enables the repetition of actions over these connections, thereby enhancing learning efficiency. As demonstrated through experiments on various traditional and deep reinforcement learning environments, TempoRL facilitates the acquisition of successful policies at a significantly faster rate, outperforming vanilla Q-learning by up to an order of magnitude.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1445", "problem_id": "14450001", "content": "A novel approach to addressing the scarcity of large-scale labeled datasets has been introduced through deep domain adaptation, a technique that surpasses traditional methods by utilizing deep networks to learn more adaptable representations. Unlike conventional techniques that rely on shared feature subspaces or the reuse of crucial source instances with shallow representations, deep domain adaptation integrates domain adaptation into the deep learning pipeline, enabling the learning of more transferable representations. While comprehensive surveys have been conducted on shallow domain adaptation, there is a lack of timely reviews on emerging deep learning-based methods, prompting the need for a thorough examination. This paper presents a comprehensive survey of deep domain adaptation methods tailored to computer vision applications, offering four key contributions: a taxonomy of deep domain adaptation scenarios based on data properties that define domain divergence, a categorization of deep domain adaptation approaches according to training loss, with a brief analysis and comparison of state-of-the-art methods, an overview of computer vision applications beyond image classification, including face recognition, semantic segmentation, and object detection, and an identification of potential shortcomings in current methods and future research directions.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1446", "problem_id": "14460001", "content": "The increasing interest in Automotive Cyber-Physical Systems (ACPS) over the past few decades has highlighted the importance of environmental perception, a critical operation that relies heavily on the analysis of complex and dynamic scenes from visual data. The application of Deep Neural Networks (DNNs) has yielded impressive results in this area, particularly when utilizing deep learning techniques. However, the short prediction horizons and real-time inference requirements of these perception systems necessitate the transformation of large pre-trained networks into smaller, more efficient models through the use of Model Compression and Acceleration (MCA) techniques. This work aims to explore optimal methods for applying novel weight sharing techniques, optimizing variables and training procedures to significantly accelerate widely adopted DNNs. Through extensive evaluation studies using state-of-the-art DNN models in object detection and tracking experiments, as seen in Figure A, B, C, we provide insights into the types of errors that occur when applying weight sharing techniques, demonstrating substantial acceleration gains with minimal accuracy losses, as discussed in References [citation].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1447", "problem_id": "14470001", "content": "Generative Adversarial Networks (GANs) enable the distribution of synthetic images as a practical alternative to sharing original datasets. These synthetic datasets can support various downstream applications, including classifier training, which traditionally relies on access to the original data. However, research indicates that adversaries with full dataset access and auxiliary information can exploit GAN models and their synthetic outputs to deduce training set membership. Existing solutions like DPGAN significantly degrade sample quality compared to standard non-private GANs. To address this, we introduce privGAN, a novel GAN framework where the generator not only fools the discriminator but also resists membership inference attacks. This approach maintains data utility while offering robust privacy protection. Furthermore, our method inherently prevents overfitting to training data, enhancing its defensive capabilities. Key contributions include: i) a privacy-preserving GAN architecture requiring no extra hyperparameter tuning or structural adjustments, ii) theoretical insights into the optimal solution for privGAN’s loss function, iii) empirical validation of its resilience against diverse white- and black-box attacks across benchmark datasets, and iv) minimal downstream performance degradation compared to non-private GANs on three standard datasets.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1448", "problem_id": "14480001", "content": "Multiview detection utilizes various camera angles to address occlusions, with the primary challenge being the aggregation of multiview data. The leading technique for this task employs convolution to process feature map projections from multiple angles onto a shared ground plane; this approach utilizes identical computations without taking into account the location of objects. However, such translation-invariant methods may not be optimal, as the features of objects are subjected to different projection distortions based on their locations and the cameras used. In this paper, we introduce a new multiview detector, MVDeTr, which leverages an innovative shadow transformer for the aggregation of multiview data. Unlike convolutions, the shadow transformer adapts its attention according to specific positions and cameras, effectively managing various shadow-like distortions. We also present an efficient training methodology that incorporates a novel view-coherent data augmentation technique, which applies random augmentations while preserving consistency across views. Our proposed system achieves new state-of-the-art accuracy on two multiview detection benchmarks. The code is accessible at https://github.com/hou-yz/MVDeTr.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1449", "problem_id": "14490001", "content": "Accurate sales forecasting is essential for E-commerce businesses, but current techniques often rely on univariate methods that only consider historical sales data for a single product. However, when numerous related time series are available, leveraging the past behavior of similar time series can enhance forecast accuracy. The product hierarchy in E-commerce platforms comprises numerous related products with correlated sales demand patterns, prompting our approach to integrate cross-series information into a unified model. To achieve this, we train a Long Short-Term Memory (LSTM) network globally to capture non-linear demand relationships within the product assortment hierarchy. Additionally, we propose a pre-processing framework to address E-commerce challenges and introduce product grouping strategies to support LSTM learning in cases where sales patterns vary across products. Our forecasting framework is empirically evaluated using a real-world dataset from Walmart.com, yielding competitive results at the category and super-departmental levels that surpass state-of-the-art techniques.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1450", "problem_id": "14500001", "content": "A major obstacle in deep reinforcement learning is achieving data efficiency, which we tackle by leveraging unlabeled data to pretrain an encoder that is subsequently fine-tuned on a limited task-specific dataset. To foster the development of representations that encompass a wide range of aspects of the underlying Markov decision process, we utilize a dual approach combining latent dynamics modeling and unsupervised goal-conditioned reinforcement learning. In a setting where interaction is restricted to 100k steps on Atari games, equivalent to approximately two hours of human experience, our method substantially outperforms existing approaches that combine offline representation pretraining with task-specific fine-tuning, and exhibits favorable comparison to other pretraining techniques that necessitate vastly larger amounts of data. Notably, our approach demonstrates considerable potential when integrated with larger models and more diverse, task-aligned observational data, ultimately approaching human-level performance and data efficiency in the Atari domain in our optimal configuration, with accompanying code available at https://github.com/mila-iqia/SGI.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1451", "problem_id": "14510001", "content": "Education is crucial for societal advancement, as it disseminates knowledge and prepares future generations. Effective educators carefully choose materials, implement appropriate methods, and design assessments tailored to student learning styles. However, artificial intelligence research has predominantly focused on machine learning, neglecting the potential of automated teaching strategies. This paper posits that teaching deserves equal or greater attention, advocating for an optimization framework to develop effective teaching strategies, termed `learning to teach'. This approach involves two interacting intelligent agents: a student model (akin to the learner in traditional machine learning) and a teacher model. The teacher model selects the data, loss function, and hypothesis space to optimize student model training. Utilizing reinforcement learning, the teacher model refines its strategies based on student model feedback, enabling a co-evolutionary learning process. To validate this approach, we apply learning to teach to train deep neural networks (DNNs), demonstrating that it achieves comparable accuracy with significantly less training data and fewer iterations across various DNN architectures (e.g., multi-layer perceptron, convolutional neural networks and recurrent neural networks) and machine learning tasks (e.g., image classification and text understanding).", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1452", "problem_id": "14520001", "content": "As machine learning gains widespread adoption, domain expertise in diverse forms has become essential for enhancing model performance, particularly in scenarios with scarce training data. However, quantifying the precise impact of domain knowledge on machine learning tasks remains poorly understood. To enhance transparency and systematically evaluate the influence of domain knowledge, we investigate its measurable contribution within informed machine learning frameworks. Our approach introduces a Shapley value-based quantification technique that equitably distributes performance gains across various domain knowledge inputs. Additionally, we employ Monte-Carlo sampling to efficiently approximate these contributions with polynomial time complexity. Experiments involving symbolic domain knowledge in semi-supervised learning tasks on MNIST and CIFAR10 datasets demonstrate quantitative assessments of different knowledge types and rigorously analyze their effects on test accuracy.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1453", "problem_id": "14530001", "content": "A semi-automated statistical approach is presented for identifying and separating objects within grayscale and color images, leveraging the fact that distinct objects are often characterized by pixel values that follow narrow Gaussian distributions centered around specific mean values. Notably, the Gaussian distributions associated with visually distinct objects exhibit minimal overlap, resulting in substantial Mahalanobis distances between them. By exploiting these statistical properties, images can be subdivided into multiple variable-sized thresholds, each isolating similar objects. The method takes into account the human eye's sensitivity to grayscale pixel values when determining threshold sizes, and achieves object separation by mapping Gaussian distributions to localized δ-functions. The efficacy of this recursive statistical algorithm is validated through its application to a diverse range of images, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1454", "problem_id": "14540001", "content": "Achieving high-density 3D reconstruction with minimal computational overhead is a key objective in SLAM research. This study introduces a framework for dense 3D reconstruction from monocular multispectral video by integrating semi-dense SLAM and Multispectral Photometric Stereo techniques. The proposed method (a) generates a semi-dense 3D model that is later refined; (b) estimates a sparse depth map, which serves as an initial input for optimization-based multispectral photometric stereo to enhance surface normal accuracy; and (c) determines camera pose, facilitating view transformation during fusion. Here, a sparse point cloud is merged with dense surface normals through an automated cross-scale fusion approach, yielding a detailed, textured dense point cloud. Experimental results demonstrate the framework's effectiveness in producing higher-density 3D reconstructions.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1455", "problem_id": "14550001", "content": "Deep neural networks for object detection are now fundamental to numerous real-world applications, but their susceptibility to adversarial threats raises significant concerns. To explore this vulnerability, we introduce the contextual camouflage attack (CCA) algorithm, designed to disrupt object detector performance. Our approach employs an evolutionary search technique combined with adversarial machine learning within a photorealistic simulated environment, identifying camouflage patterns that remain effective across diverse object positions, camera angles, and lighting scenarios. These camouflages demonstrate high efficacy against most leading object detection models.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1456", "problem_id": "14560001", "content": "Convolution and self-attention serve as core components in deep neural networks, with convolution linearly capturing local image features and self-attention modeling high-order non-local contextual relationships. While these operations are inherently complementary—addressing first-order and high-order features—current architectures like CNNs or transformers struggle to integrate both efficiently within a single module due to their differing computational patterns and the high cost of global dot-product operations in vision tasks. This study presents a theoretical framework for approximating global self-attention through convolution applied to transformed features. Leveraging this approximation, we design a multi-branch module that combines convolution and self-attention, enabling unified local and non-local feature interactions. Notably, after training, this module can be structurally re-parameterized into a single standard convolution, resulting in a purely convolution-based operator called X-volution, which can be seamlessly integrated into existing networks. Experimental results show that X-volution delivers significant performance gains (+1.2% top-1 accuracy on ImageNet, +1.7 box AP and +1.5 mask AP on COCO detection and segmentation).", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1457", "problem_id": "14570001", "content": "The existence of adversarial examples, which are natural images with subtle perturbations that cause deep networks to misclassify them, has been extensively shown in the context of image classification. This study expands the concept of adversarial examples to the more challenging tasks of semantic segmentation and object detection. Noting that both segmentation and detection involve classifying multiple targets within an image, such as pixels or object proposals, we develop a method to optimize a loss function over a set of these targets to generate adversarial perturbations. This approach leads to the proposal of a new algorithm, Dense Adversary Generation (DAG), capable of producing a wide range of adversarial examples that can be applied to various state-of-the-art deep networks for segmentation and detection. Furthermore, our findings indicate that adversarial perturbations can be effectively transferred across different networks, including those with distinct training data, architectures, and recognition tasks, with the most significant transferability observed among networks sharing the same architecture. Additionally, combining heterogeneous perturbations often enhances transfer performance, providing a viable strategy for launching black-box adversarial attacks, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1458", "problem_id": "14580001", "content": "The analysis of crowd behavior automatically is a crucial aspect of intelligent transportation systems, facilitating efficient flow management and adaptive route planning for diverse road users. A fundamental component of this analysis is crowd counting, which has seen significant advancements through the application of deep convolutional neural networks (CNNs) in recent years. Researchers have invested considerable effort into creating various CNN architectures, most of which build upon the pre-trained VGG16 model. However, due to VGG16's limited expressive power, it is typically complemented by an additional, complex network specifically tailored for enhanced counting performance. Although Inception models have surpassed VGG models in image classification, existing crowd counting networks utilizing Inception modules still consist of relatively few layers and only basic Inception configurations. To address this shortcoming, this study first evaluates the baseline Inception-v3 model on widely used crowd counting datasets and achieves unexpectedly strong performance, matching or exceeding that of many current crowd counting systems. Building on this foundational work, we advance further by introducing a Segmentation Guided Attention Network (SGANet), which uses Inception-v3 as its backbone along with a novel curriculum loss for improved crowd counting. Comprehensive experiments demonstrate that our SGANet significantly outperforms previous models, attaining state-of-the-art outcomes with mean absolute errors (MAE) of 57.6, 6.3, and 87.6 on the ShanghaiTechA, ShanghaiTechB, and UCF_QNRF datasets, respectively.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1459", "problem_id": "14590001", "content": "This study assesses the efficacy of semi-supervised learning (SSL) within a practical benchmark characterized by substantial class imbalance and the inclusion of images representing unseen classes. The benchmark is constructed from two fine-grained classification datasets, derived by sampling classes from the Aves and Fungi taxonomies. Results indicate that recent SSL techniques offer notable advantages, demonstrating an ability to leverage out-of-class data to enhance performance when training deep networks from the ground up. However, their performance is significantly lower than that of a transfer learning baseline, which presents another way of learning from limited data samples. Moreover, while current SSL methods can improve transfer learning, the existence of out-of-class data frequently has negative impacts. In this transfer learning context, fine-tuning, succeeded by self-training using distillation methods, yields the most stable outcomes. This research implies that, for realistic datasets, semi-supervised learning with expert models might necessitate strategies that differ from those typically found in the current literature.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1460", "problem_id": "14600001", "content": "The performance of deep learning models in recognizing human actions from skeleton data is noteworthy, but their vulnerability to adversarial attacks has not been thoroughly investigated, largely due to the intricate spatio-temporal characteristics of skeleton joints. This study introduces the first adversarial attack on graph convolutional networks for skeleton-based action recognition, proposing a targeted attack called Constrained Iterative Attack for Skeleton Actions (CIASA). CIASA generates adversarial action sequences by perturbing joint locations while maintaining temporal coherence, spatial integrity, and anthropomorphic plausibility, achieved by enforcing multiple physical constraints and utilizing spatial realignments and generative network regularization. The results demonstrate the effectiveness of CIASA in launching semantically imperceptible localized attacks that deceive state-of-the-art models with high confidence, exhibiting high transferability in black-box attacks, and inducing adversarial behavior in computer-generated RGB videos. A comprehensive evaluation using NTU and Kinetics datasets confirms the efficacy of CIASA in compromising graph-based skeleton action recognition, highlighting the significant threat it poses to spatio-temporal deep learning tasks in general, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1461", "problem_id": "14610001", "content": "The process of image quantization is utilized in various applications to decrease the color palette of an image, thereby reducing its size. Conversely, de-quantization involves reversing this effect to restore the original image with its full range of colors. Since de-quantization is an ill-posed problem, existing methods rely on imposing constraints on the ideal image to facilitate recovery. This study aims to establish a de-quantization framework grounded in classical statistical estimation theory, incorporating generative modeling as prior information to inform the ideal image. The proposed technique is straightforward and effective in de-quantizing images that have undergone significant quantization, and notably, it can also recover images even when the quantization process is not precisely known or contains unknown parameters, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1462", "problem_id": "14620001", "content": "The Information Bottleneck (IB) technique (\\cite) offers a principled and insightful framework for optimizing the balance between compression and prediction in representation learning. The IB objective I(X;Z)−βI(Y;Z) uses a Lagrange multiplier β to adjust this trade-off. Yet, in practice, β is often selected empirically without theoretical justification, and the relationship between β, learnability, dataset characteristics, and model capacity remains poorly understood. This work demonstrates that an inappropriate choice of β can prevent learning, causing the trivial solution P(Z|X)=P(Z) to emerge as the global optimum of the IB objective. We reveal how to circumvent this issue by identifying a sharp phase transition between learnable and unlearnable regimes as β changes, introducing the notion of IB-Learnability. We establish multiple sufficient conditions for IB-Learnability, offering theoretical insights for selecting β effectively. Additionally, we find that IB-learnability depends on the largest confident, typical, and imbalanced subset of data (the conspicuous subset) and explore its connection to model capacity. Practical algorithms are provided to estimate the minimum β for a given dataset, and we validate our theoretical findings through experiments on synthetic datasets, MNIST, and CIFAR10.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1463", "problem_id": "14630001", "content": "Identifying diverse representations of the same real-world entity is a crucial aspect of data cleaning, with numerous methods proposed to address this challenge. However, the entity resolution task is often hindered by its task-specific and user-dependent nature, prompting significant research efforts. Leveraging deep learning techniques can help mitigate these challenges. This paper proposes a novel entity resolution approach that exploits the robustness of deep autoencoders to minimize human involvement costs. By utilizing unsupervised representation learning, we reduce the training costs of deep entity resolution models, which in turn enables transfer learning and reduces the costs associated with applying the model to new datasets. Furthermore, an active learning approach, built upon the properties of deep autoencoders, is employed to decrease the costs of labeling training data. Empirical evaluation demonstrates that our approach achieves a reduction in costs while maintaining comparable effectiveness to state-of-the-art alternatives, as shown in Figure A, B, C (References [1], [2], and [3] provide further details).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1464", "problem_id": "14640001", "content": "The diagnosis of gastric cancer using deep learning techniques typically relies on convolutional neural networks, but the Visual Transformer has recently gained significant attention due to its impressive performance and efficiency, primarily in the computer vision domain. This paper proposes a novel multi-scale visual transformer model, termed GasHis-Transformer, specifically designed for Gastric Histopathological Image Classification (GHIC), enabling the automated distinction between abnormal and normal microscopic gastric images. The GasHis-Transformer model comprises two crucial modules: a global information module and a local information module, which work in tandem to effectively extract histopathological features. Experimental evaluations were conducted on a publicly available dataset of 280 hematoxylin and eosin (H&E) stained gastric histopathological images, divided into training, validation, and test sets in a 1:1:2 ratio, yielding precision, recall, F1-score, and accuracy of 98.0%, 100.0%, 96.0%, and 98.0%, respectively, on the test set. Additionally, the model's robustness was assessed by introducing ten different types of noise, including four adversarial attacks and six conventional image noises, and its performance in identifying gastrointestinal cancer was evaluated using 620 abnormal images, achieving an accuracy of 96.8%. The model's generalizability was further tested on a lymphoma image dataset and a breast cancer dataset, comprising both H&E and immunohistochemical stained images, resulting in comparable F1-scores of 85.6% and 82.8% and accuracies of 83.9% and 89.4%, respectively, demonstrating the GasHis-Transformer's high classification performance and significant potential in the GHIC task.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1465", "problem_id": "14650001", "content": "We introduce onion-peel networks for the task of video completion. Using a set of reference images alongside a target image that has gaps, our network systematically fills these gaps by drawing on the content of the reference images. The onion-peel network incrementally addresses the gaps starting from the boundary, allowing it to leverage richer contextual information with each iteration. Given an adequate number of iterations, even large gaps can be successfully inpainted. To effectively capture the missing information apparent in the reference images, we utilize an asymmetric attention block that assesses similarities between the boundary pixels of the target and the non-hole pixels in the references in a non-local fashion. This attention mechanism enables our network to have an unconstrained spatial-temporal window size, allowing for the completion of holes with coherent global content. Additionally, our framework supports image completion guided by reference images without requiring modifications, a challenge faced by prior methods. We demonstrate that our approach yields visually pleasing results in image and video inpainting across realistic test scenarios.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1466", "problem_id": "14660001", "content": "Recent advances in image inpainting have been driven by generative adversarial networks (GANs), which excel at producing realistic content. Nevertheless, many current GAN-based semantic inpainting approaches rely on auto-encoder structures with fully connected layers, often failing to preserve precise spatial details. Moreover, existing GAN discriminators face challenges in interpreting high-level contextual semantics, leading to inconsistent outputs. Traditional evaluation metrics tend to favor blurred reconstructions, inadequately assessing edge sharpness and visual realism. To address these issues, we present an enhanced GAN model featuring a fully convolutional generator for improved spatial structure retention and a refined joint loss incorporating perceptual loss to better grasp contextual semantics. Additionally, we propose two new evaluation metrics for more accurate inpainting quality assessment. Extensive experiments confirm our method's superior performance across multiple benchmarks.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1467", "problem_id": "14670001", "content": "When confronted with new tasks that come with limited data—such as introducing new classes in a classification issue or encountering a shift in the input domain—the performance of contemporary vision systems declines significantly. In this study, we demonstrate that the neural network representations that form the basis of these vision systems are prone to supervision collapse, meaning they discard any information not essential for the current training task, including that which may be vital for adaptation to new tasks or domains. We then introduce two approaches to address this challenge. The first approach utilizes self-supervised learning to promote the emergence of general-purpose features that transfer more effectively. The second approach involves a new Transformer-based neural network architecture known as CrossTransformers, which can analyze a small set of labeled images alongside an unlabeled query, establish coarse spatial correspondences between the query and labeled images, and subsequently infer class membership by calculating distances among the spatially-corresponding features. This results in a classifier that demonstrates increased resilience to shifts in tasks and domains, as evidenced by its state-of-the-art performance on the Meta-Dataset, a contemporary dataset designed to assess transfer capabilities from ImageNet to various other vision datasets.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1468", "problem_id": "14680001", "content": "Eliminating duplicates is an essential process for achieving a reasonable number of predictions in widely used proposal-based object detection systems. While straightforward and efficient, many prior algorithms rely on a greedy approach that fails to fully capitalize on the characteristics of the input data. In this study, we introduce a novel two-stage framework designed to effectively identify the suitable proposal candidate for each object. The initial stage filters out the majority of straightforward negative object proposals, whereas the second stage identifies true positives from the reduced set of proposals. Both stages utilize the same network architecture, specifically an encoder and a decoder implemented as recurrent neural networks (RNN) with global attention and context gating. The encoder processes the proposal candidates sequentially to gather global contextual information, which is subsequently used by the decoder to derive optimal proposals. Through comprehensive experiments, our proposed method demonstrates significant superiority over other existing approaches.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1469", "problem_id": "14690001", "content": "Skeleton-based action recognition is gaining traction due to its robustness in dynamic environments and its wide-ranging applications, including autonomous and anonymous surveillance. Leveraging deep learning, this field has made significant strides, achieving approximately 90\\% accuracy in favorable conditions. However, the vulnerability of skeleton-based action recognition to adversarial attacks remains largely unexplored, raising security concerns for real-world deployment. Addressing this gap is difficult due to the inherent physical limitations of skeletons and human movements. In this paper, we present a comprehensive investigation into the adversarial vulnerability of skeleton-based action recognition. We model the generation of adversarial skeleton actions as a constrained optimization problem, using mathematical formulations to represent or approximate physiological and physical constraints. Due to the intractability of the primal optimization problem with equality constraints, we propose solving its unconstrained dual using ADMM. Furthermore, we introduce an efficient plug-in defense mechanism, drawing inspiration from recent theoretical and empirical findings, to counteract adversarial skeleton actions. Extensive evaluations validate the efficacy of both the proposed attack and defense methods across various scenarios.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1470", "problem_id": "14700001", "content": "This research addresses a core challenge in Bayesian learning: identifying the most cost-effective set of data sources that meet specified performance criteria based on their data streams. Initially, we demonstrate that this data source selection problem is NP-hard. We then reformulate it as a submodular set covering problem and present a conventional greedy algorithm with established performance bounds. Additionally, we introduce an accelerated greedy approach that reduces computational time while maintaining comparable guarantees to the standard method. This efficient algorithm is also applicable to broader submodular set covering problems. Numerical experiments confirm the theoretical findings, illustrating the practical effectiveness of the proposed greedy algorithms.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1471", "problem_id": "14710001", "content": "Dropout, a widely used method for training deep neural networks, serves as a regularization technique to prevent overfitting in large models, with its effectiveness often attributed to reducing co-adaptation among nodes. However, analyzing node correlations in networks trained with and without dropout raises doubts about whether this explanation fully accounts for dropout’s impact. This paper introduces an alternative perspective on dropout’s mechanism and presents a novel approach for enhancing activation functions. We demonstrate that dropout functions as an optimization strategy, directing inputs toward the saturation region of nonlinear activation functions by facilitating gradient flow even in saturated zones during backpropagation. Building on this insight, we develop , a technique that enhances gradient propagation in saturation regions, enabling inputs to reach flatter areas and thereby improving model robustness. Experimental results validate our interpretation of dropout and show that GAAF enhances image classification performance while exhibiting the anticipated characteristics.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1472", "problem_id": "14720001", "content": "Deep neural networks typically necessitate substantial labeled data for effective training in visual feature learning from images or videos within computer vision. To circumvent the considerable expense of amassing and annotating such extensive datasets, self-supervised learning techniques—a category of unsupervised learning—have emerged. These techniques facilitate the acquisition of general image and video features from voluminous unlabeled data, thereby eliminating the need for human-provided labels. This paper comprehensively reviews deep learning-based self-supervised methodologies for general visual feature learning from images or videos. It begins by outlining the impetus, overarching framework, and key concepts of this domain. Subsequently, it consolidates prevalent deep neural network architectures employed in self-supervised learning. The core elements and evaluation criteria of self-supervised learning are then examined, succeeded by a survey of frequently utilized image and video datasets and existing self-supervised visual feature learning approaches. Furthermore, it presents and analyzes quantitative performance comparisons of the discussed methods on standard datasets for both image and video feature learning. The paper culminates with concluding remarks and a delineation of potential future research avenues in self-supervised visual feature learning.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1473", "problem_id": "14730001", "content": "The task of graph-based multi-view clustering, which seeks to partition data across multiple views, has garnered significant interest in recent years. Despite substantial progress in this area, effectively integrating characteristics from diverse views to learn a unified representation for clustering remains a formidable challenge. To address this, we introduce a novel framework, Consistent Multiple Graph Embedding Clustering (CMGEC), which leverages a multiple graph auto-encoder (M-GAE) equipped with a multi-graph attention fusion encoder to capture the complementary information inherent in multi-view data. Additionally, a Multi-view Mutual Information Maximization (MMIM) module is incorporated to ensure the learned common representation preserves the similarity of neighboring characteristics within each view. A graph fusion network (GFN) is also designed to investigate the relationships between graphs from different views, ultimately yielding a consensus graph that informs the M-GAE. Through joint training of these components, a common latent representation is obtained, encoding richer complementary information from multiple views and providing a more comprehensive depiction of the data. Experimental evaluations on three multi-view datasets demonstrate the superiority of CMGEC over existing state-of-the-art clustering methods.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1474", "problem_id": "14740001", "content": "Property inference attacks expose statistical characteristics of a training dataset; however, differentiating these attacks from the fundamental goal of statistical machine learning—creating models that reflect the statistical properties of a distribution—is challenging. Inspired by Yeom et al.'s membership inference framework, we introduce a well-defined, broad definition for property inference attacks. This definition encompasses attacks that discern between potential training distributions, going beyond prior property inference attacks focused on inferring the proportion of specific data types within the training set. We demonstrate that our definition includes existing property inference attacks and a novel attack capable of revealing a training graph's average node degree or clustering coefficient. Furthermore, our definition facilitates a theorem linking the highest achievable accuracy of inference attacks in distinguishing between distributions to the effective dataset size leaked by the model. To assess and comprehend property inference risks, we perform several experiments across diverse distributions using both black-box and white-box attack methodologies. Our findings indicate that simple attacks are often as effective as complex meta-classifier attacks and reveal unexpected asymmetries in attack effectiveness. We also adapt the most advanced property inference attack to function with convolutional neural networks and introduce methods to pinpoint parameters within a model that leak the most information, thereby considerably reducing the resource demands for meta-classifier attacks.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1475", "problem_id": "14750001", "content": "For autonomous vehicles to navigate effectively among pedestrians, it is essential to predict crowd movement rapidly and precisely, while also quantifying the uncertainty inherent in these predictions. Current methodologies often rely on repeated sampling of generative models to estimate uncertainty. Moreover, many predictive models are trained using datasets that presume full crowd visibility from an overhead perspective, which does not reflect actual vehicle sensor limitations and can result in underestimated uncertainty, particularly when sensors are obstructed. Drawing upon previous research in motion prediction that utilizes spatio-temporal graphs, we introduce Attentional-GCNN, a new Graph Convolutional Neural Network (GCNN)-based method. This approach captures implicit pedestrian interactions by assigning attention weights to graph edges. Our model can be trained to produce either a probabilistic distribution or a faster deterministic prediction, making it suitable for autonomous vehicle applications that demand either speed or accuracy with uncertainty quantification. To enhance predictive model training, we also introduce a pedestrian dataset, automatically labelled and gathered from an intelligent vehicle platform, which mirrors real-world scenarios. Experimental results across several datasets demonstrate that our proposed method surpasses state-of-the-art performance by 10% in Average Displacement Error (ADE) and 12% in Final Displacement Error (FDE), while maintaining rapid inference speeds.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1476", "problem_id": "14760001", "content": "Unsupervised domain adaptation (UDA) typically relies on simultaneous training with freely accessible source and target domain data to minimize domain discrepancies; however, this approach is often unrealistic due to data privacy concerns and transmission inefficiencies. Consequently, there is growing interest in optimizing networks within the target domain without access to labeled source data. Addressing this, we introduce a novel source data-free domain adaptive object detection (SFOD) framework, framing it as a noisy label learning problem. A common strategy involves using a pre-trained source domain network to create pseudo-labels for target domain optimization, but assessing the quality of these labels is challenging due to the absence of target domain labels. To address this, we propose a self-entropy descent (SED) metric to identify an appropriate confidence threshold for generating reliable pseudo-labels without manual annotation. Recognizing that perfectly clean labels remain elusive, our analysis reveals a prevalence of false negatives in the generated noisy labels. To mitigate this, we employ data augmentation techniques, such as Mosaic, to simulate false negatives, thereby enhancing performance. Comprehensive experiments across four distinct adaptation tasks demonstrate that our framework achieves state-of-the-art results, suggesting that existing UDA methods may not fully utilize labeled source data.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1477", "problem_id": "14770001", "content": "The susceptibility of deep networks to adversarial attacks has attracted global research attention. Similar to images, time-series data is also prone to adversarial examples, as this vulnerability stems from the model itself rather than the data type. While numerous defense strategies have been developed, especially for visual data, this study conducts a comprehensive evaluation of established adversarial defense techniques on time-series datasets, focusing on the L_ threat model. Additionally, we investigate the balance between smoothness and clean accuracy in regularization-based defenses to assess their trade-offs. Our findings reveal that these defenses provide resilience against both white-box and black-box attacks, encouraging further exploration of adversarial attack and defense mechanisms, particularly for time-series applications.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1478", "problem_id": "14780001", "content": "This study introduces a classification algorithm designed for large-scale datasets. Experimental results demonstrate that the method achieves accuracy comparable to ensemble techniques like random forests and gradient boosted trees, while offering greater interpretability. The algorithm employs a divide-and-conquer approach with two key phases: initially, a decision tree partitions the dataset, aiming for homogeneous class distributions in leaf nodes, though some heterogeneity typically remains. Subsequently, a specialized classifier assigns labels to these non-uniform leaf nodes. The decision tree offers broad segment characteristics, while the leaf-level classifier reveals attribute-level influences on labeling within each segment.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1479", "problem_id": "14790001", "content": "The accurate diagnosis of sleep disorders relies heavily on sleep staging, a complex and time-consuming process that requires a trained expert to spend several hours annotating a single patient's polysomnogram (PSG) from one night, highlighting the need for automation. Despite the success of deep learning models in achieving state-of-the-art performance in sleep staging, the aspect of interpretability has been largely overlooked. This study introduces Sleep staging via Prototypes from Expert Rules (SLEEPER), a novel approach that integrates deep learning models with expert-defined rules using a prototype learning framework, resulting in the generation of simple and interpretable models. By leveraging sleep scoring rules and expert-defined features, SLEEPER derives prototypes from PSG data fragments using convolutional neural networks, ultimately yielding models such as shallow decision trees defined over these phenotypes. The evaluation of SLEEPER on two PSG datasets from sleep studies showed that it achieved accurate sleep stage classification, comparable to human experts and deep neural networks, with a ROC-AUC of approximately 85% and a kappa of .7, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1480", "problem_id": "14800001", "content": "The application of computer vision to recognize actions in ice hockey is hindered by the presence of bulky equipment and poor image quality, presenting a significant challenge. To address this, a novel framework has been developed, comprising three primary components, to enhance the accuracy of action recognition in hockey. The framework initially estimates the pose of players using the Part Affinity Fields model, which extracts relevant cues, and then utilizes LiteFlowNet to derive temporal features via optical flow. The pose and optical flow streams are subsequently combined and fed into fully-connected layers to predict the actions of hockey players. The introduction of a new publicly available dataset, HARPET (Hockey Action Recognition Pose Estimation, Temporal), which includes annotated sequences of actions and poses of hockey players along with their sticks, has facilitated this research. The key contributions of this work include: (1) the proposed two-stream architecture, which achieves an action recognition accuracy of 85%, with a 10% increase in accuracy attributed to the incorporation of optical flows; (2) the innovative approach of localizing hand-held objects, such as hockey sticks, as an extension of human pose, resulting in a 13% increase in accuracy; and (3) the successful application of transfer learning from the larger MSCOCO dataset to the smaller HARPET dataset for pose estimation, yielding a PCKh of 87%.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1481", "problem_id": "14810001", "content": "The achievements of deep learning in visual tasks can be credited to several factors: (a) models with substantial capacity, (b) enhanced computational resources, and (c) the existence of extensive labeled datasets. Since 2012, notable improvements have been made in both model representation abilities and GPU computational power. However, it is surprising that the largest datasets have not seen a significant increase. What might occur if the size of datasets were to expand by 10 or even 100 times? This paper aims to demystify the connection between \"massive data\" and visual deep learning. By leveraging the JFT-300M dataset, which contains over 375 million noisy labels for 300 million images, we explore the potential changes in performance on current vision tasks if this data were utilized for representation learning. Our findings yield a mix of unexpected and anticipated results. Firstly, we observe that the performance in vision tasks improves logarithmically with the training data size. Secondly, our results indicate that representation learning (or pre-training) remains highly beneficial, as enhancing the base model can lead to better performance across various vision tasks. Lastly, as anticipated, we report new state-of-the-art outcomes for a range of vision tasks, such as image classification, object detection, semantic segmentation, and human pose estimation. We sincerely hope that these insights encourage the vision community to appreciate the value of data and collaborate on creating larger datasets.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1482", "problem_id": "14820001", "content": "This study explores the potential of reinforcement learning, leveraging deep neural network-based function approximation, for applications in wireless radio control and signal detection. Our preliminary findings showcase a novel approach to radio control, enabling the learning of search protocols without relying on predefined features, heuristics, or search strategies. Additionally, we propose Kerlym, an open-source collection of reinforcement learning agents built on Keras, integrated with OpenAI's Gym, to facilitate further research and development in this area, as seen in Figure A (Reference [1], Citation [2]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1483", "problem_id": "14830001", "content": "The diagnosis, treatment, and monitoring of human diseases increasingly rely on a range of imaging modalities, including magnetic resonance imaging (MRI), computed tomography (CT), Ultrasound (US), and Positron-emission tomography (PET), as well as modern optical techniques. Over the past two decades, it has become apparent that advanced image processing techniques can provide physicians with valuable insights for diagnosis, image-guided therapy and surgery, and post-treatment monitoring. Significant efforts have been invested by researchers and companies in developing sophisticated medical image analysis methods, with a focus on two key areas: medical image segmentation and registration. Segmentation of organs and lesions enables the quantification of volumes and shapes, which is crucial for diagnosis and treatment monitoring, while registration of multimodality images enhances disease detection, diagnosis, and staging, as well as image-guided surgery and therapy. The registration of images from the same modality also facilitates the monitoring of treatment progression. These clinically motivated applications pose complex mathematical problems, driving the development of advanced optimization and computing methods, particularly convex optimization, which can achieve global optima. As a result, computational medical image analysis has expanded to encompass a wide range of research topics. Notably, medical images often present unique challenges, including large volumes of acquired data (frequently in 3D or 4D), significant noise, and incomplete information, leading to large-scale optimization problems. Efficiently processing these complex \"big data\" medical images and robustly solving the associated optimization problems are critical factors in modern medical image analysis.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1484", "problem_id": "14840001", "content": "Deep generative models have evolved significantly since their introduction, now capable of producing novel and highly realistic signals such as images and sound waves. As deep learning techniques for graph-structured data have advanced, there is growing interest in adapting these generative models for graphs. While progress has been made in learning from multiple graphs (e.g., protein databases), single-graph learning remains largely unexplored. This scenario is crucial in practice, as sensitive data in fields like finance and healthcare often cannot be shared publicly, yet access to comparable datasets is essential for scientific benchmarking. Our work introduces a method to create a doppelganger graph that closely mimics the properties of a given graph while minimizing edge overlap, preventing reverse engineering. This approach integrates graph representation learning, generative adversarial networks, and graph realization algorithms. Compared to existing generative models—whether neural network-based or not—our method produces graphs that replicate key properties without directly copying the original structure. Additionally, we demonstrate that downstream tasks like node classification perform comparably on the generated graphs as on the original ones.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1485", "problem_id": "14850001", "content": "Significant advancements in multiple view geometry have been driven by reliable point feature extraction and innovations in projective geometry, enabling improved reconstruction and calibration. However, point features are sometimes unavailable, leading to unstructured point cloud outputs, while general image curves offer an alternative in keypoint-scarce scenarios, producing 3D curve geometry. Yet, these curves present unique challenges beyond conventional projective geometry for points and algebraic curves. To tackle these issues, we establish a theoretical framework grounded in the differential geometry of diverse curves—such as stationary curves, occluding contours, and non-rigid curves—focusing on stereo correspondence, camera estimation (calibration, pose, and multiview epipolar geometry), and 3D reconstruction from image curves. Consolidating prior findings into a unified theory enables three key contributions: first, deriving an image curve’s differential geometry (tangent, curvature, curvature derivative) from its 3D counterpart (tangent, curvature, curvature derivative, torsion); second, inferring a space curve’s differential geometry from two matching image curves; and third, predicting an image curve’s motion from camera movement and the space curve’s differential properties. This framework supports new curve-based multiview reconstruction and camera estimation techniques, complementing point-based methods. Applications include generating a \"3D curve sketch,\" estimating camera pose from local curve geometry, and tracking, with further developments in progress.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1486", "problem_id": "14860001", "content": "State-of-the-art machine learning models are frequently trained on large-scale distributed clusters. Importantly, these systems can be vulnerable when some computing devices display abnormal (Byzantine) behavior and provide arbitrary outputs to the parameter server (PS). This abnormal behavior can arise from various causes, including system failures and coordinated attacks. Previous research has recommended robust aggregation techniques and/or computational redundancy to mitigate the impact of distorted gradients. However, many of these strategies are ineffective if an attacker is aware of the task assignment and can strategically select which workers to target for maximum disruption. In response, our proposed method, Aspis, allocates gradient calculations to worker nodes through a subset-based assignment that facilitates multiple consistency checks on each node's behavior. By analyzing the computed gradients and implementing post-processing (clique-finding within a suitably constructed graph) at the central node, we can effectively identify and exclude adversaries from the training procedure. We demonstrate the Byzantine resilience and detection capabilities of Aspis against both weak and strong attacks and conduct extensive evaluations of the system across various large-scale training scenarios. The primary evaluation metric is test accuracy, where we show a notable improvement of approximately 30% over numerous leading approaches on the CIFAR-10 dataset, alongside a corresponding reduction in the proportion of corrupted gradients that varies from 16% to 98%.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1487", "problem_id": "14870001", "content": "The alarming rate of plant habitat loss worldwide underscores the need for collaborative conservation efforts to preserve plant biodiversity, highlighting the critical importance of accurate plant species classification in addressing this environmental issue. The field of plant taxonomy has experienced significant growth in recent years, with researchers exploring various approaches to enhance recognition performance, optimizing computational frameworks, and investigating feature extraction techniques to improve accuracy. This study presents a novel method for leaf recognition, which involves pre-processing leaves to extract refined color images, vein images, xy-projection histograms, handcrafted shape and texture features, and Fourier descriptors, and then utilizes neural network-based encoders to transform these attributes into a more effective representation, followed by classification using a support vector machine (SVM) model. The proposed approach yields state-of-the-art results on the Flavia leaf dataset, achieving an accuracy of 99.58% on test sets under random 10-fold cross-validation, outperforming previous methods, and the corresponding codes are made available at https://github.com/dinhvietcuong1996/LeafRecognition to contribute to the research community in leaf classification.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1488", "problem_id": "14880001", "content": "Many visual question answering (VQA) datasets feature basic counting tasks primarily solvable through object detection. This work investigates algorithms designed for intricate counting questions necessitating relationship analysis between objects, attribute recognition, logical inference, and related skills. To facilitate this, we introduce TallyQA, the most extensive open-ended counting dataset compiled to date. We present a novel counting algorithm leveraging relation networks in conjunction with region proposals. This approach enables the effective application of relation networks to high-resolution images. Empirical evaluation demonstrates that our method achieves state-of-the-art performance on both the TallyQA and HowMany-QA benchmarks when compared against baseline models and contemporary systems.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1489", "problem_id": "14890001", "content": "We suggest employing boosted regression trees to derive solutions for reinforcement learning challenges that are understandable to humans. Boosting aggregates multiple regression trees to enhance predictive accuracy while maintaining a high level of interpretability. Previous research has primarily concentrated separately on reinforcement learning and interpretable machine learning, with limited advancements in the area of interpretable reinforcement learning. Our experimental findings indicate that boosted regression trees generate solutions that are not only interpretable but also comparable in quality to the best reinforcement learning techniques.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1490", "problem_id": "14900001", "content": "In this study, we investigate unsupervised partitioning challenges, including clustering, image segmentation, video segmentation, and various change-point detection issues. We specifically target partitioning tasks that are grounded in the minimization of Euclidean distortions, encompassing mean-based change-point detection, K-means, spectral clustering, and normalized cuts. Our primary objective is to derive a Mahalanobis metric for these unsupervised tasks, facilitating feature weighting and/or selection. This process is approached in a supervised manner by assuming access to multiple potentially partially labeled datasets that possess the same metric. We frame the metric learning challenge as a large-margin structured prediction problem, carefully defining regularizers and loss functions, which leads to a convex optimization scenario that can be efficiently addressed using iterative methods. We demonstrate through experiments that learning the metric can significantly enhance partitioning performance across synthetic datasets, bioinformatics, video segmentation, and image segmentation tasks.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1491", "problem_id": "14910001", "content": "Effective trajectory planning is essential for autonomous unmanned aerial vehicle (UAV) deployment in next-generation communication networks. This study introduces an end-to-end reinforcement learning (RL) framework for UAV-assisted data collection from Internet of Things (IoT) devices in urban settings, where a drone autonomously gathers data from distributed sensors while adhering to flight time limits and obstacle avoidance. Unlike prior learning and non-learning methods that require costly recomputations or retraining when key parameters like sensor count, locations, or flight duration change, our approach employs a double deep Q-network (DDQN) with combined experience replay to develop a UAV control policy adaptable to varying conditions. Utilizing a multi-layer environmental map processed through convolutional layers, our network architecture enables the UAV to optimize movement decisions across diverse scenarios, balancing data collection objectives with flight efficiency and safety. Additionally, we demonstrate significant learning efficiency gains by using a UAV-centered map compared to a non-centered alternative.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1492", "problem_id": "14920001", "content": "This study introduces a novel hybrid framework combining a deep Convolutional Network with a Markov Random Field to address the complex task of estimating articulated human poses from single images. The proposed model effectively leverages structural domain knowledge, including spatial dependencies among body joints. By jointly training these complementary approaches, the system achieves enhanced performance, surpassing current leading methods in the field.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1493", "problem_id": "14930001", "content": "Automatic photo adjustment aims to replicate the retouching styles of professional photographers by learning and applying their techniques to adjust photos accordingly. Previous attempts have focused on modeling tone and color adjustments using low-level color statistics, as well as spatially varying methods that leverage high-level features and semantic label maps, which are dependent on semantic context. However, these semantics-aware methods are constrained by their reliance on pre-computed, hand-crafted features, limiting their ability to incorporate user preferences. This paper presents a deep neural network that addresses semantics-aware photo adjustment by utilizing bilinear models, which capture the multiplicative interaction between color and contextual features. The proposed semantic adjustment map serves as the contextual feature, uncovering inherent retouching presets that are applied based on scene context. Trained using a robust loss function in conjunction with a scene parsing task, the proposed method demonstrates superior performance to existing approaches in both quantitative and qualitative evaluations, as shown in Figure A, B, C (References [1], [2], [3]). Furthermore, it enables users to retouch photos according to their personal preferences by providing customized adjustment maps, allowing for a more tailored and user-centric photo adjustment experience, as discussed in [4] and [5].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1494", "problem_id": "14940001", "content": "Certain historical and more recent printed materials have been scanned or archived at very low resolutions, such as 60 dpi. While these scans are relatively legible to humans, they pose notable difficulties for optical character recognition (OCR) technologies. The prevailing approach utilizes super-resolution techniques to recreate an approximation of the original high-resolution image, subsequently inputting this into a conventional OCR system. Our innovative end-to-end method eliminates the need for the super-resolution phase and achieves superior OCR results. This strategy is influenced by our insights into the human visual system and leverages well-established neural networks for OCR tasks. Our experiments indicate that OCR can be successfully executed on 60 dpi scanned images of English text, which is a considerably lower resolution than currently accepted standards, yielding a mean character level accuracy (CLA) of 99.7% and word level accuracy (WLA) of 98.9% across approximately 1000 pages of 60 dpi text in various fonts. For 75 dpi images, we attained a mean CLA of 99.9% and a mean WLA of 99.4% on the same dataset. We are making our code and data available to the public, including a collection of low-resolution images along with their ground truths, to serve as a benchmark for future research in this area.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1495", "problem_id": "14950001", "content": "Achieving high performance in anomaly detection for multivariate time-series data relies on effectively modeling the inter-dependencies between time-series. Although recurrent neural networks (RNNs), such as GRU or LSTM, are commonly used to model these dependencies, their fully connected network structure is based on the assumption of a static and complete dependency graph, which may not be valid in many real-world applications. To address this limitation, we introduce a dynamic bipartite graph structure that encodes the inter-dependencies between time-series, where time series and time series segments (or events) are represented as two types of nodes, and the edges between them describe temporal patterns occurring on specific time series at certain times. This design enables explicit modeling of relationships between time series through dynamic connections to event nodes, and formulates the multivariate time-series anomaly detection problem as a self-supervised edge stream prediction problem in dynamic graphs, as demonstrated through extensive experiments, Figure A, B, C, (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1496", "problem_id": "14960001", "content": "The development of deep neural networks with low energy consumption has been facilitated by Adder Neural Networks (ANNs), which rely solely on additions. However, replacing conventional convolution filters with adder filters results in a decline in accuracy, primarily due to the challenges of optimizing ANNs using the \\ell_1-norm, which leads to inaccurate gradient estimation during back propagation. To address this issue, this paper proposes a novel approach that leverages a progressive kernel-based knowledge distillation (PKKD) method to enhance the performance of ANNs without increasing the number of trainable parameters. By simultaneously initializing and training a convolutional neural network (CNN) with the same architecture as a teacher network, the features and weights of both the ANN and CNN are transformed into a new space, effectively mitigating the accuracy drop. The similarity between the two networks is measured in a higher-dimensional space using a kernel-based method, allowing for the disentanglement of their distribution differences. Ultimately, the desired ANN is learned progressively, utilizing information from both the ground-truth and the teacher network. The efficacy of the proposed PKKD method is demonstrated through experiments on several benchmarks, including the ImageNet dataset, where the ANN-50 model trained using PKKD achieves a top-1 accuracy of 76.8%, outperforming the ResNet-50 by 0.6%.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1497", "problem_id": "14970001", "content": "Time-lagged autoencoders (TAEs), a deep learning technique employing regression, have been introduced for identifying slow modes within dynamical systems. Nevertheless, a comprehensive examination of nonlinear TAEs is still absent. This study investigates the strengths and weaknesses of TAEs through both theoretical and numerical methods. Theoretically, we establish performance limits for nonlinear TAEs in identifying slow modes, demonstrating that TAEs typically learn a combination of slow modes and modes of maximum variance. Numerically, we present scenarios where TAEs either succeed or fail to accurately detect the primary slowest mode in two systems: a 2D \"Washington beltway\" potential and the alanine dipeptide molecule in explicit water. Furthermore, we compare TAE outcomes with those from state-free reversible VAMPnets (SRVs), a variational neural network approach for slow mode discovery, revealing that SRVs can accurately identify slow modes in instances where TAEs are unsuccessful.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1498", "problem_id": "14980001", "content": "Machine learning (ML) compilers typically address numerous interrelated optimization challenges to produce efficient machine code. Current approaches employ heuristic-based algorithms that handle these optimizations sequentially, resulting in solutions that are difficult to maintain and often suboptimal, particularly for modern model architectures. While existing learning-based methods attempt to address these issues, they suffer from poor sample efficiency, focus on isolated optimizations, and fail to generalize to unfamiliar computational graphs, limiting their practical applicability. To overcome these challenges, we introduce an end-to-end, transferable deep reinforcement learning framework for computational graph optimization (GO), leveraging a scalable sequential attention mechanism within an inductive graph neural network. Unlike prior methods, GO processes the entire graph holistically rather than node-by-node in an autoregressive manner, significantly accelerating the optimization process. Additionally, we incorporate recurrent attention layers to optimize interdependent graph tasks collectively, achieving 33%-60% speedup over TensorFlow’s default optimizations across three tasks. Evaluated on diverse graphs with up to 80,000 nodes—including Inception-v3, Transformer-XL, and WaveNet—GO demonstrates an average 21% improvement over human experts and 18% over prior state-of-the-art methods, while converging 15x faster in a real-world device placement task.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1499", "problem_id": "14990001", "content": "The growing effectiveness of single-agent reinforcement learning (RL) in IoT applications has spurred interest in multi-agent reinforcement learning (MARL), which, though more complex, offers greater potential for large-scale IoT deployments. This paper examines a voting-based MARL scenario where agents collaborate through voting to reach collective decisions, aiming to optimize global average returns. We model the MARL problem using a linear programming framework for policy optimization and introduce a distributed primal-dual method to derive the optimal solution. Additionally, we design a voting mechanism that ensures distributed learning maintains a sublinear convergence rate comparable to centralized approaches, meaning decentralized decision-making does not hinder the attainment of global optimality. Finally, we validate our algorithm’s convergence through numerical experiments and demonstrate its applicability in real-world multi-agent IoT systems.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1500", "problem_id": "15000001", "content": "This study introduces the Object-aware Feature Aggregation (OFA) module, designed to enhance video object detection (VID) capabilities. The underlying concept is rooted in the notion that object-aware knowledge at the video level can serve as a robust semantic prior, facilitating improved object recognition. By integrating this prior knowledge into feature augmentation, significant enhancements in classification and localization performance can be achieved. To enable features to capture a more comprehensive understanding of the entire video, the proposed approach first extracts object-aware knowledge from proposals and combines it with established pair-wise contexts. Experimental results on the ImageNet VID dataset demonstrate the efficacy of this object-aware knowledge, yielding superior performance with mean average precision (mAP) scores of 83.93% and 86.09% when using ResNet-101 and ResNeXt-101, respectively. Furthermore, when paired with Sequence DIoU NMS, the approach achieves state-of-the-art mAP scores of 85.07% and 86.88%, as reported in the submission. The code for replicating these results will be made available upon acceptance, as stated in the paper.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1501", "problem_id": "15010001", "content": "This study presents a novel generative adversarial network architecture designed for pose transfer, which involves transforming the pose of an individual in an image to a desired target pose. The proposed network features a progressive generator consisting of a series of transfer blocks, each of which performs an intermediate pose transfer step by leveraging an attention mechanism to model the relationship between the input and target poses. Two distinct block types, namely the Pose-Attentional Transfer Block (PATB) and the Aligned Pose-Attentional Transfer Block (APATB), are introduced to facilitate this process. In comparison to existing approaches, the developed model produces more realistic images of persons that better preserve appearance and shape consistency with the original input images, as demonstrated through quantitative and qualitative evaluations on the Market-1501 and DeepFashion datasets. Additionally, the method is shown to be effective for data augmentation in person re-identification tasks, helping to mitigate the problem of insufficient data, and the corresponding code and pre-trained models are available at https://github.com/tengteng95/Pose-Transfer.git.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1502", "problem_id": "15020001", "content": "This research introduces a real-time Visual SLAM framework that ensures privacy by estimating camera poses and conducting bundle adjustment with combined line and point clouds. Prior work has explored localization techniques using line-cloud maps for single images or reconstructed point clouds, which enhance scene privacy by transforming point clouds into line clouds to prevent inversion attacks. However, these approaches lack computational efficiency, making them unsuitable for video sequences. Additionally, no existing method optimizes a server’s line-cloud map with a client’s reconstructed point cloud, as image observation points are unavailable to safeguard against reversibility of 3D lines. Experiments with synthetic and real-world data demonstrate that the proposed framework successfully maintains privacy and operates in real time using a line-cloud map.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1503", "problem_id": "15030001", "content": "Assessing retinal image quality (RIQA) is crucial for ensuring the quality of retinal imaging and the accuracy of diagnoses made by both ophthalmologists and automated systems. Current RIQA techniques primarily operate within the RGB color-space and are trained using limited datasets with binary quality classifications (i.e., `Accept' and `Reject'). This paper introduces an enhanced Eye-Quality (EyeQ) dataset, re-annotated from 28,792 retinal images within the EyePACS dataset, utilizing a three-tiered quality grading system (`Good', `Usable' and `Reject') designed for RIQA method evaluation. This dataset distinguishes itself through its large scale, multi-level grading, and multi-modal nature. We then investigate the impact of various color-spaces on RIQA and introduce a straightforward yet effective deep learning architecture, the Multiple Color-space Fusion Network (MCF-Net). MCF-Net integrates different color-space representations at both the feature and prediction levels to determine image quality grades. Experimental results on our EyeQ dataset demonstrate that MCF-Net achieves state-of-the-art performance, surpassing other deep learning approaches. Moreover, we evaluate diabetic retinopathy (DR) detection methods on images of varying quality, confirming that the performance of automated diagnostic systems is significantly affected by image quality.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1504", "problem_id": "15040001", "content": "To enhance deep learning performance, the mixup technique encourages neural networks to adopt simpler linear relationships between training examples. However, applying mixup to transfer learning with pre-trained models presents challenges, as high-capacity pre-trained models with large fully-connected (FC) layers can easily overfit the target dataset, even with mixed samples and labels. This paper introduces SMILE - Self-Distilled Mixup for EffIcient Transfer LEarning, which, when given mixed images, regularizes the outputs of CNN feature extractors to learn from the mixed feature vectors of inputs (sample-to-feature mixup) in addition to the mixed labels. SMILE employs a mean teacher, derived from the pre-trained model, to provide feature vectors of input samples through self-distillation and mixes up these feature vectors using a novel triplet regularizer. This regularizer balances mixup effects in both feature and label spaces while limiting linearity between samples for pre-training tasks. Comprehensive experiments demonstrate that SMILE improves performance compared to various transfer learning algorithms, including fine-tuning, L2-SP, DELTA, and RIFLE, even when combined with mixup strategies. Ablation studies reveal that standard sample-to-label mixup strategies offer limited generalizability and only marginally increase linearity between training samples. In contrast, SMILE significantly enhances mixup effects in both label and feature spaces for training and testing datasets. These empirical results support our design rationale and objectives.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1505", "problem_id": "15050001", "content": "Video summarization is crucial for video comprehension as it identifies key frames and shots. Traditionally, its purpose has been to extract the most representative and varied segments of a video to create concise summaries. Recently, a broader task known as query-conditioned video summarization has emerged, which incorporates user queries to generate more tailored summaries. In this study, we introduce a query-conditioned three-player generative adversarial network to address this issue. The generator learns the combined representation of the user query along with the video content, while the discriminator evaluates three sets of query-conditioned summaries to differentiate between an actual summary, a generated one, and a random one. A three-player loss function is proposed for the simultaneous training of the generator and the discriminator, compelling the generator to produce higher-quality summaries and preventing the creation of random trivial outputs. Tests conducted on a newly established benchmark dataset for query-conditioned video summarization demonstrate the effectiveness and efficiency of our proposed approach.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1506", "problem_id": "15060001", "content": "This study focuses on predicting nuanced fashion similarity, emphasizing the importance of specific design or attribute similarities between fashion items, such as collar design, which has implications for applications like fashion copyright protection. To achieve this, we introduce an Attribute-Specific Embedding Network (ASEN) that simultaneously learns multiple attribute-specific embeddings, enabling the measurement of fine-grained similarity in the relevant feature space. The ASEN architecture includes a global branch that processes the entire image to extract holistic features and a local branch that analyzes zoomed-in regions-of-interest (RoIs) corresponding to particular attributes, allowing for the extraction of more detailed features. These branches offer complementary perspectives on feature extraction. Furthermore, each branch incorporates Attribute-aware Spatial Attention and Attribute-aware Channel Attention modules, enabling ASEN to identify relevant regions and capture key patterns guided by the specified attribute, thereby enhancing the quality of the learned attribute-specific embeddings for fine-grained similarity assessment. Comprehensive experiments conducted on three fashion datasets, FashionAI, DARN, and DeepFashion, demonstrate ASEN's efficacy in predicting fine-grained fashion similarity and its potential for fashion reranking. The code and data are accessible at https://github.com/maryeon/asenpp .", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1507", "problem_id": "15070001", "content": "Boltzmann exploration is commonly utilized in reinforcement learning to balance exploration and exploitation. However, as demonstrated in (Cesa-Bianchi et al., 2017), traditional Boltzmann exploration exhibits subpar performance regarding regret, even in the most straightforward scenarios of stochastic multi-armed bandit (MAB) problems. In this study, we introduce a straightforward adjustment to Boltzmann exploration, inspired by a variant of the conventional doubling trick, which results in O(K\\log^ T) regret for a stochastic MAB problem with K arms, where \\alpha>0 is a parameter of the algorithm. This modification surpasses the findings in (Cesa-Bianchi et al., 2017), where an approach based on the Gumbel-softmax trick leads to O(K\\log^2 T) regret. Furthermore, we establish that our algorithm attains O(\\beta(G) \\log^ T) regret in stochastic MAB problems that involve graph-structured feedback, without requiring knowledge of the graph structure, with \\beta(G) representing the independence number of the feedback graph. Additionally, we provide comprehensive experimental results on real datasets and applications for multi-armed bandits utilizing both traditional bandit feedback and graph-structured feedback, demonstrating that our algorithm consistently matches or exceeds the performance of leading methods.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1508", "problem_id": "15080001", "content": "Referring Expression Comprehension (REC) is a growing area of interest in computer vision, focusing on identifying specific regions in an image based on a textual description. Current REC approaches typically employ multi-stage frameworks, which are computationally intensive and restrict practical deployment. This paper introduces RealGIN, a one-stage model designed for real-time REC, featuring two key innovations: Adaptive Feature Selection (AFS) and the Global Attentive ReAsoNing unit (GARAN). AFS dynamically combines multi-level semantic features to accommodate diverse expression content, while GARAN leverages textual features to gather and distribute relevant visual information across all regions, enabling robust modeling of complex linguistic conditions. Evaluated on RefCOCO, RefCOCO+, RefCOCOg, ReferIt, and Flickr30k, RealGIN surpasses most existing methods and delivers performance comparable to MAttNet. Notably, RealGIN achieves a tenfold speed improvement over conventional approaches under identical hardware conditions.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1509", "problem_id": "15090001", "content": "Recent developments in neural networks have greatly enhanced 3D object detection with LiDAR and 2D detection in video, yet integrating both modalities effectively to surpass single-modality performance remains challenging. This paper introduces the Camera-LiDAR Object Candidates (CLOCs) fusion network, a lightweight multi-modal framework that substantially enhances the accuracy of individual detectors. CLOCs processes combined detection candidates from any 2D and 3D detectors prior to Non-Maximum Suppression (NMS), utilizing their geometric and semantic correlations to refine both 3D and 2D detection outputs. Evaluated on the KITTI benchmark, CLOCs achieves notable gains in 3D and bird's eye view metrics, particularly for distant objects, outperforming existing fusion techniques. At submission, it leads all fusion-based methods on the KITTI leaderboard. The code will be made available upon acceptance.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1510", "problem_id": "15100001", "content": "Multi-Task Learning (MTL) networks offer significant potential for knowledge transfer between diverse tasks but face obstacles like overfitting on limited-data tasks, catastrophic forgetting, and negative task interference. In Natural Language Processing (NLP), optimal performance typically requires dedicated models per task, yet conventional fine-tuning methods are parameter-inefficient—often necessitating new models for each task—and prone to losing pretrained knowledge. We introduce an innovative Transformer architecture featuring a conditional attention mechanism and task-specific modules that enhance weight sharing. This design improves parameter efficiency and reduces forgetting by freezing half of the pretrained model's weights. Additionally, we employ a novel multi-task data sampling technique to counteract data imbalance effects. Our approach outperforms single-task fine-tuning methods while maintaining parameter and data efficiency, requiring only about 66% of data for weight updates. On GLUE benchmarks, our 8-task model exceeds other Adapter methods by 2.8%, and our 24-task model surpasses MTL and single-task fine-tuning approaches by 0.7-1.0%. A scaled-up version of our unified multi-task model achieves competitive performance across 26 NLP tasks, setting new benchmarks on multiple test and development sets. Our code is accessible at https://github.com/CAMTL/CA-MTL.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1511", "problem_id": "15110001", "content": "Precise medical image segmentation is crucial for diagnosis, surgical preparation, and various other applications. Convolutional Neural Networks (CNNs) have emerged as the leading automated segmentation techniques. Nevertheless, the outputs from fully automated processes might require refinement to achieve the necessary accuracy and robustness for clinical application. We introduce an interactive segmentation approach using deep learning to enhance the results produced by an automatic CNN, aiming for increased accuracy while minimizing user interactions during the refinement phase. The process involves utilizing one CNN to generate an initial automatic segmentation, which is then augmented with user inputs to highlight segmentation errors. Subsequently, a separate CNN processes the initial segmentation alongside user interactions to yield a refined segmentation result. Our method integrates user interactions with CNNs through geodesic distance transforms and introduces a resolution-preserving network to enhance dense prediction. Furthermore, we incorporate user interactions as explicit constraints within a back-propagatable Conditional Random Field. The proposed framework was evaluated on 2D placenta segmentation from fetal MRI and 3D brain tumor segmentation from FLAIR images. Empirical evaluations demonstrate that our method significantly improves upon automatic CNN performance, achieving comparable or superior accuracy with fewer user interventions and reduced time compared to traditional interactive approaches.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1512", "problem_id": "15120001", "content": "This paper introduces a novel image representation, referred to as face X-ray, designed to detect forgery in facial images by identifying whether an input image can be decomposed into a blend of two distinct source images. The face X-ray representation, a greyscale image, achieves this by visually indicating the presence of a blending boundary in forged images and its absence in authentic ones. Given that most face manipulation techniques involve blending an altered face into a background image, face X-ray offers a robust method for detecting forgeries produced by the majority of existing face manipulation algorithms. Notably, face X-ray is a generalizable approach that relies solely on the assumption of a blending step, without requiring knowledge of specific artifacts associated with individual face manipulation techniques. Consequently, the face X-ray algorithm can be trained without relying on fake images generated by state-of-the-art face manipulation methods, as demonstrated by extensive experiments that show its effectiveness in detecting forgeries created by unseen techniques, unlike most existing face forgery detection algorithms which experience significant performance degradation in such scenarios.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1513", "problem_id": "15130001", "content": "This paper introduces a novel global analysis framework for addressing low-rank matrix recovery problems on the Riemannian manifold. It examines the global behavior of Riemannian optimization using random initialization. Specifically, the study employs the Riemannian gradient descent algorithm to minimize a least squares loss function, analyzing its asymptotic behavior and convergence rate. The analysis uncovers a previously undocumented geometric characteristic of the low-rank matrix manifold: the presence of spurious critical points for the least squares function. The findings indicate that, under certain conditions, Riemannian gradient descent from a random initialization is highly likely to circumvent these spurious critical points, converging to the true solution at a nearly linear rate, achieving an \\(\\epsilon\\)-accurate solution in \\(O(\\frac{1}{\\epsilon} + n)\\) iterations. Two applications serve as illustrative examples for the global analysis: a rank-1 matrix recovery problem and a generalization of the Gaussian phase retrieval problem. The latter possesses a similar behavior to the first, with the exception of an additional saddle set, despite satisfying only the weak isometry property. The convergence guarantee is nearly optimal and almost independent of dimensionality, which validates empirical observations. The global analysis holds the potential for extension to other data problems characterized by random measurement structures and empirical least squares loss functions.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1514", "problem_id": "15140001", "content": "Approximate Bayesian computation is a well-recognized and widely-used technique for likelihood-free inference, applicable across various fields. The success of this approach heavily relies on the presence of effective summary statistics. Choosing the right summary statistics is strongly influenced by expert knowledge and the design of specific features, making it a potentially time-intensive task. Given the method's sensitivity to the dimensionality of the data, selecting summary statistics requires a careful balance between incorporating informative statistics and managing the feature vector's dimensions. This paper suggests framing the challenge of dynamically selecting suitable summary statistics from a set of candidates as a multi-armed bandit problem. This formulation allows the rejection sampling in approximate Bayesian computation to adaptively concentrate on a distribution of high-performing summary statistics rather than relying on a predetermined set. Notably, the proposed method is distinct in that it eliminates the need for pre-processing and is capable of scaling to accommodate a large array of candidate statistics. This enhances the efficient utilization of an extensive collection of potential time series summary statistics without necessitating prior feature engineering. The effectiveness of this approach is assessed against leading methods for summary statistics selection using a complex test problem sourced from the systems biology literature.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1515", "problem_id": "15150001", "content": "Material identification in real-world environments presents difficulties because of the intricate interactions between shape, surface reflectance, and lighting conditions. Traditional methods for material classification predominantly depend on manually designed visual features. This study employs a Convolutional Neural Network (convnet) to automatically learn discriminative features tailored for material recognition. By leveraging transfer learning from object recognition tasks, the model achieves more efficient training of effective features for material classification. This convnet-based transfer learning method demonstrates substantially improved accuracy over existing leading techniques. Additionally, the study examines the separate roles of reflectance and shading through an intrinsic image decomposition. Despite the advantages of transfer learning, convnet-based material classification faces challenges due to the substantial need for extensive and varied training data. To address this, we introduce a novel dataset comprising around 10,000 images categorized into 10 material types.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1516", "problem_id": "15160001", "content": "Kleinberg identified three intuitive characteristics of clustering algorithms but demonstrated that no algorithm can fulfill all of them concurrently. This paper introduces Monotonic Consistency, a novel clustering property designed to circumvent the issues associated with Kleinberg's Consistency axiom, thereby resolving the impossibility. We propose Morse Clustering, an algorithm rooted in Morse Theory from Differential Topology, which adheres to Kleinberg's original axioms, substituting Consistency with Monotonic Consistency. Morse Clustering identifies the inherent flow structure within a dataset or graph, generating a tree-based partition that signifies basins of attraction for critical vertices. Furthermore, we extend Kleinberg's axiomatic framework to encompass sparse graphs, establishing an impossibility concerning Consistency and a corresponding possibility for Monotonic Consistency when employing Morse clustering.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1517", "problem_id": "15170001", "content": "This study introduces the Mask-guided Generative Adversarial Network (MagGAN), a novel approach for editing high-resolution facial attributes, leveraging pre-trained face parser-derived semantic masks to direct the editing process. By incorporating a mask-guided reconstruction loss, MagGAN selectively edits facial regions pertinent to the desired attribute modifications while maintaining unchanged the regions unrelated to these modifications, such as accessories like hats or scarves when applying the 'To Bald' modification. Additionally, a mask-guided conditioning strategy is employed to integrate the influence region of each attribute modification into the generator, and a multi-level patch-wise discriminator structure is utilized to enable high-resolution (1024 \\times 1024) face editing. The results of experiments conducted on the CelebA benchmark demonstrate that the proposed MagGAN method substantially surpasses existing state-of-the-art approaches in terms of both the quality of the generated images and the effectiveness of attribute editing, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1518", "problem_id": "15180001", "content": "In this study, we present a Transformer-based methodology for video object segmentation (VOS). To tackle the issues of compounding errors and scalability that have been observed in previous methods, we introduce a scalable, end-to-end technique for VOS named Sparse Spatiotemporal Transformers (SST). SST generates per-pixel representations of each object in a video by utilizing sparse attention on spatiotemporal features. Our attention-centric approach enables the model to focus on a series of past frames and facilitates the necessary correspondence-like calculations essential for motion segmentation. We illustrate the superiority of attention mechanisms over recurrent networks within the spatiotemporal framework. Our approach yields competitive performance on YouTube-VOS and DAVIS 2017, demonstrating enhanced scalability and resilience to occlusions relative to existing methods. Code is available at https://github.com/dukebw/SSTVOS.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1519", "problem_id": "15190001", "content": "Recent studies have effectively integrated deep learning's feature representation learning capabilities with reinforcement learning, demonstrating success in training agents for tasks like Atari game playing from pixel data and acquiring complex manipulation skills via raw sensory information. Nevertheless, the absence of a standardized benchmark has posed challenges in measuring advancements in continuous control. To address this, we introduce a benchmark suite comprising various continuous control tasks, including classic control problems like cart-pole swing-up, high-dimensional state and action tasks such as 3D humanoid locomotion, tasks involving partial observability, and those with hierarchical structures. We present new insights derived from a thorough evaluation of several implemented reinforcement learning algorithms. To promote reproducibility and widespread use, we have released the benchmark suite and reference implementations at https://github.com/rllab/rllab.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1520", "problem_id": "15200001", "content": "Reinforcement learning algorithms can develop effective policies for intricate tasks, but these policies frequently lack robustness to minor variations in the task, particularly when such variations are not explicitly included in the training process. A potential solution involves training agents using manually introduced variations in the task or environment, but this approach may not be viable in real-world scenarios due to the difficulty of implementing perturbations or the uncertainty surrounding the selection of suitable perturbation strategies. This study reveals that acquiring diverse behaviors for task completion can inherently yield behaviors that generalize across varying environments, eliminating the need for explicit perturbations during training. By discovering multiple task solutions within a single environment, our method enables generalization to novel situations through the abandonment of ineffective solutions and the adoption of effective ones. Theoretical analysis characterizes a set of robust environments generated by our algorithm, and empirical results demonstrate that our diversity-driven approach can be extrapolated to various environmental and task changes, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1521", "problem_id": "15210001", "content": "Current Magnetic Resonance Imaging (MRI) scans often suffer from inadequate resolution due to physical, technological, and financial limitations. Super-resolution approaches can enhance MRI image quality, but conventional interpolation-based techniques for brain MRI resolution improvement compromise diagnostic precision. As the demand for high-quality brain images grows, this paper introduces a super-resolution (SR) technique leveraging overcomplete dictionaries and intrinsic image similarity to reconstruct high-resolution (HR) images from single low-resolution (LR) inputs. By exploiting nonlocal similarity, the method identifies analogous image blocks and employs a joint reconstruction framework integrating compressive sensing (CS) and similarity constraints, utilizing sparsity and self-similarity as key parameters. The approach involves first classifying image blocks—smooth, textured, or edge—through measurement domain analysis, followed by training dedicated dictionaries for each category. During reconstruction, the CS method is applied with constraints on nonlocal similarity and sparsity to restore HR brain MRI images. Experimental results demonstrate superior visual and quantitative performance compared to existing techniques.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1522", "problem_id": "15220001", "content": "We introduce a new formulation for regression and classification that aims to verify the outcomes of a machine learning algorithm. This reformulation streamlines the initial problem and assesses the results of the machine learning model using the training data. As it is essential for the validation of machine learning algorithms to be interpretable, we conduct our experiments employing the kNN algorithm alongside a newly proposed algorithm grounded in conditional probabilities. To evaluate our method, we utilized three publicly accessible data sets, analyzing three classification and two regression tasks. Additionally, the conditional probabilities-based algorithm demonstrated online applicability and necessitates significantly less memory than the kNN algorithm.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1523", "problem_id": "15230001", "content": "The detection of moving objects in three-dimensional space is a crucial aspect of dynamic scene analysis, and this paper presents a innovative approach to 3D moving object detection using Lidar sensors, drawing inspiration from the visual system of Drosophila. By leveraging the theory of elementary motion detection, a motion detector is developed that mimics the shallow visual neural pathway of Drosophila, exhibiting sensitivity to object movement while effectively reducing background noise. The proposed method employs neural circuits with varied connection modes to identify motion areas in a hierarchical manner, from coarse to fine, and extracts point clouds for each motion area to generate moving object proposals. These proposals are then refined using an enhanced 3D object detection network, which accurately estimates point clouds and produces 3D bounding boxes and object categories. The efficacy of the proposed approach is demonstrated through evaluation on the KITTI benchmark, yielding state-of-the-art results in motion detection, as shown in Figure A, B, C (References [1], [2], and [3]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1524", "problem_id": "15240001", "content": "Medical image analysis, a discipline focused on resolving clinical challenges through the examination of images produced in clinical settings, strives to extract information effectively and efficiently to enhance clinical diagnosis. Recent progress in biomedical engineering has elevated medical image analysis to a prominent area of research and development, largely due to the application of machine learning techniques. Deep learning, a successful machine learning tool, employs neural networks to automatically learn features, differing from traditional methods that rely on challenging hand-crafted feature selection and calculation. Deep convolutional networks, a subset of deep learning techniques, are frequently utilized in medical image analysis for tasks such as segmentation, abnormality detection, disease classification, computer-aided diagnosis, and retrieval. This paper presents a thorough review of the current state-of-the-art in medical image analysis using deep convolutional networks, emphasizing both the challenges and potential inherent in these techniques.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1525", "problem_id": "15250001", "content": "This study focuses on signal processing within point clouds utilizing neural networks. Currently, advanced image processing and computer vision techniques primarily rely on training deep convolutional neural networks on extensive datasets. Although similar methods apply to point-cloud processing through Graph Neural Networks (GNN), the emphasis has predominantly been on high-level tasks, such as classification and segmentation, which rely on supervised learning using labeled datasets like ShapeNet. However, these datasets are often limited and labor-intensive to create based on specific applications. In this research, we explore the application of variational models in GNNs to handle signal processing on graphs for unsupervised learning. Our contributions are two-fold: first, we demonstrate that various existing variational algorithms for graph signals can be expressed as Message Passing Networks (MPN), a specific type of GNN, which provides improved computational efficiency compared to conventional gradient-based machine learning methods. Second, we examine the unsupervised learning of feed-forward GNNs, either through direct optimization of an inverse problem or via model distillation from variational-based MPN.  Keywords: Graph Processing. Neural Network. Total Variation. Variational Methods. Message Passing Network. Unsupervised learning.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1526", "problem_id": "15260001", "content": "A significant aspect of reinforcement learning (RL) involves the method by which the agent explores its environment. Conventional exploration methods typically prioritize efficiency while neglecting safety concerns. Nevertheless, in real-world applications, ensuring the safety of the agent during exploration is vital, as any unsafe action or state could lead to permanent harm. The principal difficulty in achieving safe exploration lies in the challenge of defining unsafe states and actions within extensive continuous state or action spaces, especially in unfamiliar environments. In this paper, we introduce a novel strategy that integrates safety estimations to direct exploration and policy optimization in deep reinforcement learning. By utilizing a cost function to represent trajectory-based safety, our central concept is to define the state-action value function of this safety cost as a potential Lyapunov function and to leverage control-theoretic principles to estimate its derivative through online Gaussian Process (GP) methods. We demonstrate how these statistical models can be employed to assist the agent in uncharted environments, allowing the attainment of high-performance control policies accompanied by verifiable stability certificates.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1527", "problem_id": "15270001", "content": "The successful integration of green energy grids relies heavily on accurate wind and solar forecasts to inform the strategic placement and scheduling of renewable energy generation facilities. However, operational forecasts generated by numerical weather prediction models are limited by their spatial resolution of 10 to 20-km, resulting in less-than-optimal utilization and development of renewable energy farms. To address this issue, researchers have been exploring super-resolution techniques, but these methods often depend on simplistic interpolation techniques or complex differential equation-based models that are computationally intensive. The emergence of machine learning-based approaches, such as the physics-informed resolution-enhancing generative adversarial network (PhIREGAN), has shown promise in outperforming conventional downscaling methods. This study presents a comprehensive and adaptable benchmark of prominent deep learning-based super-resolution techniques, including the enhanced super-resolution generative adversarial network (ESRGAN) and an enhanced deep super-resolution (EDSR) network, using wind and solar data. Additionally, we introduce a novel, publicly available, processed, and machine learning-ready dataset for evaluating super-resolution methods on wind and solar data, providing a valuable resource for further research.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1528", "problem_id": "15280001", "content": "The creation of highly convincing fake images using Deepfake or GANs has significant societal implications, causing considerable disruption. Various detection methods have been developed to identify such images, but they are susceptible to adversarial perturbations, which are intentionally crafted noises that can mislead predictions. Conventional approaches to compromising fake image detectors typically involve generating perturbations that affect nearly the entire image, resulting in unnecessary redundancy and increased perceptibility. This paper presents a new approach that targets key pixels crucial to fake image detection, perturbing only these select pixels to achieve significantly lower L_0 and L_2 norms of adversarial perturbations compared to existing methods, as demonstrated through experiments on two public datasets using three fake image detectors, yielding state-of-the-art results in both white-box and black-box attacks.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1529", "problem_id": "15290001", "content": "This work addresses the challenge of forecasting time series data. While conventional methods, including both statistical models and modern neural network-based learning techniques, typically analyze raw time series data directly, we investigate the utility of persistent homology as a supplementary signal for improving forecasting accuracy. We introduce an approach that leverages attention mechanisms to focus on local topological features within historical time series data. This method can be readily incorporated into existing end-to-end trainable forecasting models, such as \\texttt, and achieves state-of-the-art results when combined with the latter on the extensive M4 benchmark dataset, which comprises 100,000 varied time series from multiple domains. Furthermore, ablation studies and comparisons against a wide spectrum of forecasting methods in a single time series training scenario validate the advantages of integrating local topological information through an attention mechanism.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1530", "problem_id": "15300001", "content": "Safeguarding individual privacy by securing sensitive attributes is a critical concern when releasing microdata, yet maintaining data quality or utility for specific workloads remains equally vital. We introduce a new privacy-preserving framework grounded in the k-anonymity model, specifically designed for applications requiring the retention of quasi-identifier variable probability distributions. This approach integrates distribution-preserving quantization with k-member clustering, developing two specialized variants that employ intra-cluster and Gaussian dithering of cluster centers to maintain distribution integrity. We theoretically analyze these schemes regarding distribution preservation and highlight their applicability in scenarios like covariate shift and transfer learning, where such preservation is essential. Through comprehensive testing on real-world Medical Expenditure Panel Survey data, we illustrate the advantages of our algorithms compared to conventional k-anonymization for a key healthcare use case involving an insurer assessing market entry risks. Additionally, empirical evaluation of reidentification risk confirms that our methods effectively uphold k-anonymity.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1531", "problem_id": "15310001", "content": "A significant challenge in systems biology involves creating statistical techniques that can autonomously identify causal relationships within gene regulatory networks, even when prior knowledge of causal connections is absent. While numerous methodologies exist for time series data, methods relying on steady-state data are frequently essential and advantageous, given the potential expense or infeasibility of acquiring time series data for numerous biological systems. Although causal Bayesian networks are a common strategy, estimating these networks is an inherently ill-posed problem. Often, it fails to pinpoint a unique causal network, instead yielding a broad set of equivalent networks that are statistically indistinguishable. This paper introduces a novel algorithm designed to uniquely determine the underlying causal network of genes. To our knowledge, this method represents the first algorithm for learning gene networks grounded in a fully identifiable causal model known as LiNGAM. We evaluate our algorithm's performance against other algorithms using synthetically generated data, noting that validation with actual microarray gene expression data would provide a more robust assessment.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1532", "problem_id": "15320001", "content": "Knowledge distillation (KD) is a widely used strategy for minimizing the computational costs associated with deep network inference, leveraging the output from a teacher model to train a smaller, more efficient student model. Hint training (such as FitNets) enhances KD by having the student model mimic the intermediate representations of the teacher model. In this paper, we present a new model compression approach called bLock-wise Intermediate representation Training (LIT), which advances the application of intermediate representations in deep network compression, surpassing both KD and hint training. LIT incorporates two main concepts: 1) it facilitates the training of a student model that has the same width but reduced depth compared to the teacher by directly contrasting their intermediate representations, and 2) it utilizes the intermediate representation from the preceding block of the teacher model as input to the current block of the student model during training, which helps to mitigate issues of instability in the student network’s intermediate representations. Our results indicate that LIT significantly decreases network depth while maintaining accuracy—specifically, it can compress a ResNeXt-110 to a ResNeXt-20 (5.5x) on CIFAR10 and a VDCNN-29 to a VDCNN-9 (3.2x) on Amazon Reviews, all without sacrificing accuracy, and it outperforms KD and hint training in terms of network size relative to a specified accuracy. Additionally, we demonstrate that applying LIT to equivalent student and teacher architectures boosts the student model's accuracy beyond that of the teacher model, outperforming the recently introduced Born Again Networks method on ResNet, ResNeXt, and VDCNN. Lastly, we show that LIT can also effectively compress GAN generators, a task that is not feasible within the KD framework due to GANs generating pixel outputs rather than probabilities.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1533", "problem_id": "15330001", "content": "Diverse perspectives on data, encompassing both naturally collected forms (such as images and audio) and synthetically generated varieties (like the introduction of different types of noise to data samples), have demonstrated significant benefits in improving representation learning. Natural perspectives are typically analyzed using multiview analysis methods, such as (deep) canonical correlation analysis [(D)CCA], while synthetic perspectives are often employed in self-supervised learning (SSL) frameworks, including SimCLR and Barlow Twins. Both approaches typically focus on training neural feature extractors to ensure that the data embeddings maintain high correlations across different views. Despite being a logical approach, the utility of correlation-driven neural embedding has only been supported through empirical evidence. This study presents a theoretically grounded framework for unsupervised multiview learning, beginning with a proposed multiview model wherein each perspective comprises a nonlinear combination of shared and individual components. Consequently, the learning challenge is centered on identifying and disentangling the shared and private components. Within this model, maximizing latent correlations is demonstrated to facilitate the extraction of shared components between views (albeit with some ambiguities). Furthermore, proper design of regularization allows for the effective disentanglement of private information in each view from the shared elements. The method has been evaluated across several tasks, such as downstream clustering, all yielding encouraging results. Additionally, our framework offers a cohesive perspective for elucidating various DCCA and SSL methodologies.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1534", "problem_id": "15340001", "content": "Weakly-supervised object detection seeks to reduce the level of supervision by eliminating the necessity for bounding boxes, yet it still relies on having image-level labels for the complete training dataset. In this research, we address the challenge of developing an object detector using only a few images with image-level labels alongside a larger collection of entirely unlabeled images. This represents a particularly rigorous scenario of semi-supervised learning, where the labeled samples are insufficient to initiate the training of the detector. Our approach involves training a weakly-supervised student detector model with pseudo-labels derived from the unlabeled dataset through a teacher classifier model, leveraging region-level similarities with labeled images. Building on the recent robust weakly-supervised framework PCL, our method can incorporate a greater number of unlabeled images to achieve performance that is either competitive with or exceeds that of many contemporary weakly-supervised detection methods.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1535", "problem_id": "15350001", "content": "The study presents an unsupervised learning classification model that performs comparably to well-known supervised classifiers like SVM and kNN in terms of classification error probability. The approach involves incrementally applying minor shift and rotation adjustments to chosen discriminative hyperplanes as new input samples are processed. Tested on a subset of the ImageNet benchmark alongside a specific feature extractor, the model achieves a Top 3 error probability of 6.2%, only slightly higher (by approximately 2%) than supervised k-Nearest Neighbor using the same extractor. In contrast, traditional unsupervised methods such as k-Means prove ineffective on the same dataset.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1536", "problem_id": "15360001", "content": "In decentralized multi-agent learning, individual agents perceive the environment as non-stationary because of the exploration and policy updates of other agents. Recent deep multi-agent reinforcement learning approaches have sought to address this issue by identifying and downweighting samples influenced by other agents' exploration or suboptimal behavior. Following this principle, we propose a decentralized quantile estimator that enhances performance by evaluating the likelihood of returns to distinguish non-stationary samples. Specifically, each agent assesses the probability of exploration or policy shifts by other agents, using its own estimates to adjust the learning rate for different samples. We present a formal approach for measuring differences in return distribution representations and techniques to leverage these differences for learning updates. Additionally, we investigate risk-seeking strategies for dynamically adapting learning and introduce adaptive risk distortion functions to control risk sensitivity. Experimental results on standard benchmarks and new environments demonstrate that our approach achieves greater stability, sample efficiency, and a higher likelihood of converging to an optimal joint policy compared to prior methods.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1537", "problem_id": "15370001", "content": "Conventional neural networks are designed to process vector inputs, organizing neurons in sequential layers. This architecture necessitates transforming non-vector data like matrices into vectors, which introduces challenges. First, vectorization may eliminate spatial relationships within the data. Second, the expanded solution space increases computational demands and requires specialized parameter handling. To overcome these limitations, we introduce matrix neural networks (MatNet), which directly process matrix inputs. Each neuron aggregates information via bilinear mappings from preceding layers, similar to traditional feedforward networks. This structure enables efficient parameter optimization through backpropagation and gradient descent. Additionally, MatNet can be easily adapted for multimodal inputs. Evaluations on MNIST digit classification and image super-resolution demonstrate its effectiveness, achieving performance comparable to state-of-the-art methods with significantly lower complexity and minimal tuning.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1538", "problem_id": "15380001", "content": "Current deep reinforcement learning (DRL) frameworks typically focus on either discrete or continuous action spaces, but many real-world applications, such as computer games, involve a combination of both. To address this limitation, we introduce a novel framework that can handle discrete-continuous hybrid action spaces without relying on approximations or relaxations. Our proposed parametrized deep Q-network (P-DQN) framework draws inspiration from both DQN, which is suited for discrete action spaces, and DDPG, which is designed for continuous action spaces, and integrates them in a seamless manner. The efficacy of our approach is demonstrated through empirical results on various test cases, including a simulation example, a RoboCup soccer scenario where the goal is to score, and the solo mode in the game King of Glory (KOG), showcasing the efficiency and effectiveness of our method.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1539", "problem_id": "15390001", "content": "This paper introduces a new weakly-supervised semantic segmentation approach that relies solely on image-level labels. Activation maps specific to each class, derived from finely-tuned classifiers, serve as guidance for training a segmentation network. Recognizing that these cues often suffer from being coarse and incomplete, super-pixel refinement is employed. Furthermore, cues extracted from classifiers trained on both color and grayscale images are integrated to address the issue of incompleteness. A conditional random field is incorporated to govern the training procedure and further refine the resulting segmentations. In addition to initializing the segmentation network, the pre-trained classifier is utilized during testing to suppress the prediction of non-existent classes. The efficacy of the proposed method is demonstrated through experimental evaluations conducted on the PASCAL VOC 2012 dataset.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1540", "problem_id": "15400001", "content": "We introduce a generalized self-supervised learning method for monocular depth estimation, capable of handling real depth across varying scenes with depth ranges from 1 to hundreds of meters. Conventional supervised techniques for monocular depth estimation necessitate precise depth measurements during the training process. This requirement has spurred the development of self-supervised methods that rely on stereo image pairs captured with a fixed camera baseline to infer disparity, which is subsequently converted to depth with known calibration. While self-supervised frameworks have achieved notable outcomes, they often fail to generalize across scenes with differing depth ranges or camera baselines. In this work, we present RealMonoDepth, a self-supervised monocular depth estimation technique that learns to accurately estimate the true scene depth for a broad spectrum of both indoor and outdoor environments. We propose a novel loss function associated with the actual scene depth, utilizing relative depth scaling and warping. This innovation facilitates the self-supervised training of a single network across multiple data sets for scenes with varying depth ranges, leveraging both stereo pairs and data from moving cameras in real-world scenarios. A thorough performance assessment across five benchmark data sets confirms that RealMonoDepth enables a single trained network to generalize depth estimation effectively in both indoor and outdoor contexts, consistently surpassing previous self-supervised methods.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1541", "problem_id": "15410001", "content": "The challenge of understanding image classification processes within deep convolutional neural networks and interpreting their outputs has drawn considerable attention from computer vision researchers and decision-makers. The opacity of these deep models, often termed \"black boxes,\" stems from a limited understanding of their internal mechanisms. In response, various explainable deep learning methodologies have emerged, including sensitivity maps (based on gradients of class output relative to input image), class activation map (CAM), and Gradient based Class Activation Maps (Grad-CAM). However, these methods often struggle with localizing multiple instances of the same class and lack universal applicability across all CNN architectures. Furthermore, Grad-CAM's tendency to incompletely capture single objects impacts performance in recognition tasks. To address these limitations and enhance visual explanations in terms of sharpness, object localization, and the ability to explain multiple object instances within a single image, we introduce Smooth Grad-CAM++ \\footnote, a novel technique integrating SMOOTHGRAD and Grad-CAM++. Smooth Grad-CAM++ enables the visualization of a specific layer, a subset of feature maps, or a subset of neurons within a feature map at each step of the inference process. Experimental results on several images demonstrate that Smooth Grad-CAM++ generates sharper visual maps and achieves superior object localization compared to existing techniques.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1542", "problem_id": "15420001", "content": "Research in deep learning on graphs has gained significant attention due to its numerous applications, yet it has primarily focused on graph embedding tasks, differing from the advancements made in generative models for images and text. The question remains whether it is feasible to apply these advancements to the graph domain. To overcome the challenges associated with linearizing discrete structures, we introduce a novel approach where a decoder generates a probabilistic fully-connected graph of a specified maximum size in a single step. Formulated as a variational autoencoder, our method is assessed on the complex task of molecule generation, as shown in Figure A (Reference [1], Citation [2]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1543", "problem_id": "15430001", "content": "This paper presents a novel algorithm that integrates calibrated prediction with generalization bounds from learning theory to establish confidence sets for deep neural networks, providing PAC guarantees that ensure the confidence set for a given input encompasses the true label with a high degree of probability. The efficacy of this approach is showcased through its application in constructing PAC confidence sets for various models, including ResNet on ImageNet, a visual object tracking model, and a dynamics model for the half-cheetah reinforcement learning problem.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1544", "problem_id": "15440001", "content": "Cooperative games are characterized by agents that share an identical payoff structure. Although value-based reinforcement learning algorithms, including different versions of Q-learning, have been utilized for learning in cooperative games, their application is limited to scenarios where both agents have full visibility of the game state. In contrast, policy search techniques present a viable option for environments where information is only partially observable. In this study, we introduce a gradient-based distributed policy search approach for cooperative games and analyze the relationship between local optimum and Nash equilibrium concepts. We empirically validate the efficacy of our method within a small, partially observable simulated soccer setting.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1545", "problem_id": "15450001", "content": "Supervised and semi-supervised learning techniques are effective but require substantial and increasing manual annotation. Unsupervised learning has gained prominence as a research area to address this issue and cater to particular applications. Within computer vision, unsupervised learning manifests in diverse forms. Concentrating on the unsupervised identification and correspondence of object categories within image datasets, inspired by Cho et al. 2015, we demonstrate that the original method can be recast and resolved as a well-defined optimization challenge. Empirical evaluations across multiple benchmark datasets validate the effectiveness of our proposed method.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1546", "problem_id": "15460001", "content": "Existing methods treat relationship detection as a classification task by assigning a single label to each relationship, considering predicate categories as entirely distinct classes. Unlike object labels with clear boundaries, predicates often share overlapping semantic meanings—for instance, \"sit_on\" and \"stand_on\" both indicate vertical relationships but differ in specific positioning details. To exploit the inherent structure of predicate categories, we first construct a language hierarchy and then apply Hierarchy Guided Feature Learning (HGFL) to enhance region features at both coarse-grained and fine-grained levels. Additionally, we introduce the Hierarchy Guided Module (HGM) to use coarse-grained features to refine fine-grained feature learning. Experimental results demonstrate that this straightforward yet effective approach significantly boosts performance (up to 33% relative improvement in Recall@50) for Scene Graph Generation across multiple datasets compared to leading baselines.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1547", "problem_id": "15470001", "content": "Software testing plays a crucial role in ensuring software quality, with testing expenses often exceeding 50% of the overall project budget. An effective and efficient approach to software testing minimizes resource consumption. Consequently, it's essential to develop procedures that enable efficient testing while reducing project resource use. The primary aim of software testing is to identify the maximum number of defects within the software system; the identification of more defects indicates higher testing efficiency. Various techniques have been suggested to uncover software defects and optimize resource use to deliver favorable outcomes. As the world increasingly adopts a data-driven approach for critical decision-making, this research paper conducts a machine learning analysis on publicly available datasets to attain maximum accuracy. The paper specifically focuses on applying diverse machine learning techniques to these datasets to determine which technique yields the most effective results. In this regard, we proposed ensemble learning models and conducted a comparative analysis among KNN, Decision Tree, SVM, and Naïve Bayes across different datasets. The findings demonstrate that the ensemble method outperforms the others in terms of accuracy, precision, recall, and F1-score. The ensemble model achieved a classification accuracy of 98.56% on CM1, 98.18% on KM2, and 99.27% on PC1, highlighting that ensemble learning is a more efficient method for defect prediction compared to the other techniques assessed.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1548", "problem_id": "15480001", "content": "Effective machine learning solutions often depend on acquiring meaningful data representations, as learning algorithms primarily rely on features to derive models. For example, classification accuracy can be enhanced by projecting data into a space where class separation is simpler, while regression benefits from identifying underlying data manifolds in the feature space. Typically, feature transformations are achieved through statistical techniques like principal component analysis or manifold learning approaches such as Isomap and locally linear embedding. Among various representation learning techniques, autoencoders stand out as highly adaptable. This paper illustrates how to shape their learned representations to meet specific learning objectives. We address multiple tasks—visualization through data embedding, image denoising, semantic hashing, anomaly detection, and instance generation—modeling them using state-of-the-art representation learning frameworks. Each task is tackled using autoencoders as the sole learning mechanism, with theoretical concepts validated through experiments on selected datasets. The results from these case studies are analyzed, along with insights into six additional learning applications. We also examine current challenges and interpretability approaches for autoencoders. Our findings highlight that, by modifying their architecture and loss functions, autoencoders can serve as a foundational tool for solving diverse problems framed as feature space transformations.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1549", "problem_id": "15490001", "content": "This study introduces a new method for creating highly realistic facial images with precise lip synchronization, based on an input audio signal. Utilizing a recurrent neural network, we extract mouth landmarks from audio features, which are then used to condition a generative adversarial network to produce highly realistic facial images. The combination of these two neural networks enables the generation of a sequence of natural-looking faces that are synchronized with the input audio track, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1550", "problem_id": "15500001", "content": "The segmentation of digital images is a crucial process in image processing and computer vision systems, as it allows for the classification of pixels into distinct regions based on their intensity levels. Numerous segmentation techniques exist, many of which involve complex computational procedures. Among these, thresholding stands out as a simple method that is easy to implement. In this paper, we introduce a novel heuristic technique for image segmentation that automatically identifies multilevel thresholds by sampling the histogram of a digital image, focusing on choosing valley points as optimal threshold values. Our findings indicate that this method surpasses the widely-used Otsu's method with respect to CPU computation time, achieving a maximum speed improvement of 35.58x and a minimum of 10.21x on standard image processing benchmarks. To validate the accuracy of our threshold determination, we calculate PSNR, SSIM, and FSIM metrics and compare them with those from Otsu's method. This comparison reveals that our approach is either comparable or superior to Otsu's method in several instances.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1551", "problem_id": "15510001", "content": "Deep learning has achieved cutting-edge results across various applications, including computer vision and natural language processing. Nevertheless, these systems are susceptible to adversarial attacks involving carefully designed inputs that induce misclassification. The perturbations required to trigger these misclassifications can be minimal, often undetectable, raising critical security issues, especially when misclassifications could endanger human safety. To address this, we introduce Deep Latent Defence, an architecture integrating adversarial training with a detection mechanism. Deep Latent Defence utilizes an adversarially trained neural network. Encoders map intermediate layer representations of data into a latent space, enabling adversarial sample detection using a k-NN classifier. Our evaluation includes grey-box and white-box attacks, alongside an adaptive L_ bounded attack designed to circumvent our defense. The findings demonstrate that our defense provides substantial security improvements, even against the most potent attack models tested.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1552", "problem_id": "15520001", "content": "Recent advancements in deep learning for 3D data have shown significant promise for end-to-end learning directly from point clouds. Nonetheless, many practical point clouds exhibit a substantial class imbalance, reflective of the natural discrepancies seen in the environment. For instance, a 3D scan of a cityscape primarily includes roadways and building facades, while other objects, like poles, are often under-represented. This paper tackles this challenge by using a weighted augmentation method to enhance the representation of less populous classes. By addressing the class imbalance in the dataset, we demonstrate that a standard PointNet++ deep neural network can perform better during inference on validation datasets. We noted an increase in the F1 score of 19% and 25% on two benchmark test datasets, ScanNet and Semantic3D, respectively, without prior processing for class imbalance. Our networks exhibited improved performance for both well-represented and less-represented classes, suggesting that the network is capable of extracting more robust and significant features when the loss function is not disproportionately focused on a limited number of classes.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1553", "problem_id": "15530001", "content": "This study demonstrates the development of a markerless tracking method specifically designed for the Windows Mobile Pocket PC platform, with the primary objective of enabling the creation of autonomous augmented reality applications for handheld devices that utilize natural feature tracking. To accomplish this, a subset of two computer vision libraries was adapted and ported to the Pocket PC platform, incorporating fixed point math to enhance the performance of the algorithms. This adaptation enables the potential execution of additional computer vision tasks on mobile devices. A model-based tracking approach leveraging edge information was employed, which is well-suited for resource-constrained devices like handhelds due to its relatively low processing requirements. The implementation leverages the OpenGL ES graphics library to harness existing graphics hardware acceleration for computer vision tasks. An augmented reality application was developed using this technique, and its tracking performance and accuracy were evaluated, as shown in Figure A, B, C (References [1], [2], [3] and citations therein).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1554", "problem_id": "15540001", "content": "We aim to evaluate and, where feasible, establish a correspondence between two distinct artificial neural networks by creating a data-driven transformation between them through manifold-learning methods. Specifically, we utilize diffusion maps alongside a Mahalanobis-like metric. Should this construction be successful, it signifies that both networks belong to the same equivalence class. Initially, we focus on transformation functions that relate solely to the outputs of the two networks; subsequently, we extend our analysis to include transformations that factor in the outputs (activations) of several internal neurons from each network. Generally, Whitney's theorem outlines the necessary number of measurements from one network to fully reconstruct every feature of the second network. The development of the transformation function depends on a coherent, intrinsic representation of the network's input space. Our algorithm is exemplified through matching pairs of neural networks trained on (a) observations of scalar functions, (b) observations of two-dimensional vector fields, and (c) representations of images of a moving three-dimensional object (a rotating horse). The formation of these equivalence classes across varying network implementations is clearly connected to transfer learning. Additionally, we anticipate that this approach will prove useful in establishing equivalence among different Machine Learning models that examine the same phenomenon observed by various instruments and research teams.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1555", "problem_id": "15550001", "content": "The capacity to learn disentangled representations of natural language is critical for numerous Natural Language Processing (NLP) applications, including conditional text generation, style transfer, and personalized dialogue systems. While analogous issues have been thoroughly investigated for data formats like images and videos, the inherent discreteness of natural language introduces complexities in disentangling textual representations, such as difficulties in manipulating the data space. Drawing inspiration from information theory, we introduce an innovative approach that effectively produces disentangled textual representations without semantic supervision. We derive and utilize a novel upper bound on mutual information to assess the dependence between style and content. By minimizing this upper bound, our method effectively separates style and content embeddings into two independent, low-dimensional spaces. Experimental results from conditional text generation and text-style transfer tasks showcase the superior quality of our disentangled representation regarding content and style retention.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1556", "problem_id": "15560001", "content": "We present a unified GAN training framework that can accommodate varying levels of labeling. This approach utilizes artificial labeling, which can seamlessly integrate manually defined labels when available, and establish an alignment between them. By leveraging the premise that neural network generators can more readily learn to map adjacent latent vectors to semantically similar data, we define artificial labels and employ them to train a classifier using generated data samples. The trained classifier is then applied to self-label real data, with the exponential moving average of the classifier used to enhance labeling accuracy. To mitigate potential errors, particularly during the initial training stages, we refine the labels using self-attention, relying on the classifier's output probability scores to determine the reliability of the labels assigned to real data samples. Our evaluation on CIFAR-10, STL-10, and SVHN datasets demonstrates that self-labeling and self-attention consistently enhance the quality of generated data, and notably, our proposed scheme can even surpass the performance of class-conditional GANs.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1557", "problem_id": "15570001", "content": "Approaches to enhance the training and prediction performance of weakly supervised machine learning models differ in their level of task specificity and integration with particular model architectures. This paper presents Knodle, a modular software framework that decouples weak data annotations, deep learning models, and techniques for refining weakly supervised training into distinct components. By enabling access to detailed information—such as dataset features, heuristic rule matches, or components of the deep learning prediction model—the framework supports diverse training methods for weak supervision. These methods range from those analyzing rule-class correlations (independent of the trained model) to those leveraging interactions between neural networks and weakly labeled data. We demonstrate the framework’s benchmarking capabilities by evaluating multiple reference implementations on datasets included in Knodle. The framework is released as the open-source Python package knodle, accessible at https://github.com/knodle/knodle.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1558", "problem_id": "15580001", "content": "The timely diagnosis of autism spectrum disorder (ASD) is crucial for enhancing the quality of life of individuals affected by the condition. Nevertheless, delays in diagnosis persist, even in affluent nations like the US, due to the time-consuming and specialized nature of standard diagnostic instruments, such as the Autism Diagnostic Observation Schedule (ADOS) and the Autism Diagnostic Interview-Revised (ADI-R). This challenge is exacerbated in resource-constrained settings, where a shortage of trained professionals further hinders diagnosis. To address this issue, alternative diagnostic approaches have been explored, leveraging the distinct responses of children with ASD to visual stimuli in controlled environments. Research has demonstrated that children with ASD exhibit a greater tendency to focus on abstract images when presented with social and abstract scenes simultaneously, as seen in Figure A, and this differential response can be utilized to develop an algorithm for rapid ASD diagnosis using eye tracking with various visual stimuli, as shown in Figure B, and cited in [citation]. This study proposes a convolutional neural network (CNN) algorithm for predicting gaze direction using images extracted from a one-minute stimulus video, which achieved high accuracy and robustness in predicting gaze direction with independent individuals and using a different camera than the one used during testing, as illustrated in Figure C. The proposed algorithm also exhibits a fast response time, enabling near real-time evaluation of ASD. By applying this method, diagnosis time can be significantly reduced, facilitating the diagnosis of ASD in low-resource regions, as referenced in References.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1559", "problem_id": "15590001", "content": "Transformers, which are typically the preferred choice in natural language processing, have received limited focus from the medical imaging sector. Their capacity to manage long-term dependencies presents an opportunity for transformer models to enhance traditional convolutional neural networks (convnets) by addressing their intrinsic limitations related to spatial inductive bias. Nevertheless, most recent transformer-based segmentation strategies have mainly utilized transformers as supplementary modules to integrate global context within convolutional features, neglecting to explore the optimal integration of self-attention—the fundamental element of transformers—with convolutional operations. To tackle this gap, we present nnFormer (Not-aNother transFormer), an advanced segmentation model featuring an interleaved architecture that empirically combines self-attention and convolution techniques. In application, nnFormer generates volumetric representations derived from 3D local volumes. This volume-centric approach significantly mitigates computational complexity, achieving reductions of approximately 98% and 99.5% on the Synapse and ACDC datasets, respectively, compared to a basic voxel-level self-attention model. Additionally, nnFormer demonstrates substantial enhancements over existing transformer-based configurations across both the Synapse and ACDC datasets. For example, nnFormer surpasses Swin-UNet by more than 7 percent on Synapse. Even against nnUNet, the currently leading fully-convolutional medical segmentation network, nnFormer exhibits a marginally superior performance on both Synapse and ACDC.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1560", "problem_id": "15600001", "content": "This study investigates visual explanations for deep metric learning and their practical uses. Metric learning, a significant area in representation learning, has garnered substantial interest; however, the interpretability of these models lags behind that of classification models. To address this, we introduce a method that identifies the image regions most influential in determining the similarity between two input images by dissecting the final activation. Going beyond merely providing a general activation map for each image, our approach generates a point-to-point activation intensity map that reveals the relationships between different regions of the images. We demonstrate the broad applicability of this framework across various metric learning tasks, providing insights into model comprehension. Experimental results highlight its efficacy in applications such as cross-view pattern discovery and interactive retrieval. The implementation is publicly accessible at \\url.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1561", "problem_id": "15610001", "content": "Outdoor vision systems used in robotics and autonomous vehicles are frequently impaired by image-quality degradations such as haze, defocus blur, and motion blur, collectively termed \"blindness issues.\" These impairments can significantly degrade the performance of robotic systems, potentially leading to hazardous outcomes. Current methods typically address only a single type of blindness or lack precise estimation of the blindness level. Furthermore, their computational demands often preclude real-time operation on practical systems. This paper introduces a novel method capable of simultaneously identifying the type of blindness and generating a pixel-level blindness map, quantifying the severity of visual impairment at each location. Both the identified blindness type and the per-pixel blindness estimate are crucial for tasks including deblurring, dehazing, and ensuring the fail-safe operation of robotic systems. Experimental results on the KITTI and CUHK datasets demonstrate that our method surpasses existing state-of-the-art techniques, achieving a processing speed of approximately 130 frames per second (fps).", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1562", "problem_id": "15620001", "content": "Recent studies [40, 24] have noted that employing a higher learning rate during the finetuning of pruned neural networks can substantially enhance performance, yet the underlying cause has remained unclear. This study aims to elucidate this phenomenon by applying the principles of dynamical isometry [42]. We investigate neural network pruning from a novel viewpoint, framing it as an initialization strategy for subsequent finetuning, and questioning the suitability of the inherited weights as an initial state. Drawing upon insights from dynamical isometry, we posit that these weights may not provide an optimal starting point, an important consideration that has been largely overlooked. We demonstrate the significance of this issue, showing that it not only clarifies the observed benefits of larger finetuning rates but also sheds light on the effectiveness of pruning itself [5, 30]. Addressing this problem offers both a more comprehensive theoretical understanding of pruning and the potential for tangible performance improvements.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1563", "problem_id": "15630001", "content": "The increasing reliance on deep learning models has highlighted the need for robustness, prompting an examination of the robustness of deep neural networks in video processing, which involves both spatial features extracted by convolutional neural networks from individual frames and temporal dynamics captured by recurrent neural networks between adjacent frames. To assess robustness, this study investigates the maximum safe radius problem, calculating the minimum distance between the optical flow sequence of a given input and that of an adversarial example in its vicinity. Under the assumption of Lipschitz continuity, the problem is approximated using finite optimisation by discretising the optical flow space, yielding an approximation with provable guarantees. A two-player cooperative game is utilised to solve the finite optimisation problem, where one player selects optical flows and the other determines the dimensions to manipulate, with an anytime approach employed to approximate the game's value by iteratively improving its bounds. The approach combines a gradient-based search algorithm for upper bounds and the admissible A* algorithm for updating lower bounds, with the framework's effectiveness demonstrated on the UCF101 video dataset, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1564", "problem_id": "15640001", "content": "We introduce a new neural network resizing component called the shape adaptor, which enhances conventional resizing layers like pooling, bilinear sampling, and strided convolution. Unlike standard resizing layers with fixed reshaping factors, our module incorporates learnable reshaping parameters. The shape adaptor can be trained end-to-end without extra supervision, enabling fully automated optimization of network architectures for specific tasks. Experiments on seven image classification datasets demonstrate consistent performance improvements when replacing traditional resizing layers with our adaptors, surpassing manually designed networks. We further validate the utility of shape adaptors in network compression and transfer learning applications. The implementation is publicly accessible at: https://github.com/lorenmt/shape-adaptor.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1565", "problem_id": "15650001", "content": "The primary objective of regression analysis is to forecast the value of a continuous outcome variable y based on a set of input values from predictor variables x. Typically, a specific combination of x values does not yield a fixed value for y, but instead a probability distribution of potential y values, denoted as p(y|x), which is characterized by its location, scale, and shape, all of which can be influenced by x and are essential for determining probable y values given x. Conventional regression methods often assume that the y values in the training data are precise numerical representations drawn from a well-behaved p(y|x). However, in practice, the actual y values in the training data may be discrete, truncated, or censored, which can affect the accuracy of the model. To address this issue, a regression approach is proposed, which utilizes an optimal transformation strategy to estimate the location, scale, and shape of p(y|x) as functions of x, even in the presence of imperfect training data, and includes validation diagnostics to evaluate the quality of the resulting solutions, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1566", "problem_id": "15660001", "content": "Multi-source transfer learning is particularly useful in scenarios where labeled data in the target domain is limited. Existing approaches mainly leverage domain similarities but rely on the assumption that source domains are either well-labeled or comparably annotated—an unrealistic condition in real-world applications. This work relaxes this assumption by tackling challenges arising from sources with varying labeling quantities and reliability levels. The first challenge is addressed by introducing a novel transfer learning technique that jointly considers source-target domain similarities and inter-source dependencies. The second challenge involves pool-based active learning where labeling is restricted to source domains, leading to a unified active transfer learning framework that integrates distribution alignment and uncertainty-based sampling. Comprehensive evaluations on synthetic and real-world datasets confirm the effectiveness of our proposed methods, outperforming existing baselines, including advanced transfer learning techniques.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1567", "problem_id": "15670001", "content": "A novel variance-reduced variant of the Greedy-GQ algorithm, termed VR-Greedy-GQ, is proposed for off-policy optimal control, leveraging a value-based reinforcement learning (RL) framework. The original Greedy-GQ algorithm has been shown to attain an \\epsilon-stationary point with a sample complexity of order (\\epsilon^) under linear function approximation and Markovian sampling, however, this is compromised by high variance stemming from Markovian samples. By incorporating an SVRG-based variance reduction scheme, VR-Greedy-GQ mitigates the stochastic variance associated with two-time-scale updates, yielding improved finite-time convergence under the same conditions. Theoretical analysis demonstrates that VR-Greedy-GQ achieves reduced bias and variance error compared to Greedy-GQ, resulting in an enhanced sample complexity of order (\\epsilon^), which is further validated through comparative experiments with Greedy-GQ in various RL settings, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1568", "problem_id": "15680001", "content": "The ability to learn from multi-relational data, which often includes noise, ambiguities, and duplicate entries, is crucial for applications like statistical inference using Web Linked Data, recommender systems, computational biology, and natural language processing. These applications typically involve the analysis of extensive and intricate datasets, such as the Web graph. However, existing multi-relational learning methods often struggle with these scenarios due to significant computational demands and limited scalability. To address these limitations, this paper introduces a new, scalable multi-relational factorization technique grounded in consensus optimization. Our model, ConsMRF, leverages the Alternating Direction Method of Multipliers (ADMM), allowing for the optimization of each target relation with a reduced parameter set compared to current state-of-the-art methods. Taking advantage of ADMM's characteristics, ConsMRF is easily parallelizable, making it well-suited for handling large multi-relational datasets. Experimental results using large Web datasets, built upon DBpedia, Wikipedia, and YAGO, demonstrate ConsMRF's efficiency and enhanced performance relative to strong baselines. Furthermore, ConsMRF's near-linear scalability suggests its potential for addressing problems at Web-scale.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1569", "problem_id": "15690001", "content": "The efficacy of reinforcement learning is contingent upon the design of a suitable action space, wherein the impact of each action is quantifiable and sufficiently granular to enable adaptable behavior. Historically, this design process has necessitated significant user input regarding the selection and frequency of available actions. This study introduces a novel reinforcement learning framework that alleviates these limitations. By operating within a routine space, a novel, higher-level action space, agents can learn effective behaviors, where each routine encompasses a set of equivalent sequences of fine-grained actions of varying lengths. The routine space is learned in an end-to-end manner, facilitating the achievement of underlying off-policy reinforcement learning objectives, as demonstrated by its application to two state-of-the-art off-policy algorithms, yielding improved performance and enhanced computational efficiency through reduced environmental interactions per episode.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1570", "problem_id": "15700001", "content": "Bounding box regression plays a critical role in object detection, with recent approaches demonstrating strong performance by utilizing Intersection over Union (IoU) as a loss function. However, IoU-based loss suffers from vanishing gradients when bounding boxes have minimal overlap, causing the model to overlook such cases. To address this, we introduce Side Overlap (SO) loss, which emphasizes maximizing side overlap between bounding boxes, thereby imposing stricter penalties for low-overlap scenarios. Additionally, Corner Distance (CD) is incorporated to accelerate convergence. By integrating Side Overlap and Corner Distance, we develop a novel regression objective function called Side and Corner Align Loss (SCALoss). SCALoss maintains a strong correlation with IoU loss, enhancing evaluation metrics while applying greater penalties for poorly overlapping cases. This function acts as a robust similarity measure, improving localization accuracy and convergence speed. Evaluations on COCO and PASCAL VOC benchmarks confirm that SCALoss consistently enhances performance, surpassing both \\ell_n loss and IoU-based loss when used with widely adopted detectors like YOLOV3, SSD, Reppoints, and Faster-RCNN.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1571", "problem_id": "15710001", "content": "This study addresses conversational fashion image retrieval using multiple turns of natural language feedback. Prior research predominantly focuses on single-turn scenarios, and current models for multiturn conversational fashion image retrieval suffer from limitations, resulting in suboptimal performance. To address these shortcomings, we introduce a new framework designed to effectively manage conversational fashion image retrieval with multiturn natural language feedback. This framework distinguishes itself by searching for suitable images through the integrated utilization of encoded reference image data, feedback text, and conversation history. Additionally, fashion attribute details of the images are incorporated through a mutual attention mechanism. Given the absence of an appropriate fashion dataset for our task's multiturn structure, we construct a large-scale dataset via manual annotation of an existing single-turn dataset. Experimental results demonstrate that our proposed model substantially surpasses current state-of-the-art approaches.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1572", "problem_id": "15720001", "content": "We investigate defense mechanisms against reward poisoning attacks within the context of reinforcement learning. Our threat model addresses attacks that influence rewards only marginally, ensuring that the attacker’s desired policy becomes the singular optimal choice under the compromised rewards, with the optimality gap dictated by an attack parameter. Our objective is to develop agents that maintain resilience against such attacks, focusing on the worst-case utility relative to the genuine, unaltered rewards while determining their policies based on the tainted rewards. We present an optimization framework for establishing optimal defense strategies, applicable in scenarios with both known and unknown attack parameters. Furthermore, we demonstrate that defense policies resulting from our proposed optimization methods come with verifiable performance guarantees. Specifically, we offer bounds concerning the genuine, untainted rewards: a) lower bounds for the expected returns of the defense policies, and b) upper bounds on the suboptimal nature of these defense strategies in relation to the attacker’s intended policy. We wrap up the paper by elucidating the reasoning behind our formal findings and demonstrating that the established bounds are significant.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1573", "problem_id": "15730001", "content": "Local descriptors are popular because of their enhanced ability to distinguish between different features. Utilizing multi-scale local neighborhoods has been shown to improve descriptor performance, albeit with increased dimensionality. This paper introduces a new approach for creating a local descriptor using multi-scale neighborhoods by determining the local directional order among intensity values at different scales in a specific direction. Local directional order represents the multi-radius relationship factor in a particular direction. The proposed local directional order pattern (LDOP) for a given pixel is calculated by establishing the relationship between the center pixel and the local directional order indexes, necessitating the transformation of the center value into the range of neighboring orders. Ultimately, the LDOP histogram is computed across the entire image to form the descriptor. Unlike existing descriptors, the dimension of the proposed descriptor is independent of the number of neighbors used to calculate the order, depending only on the number of directions. The performance of the developed descriptor is assessed within an image retrieval framework and compared against state-of-the-art descriptors using challenging face databases, including PaSC, LFW, PubFig, FERET, AR, AT&T, and ExtendedYale. Experimental results demonstrate the superiority and robustness of the LDOP descriptor.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1574", "problem_id": "15740001", "content": "The one-shot multi-object tracking approach, which combines object detection with ID embedding extraction in a single network, has made significant advancements in recent years. Nevertheless, existing one-shot trackers depend solely on single-frame detections to infer potential bounding boxes, which can prove unreliable under adverse visual conditions such as motion blur and occlusions. When a target bounding box is incorrectly identified as background by the detector, the temporal consistency of its associated tracklet is compromised, as illustrated in Fig. 1. In this study, we aim to correct these misclassified bounding boxes, referred to as fake backgrounds, through the introduction of a re-check network. This network facilitates the propagation of previous tracklets to the current frame by examining the relationship between inter-frame temporal cues and current candidates through a modified cross-correlation layer. The results from this propagation assist in reloading the \"fake background\" and ultimately reconstruct the disrupted tracklets. By incorporating the re-check network into a robust baseline tracker CSTrack (a variant of JDE), our model demonstrates significant improvements, achieving 70.7 to 76.7 and 70.6 to 76.3 MOTA on the MOT16 and MOT17 datasets, respectively. The code is freely accessible at https://github.com/JudasDie/SOTS.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1575", "problem_id": "15750001", "content": "Zero-shot learning relies on semantic attributes to bridge the gap between the search space of unseen objects. Despite the advancements brought by deep convolutional networks in visual modeling for the ZSL task, their visual features suffer from significant pattern inertia and a lack of representation of semantic relationships, resulting in substantial bias and ambiguity. To address this limitation, we introduce the Graph-based Visual-Semantic Entanglement Network, which leverages graph modeling of visual features mapped to semantic attributes via a knowledge graph, incorporating several innovative components: (1) a multi-path entangled network combining convolutional neural networks (CNNs) and graph convolutional networks (GCNs), where visual features from CNNs are input into GCNs to model implicit semantic relations, and GCNs provide feedback to refine CNN features; (2) the utilization of attribute word vectors as targets for GCN-based graph semantic modeling, forming a self-consistent regression that supervises GCNs to learn personalized attribute relations; and (3) the fusion and supplementation of hierarchical visual-semantic features refined through graph modeling into visual embeddings. Our approach surpasses state-of-the-art methods on prominent ZSL datasets, including AwA2, CUB, and SUN, by enhancing the semantic linkage modeling of visual features, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1576", "problem_id": "15760001", "content": "Modern image captioning models have surpassed human benchmarks on standard evaluation metrics like BLEU, METEOR, ROUGE, and CIDEr. However, does this indicate the problem of image captioning is fully resolved? These metrics primarily assess how closely generated captions align with human references, focusing on accuracy. Yet, images encompass numerous concepts and varying levels of detail, allowing for diverse captions that may highlight different aspects appealing to different audiences. Consequently, evaluating accuracy alone is inadequate—caption diversity must also be measured. This paper introduces a novel metric for assessing caption diversity, leveraging latent semantic analysis and kernelized CIDEr similarity. Through comprehensive experiments, we reassess recent captioning models in terms of both diversity and accuracy. Our findings reveal a significant performance gap between models and humans in both aspects, with models optimized for CIDEr accuracy exhibiting limited diversity. Additionally, we demonstrate that balancing cross-entropy loss and CIDEr reward during reinforcement learning training can effectively manage the tradeoff between caption diversity and accuracy.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1577", "problem_id": "15770001", "content": "Representation learning has been extensively investigated within meta-learning, facilitating the quick acquisition of new tasks through common representations. Recent studies, including MAML, have examined the use of fine-tuning-based metrics to evaluate how easily fine-tuning can lead to successful performance as a means to obtain representations. We introduce a theoretical framework for analyzing representations generated from a MAML-like algorithm, under the premise that the tasks share a similar underlying representation. Additionally, we provide risk bounds for the optimal predictor derived from fine-tuning via gradient descent, which illustrates that the algorithm can effectively utilize the shared structure. The upper limit is applicable to general classes of functions, as we elucidate by applying the guarantees of our framework to logistic regression and neural network scenarios. Conversely, we demonstrate that there are conditions under which any algorithm, using a representation trained without regard to task-specific fine-tuning, performs comparably to a learner without access to source tasks in the worst-case scenario. This separation result highlights the advantages of fine-tuning-based approaches, such as MAML, compared to methods that utilize \"frozen representation\" objectives in few-shot learning.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1578", "problem_id": "15780001", "content": "Timely and efficient responses are essential when a natural disaster occurs (e.g., earthquake, hurricane, etc.). Assessing building damage through satellite imagery is vital prior to the initiation of relief efforts. Utilizing a pair of satellite images captured before and after the disaster, building damage assessment focuses on estimating the level of damage inflicted on structures. Given their robust feature representation capabilities, deep neural networks have been effectively employed for this purpose. However, most current approaches merely concatenate pre- and post-disaster images as inputs for deep neural networks, overlooking their interrelations. This paper introduces a new two-stage convolutional neural network for Building Damage Assessment, termed BDANet. The first stage utilizes a U-Net for identifying building locations, with the subsequent stage sharing the weights from this initial step for damage assessment. In the second stage, a two-branch multi-scale U-Net serves as the backbone, processing pre- and post-disaster images independently. To investigate the relations between these images, a cross-directional attention module is implemented. Additionally, CutMix data augmentation is employed to address the difficulties associated with challenging classes. The proposed approach demonstrates state-of-the-art performance on the extensive xBD dataset. The code is accessible at https://github.com/ShaneShen/BDANet-Building-Damage-Assessment.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1579", "problem_id": "15790001", "content": "The Sentinel-2 mission, with its twin satellites, offers both multispectral (MS) imagery at a reasonably fine spatial resolution and a high revisit frequency, which is advantageous for Remote Sensing applications. However, of the thirteen available bands, only four possess the highest spatial resolution of 10 meters, while the remaining bands are provided at 20 or 60 meters. Specifically, the Short-Wave Infrared (SWIR) bands, which are valuable for detecting active fires, are only available at a 20-meter resolution. To generate more detailed Active Fire Detection (AFD) maps, we introduce a Convolutional Neural Network (CNN)-based super-resolution data fusion technique to enhance the spatial resolution of the SWIR bands to 10 meters. Our CNN-based method outperforms other approaches according to several accuracy metrics. Furthermore, we evaluate the utility of the super-resolved bands from an application perspective by monitoring active fires using established indices. The strengths and weaknesses of our proposed method are demonstrated using data from the Mount Vesuvius area near Naples, which experienced extensive fire damage in the summer of 2017.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1580", "problem_id": "15800001", "content": "Spatially-adaptive normalization (SPADE) has shown significant success in conditional semantic image synthesis \\cite by modulating normalized activations with spatially-varying transformations derived from semantic layouts, thereby preserving semantic information. However, a deeper understanding of its internal mechanisms is needed to mitigate its substantial computational and parameter overhead. This paper presents a return-on-investment analysis of SPADE's effectiveness, revealing that its modulation parameters benefit more from semantic-awareness than spatial-adaptiveness, particularly with high-resolution input masks. Based on this finding, we introduce class-adaptive normalization (CLADE), a lightweight yet equally effective alternative that adapts solely to semantic class. To enhance spatial-adaptiveness further, we incorporate intra-class positional map encoding, computed from semantic layouts, to modulate CLADE's normalization parameters, resulting in CLADE-ICPE, a truly spatially-adaptive variant. Extensive experiments across multiple challenging datasets demonstrate that CLADE can be generalized to various SPADE-based methods, achieving comparable generation quality to SPADE with improved efficiency, fewer parameters, and lower computational cost. The code and pretrained models are available at \\url.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1581", "problem_id": "15810001", "content": "A notable characteristic of the natural gradient is its invariance to any differentiable reparameterizations of the model. However, this property demands infinitesimal steps and tends to be diminished in practical applications using small, finite step sizes. In this study, we examine invariance characteristics through the lenses of Riemannian geometry and numerical methods for solving differential equations. We introduce the concept of the order of invariance associated with a numerical approach, defined by its convergence rate to an invariant solution. Our proposal involves employing higher-order integrators along with geodesic corrections to achieve more invariant optimization paths. We establish the numerical convergence properties of updates with geodesic corrections, demonstrating that they can be as computationally efficient as the straightforward natural gradient. Through experimental results, we illustrate that maintaining invariance facilitates quicker optimization, and our methodologies enhance traditional natural gradient methods in the training of deep neural networks and the natural policy gradient in reinforcement learning.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1582", "problem_id": "15820001", "content": "Our research demonstrates that treating graphs as collections of node features and integrating structural and positional details into a transformer framework can surpass the performance of traditional graph neural networks (GNNs). The proposed model, GraphiT, captures this information by (i) employing relative positional encoding in self-attention mechanisms using positive definite graph kernels and (ii) systematically encoding local substructures like short paths. Extensive evaluation across multiple classification and regression tasks confirms the individual and combined effectiveness of these approaches. Beyond achieving strong results on benchmark datasets, the model also provides interpretable visualizations of graph motifs that influence predictions, enhancing its suitability for scientific applications requiring explainability. Code is accessible at https://github.com/inria-thoth/GraphiT.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1583", "problem_id": "15830001", "content": "Offline reinforcement learning (RL) aims to derive an optimal policy from pre-collected datasets without requiring real-time interaction. Existing approaches in this field primarily involve: 1) generative modeling, which estimates policies from static data, and 2) estimating state-action value functions. Although much attention has been directed toward minimizing bootstrapping errors in value function approximation caused by data distribution shifts, the impact of error accumulation in generative modeling remains understudied. This work examines generative modeling errors and introduces AQL (action-conditioned Q-learning), a residual generative model designed to mitigate policy approximation errors in offline RL. Our experiments demonstrate that AQL achieves higher precision in policy approximation across various benchmark datasets. Furthermore, the proposed method enables training more capable AI agents in complex control scenarios, as validated in the multiplayer online battle arena (MOBA) game Honor of Kings.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1584", "problem_id": "15840001", "content": "We introduce a straightforward and adaptable object detection system designed for autonomous vehicles. Recognizing the high sparsity of point clouds in this context, we develop an efficient pillar-based method to address anchor-induced imbalance problems. Specifically, our technique integrates a cylindrical projection for multi-view feature extraction, estimates bounding box parameters at the pillar level instead of per point or anchor, and employs a precise pillar-to-point projection component to enhance prediction accuracy. This anchor-free design eliminates the need for hyperparameter tuning required by previous approaches, streamlining 3D object detection while achieving superior performance compared to existing methods.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1585", "problem_id": "15850001", "content": "The backpropagation algorithm is commonly regarded as crucial for developing effective feature detectors in the initial layers of artificial neural networks, enabling these detectors to contribute meaningfully to the tasks executed by higher layers. However, the conventional backpropagation method is not biologically plausible. This paper introduces a novel learning rule that is biologically plausible to a certain extent, drawing inspiration from Hebb's concept that synaptic strength modifications should be localized, relying solely on the activities of pre- and post-synaptic neurons. By incorporating global inhibition in the hidden layer, we have devised a learning algorithm capable of unsupervised learning of early feature detectors. The learned feature detectors in the lower layers can subsequently be utilized to train the weights of higher layers using standard supervised methods, resulting in a network performance comparable to that of traditional feedforward networks trained end-to-end with backpropagation, as shown in Figure A (Reference [1], Figure B, and Citation [2]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1586", "problem_id": "15860001", "content": "Video-based person re-identification (Re-ID) focuses on associating pedestrian video sequences captured by non-overlapping cameras. This task presents a significant challenge in effectively encoding both spatial and temporal information into feature representations. Current approaches primarily aggregate frame-level features and employ attention mechanisms in neural networks, but they predominantly analyze inter-frame relationships at high-level features. To address this limitation, our work enhances both intermediate and high-level features through non-local attention operations, offering two key contributions: (i) We introduce the Non-local Video Attention Network (NVAN), which integrates video-specific attributes across multiple feature levels. (ii) We also propose the Spatially and Temporally Efficient Non-local Video Attention Network (STE-NVAN), which optimizes computational efficiency by leveraging spatial and temporal redundancies in pedestrian videos. Experimental results demonstrate that NVAN achieves a 3.8% improvement in rank-1 accuracy on the MARS dataset, while STE-NVAN exhibits significantly lower computational overhead compared to existing methods.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1587", "problem_id": "15870001", "content": "The fashion industry's global transformation and increasing worldwide demand for fashion items have created a pressing need for effective fashion recommendations. Although numerous innovative solutions have been proposed to personalise fashion recommendations, they are hindered by their inability to perform well with new entities, a challenge known as the cold-start problem. This paper proposes a novel approach to address the cold-start problem for new users by utilising a visual preference modelling method based on a limited set of input images. The approach is demonstrated through feature-weighted clustering to provide personalised outfit recommendations tailored to specific occasions. The results show that the proposed visual preference modelling approach surpasses existing state-of-the-art methods in predicting clothing attributes, and a pilot study qualitatively demonstrates the system's ability to provide diverse and personalised recommendations in cold-start scenarios, as shown in Figure A, B, C (References [1], [2], and [3] cite related work).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1588", "problem_id": "15880001", "content": "This study introduces a versatile framework for developing efficient and precise algorithms for video enhancement, including super-resolution, deblurring, and denoising. The key insight is that high-quality enhancement relies on the accuracy of pixel flows rather than their density. Unlike previous approaches that compute dense yet less reliable per-pixel flows using computationally intensive methods, we propose a lightweight flow estimation technique. This method combines sparse point cloud data with even sparser and less accurate IMU data from modern autonomous systems to derive flow information. Leveraging this estimation, we present a modular framework that seamlessly incorporates flows with task-specific layers. Our approach delivers a 1.78x–187.41x speedup and a 0.42 dB–6.70 dB quality gain compared to existing methods.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1589", "problem_id": "15890001", "content": "Drawing upon recent progress in generative modeling, we present a human action generation model designed to produce sequential human motions that constitute new actions. Our approach integrates an autoencoder with a generative adversarial network (GAN) to generate diverse and continuous human actions based on the initial state and a specified class label. This model is trained end-to-end, with the autoencoder and GAN trained simultaneously. Evaluated on the NTU RGB+D dataset, our model demonstrates the capacity to generate actions with varied styles and to create novel action sequences conditioned on different action labels. Unlike conventional human action prediction and generation models, our model incorporates these features, which are crucial for real-world applications.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1590", "problem_id": "15900001", "content": "This study examines the CHARME model, a generalized mixture of nonlinear nonparametric AR-ARCH time series, and establishes its stationarity, ergodicity, and \\tau-weak dependence under mild Lipschitz-type conditions on the autoregressive and volatility functions, which are less restrictive than those found in existing literature. This theoretical foundation enables the development of an asymptotic theory for (non)parametric estimation in this model. Furthermore, leveraging the universal approximation property of neural networks (NN), a learning theory is established for NN-based autoregressive functions in the CHARME model, ensuring the strong consistency and asymptotic normality of the estimator for NN weights and biases under weak conditions, as presented in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1591", "problem_id": "15910001", "content": "Deep neural network (DNN) stereo matching techniques, which acquire prior knowledge from data, often exhibit diminished performance when applied to unfamiliar scenarios. Supervised methodologies, while typically effective with ground truth disparity maps, are hindered by the difficulty and expense of acquiring such data for every deployment environment. Consequently, several unsupervised domain adaptation techniques leveraging image-to-image translation have emerged; however, these often neglect the geometric relationships within stereo image pairs due to independent processing of each view. To mitigate this, we introduce Stereoscopic Cross Attention (SCA), an attention mechanism that consolidates features between the left and right views. By integrating SCA into an image-to-image translation network, the geometric integrity of stereo image pairs can be maintained during translation. The efficacy of our SCA-based unsupervised domain adaptation approach, which employs image-to-image translation, is validated through empirical evaluations.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1592", "problem_id": "15920001", "content": "We introduce a novel visualization technique for saliency maps in deep neural network models, with a specific application to deep reinforcement learning agents trained in Atari environments. By integrating an attention module, referred to as FLS (Free Lunch Saliency), into the feature extractor of a baseline model established by Mnih et al. (2015), we create a trainable model capable of generating saliency maps that illustrate the significance of various input components in the agent's decision-making process. Experimental results demonstrate that the incorporation of the FLS module yields a model with comparable performance to the baseline, thereby incurring no performance cost, and can seamlessly replace existing reinforcement learning agents. Furthermore, we develop an alternative feature extractor that, although scoring slightly lower, provides more accurate visualizations, and evaluate both models using saliency metrics on the Atari-HEAD dataset of human gameplay, in addition to reporting attained scores.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1593", "problem_id": "15930001", "content": "This paper presents the deep coordination graph (DCG) as a method for collaborative multi-agent reinforcement learning. DCG achieves a balance between expressive power and generalization ability by decomposing the global value function of all agents into pairwise agent payoffs using a coordination graph. This structure facilitates the maximization of value through localized message propagation within the graph, enabling end-to-end training of the value function using Q-learning. Deep neural networks are used to approximate the payoff functions, leveraging parameter sharing and low-rank approximations to substantially enhance sample efficiency. The efficacy of DCG is demonstrated by its ability to solve predator-prey problems that illustrate the overgeneralization issue, as well as complex StarCraft II micromanagement scenarios.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1594", "problem_id": "15940001", "content": "While commercial head-mounted eye trackers offer valuable functionalities for industrial and research applications, their high cost and reliance on proprietary hardware and software restrict their accessibility primarily to experts, hindering broader adoption and user-led innovation. This paper introduces Pupil—an open-source, cost-effective, and adaptable platform for mobile eye tracking and gaze-based interaction. The system includes a lightweight headset equipped with high-resolution cameras, an open-source software framework, and a graphical user interface (GUI) for visualizing gaze and video data playback. Pupil supports monocular and binocular gaze estimation with high-resolution scene and eye cameras. Its platform-independent software incorporates advanced algorithms for real-time pupil detection, tracking, calibration, and precise gaze estimation. Performance tests demonstrate an average gaze estimation accuracy of 0.6 degrees of visual angle (0.08-degree precision) with a processing latency of just 0.045 seconds.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1595", "problem_id": "15950001", "content": "This paper presents a network clustering framework designed for networks where nodes are associated with time-series data. This framework is versatile, capable of handling state clustering, identifying node clusters within states (also known as topology identification or community detection), and tracking subnetwork-state sequences. The approach is bottom-up: initially, kernel autoregressive-moving-average modeling extracts features from the original nodal time-series data to uncover non-linear relationships and low-rank representations, which are then projected onto the Grassmann manifold (Grassmannian). All clustering tasks are executed by uniquely exploiting the Riemannian geometry inherent to the Grassmannian. The framework's efficacy is demonstrated through brain-network clustering, with comprehensive numerical experiments using both synthetic and real functional magnetic resonance imaging (fMRI) data showing that the proposed learning framework performs better than several current clustering methods.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1596", "problem_id": "15960001", "content": "We introduce a novel generative autoencoder framework, the dual contradistinctive generative autoencoder (DC-VAE), which employs two complementary loss functions to enhance both inference (reconstruction) and synthesis (sampling) capabilities. The model combines an instance-level discriminative loss, ensuring fidelity in reconstruction and synthesis, with a set-level adversarial loss, promoting fidelity at the dataset level—both operating in a contradistinctive manner. DC-VAE demonstrates strong performance across multiple resolutions (32x32 to 512x512), where the synergistic interaction of these losses yields notable qualitative and quantitative improvements over standard VAEs, without requiring architectural modifications. It achieves state-of-the-art or competitive results in tasks such as image reconstruction, synthesis, interpolation, and representation learning. As a versatile VAE variant, DC-VAE is well-suited for diverse applications in computer vision and machine learning.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1597", "problem_id": "15970001", "content": "The efficacy of Generative Adversarial Networks (GANs) in producing exceptional images is well-established, but their deployment on devices with limited resources is hindered by substantial computational requirements and memory consumption. Despite notable advancements in GAN compression, existing methods still harbor potential redundancies, presenting opportunities for further compression. To address this challenge, we introduce a novel online multi-granularity distillation (OMGD) approach, designed to yield lightweight GANs capable of generating high-quality images while minimizing computational demands. Our pioneering work applies single-stage online distillation to GAN compression, leveraging a progressively refined teacher generator to enhance a discriminator-free student generator. The utilization of complementary teacher generators and network layers provides a multifaceted understanding, enriching visual fidelity from various perspectives. As demonstrated by experiments on four benchmark datasets, OMGD achieves significant compression, reducing MACs by 40x and parameters by 82.5x in Pix2Pix and CycleGAN, without compromising image quality, thereby facilitating real-time image translation on resource-constrained devices, with our code and models available at https://github.com/bytedance/OMGD.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1598", "problem_id": "15980001", "content": "Predicting detailed traffic flow from aggregated data is an increasingly important challenge, offering significant cost reductions by minimizing the need for extensive sensor deployment. This study highlights the strong relationship between traffic patterns and road networks, a factor previously overlooked or superficially addressed in prior research. To address this issue, we introduce the Road-Aware Traffic Flow Magnifier (RATFM), a novel approach that leverages road network information to comprehensively model the spatial distribution of fine-grained traffic. The method begins with a multi-directional 1D convolutional layer to extract road network features, then combines these with coarse-grained flow data to refine short-range spatial modeling. Additionally, a transformer-based architecture uses road network features as queries to capture long-range traffic patterns. The road-aware framework enables RATFM to produce accurate fine-grained traffic maps. Evaluations across three real-world datasets demonstrate that RATFM surpasses existing models in diverse scenarios.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1599", "problem_id": "15990001", "content": "Deep reinforcement learning (RL) has demonstrated remarkable performance across various fields, learning directly from complex sensory inputs. Nonetheless, when neural networks are trained within a fixed setting, such as a specific level in a video game, they often overfit, which hampers their ability to generalize to new levels. Overfitting in RL models means that even minor changes to the environment can lead to significant declines in agent performance. This paper investigates how training with procedurally generated levels can enhance generalization. We demonstrate that in some games, procedural generation allows for generalization to new levels within the same distribution. Furthermore, adjusting the level difficulty based on agent performance can improve results while utilizing less data. The generalization of learned behaviors is also assessed using a set of levels designed by humans. Our findings indicate that the ability to adapt to human-designed levels is strongly influenced by the architecture of the level generators. We utilize dimensionality reduction and clustering methods to visualize the level distributions produced by the generators and examine their capacity to create levels akin to those crafted by humans.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1600", "problem_id": "16000001", "content": "The need to embed large, high-dimensional data into low-dimensional vector spaces is crucial for efficient computational processing of modern datasets. Recent methods, such as word2vec and node2vec, have surpassed traditional latent semantic analysis and are widely recognized as effective tools in this domain. This paper extends existing research by introducing fca2vec, a novel family of embedding techniques tailored to formal concept analysis (FCA). Our study contributes to two primary areas of research, firstly by facilitating the application of FCA principles to large datasets, including the retrieval of the cover relation of a concept lattice from a computationally efficient embedding, and secondly by enhancing the classical node2vec approach in low-dimensional spaces. Throughout, we maintain the fundamental FCA constraint of ensuring explainable results. The efficacy of our new procedures is demonstrated through evaluations of fca2vec on various datasets, including wiki44 (a dense subset of the Wikidata knowledge graph), the Mushroom dataset, and a publication network derived from the FCA community, as seen in Figure A, B, C (citations remain unchanged, e.g., [1]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1601", "problem_id": "16010001", "content": "This study proposes a methodology for segregating foreground objects from their background in color images. The process commences with the input of a color image of arbitrary size, which is then transformed into a grayscale representation. Subsequently, the Canny edge detector is applied to identify the boundaries of the foreground object. The approach then involves determining the maximum distance between boundary pixels, both column-wise and row-wise, and filling the enclosed region defined by these edges. By doing so, the grayscale values of pixels within the bounded area are extracted, and ultimately, the grayscale image is converted back to its original color format, yielding an image that solely contains the foreground object.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1602", "problem_id": "16020001", "content": "The domain of large-scale video retrieval is currently a vibrant area of research. A significant portion of this work focuses on retrieving videos using text-based queries, employing methods like VSE++. However, there is a scarcity of research related to video retrieval via image queries, and the existing studies primarily depend on image queries sourced from within the video datasets or analyze videos frame by frame. These methodologies lack generalization for queries originating from outside the dataset and face scalability challenges with extensive video collections. To address these limitations, we introduce a novel method for video retrieval using image queries, which involves constructing an undirected graph from the amalgamated set of frames of all videos targeted for search. The features of the nodes in this graph are leveraged for the video retrieval process. We perform experiments on the MSR-VTT dataset utilizing query images not included in the dataset. The effectiveness of this new approach is assessed using P@5, P@10, and P@20 metrics. This study incorporates two distinct ResNet models, specifically ResNet-152 and ResNet-50.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1603", "problem_id": "16030001", "content": "Pedestrian detection, as a specialized form of object detection, has advanced considerably when trained on extensive manually labeled datasets in generic settings. However, models trained on such datasets perform poorly when applied directly to specific scenes due to distinct viewpoints, lighting conditions, and backgrounds. Adapting generic pedestrian detectors to these specialized environments requires labeled data from the target scenes, yet manual annotation is time-consuming, costly, and prone to inconsistencies in pixel-level accuracy. To address the scarcity of labeled data, this paper introduces an ACP-based approach leveraging augmented reality to construct virtual representations of specific scenes, simulating pedestrian movement within them. Experimental results demonstrate that synthetic data from virtual environments effectively enhances the adaptation of generic detectors to specialized real-world scenarios.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1604", "problem_id": "16040001", "content": "Projecting the point cloud onto a 2D spherical range image converts the LiDAR semantic segmentation into a 2D segmentation challenge on the range image. Nonetheless, the LiDAR range image fundamentally differs from conventional 2D RGB images; for instance, each location on the range image conveys specific geometric information. In this work, we introduce a novel projection-based LiDAR semantic segmentation framework featuring a unique network design and an effective post-processing phase. Our network incorporates a FID (fully interpolation decoding) module that upscales the multi-resolution feature maps directly through bilinear interpolation. Drawing inspiration from the 3D distance interpolation utilized in PointNet++, we propose that this FID module represents a 2D distance interpolation in the (\\theta, \\phi) space. As a parameter-free decoding module, the FID significantly decreases model complexity while preserving strong performance. Additionally, we observe that our model's predictions exhibit distinct boundaries among various semantic classes, prompting us to reconsider the necessity of the commonly employed K-nearest-neighbor post-processing within our pipeline. We recognize that the many-to-one mapping induces a blurring effect, resulting in certain points being assigned to the same pixel and sharing the same label. Consequently, we suggest addressing these occluded points by designating the closest predicted label to them. This NLA (nearest label assignment) post-processing demonstrates superior performance compared to KNN, along with improved inference speed, as shown in our ablation study. On the SemanticKITTI dataset, our pipeline achieves the highest performance among all projection-based methods with a resolution of 64 \\times 2048 and all point-wise solutions. Utilizing ResNet-34 as the backbone, both training and testing of our model can be completed on a single RTX 2080 Ti with 11G memory. The code is available for release.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1605", "problem_id": "16050001", "content": "Advances in smart home technology, autonomous vehicles, healthcare systems, and robotic aids, along with stricter legal requirements, have significantly shaped scholarly investigations into interpretable machine learning. Many researchers have developed techniques to explain arbitrary black box models for classification problems. However, a limitation of these model-agnostic explanation methods is their generic neighborhood generation approach, which fails to ensure meaningful proximity between generated samples and the original instance. This study presents an approach for locally explaining neural network decisions by incorporating the model’s architecture into the neighborhood generation process, thereby ensuring valid adjacency between the instance and its generated neighbors.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1606", "problem_id": "16060001", "content": "The foundation of social structure in daily life has long been rooted in the social relationships that emerge from individual interactions, dating back to the dawn of civilizations. While significant advancements have been achieved in the field of computer vision, particularly in scene understanding through object detection and scene parsing, recent studies have shifted focus towards examining the relationships between objects based on their functional and geometric properties. This study seeks to investigate the recognition of social relationships within still images, introducing a novel dual-glance model that initially concentrates on the individual pair of interest before utilizing an attention mechanism to examine contextual cues. To support this research, a large-scale dataset known as People in Social Context (PISC) has been compiled, consisting of 22,670 images and 76,568 annotated samples representing 9 distinct types of social relationships. The efficacy of the proposed model is demonstrated through benchmark results on the PISC dataset and qualitative assessments, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1607", "problem_id": "16070001", "content": "This paper presents a new machine learning framework designed for facial reenactment, differing from existing model-based approaches and recent frame-based methods that utilize Deep Convolutional Neural Networks (DCNNs) to produce individual frames. Our approach uniquely leverages the distinct structure of facial movements, with a focus on mouth motion, while also ensuring temporal consistency. The results show that our proposed method can effectively transfer the facial expressions, pose, and gaze of a source actor to a target video with enhanced photo-realistic accuracy, outperforming current state-of-the-art methods.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1608", "problem_id": "16080001", "content": "Deep convolutional neural networks (DCNNs) are widely used in medical imaging, offering impressive versatility and performance in image analysis tasks like segmentation, localization, and prediction. However, their substantial representational capacity requires significant computational resources, hindering their use in image-guided interventions and point-of-care diagnostics on mobile devices lacking graphics processing units (GPUs). We introduce a novel method that approximates both trainable weights and neural activations in deep networks using ternary values, addressing the challenge of backpropagation with non-differentiable functions. This allows for the elimination of resource-intensive floating-point matrix multiplications in convolutional neural networks, replacing them with energy-efficient binary operators and population counts. Our approach, demonstrated on a fully-convolutional network (FCN) for CT pancreas segmentation, reduces memory requirements by over 10-fold and facilitates sub-second inference without GPUs. This ternary approximation achieves high accuracy (with no post-processing), yielding a Dice overlap of 71.0%, statistically comparable to networks using high-precision weights and activations. We also show significant gains compared to binary quantization and the absence of our proposed ternary hyperbolic tangent continuation. This technique enables highly efficient DCNN inference without GPUs, facilitating the adoption of deep learning in practical clinical settings and potentially enhancing accuracy in large-scale medical data retrieval.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1609", "problem_id": "16090001", "content": "Print quality significantly influences a printer's overall performance. Identifying, categorizing, and evaluating printing flaws can reveal the printer's operational condition and aid in pinpointing internal mechanical issues. To address these concerns, an effective algorithm is necessary as a substitute for conventional visual inspection. This paper concentrates on pages exhibiting localized imperfections, specifically gray and solid spots. We introduce a coarse-to-fine approach for detecting such defects in a block-wise fashion, subsequently aggregating these block attributes to form a feature vector representing the entire test page for ranking purposes. In the detection phase, initial candidate regions are selected by applying a threshold to a single feature. Subsequently, more refined features of these candidate blocks are computed and input into a decision tree, pre-trained on our dataset. The decision tree model then provides the final result, managing the false alarm rate while preserving the required miss rate. Empirical evidence demonstrates that our algorithm outperforms existing techniques in both detecting and classifying localized printing defects.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1610", "problem_id": "16100001", "content": "Radar-based detection of road users plays a crucial role in autonomous driving systems. Traditional automotive radar sensors produce sparse data, making it difficult to enhance during signal processing. However, emerging radar technologies show promise for addressing these challenges. This study compares two radar sensors from different generations, focusing on their effectiveness in detecting and classifying moving road users. The analysis involves datasets from a standard radar and a next-generation high-resolution radar, with careful consideration given to ensuring comparable dataset assembly. The detection framework incorporates clustering, feature extraction, and a recurrent neural network ensemble for classification. Each component is assessed both separately and collectively, revealing the source of performance gains within the processing pipeline. Additionally, the study examines generalization capabilities and key evaluation metrics for radar-based object detection. Findings demonstrate significant advantages of the next-generation radar, primarily due to enhanced clustering performance rather than classification improvements.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1611", "problem_id": "16110001", "content": "We introduce a novel algorithm that enhances the exploration efficiency of deep Q-learning agents in dialogue systems. These agents utilize Thompson sampling, generating Monte Carlo samples from a Bayes-by-Backprop neural network for exploration. Compared to standard approaches like \\epsilon-greedy, Boltzmann, bootstrapping, and intrinsic-reward-based methods, our algorithm achieves faster learning. Furthermore, we demonstrate that enriching the replay buffer with a small number of successful episodes can enable Q-learning in scenarios where it would typically struggle.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1612", "problem_id": "16120001", "content": "Current domain adaptation techniques focus on acquiring features that transfer well across domains. Typically, these approaches necessitate modifying the source classifier to align with the target domain, often failing to balance performance between the source and target domains effectively. In contrast, our approach introduces a distinct component, termed a data calibrator, which assists a fixed source classifier in regaining discriminative capability in the target domain without compromising its original performance. For minor domain discrepancies, the source classifier's existing representations suffice, surpassing GAN-based methods in digit recognition tasks. For larger domain gaps, our technique enhances performance by utilizing synthetic images produced by GANs, achieving top results in digit datasets and driving scene semantic segmentation. Additionally, our findings suggest that adversarial attacks on domain discriminators can address specific performance-reducing factors caused by domain shifts.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1613", "problem_id": "16130001", "content": "The development of techniques for generating counterfactual explanations for opaque black-box systems has been a prolific area of research, yet the uncertainty associated with these explanations has received relatively scant attention. This oversight is particularly problematic in high-stakes applications, such as medical diagnosis and treatment planning, where misleading or uncertain explanations can have severe consequences. Furthermore, it is often challenging to assess whether the generated explanations are firmly rooted in the training data and robust to changes in the data distribution. To address these challenges, this paper presents a set of practical solutions that draw on novel connections to existing research in explainability, including trust scores, and uncertainty estimation, such as Monte Carlo Dropout, and evaluates their effectiveness through two experiments, as shown in Figure A and Figure B (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1614", "problem_id": "16140001", "content": "Unsupervised domain adaptive object detection seeks to transfer object detectors from a source domain with annotations to an unannotated target domain. Current methodologies predominantly employ a two-stage approach, initially producing region proposals and subsequently identifying objects, frequently utilizing adversarial learning to reduce disparities between domains at both stages. Nevertheless, adversarial learning can negatively impact the alignment of already well-aligned samples because it focuses solely on aligning global distributions across domains. To overcome this limitation, we propose an uncertainty-aware domain adaptation network (UaDAN) that uses conditional adversarial learning to align samples based on their alignment quality, treating well-aligned and poorly-aligned samples differently. Specifically, we develop an uncertainty metric to evaluate the alignment of each sample, dynamically adjusting the intensity of adversarial learning for both well-aligned and poorly-aligned samples. Furthermore, we leverage the uncertainty metric to implement curriculum learning, starting with simpler image-level alignment and progressively advancing to more complex instance-level alignment. Comprehensive experiments conducted on four demanding domain adaptive object detection datasets demonstrate that UaDAN outperforms existing state-of-the-art techniques.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1615", "problem_id": "16150001", "content": "Deep neural networks are susceptible to adversarial examples, which are intentionally manipulated images designed to mislead the model. Existing methods for generating these examples primarily focus on introducing minimal perturbations that maximize prediction errors. However, these images often exhibit artificial distortions that differentiate them from genuine images. This characteristic is exploited by various defense mechanisms that employ denoising techniques or train models to withstand minor disturbances. This paper presents \"Semantic Adversarial Examples,\" a novel category of adversarial examples where images are significantly altered to deceive the model while preserving the original object's semantic meaning. We frame the generation of these examples as a constrained optimization challenge and propose an adversarial transformation grounded in the shape bias observed in human cognition. Our approach involves converting RGB images to the HSV color space and then randomly adjusting the Hue and Saturation components while maintaining the Value component. Experimental evaluations conducted on the CIFAR10 dataset demonstrate that the VGG16 network achieves an accuracy of 5.7% when processing these color-shifted adversarial images.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1616", "problem_id": "16160001", "content": "Fully convolutional neural networks (F-CNNs) have achieved exceptional performance in image segmentation across various applications, primarily driven by advancements in spatial encoding and network connectivity to enhance gradient flow. This paper investigates an alternative approach, focusing on the adaptive recalibration of feature maps to selectively emphasize meaningful features while diminishing weaker ones. Drawing on the concept of the squeeze and excitation (SE) module, initially proposed for channel recalibration in image classification, we develop three novel SE module variants tailored for image segmentation: (i) spatial squeezing and channel-wise exciting (cSE), (ii) channel-wise squeezing and spatial exciting (sSE), and (iii) concurrent spatial and channel squeeze and excitation (scSE). By integrating these SE modules into three state-of-the-art F-CNN architectures (DenseNet, SD-Net, U-Net), we observe a consistent improvement in performance across all models, with minimal impact on model complexity. The effectiveness of our approach is demonstrated through evaluations on two challenging applications: whole brain segmentation using MRI scans (Multi-Atlas Labelling Challenge Dataset) and organ segmentation using whole body contrast-enhanced CT scans (Visceral Dataset).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1617", "problem_id": "16170001", "content": "We introduce a deep learning-based inverse reinforcement learning framework, termed PQR, designed to estimate reward functions using energy-based policies. PQR operates by sequentially estimating the Policy, the Q-function, and the Reward function. Unlike conventional methods, PQR accommodates reward functions that are dependent on both state and action, and it is also applicable to environments with stochastic state transitions. To address identifiability, PQR relies on the assumption of a single anchor action with a known reward, such as a no-operation action resulting in zero reward. We detail both the estimators and algorithmic implementation of PQR. When the environment's transition dynamics are known, we demonstrate that the PQR reward estimator can uniquely identify the true reward function. In scenarios with unknown transition dynamics, we provide bounds on the estimation error of PQR. Empirical evaluations on both synthetic and real-world datasets validate the effectiveness of PQR.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1618", "problem_id": "16180001", "content": "We present a novel, multiplicative-weight update algorithm for learning undirected graphical models, specifically Markov random fields (MRFs), with a simple and efficient approach. For the well-studied cases of Ising models and Boltzmann machines, our algorithm achieves a nearly optimal sample complexity and quadratic running time, up to logarithmic factors, thereby improving upon and subsuming existing work, as seen in Figure A (Reference [1]). Furthermore, we develop the first efficient algorithm for learning Ising models over general alphabets. Our primary application is an algorithm for learning the structure of t-wise MRFs, which boasts nearly-optimal sample complexity, up to polynomial losses in necessary terms that depend on the weights, and a running time of n^, as illustrated in Figure B. With n^ samples, we can also learn the model parameters and generate a hypothesis that is statistically close to the true MRF, outperforming prior work that runs in time n^ for bounded degree graphs and fails to produce a statistically close hypothesis, even for t=3, as shown in Figure C (cited in [2]). Notably, our algorithm's runtime exhibits the correct dependence on n and t, assuming the hardness of learning sparse parities with noise. The proposed algorithm, termed the Sparsitron, is straightforward to implement, featuring only one parameter, and operates effectively in the online setting, with its analysis leveraging a regret bound from Freund and Schapire's classic Hedge algorithm, ultimately providing the first solution to the problem of learning sparse Generalized Linear Models (GLMs).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1619", "problem_id": "16190001", "content": "Defining a Reinforcement Learning (RL) task requires the selection of an appropriate planning horizon, which is commonly represented by a discount factor. It has been established that utilizing RL algorithms with a lower discount factor can serve as a regularizer, enhancing performance in scenarios with limited data. However, the specific characteristics of this regularizer have yet to be explored. This study aims to address that gap. We demonstrate a clear equivalence for multiple Temporal-Difference (TD) learning approaches between employing a diminished discount factor and incorporating an explicit regularization term into the loss function of the algorithm. Inspired by this equivalence, we conduct empirical investigations to compare this method with standard L_2 regularization, performing extensive experiments across both discrete and continuous domains, utilizing tabular and functional representations. Our findings indicate that the effectiveness of the regularization is closely tied to the traits of the available data, including its size, distribution, and mixing rate.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1620", "problem_id": "16200001", "content": "This paper introduces a novel approach for utilizing deep neural networks to enhance physics-based simulations. Specifically, we combine traditional Lagrangian mechanics with a deep autoencoder to expedite the elastic simulation of deformable solids. The inertia effect necessitates the evaluation of second-order derivatives of the deep autoencoder network to establish dynamic equilibrium, which goes beyond the capabilities of standard automatic differentiation tools that primarily focus on gradient computation. Achieving nonlinear force equilibrium presents additional challenges when applying standard Newton's method, as it requires calculating the network's third-order derivative to derive the variational Hessian. We address these challenges using complex-step finite difference techniques alongside reverse automatic differentiation. This method allows us to leverage the benefits of complex-step finite difference, while efficiently employing complex-value perturbations to minimize unnecessary network evaluations. With a GPU-based implementation, we can effectively manage deep autoencoders (e.g., over 10 layers) operating in a high-dimensional latent space in real time. Additionally, we create a sampling network and a weighting network to facilitate Cubature integration, allowing for the incorporation of nonlinearity in model reduction. We anticipate that this work will inspire and support future research in nonlinearly reduced physical simulation challenges.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1621", "problem_id": "16210001", "content": "A function refers to the set of tasks that allow a product to fulfill its intended purpose. Functional modeling and similar tools assist in decision-making during the early stages of product design when specific choices remain undefined. Since function-based design data tends to be limited and subjective, automated function classification can enhance data reliability and support intelligent design agents through improved function representation. Typically, such data is archived in manually curated repositories, which compile expert insights and interpretations of product functions, structured by function-flow and component taxonomies. This study transforms a structured taxonomy-based repository into assembly-flow graphs and employs a graph neural network (GNN) for automated function classification. By training on repository data, the model establishes a baseline for component function assignment. Testing reveals micro-average F-scores of 0.832 for tier 1 (broad), 0.756 for tier 2, and 0.783 for tier 3 (specific) functions, demonstrating promising performance despite data imbalances. This work lays the foundation for advanced applications in knowledge-based CAD systems and Design-for-X approaches within function-based design.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1622", "problem_id": "16220001", "content": "The rapid evolution of fashion design and production has been driven by the influence of social media, with trends often emerging from celebrities, renowned designers, and fashion influencers, thereby compressing the fashion cycle. Nevertheless, the vast amount of fashion-related content and user-generated photos on social media platforms poses a significant challenge for fashion designers seeking to identify and compile current fashion trends, underscoring the need for in-depth analysis of social media photos to detect and categorize multiple fashion items. Although object detection competitions like MSCOCO boast extensive sample collections for various object categories, compiling large labeled datasets for fast fashion items is notoriously difficult. Furthermore, existing state-of-the-art object detectors are unable to leverage the vast amounts of unlabeled social media data to refine their performance using labeled datasets. This study demonstrates the application of a generic object detector that can be pre-trained in an unsupervised manner, utilizing 24 categories from the recently released Open Images V4 dataset. By initially training the detector's base architecture using unsupervised learning on 60K unlabeled social media photos from 24 categories, and subsequently fine-tuning it on 8.2K labeled photos from Open Images V4, we achieve a 72.7% mAP on a test dataset of 2.4K photos with 300 X 300 image inputs, outperforming state-of-the-art object detectors by 11% to 17%. This improvement is attributed to the chosen architecture, which enables unsupervised learning and excels at identifying small objects, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1623", "problem_id": "16230001", "content": "The development of fine-grained image understanding models that surpass object detection capabilities has sparked a growing interest in image scene graph generation, which encompasses object, attribute, and relationship detection. However, the absence of a standardized benchmark has hindered the comparability of results across different scene graph generation models, thereby slowing research advancements. To address this limitation, we introduce a novel scene graph generation benchmark, built upon the maskrcnn-benchmark and incorporating several prominent models. This paper provides an overview of the key features of our benchmark and presents a thorough ablation study of scene graph generation models, utilizing the Visual Genome and OpenImages Visual relationship detection datasets, with our codebase available at https://github.com/microsoft/scene_graph_benchmark.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1624", "problem_id": "16240001", "content": "Graph Representation Learning (GRL) has advanced considerably, offering effective methods for extracting structural information for downstream learning. While current methods, such as shallow embeddings and Graph Neural Networks, are often evaluated using node classification and link prediction, this study adopts an application-focused viewpoint to assess the representational capacity of well-known embedding techniques in relation to real-world graph characteristics. We implement a comprehensive empirical framework, driven by data, to question established beliefs about the expressive power of embedding methods in graphs exhibiting diverse patterns, complemented by a theoretical examination of the limitations identified. Our findings indicate that universally applicable GRL methods are difficult to establish in practical settings. As new techniques emerge, they should explicitly address their capacity to capture graph properties and their applicability to datasets with significant structural variations.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1625", "problem_id": "16250001", "content": "Reliable assessment of predictive uncertainty (model calibration) is crucial for the secure deployment of neural networks. Numerous cases of miscalibration in contemporary neural networks have been documented, indicating a tendency where newer, higher-accuracy models yield less reliable uncertainty estimates. In this study, we re-examine this issue for cutting-edge image classification models. We methodically analyze the relationship between model calibration and accuracy, discovering that the latest architectures, particularly those not employing convolutions, exhibit superior calibration. Previously observed patterns, such as calibration degradation under distribution shifts or with increased model size, are less evident in modern designs. Additionally, we demonstrate that neither model scale nor pretraining extent entirely accounts for these variations, implying that architectural choices significantly influence calibration characteristics.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1626", "problem_id": "16260001", "content": "Data augmentation encompasses various strategies aimed at enhancing model generalization through the expansion of training samples. These approaches frequently demand domain-specific insights, leading to a surge in research on automated augmentation methods. In this study, we employ bilevel optimization to address graph classification on the ogbg-molhiv dataset. Our top-performing augmentation yielded a test ROCAUC score of 77.77% using a GIN+virtual classifier, establishing it as the leading augmentation technique for this model on the leaderboard. This approach integrates a GIN layer augmentation generator with a bias transformation, surpassing the performance of the same classifier when augmented with the advanced FLAG method.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1627", "problem_id": "16270001", "content": "The quest for effective neural networks is a crucial and pragmatic area within deep learning. In addition to determining the architecture's depth, convolution type, normalization, and nonlinear functions, the topological connectivity of neural networks also plays a vital role. Previous modular design principles based on rules have streamlined the complexities of constructing effective architectures but have restricted possible topologies to limited configurations. In this study, we aim to enhance the connectivity in neural networks by proposing a topological approach that represents a network as a complete graph for analysis. In this representation, nodes perform the aggregation and transformation of features, while edges dictate the flow of information. By assigning learnable parameters to the edges to reflect the strength of connections, the learning process can occur in a differentiable fashion. Additionally, we introduce an auxiliary sparsity constraint on the connectivity distribution, which encourages the learned topology to emphasize essential connections. This learning approach is compatible with existing networks and can adapt to larger search spaces and various tasks. Experimental quantitative results demonstrate that the connectivity learned through this method outperforms traditional rule-based approaches, such as random, residual, and complete designs. Furthermore, it achieves notable enhancements in image classification and object detection without imposing an excessive computational burden.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1628", "problem_id": "16280001", "content": "Image segmentation is a critical component in medical applications for diagnosis and intervention, with methods ranging from manual to semi-automated or fully automated. While manual segmentation ensures high-quality outcomes, it is labor-intensive, time-consuming, and subject to user bias. Fully automated techniques eliminate human involvement but often produce less accurate results without allowing adjustments. Semi-automated methods strike a balance by enabling user interaction while maintaining precision. This paper introduces a deep learning-based semi-automated segmentation tool designed as an intelligent interactive solution for outlining regions of interest in medical imaging. We illustrate its effectiveness in segmenting abdominal organs from CT scans, addressing key clinical needs: (i) achieving high-quality 2D segmentations with minimal user input, (ii) adapting to unfamiliar structures and edge cases, (iii) allowing intuitive, adjustable corrections for refined accuracy, and (iv) ensuring reliable performance. Our method is evaluated against existing techniques to highlight its superior capabilities.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1629", "problem_id": "16290001", "content": "This study addresses the challenge of extracting semantic attributes using only classification labels as supervision. In the context of classifying bird images by species, the goal is to develop features analogous to those employed by zoologists. To achieve this, we introduce a neural network architecture incorporating discrete features in its final layer, succeeded by both a multi-layered perceptron (MLP) and a decision tree. By leveraging decision trees with simple binary decision stumps, we anticipate the development of semantically meaningful discrete features. We provide both a theoretical framework and a practical approach for learning within the shared space of two hypothesis classes. Empirical evaluations on various benchmark datasets demonstrate an enhanced capacity to extract feature sets that exhibit strong correlations with unobserved attributes.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1630", "problem_id": "16300001", "content": "Efficiently solving intricate, multi-task problems in sparse-reward reinforcement learning remains challenging. Achieving sample efficiency in multi-task learning necessitates the reuse and sharing of low-level policies. To enable automatic hierarchical task decomposition, we suggest leveraging step-by-step human demonstrations through natural language instructions and action trajectories. We present a dataset of these demonstrations within a crafting-oriented grid world. Our framework includes a high-level language generator and a language-conditioned low-level policy. Human demonstrations enhance performance on the most complex tasks, while natural language integration enables zero-shot generalization to novel tasks and rapid learning from limited demonstrations. This generalization extends to both the agent’s actions and its ability to produce natural language instructions for unfamiliar tasks. Additionally, our method enhances interpretability by allowing the trained agent to generate high-level action descriptions.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1631", "problem_id": "16310001", "content": "The application of graph neural networks (GNNs) to algorithmic learning has been explored, but existing studies have primarily focused on algorithms that are well-suited to standard GNN architectures, such as sorting, Breadth-First search, and shortest path finding. In contrast, this study demonstrates the use of neural execution to learn a more complex algorithm, specifically maximum bipartite matching, by reformulating it as a flow problem and leveraging the Ford-Fulkerson method to compute the maximum flow. Notably, this is accomplished using a single GNN to generate features, which are then used to guide the neural execution process. The results of the evaluation are highly promising, with the network achieving optimal matchings in nearly 100% of cases, exhibiting strong generalization capabilities.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1632", "problem_id": "16320001", "content": "This study delves into the application of Deep Reinforcement Learning within parameterized action spaces, focusing on the development of sample-efficient end-to-end training methodologies. To address this challenge, a novel, compact architectural design is proposed, wherein the parameter policy is conditioned on the output generated by the discrete action policy. Furthermore, two innovative training methods are introduced, building upon the established algorithms of Trust Region Policy Optimization (TRPO) and Stochastic Value Gradient (SVG), to facilitate the training of this architecture. The efficacy of these methods is demonstrated through comparative analysis, showcasing superior performance to the existing state-of-the-art method, Parameterized Action DDPG, across various test domains.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1633", "problem_id": "16330001", "content": "This research concentrates on creating efficient adversarial patch attacks, with a novel approach to reconciling the conflicting goals of attack effectiveness and inconspicuousness through the development of innovative, semi-transparent patches. Driven by the need for a comprehensive analysis of patch attack robustness in the face of geometric transformations, we investigate the crucial factors influencing patch attack success and the effects of distributional shift between training and testing environments, utilizing the Expectation over Transformation (EoT) framework. Our analysis, which centers on three primary transformation categories (rotation, scale, and location), yields quantitative insights into the design of effective patch attacks, revealing that scale has a profound impact on attack success. To overcome the limitations of scale in real-world attack deployment, particularly the noticeable nature of large patches, we propose a new optimization method to design irregularly-shaped, semi-transparent patches that balance the trade-off between minimizing obtrusiveness and maximizing attack effectiveness, as seen in Figure A, B, C (References [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1634", "problem_id": "16340001", "content": "Neural Machine Translation has recently attracted significant interest due to the emergence of increasingly advanced yet highly effective models. The attention mechanism has played a crucial role in this progress by assigning importance weights to input words, enabling the decoder to focus on contextually relevant terms. However, as more complex attention models have been developed, they require extensive computations, leading to slower inference speeds. This paper introduces an attention network modeled using principles from social choice theory, while also framing the attention mechanism as a Markov Decision Process optimized through reinforcement learning. We propose replacing traditional attention networks with an election-based approach (k-Borda), refined via Q-learning, which reduces inference time compared to standard Bahdanau translators while maintaining comparable translation quality. Our experiments validate these claims and demonstrate improved inference efficiency.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1635", "problem_id": "16350001", "content": "Numerous applications, including autonomous navigation, urban planning, and asset monitoring, depend on precise information regarding objects and their geographical locations. This paper presents a method for the automatic detection and calculation of GPS coordinates of frequently appearing stationary objects of interest by utilizing street view images. Our processing framework employs two fully convolutional neural networks: the first one segments objects in the images, while the second estimates their distance from the camera. To achieve consistent geolocation of all identified objects, we introduce an innovative custom Markov Random Field model for object triangulation. The uniqueness of our pipeline lies in the integration of monocular depth estimation and triangulation, which facilitates the automated mapping of intricate scenes containing multiple visually similar objects of interest. We empirically assess the effectiveness of our approach across two object categories: traffic lights and telegraph poles. The experiments demonstrate high recall rates for object detection and GPS accuracy within 2 meters, which matches the precision of single-frequency GPS receivers.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1636", "problem_id": "16360001", "content": "Deep neural networks have demonstrated success in reinforcement learning, with deep reinforcement learning exhibiting the capacity to learn effective policies directly from high-dimensional sensory inputs, surpassing traditional methods. Addressing the lack of fundamental advancements in existing training frameworks, we introduce a novel, conceptually clear framework potentially applicable to various reinforcement learning algorithms. This approach utilizes Monte-Carlo sampling for raw data input, which is then trained in batches to create Markov decision process sequences. Network parameters are updated synchronously, replacing experience replay. This framework optimizes the unbiased approximation of the loss function, aligning its estimation with the real probability distribution of data inputs, resulting in improved sample efficiency and convergence rate compared to existing deep reinforcement learning methods. We evaluate this on both discrete action spaces and continuous control problems. Furthermore, we present several algorithms integrated with our new framework for typical discrete and continuous scenarios. These algorithms demonstrate enhanced efficiency compared to their original deep reinforcement learning counterparts, offering a pathway for generalizing existing and future algorithms to our new framework \\cite.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1637", "problem_id": "16370001", "content": "Reward decomposition presents a significant challenge in the centralized training with decentralized execution (CTDE) approach for multi-agent reinforcement learning. To effectively utilize global information—which incorporates states from all agents and the surrounding environment to break down Q values into individual contributions—we introduce a meta-learning-based Mixing Network with Meta Policy Gradient (MNMPG) framework, designed to extract the global hierarchy for precise reward allocation. The learning signal for this global hierarchy is derived from the difference in episodic rewards before and after \"exercise updates\" via the utility network. Our approach is broadly compatible with CTDE methods employing monotonic mixing networks. Evaluations on the StarCraft II micromanagement benchmark show that our method, even with a basic utility network, surpasses existing state-of-the-art MARL algorithms in 4 out of 5 highly challenging scenarios. Further performance improvements are attainable when integrating a role-based utility network.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1638", "problem_id": "16380001", "content": "Objective: The precise assessment of root canal filling outcomes in X-ray images is crucial for root canal treatment, which categorizes results as correct filling, under-filling, or over-filling based on the spatial relationship between the apical area boundary of the tooth root and the top of the filled gutta-percha, as well as the tooth root's shape. Methods: We introduce an innovative anatomy-guided Transformer diagnosis network. To capture accurate anatomy-guided features, we utilize a polynomial curve fitting segmentation technique to delineate the indistinct boundaries. Additionally, we present a Parallel Bottleneck Transformer network (PBT-Net) as the classification network for the final assessment. Results and conclusion: Our numerical experiments demonstrate that the anatomy-guided PBT-Net enhances accuracy from 40% to 85% compared to the baseline classification network. Further comparisons with the state-of-the-art segmentation network reveal a significant reduction of 30.3% in the Adaptive Segmentation Divergence (ASD) due to our fitting segmentation approach. Significance: The polynomial curve fitting segmentation proves effective for very indistinct boundaries, and the classification network guided by prior knowledge is exceptionally well-suited for evaluating root canal therapies. The newly proposed Parallel Bottleneck Transformer, designed for self-attention, is versatile, encouraging widespread application across various backbone networks.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1639", "problem_id": "16390001", "content": "Image correction seeks to transform an image into a more visually appealing version. Current methods primarily focus on manipulating image pixels, which are often insufficient for recovering details in under- or over-exposed areas. In this paper, we re-examine the image formation process and observe that the missing details in these areas are present in the corresponding high dynamic range (HDR) data. These details, although easily perceived by humans, are diminished in the low dynamic range (LDR) domain due to tone mapping. Consequently, we frame image correction as an HDR transformation task and introduce a new method called Deep Reciprocating HDR Transformation (DRHT). Starting with an input LDR image, we initially reconstruct the missing details within the HDR domain. Subsequently, we apply tone mapping to the predicted HDR data to produce an output LDR image with the recovered details. To achieve this, we present a unified framework comprising two CNNs for HDR reconstruction and tone mapping, integrated end-to-end for joint training and prediction. Experimental results on standard benchmarks indicate that the proposed method outperforms state-of-the-art image correction methods.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1640", "problem_id": "16400001", "content": "Three-dimensional point cloud instance segmentation is a widely studied field critical for advancing autonomous vehicles and robotics. Some current approaches divide input point clouds into smaller sections, like 1m x 1m regions, due to the high space complexity of models that limits their ability to process large point sets. However, these small regions sometimes contain only a few instances of the same class, causing existing evaluation metrics like mAP to be heavily influenced by category recognition accuracy. To overcome these challenges, we introduce a method with O(Np) space complexity, enabling the processing of larger regions, along with new metrics designed to be unaffected by input categories or sizes. Our approach maps input point clouds to an embedding space where instances form distinct clusters, facilitating instance differentiation during inference. The method demonstrates superior performance under both conventional and our proposed metrics. Additionally, we validate that our new metric evaluates task performance independently of external conditions.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1641", "problem_id": "16410001", "content": "The foundation of machine learning theory relies heavily on exponential inequalities, and establishing these inequalities for non-identically and independently distributed (non-i.i.d) random variables enables the extension of various learning techniques to this domain. Over the past 15 years, significant research has been conducted on both inequalities and learning theory, particularly in the context of time series. Nevertheless, the majority of existing results pertain to stationary time series, thereby limiting their applicability to numerous important cases, such as series exhibiting periodic behavior, which are inherently non-stationary. This paper builds upon the work of Dedecker and Fan (2015) by extending their fundamental tools to non-stationary Markov chains, yielding a Bernstein-type inequality as an application, and subsequently deriving risk bounds for predicting periodic autoregressive processes with unknown periods.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1642", "problem_id": "16420001", "content": "Multi-task learning has the potential to enable robots to develop a wide range of valuable skills. However, existing multi-task reinforcement learning approaches often presume that robots can gather data from all tasks continuously. In practice, the tasks that robots are trained on are presented sequentially, influenced by the user's needs and the robot's current surroundings. This study investigates a realistic sequential multi-task reinforcement learning challenge that arises from the limitations of real-world robotic systems. We propose a method that effectively utilizes the data and policies acquired from prior tasks to progressively enhance the robot's capabilities. Through a series of simulated robotic manipulation trials, our method demonstrates that it requires fewer than half the number of samples compared to training each task independently, while also avoiding the impracticality of round-robin data collection. Using a Franka Emika Panda robot arm, our method successfully incrementally learns ten demanding tasks, such as bottle capping and block insertion.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1643", "problem_id": "16430001", "content": "Despite the Transformer architecture's significant achievements across numerous fields, particularly in Natural Language Processing (NLP), its application to time series forecasting remains problematic. The autoregressive decoding inherent in standard Transformer models can lead to substantial error accumulation in this context. Furthermore, effectively handling spatial-temporal dependencies with Transformers presents considerable challenges. To address these issues, this paper introduces a novel Non-Autoregressive Transformer architecture specifically designed for time series forecasting, mitigating time delay and cumulative error problems associated with conventional Transformers. A new spatial-temporal attention mechanism is also proposed, leveraging a learned temporal influence map to integrate spatial and temporal attention, thereby enabling the cohesive processing of spatial and temporal dependencies. Empirical evaluations conducted on various ego-centric future localization datasets demonstrate that the proposed model achieves state-of-the-art performance in both real-time operation and accuracy.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1644", "problem_id": "16440001", "content": "Developing artificial intelligence that can effectively generalize poses a substantial challenge, particularly in creating agents that can autonomously learn about their environment and adapt this knowledge to tackle complex tasks with limited rewards. Traditional reinforcement learning methods often decouple learning and decision-making processes, which diverges from the principles of biological learning. By drawing inspiration from experimental neuroscience research, including spike-timing dependent plasticity, memory consolidation, and the neurochemical basis of curiosity-driven behavior, we propose the Neurons-in-a-Box architecture, which integrates computationally efficient models of biological algorithms. This approach enables wholly generalizable learning, allowing the agent to construct and apply representations without explicit optimization over predefined criteria or actions, as demonstrated in various environments, such as OpenAI Gym's Mountain Car, Inverted Pendulum, a video stream, and Google Chrome's Dinosaur Game, including Figure A, B, C, as referenced in [citation].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1645", "problem_id": "16450001", "content": "Convolutional neural networks (CNN) are integral to image processing applications such as image classification, object detection, and semantic segmentation. Typically, CNNs consist of multiple stacked layers, ranging from several to hundreds, with weights totaling several megabytes. One effective strategy for decreasing both complexity and memory usage is pruning, which involves eliminating weights that link neurons between adjacent layers within the network. Achieving a near-optimal solution with a predetermined accuracy reduction can become more complex as the deep learning model incorporates a greater number of convolutional layers. This paper outlines and contrasts several methods, including those based on retraining and those that do not require retraining.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1646", "problem_id": "16460001", "content": "This paper addresses the complex problem of video scene graph generation (VidSGG), aiming to create structured video representations for advanced understanding tasks. We introduce a novel approach that separates context modeling for relation prediction from intricate low-level entity tracking. Specifically, we develop an efficient frame-level VidSGG method, named TRACE, emphasizing the capture of spatio-temporal context for relation recognition. Our TRACE framework utilizes a modular design to streamline the VidSGG pipeline, incorporating two key components: Hierarchical Relation Tree (HRTree) construction and Target-adaptive Context Aggregation. The HRTree facilitates an adaptive structure for organizing potential relation candidates and directs the context aggregation module to effectively capture spatio-temporal structural information. This process yields contextualized feature representations for each relation candidate, which are then fed into a classification head to identify its relation category. Finally, a straightforward temporal association strategy is used to track TRACE's detected results, producing the video-level VidSGG. Experiments conducted on two VidSGG benchmarks, ImageNet-VidVRD and Action Genome, demonstrate that TRACE achieves state-of-the-art performance. The code and models are available at \\url.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1647", "problem_id": "16470001", "content": "Graph coarsening, or graph reduction, has long been a highly effective and widely used technique in scientific computing, and it is increasingly gaining prominence in machine learning. This paper examines established coarsening approaches from scientific computing and explores their adaptation to emerging machine learning applications. In scientific computing, coarsening is fundamental to algebraic multigrid methods and multilevel incomplete LU factorizations, while in machine learning, it is often referred to as graph downsampling or reduction. The primary objective is typically to construct a smaller graph that retains structural and functional similarities to the original. A recurring theme across these methods is the reliance on spectral properties to define the coarser graph.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1648", "problem_id": "16480001", "content": "Deepfakes, generated via digital manipulation to produce convincing and deceptive videos, leverage deep learning methodologies like autoencoders and GANs. The increasing accessibility and precision of these techniques leads to the creation of highly realistic fake videos. While Convolutional Neural Networks (CNNs), particularly EfficientNet B7-based approaches, have been conventionally employed for deepfake detection, this research explores the integration of diverse Vision Transformers with a convolutional EfficientNet B0, serving as a feature extractor. The results are on par with recent Vision Transformer-based methods. Unlike current state-of-the-art techniques, this study avoids the use of distillation or ensemble methods, with the optimal model attaining an AUC of 0.951 and an F1 score of 88.0%, closely approaching state-of-the-art performance on the DeepFake Detection Challenge (DFDC).", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1649", "problem_id": "16490001", "content": "Coverage path planning is a thoroughly researched issue in robotics where a robot is required to devise a route that continuously traverses every point within a specified area, typically at a consistent frequency. To accommodate situations where certain points necessitate more frequent visits, the problem has evolved into non-uniform coverage planning. This study examines a variant of non-uniform coverage where the robot lacks prior knowledge of the distribution of relevant events, yet must learn to enhance the detection rate of significant occurrences. The continual area sweeping problem has been previously defined under stringent environmental assumptions, with only a greedy strategy proposed thus far. We broaden the continual area sweeping framework to impose fewer restrictions on the environment and introduce an innovative method based on reinforcement learning within a Semi-Markov Decision Process. This method is assessed through an abstract simulation as well as a high-fidelity Gazebo simulation, demonstrating substantial improvements over existing methods in general scenarios, which is particularly pertinent to the expanding field of service robotics.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1650", "problem_id": "16500001", "content": "Smartphones facilitate the scanning and digital archiving of physical photographs, but the quality of these scanned images often suffers. While supervised deep neural networks offer a potential solution, they demand extensive paired training data, which is costly to acquire. Prior research has explored synthetic training data generated through image processing techniques to simulate degradation; however, these approaches typically rely on ideal latent space representations and fail to capture the complexities of real-world smartphone scanning degradations stemming from factors like lens defocus, lighting variations, and detail loss from printing. Furthermore, structural misalignments caused by distortions in 3D object capture can hinder both restoration effectiveness and quantitative assessment accuracy. To address these challenges, we introduce a semi-supervised Deep Photo Scan (DPScan) approach. First, we introduce a method for generating realistic degradation and present the DIV2K-SCAN dataset tailored for smartphone-scanned photo restoration. We also propose Local Alignment to mitigate minor data misalignments. Second, we simulate diverse real-world degradation variants using low-level image transformations, thereby enhancing generalization across different smartphone scanning characteristics. We then train a degradation network to generalize various degradation styles and generate pseudo-scanned versions of unscanned images, mimicking smartphone scanning. Finally, we employ a semi-supervised learning framework to train our restoration network using both scanned and unscanned images, broadening the diversity of training content. Experimental results demonstrate that DPScan achieves superior quantitative and qualitative performance compared to baseline architectures, current academic methods, and commercial applications for smartphone photo scanning.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1651", "problem_id": "16510001", "content": "We introduce GAMesh, a novel meshing technique that leverages a mesh prior to construct surfaces from point network outputs. The method projects these points onto the prior and refines the mesh, maintaining the prior's topology while allowing the point network to dictate geometric accuracy. This approach eliminates dependence on point density or distribution, addressing limitations in conventional surface reconstruction methods. Separating geometry from topology offers benefits in single-view shape prediction, unbiased point network assessment, and handling sparse point clouds. Additionally, training point networks with GAMesh enables direct vertex optimization, producing adaptive meshes with diverse topologies.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1652", "problem_id": "16520001", "content": "This study focuses on unsupervised discovery of interpretable paths within the latent space of pretrained GANs, offering an intuitive approach to control generative factors. It tackles key limitations in existing methods, such as (a) identifying latent code-independent linear paths and (b) relying on visual assessment or manual labeling for evaluation. Specifically, we introduce non-linear latent space transformations, each defined by RBF-based warping functions, where each transformation generates a family of non-linear paths through function gradients. Extending Voynov and Babenko’s linear path discovery, we optimize RBF parameters to ensure discriminator networks can easily distinguish images generated along different paths. This results in clear transformations, such as facial pose and expression variations. Our method generalizes linear paths as a special case and demonstrates experimentally that non-linear paths yield sharper, more disentangled, and interpretable image-space changes compared to state-of-the-art techniques, both qualitatively and quantitatively. Code and pretrained models are available at: https://github.com/chi0tzp/WarpedGANSpace.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1653", "problem_id": "16530001", "content": "The Earth Mover's Distance (EMD) is a highly effective metric for comparing discrete probability distributions; however, its computational demands are substantial. While linear-complexity approximation algorithms have been developed to enhance scalability, they are constrained by either low-dimensional vector spaces or diminished effectiveness with increased overlap between probability distributions. To address these limitations, we introduce new approximation algorithms that maintain linear time complexity. Leveraging the capabilities of massively parallel computing engines, like Graphics Processing Units (GPUs), our data-parallel algorithms demonstrate significant performance improvements. Evaluated on the 20 Newsgroups text dataset, our algorithms achieve speeds four orders of magnitude greater than a multi-threaded CPU implementation of Word Mover's Distance, while preserving comparable nearest-neighbors-search accuracy. Furthermore, when applied to MNIST images, our algorithms outperform a GPU implementation of Sinkhorn's algorithm by four orders of magnitude in speed and yield a slight improvement in nearest-neighbors-search accuracy.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1654", "problem_id": "16540001", "content": "This paper introduces an access control technique designed to safeguard semantic segmentation models, leveraging the spatially invariant permutation of feature maps through a secret key. The methodology involves training and testing segmentation models by permuting specific feature maps using this key. This approach ensures that authenticated users possessing the correct key gain complete access to the model's functionality, while simultaneously diminishing performance for unauthorized users. Unlike existing access control mechanisms predominantly focused on image classification, this method is specifically tailored for semantic segmentation. Experimental results indicate that protected models enable authorized users to achieve performance levels comparable to unprotected models, while effectively resisting access attempts by unauthorized users lacking the correct key. Furthermore, the evaluation confirms the reduced performance of a conventional block-wise transformation method when applied to semantic segmentation models.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1655", "problem_id": "16550001", "content": "Iris recognition stands out as a paramount biometric identification method, owing to the distinctive characteristics of the iris texture, including freckles, coronas, stripes, furrows, and crypts, which are unique to each individual and thus distinguishable. The anatomical structure of the iris, replete with these unique features, enables the differentiation of individuals, prompting a substantial effort in recent years to enhance its performance. This article provides an overview of the common steps involved in an Iris recognition system, followed by the utilization of a specialized neural network for the recognition process, as seen in Figure A, B, C. The experimental results, as cited in References [citation], demonstrate that high accuracy can be achieved, particularly when the preliminary steps are executed effectively, highlighting the importance of meticulous preparation in the iris recognition process.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1656", "problem_id": "16560001", "content": "This study presents a novel automated vision system designed to detect animals in images captured by trail cameras in a Texas Parks and Wildlife Department-managed field, offering a non-intrusive alternative to traditional wildlife counting methods. The large volume of images generated by trail cameras renders manual analysis time-consuming and inefficient, prompting the development of a two-stage deep convolutional neural network pipeline that identifies animal-containing images and subsequently detects birds. The system demonstrates high accuracy, with the animal classification component achieving 93% sensitivity and 96% specificity, while the bird detection component attains over 93% sensitivity, 92% specificity, and a 68% average Intersection-over-Union rate, processing images in under 0.5 seconds compared to an average of 30 seconds for human labelers. To address post-deployment issues related to seasonal changes in image features, the system incorporates an automatic retraining algorithm that detects data drift and updates the model, utilizing a novel technique to identify drifted images and trigger retraining. Additionally, two statistical experiments are conducted to elucidate the prediction behavior of the animal classification system, revealing that the presence of an animal in the input image significantly influences the system's decisions, as confirmed by statistical hypothesis testing, Figure A, Figure B, and Figure C, as referenced in References [citation].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1657", "problem_id": "16570001", "content": "The goal of weakly-supervised instance segmentation is to accurately identify and segment individual object instances using only image-level labels. In contrast to existing methods that rely on multiple offline stages, this work introduces the Sequential Label Propagation and Enhancement Networks (Label-PEnet), which progressively refines image-level labels into pixel-wise labels through a coarse-to-fine approach. The proposed architecture consists of four sequential modules - multi-label classification, object detection, instance refinement, and instance segmentation - that share the same backbone and are trained alternately using a curriculum learning strategy, allowing for gradual label generalization from images to pixels with increasing precision. Furthermore, a proposal calibration module is designed to leverage the capabilities of classification networks in identifying key pixels that define object parts, serving as a post-validation strategy that operates in reverse order. The effectiveness of Label-PEnet is evaluated on the PASCAL VOC 2007 and 2012 benchmarks, with experimental results demonstrating that it surpasses state-of-the-art algorithms by a significant margin and achieves performance comparable to fully-supervised approaches.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1658", "problem_id": "16580001", "content": "Over the past decade, automatic emotion recognition has emerged as a prominent area of research, with a plethora of studies focusing on facial expressions and speech; however, the recognition of emotions from body gestures remains a relatively underinvestigated area. This survey aims to provide a comprehensive overview of the field, with the goal of stimulating further research. The concept of emotional body gestures, a key aspect of non-verbal communication, is introduced, along with discussions on gender differences and cultural dependencies. A thorough framework for automatic emotional body gesture recognition is then outlined, encompassing person detection, static and dynamic body pose estimation in both RGB and 3D formats, as well as recent advances in representation learning and emotion recognition from images of emotionally expressive gestures. Additionally, multi-modal approaches that integrate speech or facial expressions with body gestures to enhance emotion recognition are explored. Despite the maturity of pre-processing techniques, such as human detection and pose estimation, which are now robust and scalable, the field of emotion recognition from body gestures is hindered by a scarcity of labelled data, a lack of consensus on output spaces, and overly simplistic representations that rely heavily on naive geometrical features, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1659", "problem_id": "16590001", "content": "Machine learning, particularly in the realm of deep learning, is significantly transforming the approaches taken in the inverse design of optical thin-films. Most of the existing studies have concentrated on optimizing parameters such as layer thickness and structural dimensions of optical thin-films. A notable challenge that emerges is the automation of material discovery. In this research, we introduce a novel end-to-end algorithm for optical thin-film inverse design. This approach integrates the capabilities of unsupervised learning, reinforcement learning (RL), and a genetic algorithm to create optical thin-films autonomously, without any human oversight. Additionally, through various practical examples, we demonstrate how this technique can be utilized to enhance the spectra of a multi-layer solar absorber device.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1660", "problem_id": "16600001", "content": "In the field of data science, synthetic data has traditionally been employed for method refinement, feature selection, and feature engineering. Recent advancements in explainability research have renewed interest in synthetic data, particularly as modern datasets grow larger and more intricate, often necessitating less interpretable models. Unlike other domains, explainability lacks definitive ground truth for interpretations. Drawing from recent studies that establish ground truth for explaining image classifiers, we introduce an analogous approach for tabular data. By leveraging copulas—a compact representation of a dataset’s statistical properties—users can develop insights into explainability through controlled datasets and experiments. This approach is illustrated through three applications: one-dimensional logistic regression, the influence of correlation from informative features, and the effect of correlation from redundant variables.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1661", "problem_id": "16610001", "content": "Graph neural networks (GNNs) have become the dominant approach for various tasks involving graph-structured data, including node classification. Given that real-world graphs frequently evolve and may exhibit emergent classes, we frame these complexities as a lifelong learning problem, where a learner leverages knowledge gained from previous tasks. This knowledge can be stored either explicitly as historical data or implicitly within the model's parameters. In this study, we conduct a thorough analysis of the impact of both implicit and explicit knowledge. Accordingly, we propose an incremental training strategy tailored for lifelong learning on graphs and introduce a novel metric, based on k-neighborhood time differences, to manage variations in historical data. We evaluate our training method across five established GNN architectures using three newly created lifelong node classification datasets. The results indicate that retaining only 50% of the GNN's receptive field is sufficient to maintain at least 95% accuracy compared to training on the entire historical graph data. Additionally, our experiments demonstrate that the importance of implicit knowledge increases as the availability of explicit knowledge decreases.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1662", "problem_id": "16620001", "content": "The efficient management of urban traffic is crucial for any smart city project, making the quality of sensor data related to traffic extremely significant. However, similar to other sensory data, urban traffic data may experience issues, resulting in incomplete measurements. This paper concentrates on the completion of inter-region traffic data, modeling it as a spatiotemporal tensor with missing entries. To address the gaps in data, we introduce an advanced CANDECOMP/PARAFAC (CP) completion technique that takes into account both urban and temporal factors related to traffic. We identify urban characteristics by subdividing the study area into distinct regions, calculating urban feature vectors based on biodiversity principles to create an urban similarity matrix. For the temporal aspect, we perform entropy analysis to identify the most consistent time-series, followed by a joint Fourier and correlation analysis to determine periodicity and develop a temporal matrix. Both the urban and temporal matrices are incorporated into a revised CP-completion objective function, which we tackle using an alternating least squares method that works on the vectorized input data. We conduct a thorough comparative analysis with two scenarios: one simulating random missing values and another focusing on specific areas and time periods for missing data. Our findings indicate that our method achieves significant recovery performance, with a 26% enhancement over leading CP methods and a 35% improvement compared to advanced generative model-based approaches.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1663", "problem_id": "16630001", "content": "This paper presents a novel approach to bi-level image thresholding, leveraging the Kaniadakis entropy within a maximum entropy principle framework. The entropic index of Kaniadakis entropy is examined for its impact on threshold determination and its ability to induce an \"image transition\", characterized by a sudden change in the bi-level image's appearance. To demonstrate the efficacy of this method, several examples are provided, including a comparative analysis with the Tsallis entropy-based approach, as seen in Figure A, B, C (References [1], [2], and citations therein).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1664", "problem_id": "16640001", "content": "Recent advancements have demonstrated significant progress in generating facial images. Nevertheless, current methodologies primarily produce faces from stochastic noise, lacking the ability to synthesize images based on particular attributes. This paper addresses the challenge of attribute-driven face synthesis, concentrating on generating faces exhibiting characteristics dictated by specified attributes. To achieve this, we introduce a novel attribute-aware face image generator, termed AFGAN, leveraging generative adversarial networks. The approach incorporates a dual-path embedding layer and a self-attention mechanism to transform binary attribute vectors into comprehensive attribute features. Subsequently, three cascaded generators produce facial images at resolutions of 64 \\times 64, 128 \\times 128, and 256 \\times 256, utilizing the derived attribute features as input. Furthermore, an image-attribute matching loss function is introduced to strengthen the association between the generated images and the provided attributes. Comprehensive experiments conducted on the CelebA dataset validate the superior performance of our AFGAN model through qualitative and quantitative assessments.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1665", "problem_id": "16650001", "content": "This study focuses on the creation and application of an automated anomaly detection algorithm tailored for meteorological time-series data. To accomplish this, we introduce a methodology that builds an ensemble of anomaly detectors, coupled with an adaptive threshold selection technique informed by synthetically generated anomalies. The effectiveness of the developed method is validated through its integration into the \"Minimax-94\" road weather information system.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1666", "problem_id": "16660001", "content": "This study focuses on the task of pinpointing a specific segment within an untrimmed video using a textual query, a complex challenge due to the interdependencies between temporal moments in the video. Current approaches often fail to address this issue effectively as they treat moments in isolation, ignoring their temporal connections. To overcome this limitation, we introduce a two-dimensional temporal map that captures moment relationships by representing start and end times along separate axes. This structure accommodates moments of varying durations while preserving their sequential context. Building upon this framework, we present the Temporal Adjacent Network (2D-TAN), a single-shot model designed for moment localization that simultaneously encodes temporal adjacency and learns distinctive features to align video segments with corresponding text queries. Experimental results on Charades-STA, ActivityNet Captions, and TACoS datasets demonstrate that 2D-TAN achieves superior performance compared to existing methods.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1667", "problem_id": "16670001", "content": "Variations in atmospheric conditions and data acquisition frequently lead to substantial spectral differences in satellite imagery from different locations. These spectral disparities between training and test datasets degrade the performance of contemporary supervised learning techniques, resulting in suboptimal segmentation maps. To address this, we introduce a novel semantic segmentation framework designed to be resilient to these spectral shifts. At the core of our framework is the Color Mapping Generative Adversarial Network (ColorMapGAN), which synthesizes artificial training images. These images maintain the semantic integrity of the original training data while exhibiting a spectral distribution akin to that of the test images. Subsequently, we refine a pre-trained classifier using the generated images and the ground-truth data associated with the training images. Unlike conventional Generative Adversarial Networks (GANs), ColorMapGAN's generator lacks convolutional and pooling layers. Instead, it employs a single element-wise matrix multiplication and matrix addition to effectively map the training data's color space to that of the test data. This simple yet effective architecture enables our framework to significantly surpass existing methods in both accuracy and computational efficiency.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1668", "problem_id": "16680001", "content": "The capacity to manage the internal representation space within a neural network is advantageous, enabling the generation of novel data under supervision. This paper presents a method for achieving this control while simultaneously constructing a low-dimensional representation of the input data stream, developing a generalized algorithm rooted in Self-Organizing Maps (SOMs). SOMs, a type of neural network trained via unsupervised learning, create a discretized, low-dimensional mapping of the input space. These mappings facilitate the generation of new data through backward propagation of interpolations derived from the mapping grid. However, the inherent challenge lies in the unknown final topology of a SOM's mapping space prior to training, complicating supervised data interpolation. Our work introduces a SOM-based variation that constrains prototype updates based on their distance from externally specified targets within the mapping space. We demonstrate that these variants, termed Supervised Topological Maps (STMs), enable supervised mapping where the user determines the location of internal representations within the mapping space. Controlling the internal representation space in STMs proves to be more straightforward compared to existing methods employing variational or adversarial autoencoders.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1669", "problem_id": "16690001", "content": "In the realm of object detection, the definition of positive and negative samples relies on a predetermined intersection over union (IoU) threshold. Training an object detector with a relatively low IoU threshold, such as 0.5, often results in noisy detections, whereas increasing the IoU threshold tends to compromise detection performance due to two primary factors: overfitting during training caused by an exponential decrease in positive samples, and a mismatch between the optimal IoU of the detector and that of the input hypotheses at inference time. To mitigate these issues, a novel multi-stage object detection framework, known as the Cascade R-CNN, is proposed, comprising a series of detectors trained with progressively increasing IoU thresholds to enhance selectivity against close false positives. By training detectors in a sequential manner, leveraging the observation that the output of one detector provides a suitable distribution for training the next, higher-quality detector, the Cascade R-CNN ensures that each detector has a positive sample set of comparable size, thereby reducing overfitting. This cascade procedure is also applied during inference, allowing for a closer alignment between the hypotheses and the quality of each stage's detector. As demonstrated by experiments, a straightforward implementation of the Cascade R-CNN outperforms all single-model object detectors on the COCO dataset, and its applicability is shown to be consistent across various detector architectures, yielding significant gains regardless of the baseline detector's strength, with the code available at https://github.com/zhaoweicai/cascade-rcnn.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1670", "problem_id": "16700001", "content": "This study tackles the challenge of optimizing communication between servers and clients in federated learning (FL) by introducing a novel client selection method. Existing sampling methods in FL often suffer from bias or suboptimal server-client communication and training stability. To mitigate this, we propose clustered sampling, which theoretically yields improved client representativeness and reduced variance in stochastic aggregation weights. Our approach is supported by two clustering methods that enable client aggregation based on 1) sample size and 2) model similarity. Experimental results in non-iid and unbalanced scenarios demonstrate that clustered sampling achieves better training convergence and reduced variability compared to traditional sampling methods. Notably, our method does not require additional client-side operations and can be easily integrated into standard FL frameworks. Furthermore, clustered sampling is compatible with existing privacy enhancement techniques and communication reduction methods via model compression, as shown in Figure A, B, C (References [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1671", "problem_id": "16710001", "content": "We present an unsupervised, iterative algorithm for point-set registration of an unlabeled N-dimensional Euclidean point cloud, where the correspondence between points is not predetermined. This method utilizes linear least squares and examines every potential point pairing, progressively aligning the two sets until the number of point pairs remains within the defined limit of permissible one-to-one pairings.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1672", "problem_id": "16720001", "content": "In the context of continual learning, where tasks are presented in a sequential manner, the primary objectives are to facilitate learning while mitigating catastrophic forgetting, optimizing model capacity utilization, and leveraging both forward and backward transfer learning. This study delves into the efficacy of the Variational Continual Learning (VCL) framework in achieving these goals, using split MNIST and permuted MNIST as benchmark continual learning tasks. By introducing a novel best practice approach for mean-field variational Bayesian neural networks, we demonstrate substantial enhancements to the VCL framework, which already represented a competitive method. Furthermore, through an in-depth examination of the solutions obtained, we gain insight into the underlying factors contributing to VCL's performance and compare it to an idealized continual learning solution, as shown in Figure A, B, C (References [1], [2], and [3] provide additional context).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1673", "problem_id": "16730001", "content": "The acceleration of deep convolutional neural networks has garnered significant attention in both academic and industrial circles, driving innovation in this area. This paper introduces LANCE, an efficient algorithm that leverages the strengths of both fast convolution and quantization techniques through the implementation of low-precision quantized Winograd convolution. By integrating linear quantization into the Winograd domain, LANCE enables the efficient execution of fast convolution under low-precision computation on graphics processing units. Experimental evaluations of neural network models utilizing LANCE are conducted on benchmark image classification datasets, including SVHN, CIFAR, and ImageNet, as shown in Figure A, B, C (References [1], [2], [3]). The results demonstrate that the 8-bit quantized Winograd convolution achieved by LANCE yields a performance improvement of up to 2.40x compared to full-precision convolution, with negligible loss of accuracy, as discussed in citations [4] and [5].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1674", "problem_id": "16740001", "content": "This study demonstrates a method for performing global image edits using text-based instructions, where a source image is modified according to a given textual prompt. Three distinct trainable models—bucket, filter bank, and end-to-end—are developed, leveraging RNN and GAN architectures, with varying degrees of encoded expert knowledge (the end-to-end model being the most general). Training data consists of approximately 2000 image pairs with corresponding textual descriptions collected via Amazon Mechanical Turk. Experiments conducted on this dataset confirm the effectiveness of the proposed approaches. Further analysis reveals that the filter bank model strikes a balance between generality and performance; replacing RNN with Graph RNN in this model yields improved results. To our knowledge, this represents the first computational photography framework for global image editing driven solely by free-form textual instructions.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1675", "problem_id": "16750001", "content": "The first mobile camera phone was introduced merely two decades ago, at a time when capturing images with a phone was considered unusual and online photo sharing was not a common practice. Currently, smartphones prioritize camera functionality over calling features. What spurred this change? The shift can be attributed to advancements in computational photography, which focuses on producing high-quality images using compact, mobile cameras. Recent developments in algorithms and computing, particularly through machine learning, have redefined the norms of photography, introducing innovative methods for capturing, processing, storing, and sharing images. In this paper, we provide a concise overview of the evolution of mobile computational photography and outline several critical technological elements such as burst photography, noise reduction, and super-resolution. Throughout the discussion, we may draw simple comparisons to the human visual system.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1676", "problem_id": "16760001", "content": "In this study, we introduce an innovative framework for medical image segmentation based on iterative deep learning. We integrate an iterative learning strategy with an encoder-decoder architecture to enhance segmentation accuracy, facilitating the precise identification of regions of interest (ROIs), including intricate shapes and detailed textures found in medical images through an iterative process. The suggested iterative deep convolutional encoder-decoder network features two primary pathways: a convolutional encoder pathway and a convolutional decoder pathway, both incorporating iterative learning. Experimental findings demonstrate that our iterative deep learning framework achieves outstanding segmentation results for a variety of medical images. The effectiveness of the proposed approach has been validated through comparisons with other leading-edge medical image segmentation techniques.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1677", "problem_id": "16770001", "content": "This study tackles Bayesian reinforcement learning through efficient model-based online planning. We introduce an optimism-free Bayes-adaptive method designed to promote deeper yet sparser exploration, accompanied by a theoretical performance guarantee relative to the Bayes optimal policy while maintaining reduced computational overhead. A key innovation involves employing a candidate policy generator to produce long-term options within belief-based planning trees, enabling the construction of substantially sparser and deeper structures. Empirical evaluations across various environments demonstrate that our approach outperforms existing state-of-the-art methods in computational efficiency and achieves notably higher rewards in discrete settings.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1678", "problem_id": "16780001", "content": "We introduce an innovative second-order optimization framework for training emerging deep continuous-time models, particularly the Neural Ordinary Differential Equations (Neural ODEs). The inherent complexity of training these models involves costly gradient calculations through solving a backward ODE, making the derivation of efficient second-order methods quite challenging. However, drawing inspiration from the recent Optimal Control (OC) view on training deep networks, we demonstrate that a specific continuous-time OC approach known as Differential Programming can be utilized to derive backward ODEs for higher-order derivatives while maintaining an O(1) memory requirement. Additionally, we investigate a low-rank representation of the second-order derivatives, which facilitates efficient preconditioned updates through Kronecker-based factorization. The resulting methodology achieves significantly faster convergence compared to first-order baselines in terms of wall-clock time, with this advancement sustaining across a range of applications, including image classification, generative flow, and time-series forecasting. Our framework also supports direct optimization of architectures, such as the integration time of Neural ODEs, using second-order feedback policies, thus reinforcing the OC perspective as a structured method for understanding optimization in deep learning.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1679", "problem_id": "16790001", "content": "Motivated by the characteristics of human vision, this study presents a novel driving model that leverages a multi-resolution approach, incorporating both periphery and fovea components, to predict vehicle speed from dash camera videos. The model's peripheral vision component processes full video frames at a lower resolution, while its foveal vision component selectively focuses on specific sub-regions, utilizing high-resolution inputs to enhance driving performance. The fovea selection module is trained using supervised driver gaze data, and the results indicate that incorporating high-resolution inputs from predicted driver gaze locations substantially enhances the model's driving accuracy. Compared to a uni-resolution model with equivalent computational complexity, the proposed periphery-fovea multi-resolution model exhibits superior performance, with particularly notable improvements in critical situations involving pedestrians, as opposed to non-critical scenarios, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1680", "problem_id": "16800001", "content": "The performance of self-supervised monocular depth estimation in driving scenarios has approached that of supervised methods, yet it remains susceptible to errors in predicting the depth of traffic participants when the static world assumption is violated, which can compromise safety. To address this limitation, we introduce R4Dyn, a novel approach that leverages cost-effective radar data to enhance a self-supervised depth estimation framework. Specifically, our method utilizes radar as a weak supervision signal during training and as an additional input to bolster estimation robustness at inference time, enabling the collection of training data from various existing vehicles equipped with automotive radars. By mitigating inherent radar issues such as noise and sparsity through signal filtering and expansion, we render radar data compatible with learning-based approaches. Our R4Dyn approach effectively overcomes the major limitation of self-supervised depth estimation, notably improving predictions for dynamic objects like cars by 37% on the nuScenes dataset, thereby demonstrating the value of radar as a supplementary sensor for monocular depth estimation in autonomous vehicles, with plans to release the code publicly.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1681", "problem_id": "16810001", "content": "Monocular 3D object detection is crucial for autonomous driving, but its effectiveness is compromised by changes in the ego-car's pose relative to the ground plane, a common occurrence caused by minor variations in road conditions. Current methodologies, often overlooking camera pose information, are vulnerable to camera extrinsic parameters, a significant limitation given the prevalence of object perturbation in real-world autonomous driving scenarios. To address this, we introduce a new approach that integrates camera pose estimation, rendering the detector robust to extrinsic perturbations. Our framework estimates camera extrinsic parameters via vanishing point and horizon change detection, employing a converter to correct perturbation-related feature distortions in the latent space. This enables our 3D detector to function independently of extrinsic parameter variations, ensuring accurate results in challenging real-world conditions, such as uneven roads, where conventional monocular detectors struggle. Experimental results on the KITTI 3D and nuScenes datasets demonstrate our method's superior performance, surpassing state-of-the-art alternatives by a substantial margin.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1682", "problem_id": "16820001", "content": "The issue of \"nonstationarity\" represents a critical challenge in cooperative multi-agent reinforcement learning (MARL). This occurs as each agent's policy evolves during the learning process, while simultaneously being perceived as part of the environment by other agents. Such dynamics lead to an inherent fluctuation of information among agents throughout learning, significantly hindering convergence. Utilizing the MAILP model of information transfer in multi-agent learning, we demonstrate that increasing the level of centralization during the learning process can effectively alleviate the delays in convergence caused by nonstationarity. The most centralized approach involves parameter sharing, which is relatively underutilized in MARL applications with homogeneous agents. This method enhances single-agent reinforcement learning (RL) techniques by enabling all agents to adopt the same policy. Our experiments corroborate the theoretical finding that heightened learning centralization improves performance. Additionally, we introduce parameter sharing to eight contemporary single-agent deep RL strategies for the first time, resulting in up to 44 times more average reward in just 16% of the episodes compared to earlier parameter sharing studies. Finally, we provide a formal proof for a set of techniques that enable parameter sharing in settings with heterogeneous agents.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1683", "problem_id": "16830001", "content": "This paper introduces the Transfer-Editing and Recognition Generative Adversarial Network (TER-GAN), a comprehensive framework that enables three primary functions: Facial Expression Transfer (FET), where facial expressions are transferred between different identities; Facial Expression Editing (FEE), which involves transforming a given image's expression to a target expression while preserving its identity; and Facial Expression Recognition (FER), which recognizes the facial expression in a face image. The TER-GAN architecture leverages generative models to produce synthetic images while extracting crucial information from input images during reconstruction. Specifically, it utilizes two encoders to extract identity and expression information from two input images, and a decoder generates a synthetic expression image. To enhance feature disentanglement and extraction, the authors propose a novel expression consistency loss and an identity consistency loss, which utilize additional expression and identity information from generated images. The experimental results demonstrate the effectiveness of the proposed method in facial expression transfer, editing, and recognition, with evaluations conducted using the Oulu-CASIA dataset to compare with state-of-the-art methods, as shown in Figure A, B, C (References [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1684", "problem_id": "16840001", "content": "Feature matching for identifying image correspondences is fundamental to many computer vision applications. Existing research includes numerous detectors and descriptors designed for efficient feature generation from image keypoints. This study examines eight binary descriptors (AKAZE, BoostDesc, BRIEF, BRISK, FREAK, LATCH, LUCID, and ORB) and eight interest point detectors (AGAST, AKAZE, BRISK, FAST, HarrisLapalce, KAZE, ORB, and StarDetector). To analyze interest point detectors and evaluate the performance of different detector-descriptor pairings, the detection and description phases were separated. Experiments were performed on a standard dataset to comparatively analyze each method's performance under various image transformations, revealing that: (1) FAST, AGAST, and ORB detectors exhibited faster performance and identified a greater number of keypoints; (2) AKAZE and KAZE detectors demonstrated superior performance under photometric variations, while ORB showed greater robustness to geometric changes; (3) descriptors generally performed better when combined with KAZE and AKAZE detectors; (4) BRIEF, LUCID, and ORB descriptors were comparatively faster; and (5) no descriptor performed exceptionally well under geometric transformations, although BRISK, FREAK, and AKAZE exhibited moderate resilience.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1685", "problem_id": "16850001", "content": "Image harmonization seeks to adjust the color of the composited area relative to the designated background. Earlier approaches framed this problem as pixel-wise image-to-image translation employing UNet architectures. Nonetheless, the size of the models and their computational demands hinder their effectiveness on edge devices and with high-resolution images. To address this limitation, we introduce a novel spatial-separated curve rendering network (S^2CRNet) designed for efficient and high-resolution image harmonization for the first time. In S^2CRNet, we begin by extracting spatial-separated embeddings from the thumbnails of the masked foreground and background independently. Next, we develop a curve rendering module (CRM) that learns and integrates spatial-specific knowledge through linear layers to produce the parameters for pixel-wise curve mapping in the foreground area. Ultimately, we directly render the original high-resolution images utilizing the learned color curve. Additionally, we have created two extensions of the proposed framework termed Cascaded-CRM and Semantic-CRM for refined cascading and semantic support, respectively. Experimental results indicate that our method reduces the number of parameters by over 90% compared to previous techniques while still achieving state-of-the-art performance on both the synthesized iHarmony4 and the real-world DIH test set. Furthermore, our approach operates effectively on higher resolution images in real-time, exceeding the speed of existing methods by more than 10 times. The code and pre-trained models will be publicly available at https://github.com/stefanLeong/S2CRNet.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1686", "problem_id": "16860001", "content": "Recent advancements in neural network compression techniques, particularly channel pruning, have gained prominence for reducing the size and computational demands of deep neural networks (DNNs), making them suitable for resource-limited environments like embedded systems. To streamline the pruning process and eliminate manual effort, reinforcement learning (RL)-based auto-pruning methods have been introduced. However, these approaches suffer from prolonged training durations and high sampling costs, hindering their practical deployment. To address these challenges, this paper presents an efficient auto-pruning framework that leverages historical pruning data to enhance performance. The framework accelerates RL-pruner convergence through transfer learning, employs an augmented transfer learning strategy to further optimize training speed, and incorporates an assistant learning mechanism to improve sample efficiency. Experimental results demonstrate that the proposed framework achieves a 1.5-2.5x speedup for ResNet20 and 1.81-2.375x for other architectures, including ResNet56, ResNet18, and MobileNet v1.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1687", "problem_id": "16870001", "content": "Change-point detection aims to identify shifts in the distribution of time series data. Current leading methods for change-point detection often rely on directly estimating density ratios. This study demonstrates how existing algorithms can be broadened through the application of diverse binary classification and regression models. Specifically, it is shown that Gradient Boosting with Decision Trees and Neural Networks are suitable for this task. The algorithms' performance is evaluated using both simulated and real-world datasets. The findings indicate that the presented methods surpass the conventional RuLSIF algorithm. Furthermore, the discussion highlights scenarios where the proposed algorithms offer benefits compared to existing approaches.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1688", "problem_id": "16880001", "content": "Optical marker-based motion capture is essential for applications like motion analysis, animation, and biomechanics. A significant bottleneck in current motion capture workflows is the manual labelling process, which involves assigning optical markers to specific body locations, a time-consuming and laborious post-processing task. This problem can be framed as a ranking challenge, where markers are initially scrambled by an unknown permutation matrix, and the goal is to restore the correct order. In this paper, we introduce an automatic marker labelling framework that estimates a permutation matrix for each frame using a differentiable permutation learning model. It then leverages temporal consistency to detect and rectify any remaining labelling inaccuracies. Experimental results demonstrate the efficacy of our proposed approach.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1689", "problem_id": "16890001", "content": "Convolutional neural networks (CNNs) often exhibit channel redundancy in feature maps, leading to excessive memory usage and computational demands. To address this, we propose a new Slim Convolution (SlimConv) module that enhances CNN performance by minimizing redundant channels. The SlimConv process involves three key stages: Reconstruct, Transform, and Fuse, which efficiently split and reorganize features to enable effective weight compression. A central component is the weight flipping operation, significantly enhancing feature diversity and boosting performance. Designed as a plug-and-play unit, SlimConv can seamlessly replace standard convolutional layers in CNNs. Extensive testing on ImageNet, MS COCO2014, Pascal VOC2012 segmentation, and Pascal VOC2007 detection datasets confirms SlimConv's efficacy, demonstrating consistent performance improvements with reduced memory and computational costs. For instance, SlimConv-enhanced ResNet-101 attains 77.84% top-1 accuracy on ImageNet with 4.87 GFLOPs and 27.96M parameters, outperforming the baseline by nearly 0.5% while cutting 3 GFLOPs and 38% of parameters.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1690", "problem_id": "16900001", "content": "A comprehensive understanding of scene geometry is crucial for autonomous driving, particularly in contexts like parking and urban navigation. This understanding is typically achieved with surround-view fisheye cameras that concentrate on the immediate vicinity of the vehicle. Most current depth estimation techniques rely on a single camera, limiting their adaptability when applied to multiple camera systems. It is essential for the depth estimation model to be validated across various cameras installed on millions of vehicles, each with different camera geometries. Even within a single vehicle, intrinsic parameters can differ because of manufacturing variances. Deep learning models often struggle with these variations, making it nearly impossible to train and validate each camera type individually. To address this, we introduce innovative camera-geometry adaptive multi-scale convolutions that integrate camera parameters as a conditional input, allowing the model to adapt to new fisheye cameras. Furthermore, we enhance distance estimation through pairwise and patchwise vector-based self-attention encoder networks. Our method is assessed using the Fisheye WoodScape surround-view dataset, demonstrating significant advancements compared to earlier methods. We also illustrate the adaptability of our approach across various camera angles and conduct thorough experiments to bolster our findings. For comparative analysis with other methods, we evaluate front camera data on the KITTI dataset (using pinhole camera images) and achieve leading performance among self-supervised monocular approaches. A video overview showcasing qualitative results is available at https://youtu.be/bmX0UcU9wtA. Baseline code and dataset will be released publicly.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1691", "problem_id": "16910001", "content": "The task of sentiment analysis can be hindered by limited data availability, as datasets are typically created and annotated manually. When training data is scarce, generative models can enhance the training process through data augmentation. Notably, Generative Adversarial Networks (GANs) have achieved state-of-the-art results in various tasks, including image and text generation. This paper investigates the use of GAN models for data augmentation in sentiment analysis, focusing on low-resource datasets. To address the challenges of limited data, several techniques are explored for training GAN models, and the quality of generated data is evaluated as both a test set and a training set for classification models. Additionally, the analysis includes a visual examination of the generated and real data using the t-Distributed Stochastic Neighbor Embedding (t-SNE) method to project the data into a two-dimensional space, as well as an assessment of how the quality of generated data improves with increased training data for the GAN.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1692", "problem_id": "16920001", "content": "The precise segmentation of slender structures is crucial in various imaging applications, including the analysis of vessels, retina, plant roots, and road networks captured by satellites, as it enables subsequent examination of biomarkers, organ structure, and topology. However, gaps in the extracted foreground can impede downstream image-based analysis. To address this issue, this paper presents a general post-processing technique for recovering gaps in large-scale segmentation masks, framing the problem as a blind inpainting task where the algorithm is unaware of the regions with missing lines. This task is solved using an adversarially trained neural network. A key challenge in processing large images is the limited memory capacity of current GPUs, which often necessitates dividing the image into smaller patches, potentially compromising the global coherence and structural integrity of the reconstructed image. To overcome this, the proposed method leverages adversarial training and reinforcement learning (Policy Gradient) to equip the model with both global context awareness and attention to local details. The approach is evaluated across multiple datasets in medical imaging, plant science, and remote sensing, with experiments showing that it produces the most realistic and complete inpainted results, surpassing other methods. Furthermore, a dedicated study on plant roots reveals that the proposed approach achieves performance comparable to that of human annotators, as demonstrated in Figure A, B, C (see References [citation] for more details), with the implementation available at \\url.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1693", "problem_id": "16930001", "content": "The field of reinforcement learning has undergone substantial advancements, with model-free approaches demonstrating greater versatility and applicability compared to model-based methods. This study introduces a novel reinforcement learning framework, termed \"neural network iterative linear quadratic regulator (NNiLQR)\", which leverages iterative linear quadratic regulator (iLQR) without requiring prior knowledge of the system model. By relying solely on measurement data, this non-parametric approach enables the establishment of an optimal policy through iterative neural network refinements, eliminating the need for system modeling. The NNiLQR method significantly outperforms the traditional iLQR approach in terms of the objective function, owing to its innovative exploration strategy. The effectiveness of the NNiLQR method is clearly illustrated through two exemplary cases, highlighting its notable advantages, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1694", "problem_id": "16940001", "content": "Recent advancements in geometric representation learning have demonstrated significant potential across various machine learning domains, including relational learning, language processing, and generative modeling. This study focuses on manifold-valued regression in hyperbolic space as a key step for multiple machine learning applications. Specifically, we reframe tree node prediction as a manifold regression task in hyperbolic space, offering new insights into two complex problems: 1) hierarchical classification through label embeddings and 2) taxonomy expansion using hyperbolic representations. To solve the regression challenge, we evaluate existing techniques and introduce two computationally efficient alternatives: a parametric deep learning model guided by target space geodesics and a non-parametric kernel method with proven excess risk bounds. Experimental results confirm the effectiveness of hyperbolic geometry, particularly for taxonomy expansion, where hyperbolic-based models substantially outperform Euclidean space regression approaches.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1695", "problem_id": "16950001", "content": "Purpose: This study aims to assess whether deep learning networks can be trained to predict the future 24-2 Humphrey Visual Field (HVF). Participants: The study involved all patients who received a HVF 24-2 at the University of Washington. Methods: Data points from consecutive 24-2 HVFs collected between 1998 and 2018 were retrieved from a University of Washington database. A ten-fold cross validation approach, utilizing a set aside test group, was employed to carry out three primary stages of model development: selection of model architecture, combination of datasets, and training of time-interval models through transfer learning, aimed at creating a deep learning artificial neural network proficient in producing point-wise visual field predictions. Results: Over 1.7 million perimetry points, measured to the hundredth decibel, were gathered from 32,443 24-2 HVFs. The top-performing model, CascadeNet-5, with 20 million trainable parameters, was chosen. The overall mean absolute error (MAE) for the test group stood at 2.47 dB (95% CI: 2.45 dB to 2.48 dB). The one hundred fully trained models demonstrated the capability to accurately foresee progressive field loss in glaucomatous eyes for up to 5.5 years in advance, achieving a correlation of 0.92 between the mean deviation (MD) of predicted and actual future HVF (p < 2.2 x 10 -16) and an average discrepancy of 0.41 dB. Conclusions: The results indicate that deep learning networks, when applied to unfiltered real-world data, exhibit a remarkable ability to comprehend spatio-temporal changes in HVFs and to make predictions regarding future HVFs for as long as 5.5 years based solely on a single HVF input.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1696", "problem_id": "16960001", "content": "Seglearn is a publicly available Python library designed for machine learning applications involving time series or sequential data, utilizing a sliding window segmentation methodology. It offers a versatile framework for addressing classification, regression, and forecasting tasks that incorporate multivariate sequence data and contextual information. Compatible with the scikit-learn ecosystem, Seglearn is recognized as one of the scikit-learn Related Projects, with dependencies on numpy, scipy, and scikit-learn. Licensed under the BSD 3-Clause License, the package is accompanied by comprehensive documentation, including an API description, user guide, and illustrative examples, as well as a robust set of unit tests that ensure a high level of code coverage.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1697", "problem_id": "16970001", "content": "In recent years, tracking-by-detection methods have shown promising results, but their performance is heavily dependent on the quality of the training data. However, the limited availability of labeled training data often necessitates the extraction and labeling of additional samples by the tracker itself, which can lead to the inclusion of noisy or corrupted data due to various perturbations such as occlusions and misalignments. While existing tracking-by-detection approaches either neglect this issue or utilize a separate component to manage the training set, we introduce a novel approach that dynamically assesses the quality of the samples and integrates this estimation into a unified formulation. By minimizing a single loss function over both the target appearance model and the sample quality weights, our method jointly optimizes the tracking model and down-weights corrupted samples while emphasizing correct ones. We evaluate our approach on three benchmarks, including OTB-2015 with 100 videos, VOT-2015 with 60 videos, and Temple-Color with 128 videos, and achieve significant improvements, such as a 3.8% gain in mean overlap precision on OTB-2015, ultimately obtaining state-of-the-art results on all three datasets, with code and supplementary material available at http://www.cvl.isy.liu.se/research/objrec/visualtracking/decontrack/index.html.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1698", "problem_id": "16980001", "content": "Predicting the future states of a dynamical system, its response to external disturbances, and its underlying causal relationships is crucial, and incorporating nonlinearity is essential for achieving this goal. Nevertheless, conventional methods for detecting causality and impulse response, such as Vector Autoregression (VAR), are limited by their assumption of linearity, which hinders their ability to capture complex dynamics. To address this limitation, we propose a novel vector autoencoder nonlinear autoregression neural network (VANAR) that can automatically extract relevant features from time series data and estimate the underlying functional form. The performance of VANAR is evaluated in three aspects: forecasting accuracy, causality detection, and impulse response modeling, using both a simulated nonlinear chaotic system and empirical data from the Philippine macroeconomy. The results indicate that VANAR outperforms VAR in terms of forecasting and causality detection, and its accuracy surpasses that of state-of-the-art models, including SARIMA and TBATS. Although both models struggle to predict the trajectories of the nonlinear chaotic system under external shocks, VANAR demonstrates robustness in modeling diverse dynamics, including chaotic, noisy, and data-scarce environments, as well as macroeconomic systems, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1699", "problem_id": "16990001", "content": "In the contexts of domain generalization (DG) and unsupervised domain adaptation (UDA), the alignment of features across different domains has been extensively investigated to reconcile their distributions and facilitate the acquisition of domain-invariant representations. Nevertheless, this feature alignment often lacks consideration of the specific tasks involved, which can diminish the discriminative capability of the representations and, consequently, impede overall performance. In this study, we introduce a comprehensive framework called Feature Alignment and Restoration (FAR) designed to concurrently enhance both the generalization and discrimination abilities of networks for effective DG and UDA. Our approach entails performing feature alignment (FA) between domains by synchronizing the moments of the distributions of strategically chosen features to minimize their differences. To maintain high discriminatory power, we introduce a Feature Restoration (FR) process that extracts task-relevant features from residual data, employing these features to augment the aligned ones. Additionally, to promote better disentanglement, we implement a dual ranking entropy loss constraint during the FR phase to foster the distinction between task-relevant and task-irrelevant features. Comprehensive evaluations across various classification benchmarks validate the superior performance and robust generalization capabilities of our FAR framework for both domain generalization and unsupervised domain adaptation.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1700", "problem_id": "17000001", "content": "Estimating lighting from a single image is a crucial but difficult task in the fields of computer vision and computer graphics. Current approaches to lighting estimation, which involve either regressing representative illumination parameters or directly generating illumination maps, often result in limited accuracy and generalizability. To address this, we introduce Geometric Mover's Light (GMLight), a novel framework for lighting estimation that combines a regression network with a generative projector to achieve effective illumination estimation. By parameterizing illumination scenes in terms of geometric light distribution, light intensity, ambient term, and auxiliary depth, we formulate the estimation process as a regression task. Drawing inspiration from the earth mover's distance, a custom geometric mover's loss function is designed to facilitate precise regression of light distribution parameters. The estimated lighting parameters are then utilized by the generative projector to synthesize panoramic illumination maps that exhibit realistic appearance and frequency characteristics. As demonstrated through extensive experiments, GMLight yields accurate lighting estimates and superior relighting fidelity for 3D object insertion, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1701", "problem_id": "17010001", "content": "This study introduces 3D-RecGAN++, an innovative method for reconstructing a complete 3D model of an object from a single arbitrary depth view using generative adversarial networks. While prior approaches often rely on multiple object views or class labels to infer full 3D geometry, 3D-RecGAN++ operates solely on a voxel grid representation of a single depth view, generating a high-resolution 256^3 3D occupancy grid by accurately predicting occluded or missing regions. The approach leverages the strengths of autoencoders and conditional Generative Adversarial Networks (GANs) to infer detailed 3D structures in high-dimensional voxel space. Comprehensive evaluations on synthetic and real-world Kinect datasets demonstrate that 3D-RecGAN++ surpasses existing techniques in single-view 3D reconstruction and generalizes effectively to previously unseen object categories.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1702", "problem_id": "17020001", "content": "Sparse phase retrieval is a significant problem across various scientific disciplines, garnering considerable research interest. This paper introduces a chastic altenating inimizing method for arse phse etrieval () algorithm designed to reconstruct n-dimensional s-sparse signals using only O(s\\,\\mathrm\\, n) measurements. Unlike many current techniques,  does not necessitate a precise initial estimate. The algorithm incorporates the hard-thresholding pursuit (HTP) to address sparse constraint least square sub-problems. A key advantage of  is its global convergence property, achieving optimal sample complexity with random initialization. Comprehensive numerical experiments are presented to confirm the effectiveness of the proposed algorithm.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1703", "problem_id": "17030001", "content": "The creation of autonomous vehicles relies heavily on the availability of extensive data related to diverse driving scenarios, but manually annotating these scenarios is not only expensive, but also prone to errors due to the limitations of rule-based systems for labeling trajectories. To overcome this challenge, a novel non-parametric framework for clustering trajectories is introduced, comprising a five-stage process: (1) synchronizing trajectories and measuring their temporal differences, (2) mapping these differences into a vector space, (3) identifying transitive relationships, (4) representing these relationships in a new vector space, and (5) grouping trajectories into an optimal number of clusters. This framework is tested on a complex, real-world dataset of labeled trajectories, yielding promising outcomes despite the added complexity of trajectories of varying lengths, as shown in Figure A, B, C (References [1], [2], [3]). Additionally, the framework is expanded to assess the effectiveness of augmenting the real dataset with synthetic data produced by a Generative Adversarial Network (GAN), examining the consistency of the generated trajectories with the underlying true clusters, and comparing the results to those presented in Figure A, B, C, and cited in References [1], [2], [3], and [4].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1704", "problem_id": "17040001", "content": "Unsupervised image translation, which involves transforming images between two unpaired datasets, faces difficulties in identifying accurate mappings without supervised data. Current approaches leverage Generative Adversarial Networks (GANs) to ensure the translated images match the target distribution. However, these set-level constraints fail to capture instance-specific relationships (e.g., aligned semantic components in object configuration tasks), often producing errors such as geometric or semantic distortions and contributing to mode collapse. To overcome these challenges, we introduce a new framework called Deep Attention GAN (DA-GAN) for instance-level translation, which decomposes the task into transforming instances within a structured latent space. Our method jointly trains a deep attention encoder to identify instance-level correspondences by focusing on learned instance pairs, enabling constraints at both set and instance levels. Experimental results show our approach outperforms existing state-of-the-art methods, while its versatility in applications like pose morphing and data augmentation advances the boundaries of domain translation.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1705", "problem_id": "17050001", "content": "In self-supervised learning, a model is trained to perform a pretext task using a dataset without requiring human labels. The primary goal is to adapt this model for a specific target task and domain. Currently, fine-tuning is the most efficient strategy for transferring knowledge, which necessitates utilizing the same model or its components for both the pretext and target tasks. This paper introduces a new framework for self-supervised learning that addresses challenges related to the design and comparison of various tasks, models, and data domains. Specifically, our framework separates the architecture of the self-supervised model from the final model that is fine-tuned for the specific task. This separation enables us to: 1) quantitatively evaluate models that were previously incompatible, including those with hand-crafted features; 2) demonstrate that deeper neural networks can obtain improved representations from the same pretext task; and 3) transfer knowledge from a more complex model to a simpler one, thereby enhancing its learning capability. We apply this framework to create a new self-supervised task, which achieves state-of-the-art results on widely used benchmarks such as PASCAL VOC 2007, ILSVRC12, and Places, significantly outperforming previous methods. Our learned features reduce the mean average precision gap between models trained with self-supervised and supervised learning from 5.9% to 2.6% in object detection on PASCAL VOC 2007.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1706", "problem_id": "17060001", "content": "Traditional game testing depends on human testers, scripted play tests, and existing knowledge of focal areas to generate pertinent testing data. In this work, we propose the incorporation of a self-learning mechanism into the game testing framework through deep reinforcement learning (DRL). This framework leverages DRL to either explore or exploit game mechanics guided by a user-defined reward signal. Consequently, it enhances test coverage while identifying unintended gameplay mechanics, exploits, and bugs across various game genres. In this paper, we demonstrate that DRL effectively improves test coverage, uncovers exploits, evaluates map difficulty, and identifies common issues during the testing of first-person shooter (FPS) games.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1707", "problem_id": "17070001", "content": "Common techniques for training generative models, including Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), typically impose that the latent representations conform to simple distributions like isotropic Gaussian. In this paper, we contend that allowing the latent space of an auto-encoder to represent more complex distributions facilitates a more precise modeling of intricate data distributions. Building on this insight, we introduce a two-stage optimization process aimed at maximizing an approximate implicit density model. Our experimental results demonstrate that this method surpasses the performance of GANs and VAEs on two image datasets (MNIST, CELEB-A). We further illustrate that our technique can be effectively applied to generating sequential data, exemplified by its capability to produce speech and music.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1708", "problem_id": "17080001", "content": "This study addresses the challenge of determining the content level in a cup or drinking glass from a single image, a task complicated by factors such as transparency, varying shapes, and partial occlusions, as well as limited training data. To overcome these hurdles, we employ a transfer learning approach, utilizing adversarial training on a broad source dataset, followed by fine-tuning on a task-specific dataset. Our investigation includes an examination of various training strategies and their combinations, with experiments conducted on diverse container types from the CORSMAL Containers Manipulation dataset. The results demonstrate that incorporating adversarial training in the source domain through transfer learning yields consistently improved classification accuracy on the test set, while also mitigating the classifier's tendency to overfit to specific features of the training data.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1709", "problem_id": "17090001", "content": "This study presents a deep learning-based method for detecting roads by combining LIDAR point clouds and camera images. The approach involves projecting an unstructured LIDAR point cloud onto the camera image plane, followed by upsampling to generate dense 2D images that capture spatial information. Multiple fully convolutional neural networks (FCNs) are trained for road detection, utilizing either single-sensor data or one of three fusion strategies: early, late, and the novel cross fusion method. The cross fusion FCN is uniquely designed to learn the optimal integration of multimodal information directly from the data, leveraging trainable cross connections between the LIDAR and camera processing branches. The effectiveness of a multimodal system for road detection is demonstrated using a challenging dataset extracted from the KITTI raw data set, where a camera-based FCN struggles to perform, whereas a multimodal system achieves high accuracy. The proposed cross fusion FCN is evaluated on the KITTI road benchmark, yielding a MaxF score of 96.03%, which positions it among the top-performing approaches, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1710", "problem_id": "17100001", "content": "Recently, there has been considerable advancement in comprehending reinforcement learning within discounted infinite-horizon Markov decision processes (MDPs) through the establishment of strict sample complexity limits. Nevertheless, in various real-world contexts, an interactive learning agent functions over a specified or restricted time frame, such as in tutoring students for examinations or managing customer service inquiries. These situations are often more effectively modeled as episodic fixed-horizon MDPs, where only less precise sample complexity bounds are available. A relevant concept of sample complexity in this context is the number of episodes needed to ensure a certain level of performance with high probability (PAC guarantee). In this study, we present an upper PAC bound \\tilde O(\\frac \\ln\\frac 1 \\delta) and a lower PAC bound \\tilde \\Omega(\\frac \\ln \\frac 1 ), which aligns up to logarithmic factors and includes an additional linear relationship with the number of states | S|. The lower bound is the first to be established for this scenario. Our upper bound utilizes Bernstein's inequality to enhance previous limits for episodic finite-horizon MDPs that had a time-horizon dependency of at least H^3.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1711", "problem_id": "17110001", "content": "With the daily and accelerated production of data, data stream mining is becoming increasingly crucial, highlighting the necessity for algorithms capable of swiftly processing this information. These algorithms are specifically designed to facilitate real-time knowledge extraction from ongoing data challenges. However, many existing data stream mining algorithms come with substantial processing and memory expenses; typically, enhanced predictive performance correlates with increased costs. To enhance predictive performance without significantly escalating memory and processing time, this paper presents a new algorithm called Online Local Boosting (OLBoost), which can be integrated into online decision tree algorithms to boost their predictive performance without altering the structure of the generated decision trees. OLBoost focuses on applying boosting to small, discrete areas within the instance space. The experimental findings in this paper demonstrate that OLBoost enables online learning decision tree algorithms to considerably enhance their predictive capabilities, with smaller trees achieving performance that is comparable to or even surpasses that of larger trees.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1712", "problem_id": "17120001", "content": "In the healthcare domain, where data is often limited and models are required to make predictions for patients with rare conditions, quantifying the uncertainty associated with a model's predictions can enhance the efficacy of decision support systems and foster greater user confidence. This research contributes to the development of uncertainty estimation methods for classification and risk prediction in medical tabular data in two key ways. Firstly, it extends and refines the existing set of heuristics for selecting uncertainty estimation techniques, incorporating tests for clinically relevant scenarios such as handling uncommon pathologies, adapting to changes in clinical protocols, and simulating data corruption, with these heuristics tailored to specific clinical use cases. Secondly, the study reveals that ensemble methods and related techniques are less effective in identifying out-of-domain examples, a crucial task that auto-encoders perform more successfully, with additional insights provided on the interplay between uncertainty estimation, class imbalance, post-modeling calibration, and other modeling procedures, as demonstrated through a range of experiments on both synthetic and real-world data.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1713", "problem_id": "17130001", "content": "Matching articulated shapes represented by voxel sets simplifies to the problem of maximal sub-graph isomorphism when each set is articulated through a weighted graph. Spectral graph theory facilitates the mapping of these graphs into lower-dimensional spaces, allowing for shape matching by aligning embeddings due to their invariance to pose alterations. Traditional graph isomorphism methods that depend on the ordering of eigenvalues for eigenspace alignment struggle with large and noisy datasets. We propose a novel formulation that identifies the optimal alignment between two congruent K-dimensional point sets by selecting the most suitable subset of eigenfunctions from the Laplacian matrix. This selection process utilizes eigenfunction signatures constructed from histograms, providing an effective initialization for the alignment problem and significantly enhancing overall performance. Consequently, dense shape matching reformulates into point registration of embeddings under orthogonal transformations, which is addressed through unsupervised clustering and the EM algorithm. The maximal subset matching of non-identical shapes is approached by defining a suitable outlier category. Experimental results on challenging cases demonstrate the algorithm's ability to manage changes in topology, variations in shape, and different sampling densities effectively.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1714", "problem_id": "17140001", "content": "Machine learning techniques have demonstrated significant effectiveness in enhancing image resolution from low-quality inputs. Video super-resolution extends this by leveraging information across multiple frames, typically using optical flow and sequential image warping. This study introduces an end-to-end video super-resolution network that integrates optical flow estimation within the architecture, unlike prior approaches. Our analysis reveals that conventional image warping methods offer limited benefits for video super-resolution when using optical flow. Instead, we introduce a motion compensation technique that directly warps from low to high resolution. This network design enables video super-resolution to effectively utilize optical flow, achieving leading performance on standard benchmarks. Additionally, we demonstrate that processing entire images, rather than isolated patches, substantially improves accuracy.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1715", "problem_id": "17150001", "content": "Recognizing materials is an integral aspect of daily life, as humans can effortlessly identify objects and their constituent materials under normal viewing conditions. However, elucidating the underlying perceptual mechanisms that enable accurate discernment of an object's visual properties remains a longstanding challenge. This study undertakes a comprehensive examination of how the interplay between geometric, illuminative, and spatial frequency factors influences human performance in material recognition tasks. Through large-scale behavioral experiments, participants identify reference materials from a set of candidate samples, with careful consideration given to sampling information in the frequency domain. The analysis reveals substantial first-order interactions between geometry and illumination for both reference and candidate materials. Furthermore, the findings indicate that simple image statistics and higher-order image histograms do not correlate with human performance, prompting the use of a deep neural network to compare highly non-linear statistics in material recognition tasks. The results demonstrate that these models can accurately classify materials, suggesting their ability to define a meaningful representation of material appearance from labeled image data. Additionally, preliminary evidence suggests that humans and these highly non-linear models may utilize similar high-level factors for material recognition tasks, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1716", "problem_id": "17160001", "content": "We introduce a straightforward approach for evaluating the quality of images produced by Generative Adversarial Networks (GANs). This method is applicable to any type of GAN without disrupting the training process or influencing the training goals. The key concept involves establishing a likelihood function that is associated with the quality of the generated images. Specifically, we derive a Gaussian likelihood function from the embeddings (hidden activations) of real images in the discriminator, which allows us to formulate two simple metrics for assessing the likelihood that the embeddings of generated images originate from the distribution of real image embeddings. This results in a basic measure of suitability for generated images across various GAN models. Empirical findings on CIFAR-10 reveal a significant correlation between the proposed metrics and the perceived quality of the generated images.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1717", "problem_id": "17170001", "content": "Supervised learning techniques applied to molecules offer significant potential in fields such as chemistry, drug discovery, and materials science. Fortunately, several neural network models, which are invariant to molecular symmetries and closely related, have been previously documented. These models utilize a message-passing algorithm and aggregation methods to determine a function based on the entire input graph. The logical progression is to identify a particularly efficient version of this general method and assess its performance on chemical prediction benchmarks, continuing until either a solution is achieved or the approach's limitations are identified. In this study, we consolidate existing models into a unified framework termed Message Passing Neural Networks (MPNNs) and investigate innovative variations within this framework. Utilizing MPNNs, we present state-of-the-art results on a key molecular property prediction benchmark, achieving results that suggest future research should concentrate on datasets featuring larger molecules or more precise ground truth labels.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1718", "problem_id": "17180001", "content": "The increasing demand for indoor Location-based Services (LBS) has led to a surge in the development of Internet of Things (IoT)-based indoor localization systems, with Inertial Measurement Unit (IMU)-based localization emerging as a promising solution due to its scalability and independence from proprietary sensors or modules. However, existing IMU-based methods, which rely on statistical techniques for heading and step length estimation, are hindered by cumulative errors and high computational requirements, limiting their suitability for real-time indoor positioning. To overcome these limitations, this study proposes an Online Dynamic Window (ODW)-assisted two-stage Long Short Term Memory (LSTM) localization framework, comprising three distinct ODW models: a Natural Language Processing (NLP)-inspired Dynamic Window (DW) approach, a Signal Processing Dynamic Windowing (SP-DW) approach, and a hybrid SP-NLP model that combines the strengths of the first two. The proposed ODW-assisted models demonstrate significant improvements in computational efficiency and accuracy compared to traditional LSTM-based positioning approaches, enabling near-real-time indoor localization with high accuracy, as validated through experiments using a real Pedestrian Dead Reckoning (PDR) dataset, which highlight the potential of these techniques for achieving high classification accuracy while substantially reducing computational time, making them suitable for near real-time applications.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1719", "problem_id": "17190001", "content": "The development of deep learning has been significantly advanced by Batch Normalization (BN), a technique that facilitates the training of various networks. Nevertheless, normalizing along the batch dimension poses challenges, as BN's error escalates rapidly with decreasing batch sizes due to the inaccurate estimation of batch statistics. This limitation restricts BN's applicability in training larger models and transferring features to computer vision tasks, such as detection, segmentation, and video analysis, which often require small batches due to memory constraints. As an alternative, this paper proposes Group Normalization (GN), a straightforward approach that divides channels into groups and calculates the mean and variance for normalization within each group. Unlike BN, GN's computation is batch-size independent, and its accuracy remains stable across a wide range of batch sizes. Experimental results on ResNet-50 trained with ImageNet demonstrate that GN achieves a 10.6% lower error rate than BN when using a batch size of 2, and is comparable to BN with typical batch sizes, outperforming other normalization variants. Furthermore, GN can be seamlessly transferred from pre-training to fine-tuning and outperforms BN-based counterparts in object detection and segmentation tasks on COCO, as well as video classification on Kinetics, showcasing its potential to replace BN in a variety of tasks. Additionally, GN can be easily implemented with minimal code in modern libraries, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1720", "problem_id": "17200001", "content": "Data augmentations play a crucial role in enhancing the robustness of neural networks, particularly in computer vision applications. A key inquiry is whether neural network features inherently capture the transformations applied during data augmentation. To explore this, we present a methodical framework to determine which network layers are most indicative of augmentation effects. By leveraging features from pre-trained vision models with minimal post-processing, we predict common augmentation-altered properties such as scale, aspect ratio, hue, saturation, contrast, and brightness. Notably, neural network features not only detect augmentation transformations but achieve high accuracy in predicting many of them. Upon confirming that these features encode augmentation-related information, we demonstrate that they predominantly reside in the initial layers of contemporary CNNs, diminishing in deeper layers.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1721", "problem_id": "17210001", "content": "Image interpolation, often referred to as image morphing, involves creating a visual transition between two or more input images. For this transition to achieve an aesthetically pleasing appearance, it is essential that it possesses three key attributes: (i) smoothness, (ii) minimal alterations to the images, and (iii) a realistic appearance devoid of unnatural artifacts throughout the transition. To ensure a seamless and direct transition, one can utilize the well-established Wasserstein Barycenter Problem (WBP). Although this method ensures minimal changes in accordance with the Wasserstein metric, the images generated may appear unrealistic. In this study, we introduce an innovative method for image morphing that fulfills all three essential criteria. Specifically, we propose a constrained variant of the WBP that compels the intermediate images to adhere to an image prior. We outline an algorithm designed to address this issue and illustrate its application using sparse priors and generative adversarial networks.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1722", "problem_id": "17220001", "content": "Efficient evaluation techniques for new algorithms are essential for enhancing interactive bandit and reinforcement learning systems, like recommendation platforms. Although A/B tests are dependable, they require significant time and resources and carry a potential risk of unsuccessful outcomes. In this paper, we propose an alternative approach that forecasts algorithm performance based on historical data that may originate from a different algorithm. Our estimator has the characteristic that its predictions converge in probability to the actual performance of a counterfactual algorithm at a specified rate as the sample size N increases. Additionally, we demonstrate a valid method to estimate the variance of our predictions, enabling analysts to assess the uncertainty associated with them. These characteristics remain valid even when the analyst is unaware of which among many potentially significant state variables are genuinely relevant. We validate our approach through a simulation experiment in reinforcement learning and subsequently apply it to enhance advertisement design for a prominent advertising firm. Our results indicate that our method achieves lower mean squared errors compared to leading existing methods.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1723", "problem_id": "17230001", "content": "Declarative process modeling approaches, which focus on defining high-level process restrictions, are increasingly popular, particularly for adaptable process modeling. This paper introduces DisCoveR, a highly effective and precise declarative mining technique designed to extract Dynamic Condition Response (DCR) Graphs from event logs. The algorithm is formally defined, its optimized bit vector implementation is detailed, and its performance is rigorously assessed against two leading declarative miners in Declare and DCR Graphs mining. In a binary classification task, DisCoveR surpassed the performance of these alternatives, demonstrating an average accuracy of 96.2% in the Process Discovery Contest 2019. Owing to its linear time complexity, DisCoveR also achieves significantly faster run-times, by one to two orders of magnitude, compared to other declarative methods. The paper also illustrates the integration of this miner into an advanced declarative process modeling framework as a model recommendation tool, examines the vital role discovery can play in the modeling process, and presents findings on how this integration has enhanced the end-user modeling experience.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1724", "problem_id": "17240001", "content": "This study explores empirical scaling relationships in unsupervised transfer learning using a fine-tuning approach. While training progressively larger neural networks from scratch on a dataset of fixed size leads to a performance plateau (cross-entropy loss) due to data limitations, pre-trained models on large language datasets exhibit a reduction in the performance gain slope, rather than complete stagnation. We quantify the effective data transferred through pre-training by estimating the data volume a transformer of equivalent size would need to achieve the same loss when trained from scratch, thereby isolating data as the key variable. Our findings indicate that, in the low data regime, the effective data transferred follows a power-law relationship with the parameter count and the fine-tuning dataset size. We posit that the power-law exponents reflect both the model's generality and the proximity of the distributions in a directed manner. Essentially, pre-training amplifies the effective size of the fine-tuning dataset. The scaling behavior of transfer, mirroring overall performance, can be predicted based on parameters, data, and computational resources.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1725", "problem_id": "17250001", "content": "This paper presents a video surveillance system designed to identify traffic incidents within footage collected by stationary cameras on highways. The events of interest involve specific sequences of occurrences in the video, such as a vehicle halting in the emergency lane. Consequently, identifying these events necessitates the analysis of a temporal sequence in the video stream. We examine various methodologies utilizing architectures that incorporate Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs). The initial approach extracts feature vectors, primarily related to motion, from each frame of the video, which are then input into an RNN that processes the resulting sequence of vectors. Other methods directly use the sequence of frames, which may be enhanced with pixel-level motion data. This processed stream is analyzed using a combination of a CNN and an RNN architecture, and we also explore a model based on transfer learning. The results are encouraging, and the most effective architecture will be evaluated in real operational settings.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1726", "problem_id": "17260001", "content": "Recent advancements in graph generative models have shown significant promise in creating increasingly realistic graphs, assessed by global graph characteristics like degree distribution, density, and clustering coefficients. Deep generative models have also progressed substantially by more effectively modeling local correlations within the graph topology. This improved modeling has proven valuable for predicting unobserved graph elements, such as link existence or node classification, based on neighboring observed components. A thorough scientific comprehension of graph data requires consideration of both global and local structures. In this work, we introduce a combined model addressing both aspects as complementary goals within a graph VAE framework. Global structure is represented by integrating graph kernels into a probabilistic model. The loss function of this model is closely associated with the maximum mean discrepancy (MMD) between the global structures of the reconstructed and input graphs. The ELBO objective, derived from the model, uses an MMD term to regularize a standard local link reconstruction term. Our experimental results demonstrate a considerable enhancement in the realism of the generated graph structures, typically by 1-2 orders of magnitude in terms of graph structure metrics, when compared to state-of-the-art graph VAE and GAN models. Local link reconstruction also improves in numerous instances.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1727", "problem_id": "17270001", "content": "This paper presents a fully automated approach for transforming a static image into a realistic, looping animated video, focusing on scenes characterized by continuous fluid motion, such as flowing water or billowing smoke. By leveraging the concept that natural motion of this type can be effectively replicated using a static Eulerian motion description - a single, time-invariant flow field defining particle motion at specific 2D locations - our method utilizes an image-to-image translation network to capture motion priors from online videos of natural scenes. This enables the synthesis of a corresponding motion field for a new image, which is then animated through a deep warping process involving the encoding of pixels as deep features, warping these features via Eulerian motion, and decoding the resulting warped feature maps into images. To generate seamless, continuously looping video textures, a novel video looping technique is proposed, where features are flowed both forward and backward in time before being blended. The efficacy and robustness of this approach are demonstrated through its application to a diverse range of examples, including beaches, waterfalls, and flowing rivers, as shown in Figure A, B, C (see References [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1728", "problem_id": "17280001", "content": "We develop a deep reinforcement learning (RL) model designed to assess an individual's probability of malaria infection by dynamically selecting relevant household survey questions. The RL agent learns to sequentially choose the next question and determine when to stop, making predictions based on accumulated responses. It faces a minor penalty for each question posed and a substantial reward or penalty for accurate or incorrect predictions, incentivizing a balance between survey brevity and prediction precision. Our approach employs a Deep Q-network that derives a policy directly from question responses, with actions corresponding to each survey question and prediction outcome. Focusing on Kenya, where malaria poses a significant health challenge, we train the model using data from 6481 households in the Kenya Malaria Indicator Survey 2015. To assess the benefit of adaptive questioning, we compare our RL model with a supervised learning (SL) baseline that uses a predetermined set of questions. Evaluation on a holdout dataset shows the RL model achieves 80% prediction accuracy while averaging only 2.5 questions per survey. Furthermore, the RL agent adapts dynamically to responses, matching the SL baseline's accuracy while substantially shortening survey length.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1729", "problem_id": "17290001", "content": "This research introduces a novel dynamic graph representation learning model, designed to predict network capacity between viewers in live video streaming events on weighted graphs. The proposed EGAD model utilizes a neural network architecture that incorporates self-attention mechanisms between consecutive graph convolutional networks to capture graph evolution. To mitigate the issue of large neural architectures requiring extensive parameters, which can increase online inference latency and degrade user experience, a knowledge distillation strategy is employed. This involves pretraining a teacher model on offline data and then transferring its knowledge to a smaller student model with reduced parameters using a specially designed distillation loss function. The model's performance is evaluated on a link prediction task using three real-world datasets from live video streaming events, each lasting 80 minutes, where viewers utilized the distribution solution provided by Hive Streaming AB. The results demonstrate the model's effectiveness in terms of link prediction accuracy and parameter requirements, outperforming state-of-the-art approaches. Furthermore, the distillation performance is examined, showing that the model can achieve a compression ratio of up to 15:100 while maintaining high link prediction accuracy. The evaluation datasets and implementation are available at https://stefanosantaris.github.io/EGAD for reproducibility purposes.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1730", "problem_id": "17300001", "content": "Analyzing high-dimensional data presents difficulties in statistical learning and modeling, particularly when predictors exhibit natural groupings that require between-group sparsity. In practical applications, collinearity issues arise, causing conventional l_1 methods to struggle with inconsistent variable selection and reduced prediction accuracy. Additionally, many real-world scenarios extend beyond Gaussian assumptions. To address these issues, this study explores nonconvex penalized generalized linear models with grouped predictors and introduces a computationally efficient algorithm. Theoretical analysis confirms its convergence and establishes precise scaling conditions. The framework supports grouped predictors and various nonconvex penalties, such as discrete l_0 and combined `l_0+l_2' penalties, while also investigating penalty design and parameter tuning. Demonstrations in super-resolution spectrum estimation for signal processing and joint gene selection for cancer classification highlight the enhanced performance of nonconvex penalized estimation methods.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1731", "problem_id": "17310001", "content": "The widespread adoption of reinforcement learning remains elusive, despite its notable achievements in gaming and robotics, due to limitations in sample efficiency and inconsistent performance in uncommon yet demanding situations. Taking cue from the human learning paradigm, where deliberate practice fosters expertise, we introduce an adversarial sampling methodology, leveraging a failure prediction model termed \"CoachNet\". CoachNet, trained concurrently with the agent, forecasts the likelihood of failure, which in turn informs a stochastic sampling strategy, directing the agent towards more difficult episodes. By doing so, the training process concentrates on addressing the agent's vulnerabilities, rather than redundantly revisiting already mastered scenarios. We elaborate on the design and operational principles of CoachNet and provide empirical evidence of its capacity to enhance sample efficiency and robustness during testing in standard continuous control tasks, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1732", "problem_id": "17320001", "content": "This paper introduces a novel approach for learning the acyclic mixed graph structure of a linear non-Gaussian structural equation model from observational data. Expanding on the work of Wang and Drton, we demonstrate that the hidden variable structure can be enriched by identifying multi-directed edges in addition to directed and bidirected connections. Such multi-directed edges arise when multiple observed variables share a latent common cause. By analyzing higher-order cumulants and applying the multi-trek rule, our method detects these hidden influences. The proposed technique accurately reconstructs the graph structure for bow-free acyclic mixed graphs that may contain multi-directed edges.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1733", "problem_id": "17330001", "content": "The identification of imaging biomarkers for autism spectrum disorder (ASD) is essential for elucidating the underlying mechanisms of ASD and predicting or monitoring treatment outcomes. Recent studies have utilized deep learning classifiers to identify ASD from functional magnetic resonance imaging (fMRI) data with improved accuracy compared to traditional learning strategies. Nevertheless, a major challenge associated with deep learning models is interpreting the image features that the network utilizes to define biomarkers. Existing methods typically extract biomarkers by analyzing the impact of ignoring one feature at a time on the prediction outcome. In this study, we employ Shapley value explanation (SVE) from cooperative game theory to move beyond individual feature analysis, as it considers the interactions between features and can be applied to any machine learning method, providing a novel and more accurate approach to determining instance-wise biomarker importance from deep learning models. Although SVE is computationally complex, with a complexity of 2^N for N features, we address this limitation by introducing two approaches based on the graph structure of the input data: 1) considering only the centralized coalition of each feature, and 2) a hierarchical pipeline that clusters features into small communities and applies SVE within each community, with the option to use Monte Carlo approximation for large permutation sets. We validate our methods using the MNIST dataset and compare the results to human perception, and then assess the plausibility of our biomarker findings by training a Random Forest (RF) classifier to distinguish between ASD and control subjects from fMRI data and comparing SVE results to standard RF-based feature importance, ultimately presenting initial results on ranked fMRI biomarkers using SVE on a deep learning classifier for the ASD/control dataset.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1734", "problem_id": "17340001", "content": "Multi-label image recognition is a complex yet important task, where significant advancements have been made by identifying semantic regions and leveraging label relationships. However, existing approaches often rely on RNN/LSTM models to implicitly capture sequential dependencies between regions and labels, failing to fully model mutual interactions or explicitly incorporate label co-occurrences. Furthermore, these methods demand extensive training data per category and struggle with novel categories under limited samples. To overcome these limitations, we introduce a knowledge-guided graph routing (KGGR) framework that integrates statistical label correlations with deep learning. This framework uses prior knowledge to enable adaptive information flow across categories, enhancing multi-label recognition while reducing sample dependency. Specifically, it constructs a structured knowledge graph based on label co-occurrence statistics, employs label semantics to initialize semantic-specific features, and utilizes a graph propagation network to model contextualized image representations. Additionally, each graph node is initialized with classifier weights for its respective label, and a separate propagation network facilitates message passing to improve classifier training. Extensive experiments on multi-label image recognition (MLR) and multi-label few-shot learning (ML-FSL) tasks demonstrate that KGGR substantially outperforms existing state-of-the-art methods on standard benchmarks.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1735", "problem_id": "17350001", "content": "This study proposes a unified framework for estimating the structure and parameters of latent tree graphical models, employing a \"divide-and-conquer\" strategy that initially learns models for subsets of variables and progressively integrates them into a comprehensive solution. The structure estimation component utilizes combinatorial techniques, including minimum spanning tree construction and recursive local grouping, while parameter estimation relies on the method of moments and tensor decomposition. The approach ensures accurate recovery of the underlying tree structure and model parameters with minimal sample complexity for linear multivariate latent tree models, encompassing discrete, Gaussian, and Gaussian mixture distributions. Furthermore, the developed bulk asynchronous parallel algorithm achieves efficient parallelization, with computational complexity increasing logarithmically with the number of variables and linearly with variable dimensionality.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1736", "problem_id": "17360001", "content": "This paper tackles the complex problem of forecasting human-object interaction (HOI) using first-person video. Departing from prevalent approaches that overlook the camera wearer's interaction with objects or treat body motion as an independent input, we highlight the significance of intentional hand movements in predicting future activities. We introduce a new deep learning architecture that simultaneously models and predicts egocentric hand motion, interaction hotspots, and future actions, leveraging intentional hand movement as a representation of the future. Specifically, we conceptualize future hand motion as motor attention and capture this attention within our deep model using latent variables. The predicted motor attention is then used to refine the extraction of distinctive spatial-temporal visual features, enhancing the prediction of actions and interaction hotspots. Comprehensive experiments showcase the advantages of our proposed joint model, achieving state-of-the-art action anticipation performance on both the EGTEA Gaze+ and the EPIC-Kitchens datasets. Our project page is available at https://aptx4869lm.github.io/ForecastingHOI/", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1737", "problem_id": "17370001", "content": "The task of creating images from scene graphs has garnered significant attention due to its inherent complexity. Existing methodologies often involve producing an initial layout representation of the desired image. Nevertheless, these methods typically generate each object within the layout in isolation, leading to problems such as significant overlap, inadequate coverage, and a lack of clarity in the layout. To address these limitations, we introduce a new technique that progressively constructs the complete layout description, thereby enhancing the relationships between objects. Empirical results on the COCO-STUFF dataset demonstrate that our method enhances the quality of both the resulting layout and the final generated image. Specifically, our approach increases layout coverage by nearly 20 percentage points and minimizes object overlap to minimal levels.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1738", "problem_id": "17380001", "content": "Recent interest in continual learning has led to the development of numerous methodologies. Nevertheless, the heterogeneity of evaluation scenarios complicates comparative analysis. This study introduces a structured categorization of these scenarios and assesses them using a unified framework that incorporates both robust baselines and cutting-edge techniques. The findings reveal the varying degrees of difficulty presented by different scenarios and indicate that straightforward baselines, such as Adagrad, L2 regularization, and basic rehearsal strategies, can attain performance levels comparable to those of contemporary mainstream methods. The paper concludes by proposing recommendations for designing more challenging evaluation scenarios and suggesting potential avenues for future research. The code is available at https://github.com/GT-RIPL/Continual-Learning-Benchmark.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1739", "problem_id": "17390001", "content": "Deep metric learning has gained considerable traction recently by integrating distance metric learning with deep neural networks. Much work focuses on developing various pair-based angular loss functions that separate the magnitude and directional components of embedding vectors, thereby maintaining consistency between training and testing measurements. However, these existing angular losses do not ensure that all sample embeddings reside on the same hypersphere during training, potentially leading to gradient instability in batch optimization and hindering the rapid convergence of embedding learning. In this paper, we initially analyze the impact of the embedding norm in deep metric learning with angular distance. Subsequently, we introduce a spherical embedding constraint (SEC) to standardize norm distributions. SEC dynamically aligns embeddings onto a shared hypersphere, facilitating more balanced directional updates. Comprehensive experiments across deep metric learning, face recognition, and contrastive self-supervised learning demonstrate that the SEC-based angular space learning approach substantially enhances the performance of current state-of-the-art methods.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1740", "problem_id": "17400001", "content": "Malicious URLs, also known as harmful websites, pose a widespread and significant risk to cybersecurity. These URLs host harmful content such as spam, phishing, or drive-by exploits, tricking users into falling victim to scams that result in financial losses, stolen personal data, or malware infections, costing billions annually. Timely detection and mitigation of these threats are crucial. While traditional methods rely heavily on blacklists, these are incomplete and fail to identify newly created malicious URLs. To enhance detection capabilities, machine learning approaches have gained prominence in recent years. This paper offers a detailed review and structured analysis of machine learning-based techniques for Malicious URL Detection, outlining the problem formulation and examining key aspects such as feature representation and algorithmic design. The survey serves as a valuable resource for both academic researchers and industry professionals, providing insights into current advancements and practical applications. Additionally, it addresses system design challenges, unresolved research questions, and potential future directions in the field.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1741", "problem_id": "17410001", "content": "The convergence of edge computing and artificial intelligence, particularly deep learning, is giving rise to a new paradigm known as edge intelligence. Nevertheless, the development of edge intelligence systems is hindered by the mismatch between computationally intensive deep learning algorithms and the limited capabilities of edge systems, resulting in a significant performance gap. To address this issue, numerous deep learning techniques and optimization methods have been proposed, including lightweight models, network compression, and efficient neural architecture search. Despite the existence of partial reviews, a comprehensive and systematic examination of these techniques is lacking, which is essential for the successful implementation of edge intelligence. As the field continues to evolve with diverse methods being developed for edge systems, a thorough review is necessary to inform edge computing engineers and the broader community about the current state-of-the-art deep learning techniques that can facilitate the development of edge intelligence systems. This paper provides an overview of the latest and most representative deep learning techniques for edge intelligence, encompassing hand-crafted models, model compression, hardware-aware neural architecture search, and adaptive deep learning models, and ultimately discusses potential future directions based on observations and experimental findings.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1742", "problem_id": "17420001", "content": "In large distributed data center networks, error events trigger a cascade of messages from affected hardware and software components, resulting in a vast message log. This paper tackles the unsupervised learning challenge of identifying error event signatures, which are essentially the unique combinations of messages generated by each event. A key contribution is the formulation of this problem as a topic discovery task, where events and messages are analogous to topics and words, respectively. To address the lack of a direct document equivalent, a non-parametric change-point detection algorithm with linear computational complexity is employed to segment the message log into episodes, effectively creating document-like subsets. The transformed problem is then solved using Latent Dirichlet Allocation (LDA), a well-established topic discovery algorithm. Theoretical analysis confirms the consistency and low sample complexity of the change-point detection algorithm, while its scalability is demonstrated on a large dataset of 97 million messages collected over 15 days from a distributed data center network supporting a major wireless service provider, as shown in Figure A, B, C (References [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1743", "problem_id": "17430001", "content": "Deep learning, a feature learning approach characterized by its robust nonlinear feature transformation capabilities, has become increasingly crucial in various artificial intelligence domains. A prominent deep learning technique, the deep autoencoder, is effective in extracting abstract information from datasets but has limitations, including neglecting the complementary relationship between deep and original features during transformation and being susceptible to the small sample problem. To address these issues, this paper proposes a novel deep autoencoder, termed hybrid feature embedded stacked sparse autoencoder (HFESAE), which learns discriminative deep features by incorporating original features to filter weak hidden-layer outputs during training. Furthermore, to mitigate the limited class representation ability of abstract information due to the small sample problem, a feature fusion strategy is designed, combining abstract information learned by HFESAE with original features to obtain hybrid features for reduction, utilizing a hybrid feature selection strategy based on L1 regularization and a support vector machine (SVM) ensemble model, which employs weighted local discriminant preservation projection (w_LPPD) on each base classifier. The efficacy of the proposed algorithm is validated using several public datasets, with experimental results demonstrating superior performance compared to existing feature learning algorithms, including state-of-the-art deep autoencoder methods, as shown in Figure A, B, C (References [1], [2], and [3] cite related work).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1744", "problem_id": "17440001", "content": "Most machine learning algorithms operate under the assumption that objects are depicted as vectors. However, many of the objects we aim to analyze are more effectively represented by alternative data structures, including sequences and time series. In these cases, numerous conventional learning algorithms are not applicable. We extend gradient-based learning algorithms to time series using dynamic time warping. To achieve this, we propose elastic functions, which broaden the application of functions on time series to matrix spaces. We present necessary conditions that ensure consistency in generalized gradient learning for time series. Furthermore, we demonstrate how these findings can be applied to any elastic distance functions and to sequences made up of symbolic elements. Specifically, we enhance four linear classifiers for use with time series under dynamic time warping and test them on benchmark datasets. The results suggest that generalized gradient learning utilizing elastic functions could effectively augment current advancements in statistical pattern recognition for time series.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1745", "problem_id": "17450001", "content": "In a Multiclass Classification (MCC) framework characterized by a set of labeled point-clouds defined by a feature-set, we derive stochastic partial orderings from all possible triplets of point-clouds without directly calculating the distances between them. This collection of partial orderings enables the efficient computation of a label embedding tree geometry in the Label-space, which in turn yields a predictive graph with weighted linkages. These two multiscale geometries collectively represent the coarse scale information content of MCC, providing insights into the underlying mechanisms of labeling and facilitating accurate predictions with multiple potential labels supported by the data, as seen in Figure A (Reference [1]). To uncover within-label heterogeneity, we also identify natural clusters within each point-cloud and derive their corresponding multiscale geometry, which serves as the fine-scale information content. This approach demonstrates the scalability of our computational method to MCC settings with large label-spaces, as illustrated in Figure B (Reference [2]). Ultimately, the computed multiscale collective of data-driven patterns and knowledge will form the foundation for constructing transparent and explainable subject matter intelligence related to the system of interest, as discussed in Figure C (Reference [3] and Citation [4]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1746", "problem_id": "17460001", "content": "Identifying inheritance patterns within a family is crucial for diagnosing inherited diseases. To address this, we utilize hypergraphs and latent state space models to represent family trees with genetic inheritance patterns, enabling explainable predictions of these patterns. Our methodology facilitates precise causal inference regarding a patient's potential genotypes based on their relatives' observed traits. The framework is designed to allow detailed examination of the inference process, thereby offering explainable predictions. Moreover, we incorporate human insight by enabling the assignment of hypothetical evidence to inherited gene variants. This research demonstrates the potential of latent state space models to enhance patient care, particularly in cases involving rare inherited diseases where access to genetic experts is restricted.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1747", "problem_id": "17470001", "content": "We investigate the approximation characteristics of convolutional architectures in the context of time series modeling, which can be expressed as a problem of functional approximation. In the framework of recurrent architectures, recent findings indicate a complex relationship between approximation efficiency and the memory structures associated with the data generation process. In this paper, we present similar findings for convolutional architectures, using WaveNet as a key example. Our findings demonstrate that in this alternative setting, approximation efficiency is influenced not only by memory aspects but also by additional intricate structures present in the target relationship. This leads to a novel definition of spectrum-based regularity that quantifies the complexity of temporal relationships within the convolutional approximation framework. These investigations establish a basis for comprehending the distinctions among various architectural options for time series modeling and offer theoretically informed guidance for practical applications.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1748", "problem_id": "17480001", "content": "Deep generative models have achieved notable success in the modeling of continuous data, yet the task of accurately representing discrete structures governed by formal grammars and semantics, such as computer programs and molecular configurations, poses significant challenges. The question of how to produce data that is both syntactically and semantically correct remains largely unresolved. Drawing inspiration from compiler theory, where syntax and semantics are verified through syntax-directed translation (SDT), we introduce a novel syntax-directed variational autoencoder (SD-VAE) that employs stochastic lazy attributes. This method transforms the traditionally offline SDT verification into a real-time generative mechanism that guides the decoder's constraints. In comparison to leading existing methods, our approach imposes restrictions on the output space, ensuring that the results are not only syntactically valid but also semantically sound. We assess the performance of the proposed model through applications related to programming languages and molecular structures, focusing on reconstruction and optimization tasks. Our findings illustrate the model's effectiveness in integrating syntactic and semantic constraints in discrete generative frameworks, surpassing the performance of current state-of-the-art solutions.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1749", "problem_id": "17490001", "content": "Recent advancements in deep learning have enabled image colorization techniques to produce highly realistic results, making it challenging for human observers to detect artificially colorized images. This study introduces a new forensic approach using convolutional neural networks (CNNs) to differentiate between natural images (NIs) and colorized images (CIs), even in blind detection scenarios where no training samples from \"unknown\" colorization methods are available during testing. The proposed method achieves strong classification accuracy and generalization capabilities. Initially, a base network is developed, outperforming existing techniques in both accuracy and generalization. Additionally, a supplementary branch is integrated into this network to analyze localized feature regions, further boosting classification precision and generalization performance. To enhance blind detection, negative samples are generated via linear interpolation of paired NIs and CIs, which are then incrementally incorporated into the training process. Experiments confirm the method's robustness and superior generalization when evaluated against various state-of-the-art colorization algorithms.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1750", "problem_id": "17500001", "content": "The existing deep reinforcement learning (RL) methods are hindered by their lack of prior knowledge about the environment, resulting in limited computational and sample efficiency. Recent studies have focused on unsupervised object representation learning using priors and losses based on static object properties, such as visual consistency, to provide a concise and causal description of the world. Nevertheless, the dynamics and interactions of objects are equally important indicators of objectness. This paper presents a framework that enables reasoning about object dynamics and behavior to quickly determine minimal and task-specific object representations. The importance of considering object behavior and dynamics is demonstrated through a set of RGBD MuJoCo object collection and avoidance tasks, which, despite being visually simple, pose challenges to state-of-the-art unsupervised object representation learning algorithms, as shown in Figure A, B, C (References [citation]). Furthermore, the potential of this framework is showcased in several Atari games, where the combination of our object representation with standard RL and planning algorithms yields significantly faster learning compared to existing deep RL algorithms.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1751", "problem_id": "17510001", "content": "Deep neural networks have greatly advanced semantic segmentation, especially in medical imaging. However, training effective models demands substantial pixel-level ground truth masks, which are often difficult to acquire in the medical field. Moreover, limited data availability during training significantly elevates the risk of overfitting. To mitigate the need for extensive annotation, recent research has explored few-shot learning strategies, enabling the learning of new classes from only a few labeled examples. These models are trained on episodes representing diverse segmentation problems, each trained with a minimal labeled dataset. This paper introduces a new few-shot learning framework for semantic segmentation that incorporates unlabeled images within each episode. To accommodate this paradigm, we introduce surrogate tasks that utilize strong supervisory signals—derived from the data—to facilitate semantic feature learning. We demonstrate that incorporating these unlabeled surrogate tasks during episodic training enhances feature representation, resulting in improved generalization to unseen tasks. The effectiveness of our method is demonstrated through skin lesion segmentation on two public datasets. Our approach is general, model-agnostic, and compatible with various deep architectures.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1752", "problem_id": "17520001", "content": "We propose a novel approach to reconstruct the three-dimensional shape, texture, and motion of an object from a single image affected by motion blur, by modeling the object's properties in the 3D domain. Unlike previous methods that only address deblurring in the 2D image domain, our 3D modeling approach accurately captures arbitrary object motion, resulting in improved image decomposition and sharper deblurring outcomes. By representing the motion-blurred object's appearance as a combination of the background and a 3D object undergoing constant translation and rotation, our method leverages differentiable rendering and suitable regularizers to minimize the loss in reconstructing the input image. This enables the estimation of the blurred object's textured 3D mesh with high accuracy, outperforming existing approaches on several fast-moving object deblurring benchmarks, as seen in Figure A, B, C (References [1], [2], [3]). The reconstructed 3D mesh also allows for high-quality temporal super-resolution and novel views of the deblurred object, demonstrating the effectiveness of our approach (citations [4], [5]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1753", "problem_id": "17530001", "content": "We investigate linear two-time-scale stochastic approximation, an iterative technique employed to solve systems of two equations with differing step sizes, driven by its extensive utility in reinforcement learning. Our primary objective is to define the finite-time complexity of this method when subjected to time-varying step sizes and Markovian noise. Specifically, we demonstrate that the mean square errors of the variables produced by the method converge to zero at a sublinear rate of \\Ocal(k^), with k representing the iteration count. To enhance performance, we implement a restarting mechanism that periodically restarts the algorithm. Our analysis reveals that this restarting approach achieves a complexity comparable to that of constant step sizes under time-varying step sizes, while still ensuring convergence to the precise solution. Furthermore, the restarting scheme mitigates the issue of excessively small step sizes, thereby benefiting the practical application of linear two-time-scale stochastic approximation.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1754", "problem_id": "17540001", "content": "Scene graph generation seeks to offer a semantic and structural interpretation of an image by representing objects as nodes and their relationships as edges. Current state-of-the-art methods rely on leveraging contextual information around objects and relations, such as through information exchange between objects. A crucial step in these methods involves transforming the representation of source objects to extract relevant information for target objects. In this study, we posit that a source object should provide information tailored to the specific needs of each target object, rather than offering uniform information to all targets. To this end, we introduce a Target-Tailored Source-Transformation (TTST) method designed to efficiently disseminate information among object proposals and relations. Specifically, for a source object proposal contributing information to other target objects, we transform the source object feature into the target object feature domain, considering both the source and target objects simultaneously. Furthermore, we explore enhanced representations by incorporating language priors with the visual context during the transformation process for scene graph generation. This enables target objects to extract target-specific information from source objects and source relations, thereby refining their own representations. Validated on the Visual Genome benchmark, our framework achieves state-of-the-art performance in scene graph generation. Experimental results demonstrate that our method facilitates mutual improvement in both object detection and visual relationship detection.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1755", "problem_id": "17550001", "content": "Transformers have proven effective in various natural language processing applications, yet their adaptation to video-related tasks like long-term video generation and scene comprehension has been challenging because of computational intensity and the absence of inherent tokenization. This paper introduces the Object-Centric Video Transformer (OCVT), which employs an object-centric strategy to break down scenes into tokens compatible with generative video transformers. By segmenting videos into objects, our unsupervised model effectively captures intricate spatiotemporal interactions among multiple objects and synthesizes subsequent video frames. Additionally, our approach is considerably more memory-efficient than pixel-based models, enabling training on sequences as long as 70 frames using a single 48GB GPU. We evaluate OCVT against prior RNN-based methods and alternative video transformer baselines, showing its superior performance in frame prediction. OCVT also produces meaningful representations for video reasoning, attaining state-of-the-art results on the CATER task.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1756", "problem_id": "17560001", "content": "The development of deep learning models often relies on extensive labeled datasets, but in numerous practical scenarios, obtaining supplementary data can be prohibitively costly or unfeasible. To address this challenge, we introduce semi-supervised deep kernel learning (SSDKL), a novel semi-supervised regression approach that operates within the posterior regularization framework by minimizing predictive variance. By integrating the hierarchical representation capabilities of neural networks with the probabilistic modeling strengths of Gaussian processes, SSDKL effectively harnesses unlabeled data to enhance performance. Our experiments demonstrate that SSDKL achieves significant improvements over supervised deep kernel learning and other semi-supervised methods, including VAT and mean teacher adapted for regression, across a broad range of real-world regression tasks.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1757", "problem_id": "17570001", "content": "To facilitate the robust transfer of reinforcement learning policies from simulation to real-world environments, practitioners typically employ compute-intensive domain randomization techniques. Nevertheless, the presence of unmodeled nonlinearities in real systems can still compromise the stability of simulated policies, hindering their ability to acquire experience in real environments. This paper introduces a novel approach that ensures a stable region of attraction for the output of a simulated policy, even in the context of highly nonlinear systems. By utilizing \"bias-shifted\" neural networks to construct and train the controller within a simulator, our method captures the system's nonlinearities while preserving linearity within a specific region of the state space, allowing it to be tuned to mimic a stable linear quadratic regulator for the real system, as demonstrated through the successful transfer of simulated policies to real systems in the case of a swing-up inverted pendulum.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1758", "problem_id": "17580001", "content": "The current success of leading techniques in semantic segmentation relies heavily on large datasets. These data are considered valuable assets that require protection, as their collection and annotation demand considerable effort and incur substantial costs. Furthermore, visual data may contain private or sensitive information, rendering it unsuitable for public dissemination. Unfortunately, recent research in the area of adversarial machine learning has indicated that even black box classifiers can leak information regarding the datasets on which they were trained. Our findings demonstrate that such membership inference attacks can effectively target advanced semantic segmentation models. To address the associated risks, we investigate a variety of defenses against these attacks and identify effective countermeasures that do not significantly compromise the utility of the segmentation methods. Lastly, we conduct thorough evaluations of our attacks and defenses using several pertinent real-world datasets: Cityscapes, BDD100K, and Mapillary Vistas.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1759", "problem_id": "17590001", "content": "Traditional deep learning-based Computer Aided Diagnosis (CAD) systems frame the problem as an image classification task, categorizing images as either Normal or Abnormal, and achieve high accuracy in detecting specific diseases for which they are trained. However, these systems often fail to provide a clear explanation for their classification decisions, and the activation maps corresponding to these decisions do not accurately correlate with the regions of interest associated with specific diseases. This study addresses this limitation by proposing an approach that emulates clinical practice, where evidence is sought prior to diagnosis, by training a CAD model using a combination of class labels for the entire training image set and rough localizations of suspect regions as additional input for a subset of training images. The effectiveness of this approach is demonstrated through the detection of diabetic macular edema (DME) from OCT slices, where testing on a large public dataset yields state-of-the-art classification accuracy with only a third of images requiring roughly segmented fluid-filled regions, while also providing anatomically accurate heatmaps and regions of interest. Furthermore, the proposed solution is successfully adapted for Breast Cancer detection from mammographic images, with promising evaluation results on public datasets highlighting the generalizability of the approach, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1760", "problem_id": "17600001", "content": "The goal of point cloud completion is to reconstruct the complete geometry of 3D objects from incomplete data by inferring the missing regions. Traditional approaches typically rely on a global shape representation extracted from the incomplete input to predict the complete point cloud, but this often results in the loss of detailed structural information in local regions. To mitigate this issue, this study introduces the Skip-Attention Network (SA-Net) for 3D point cloud completion, which makes two key contributions. Firstly, a novel skip-attention mechanism is proposed to leverage local structure details from incomplete point clouds during the completion process, allowing for the selective conveyance of geometric information at various resolutions and providing an interpretable completion process. Secondly, a structure-preserving decoder with hierarchical folding is developed to utilize the geometric information encoded by the skip-attention mechanism, preserving the structure of the complete point cloud by progressively detailing local regions using the skip-attentioned geometry at the same resolution. Experimental results on the ShapeNet and KITTI datasets, as shown in Figure A, B, C (References [citation]), demonstrate the superiority of the proposed SA-Net over existing state-of-the-art point cloud completion methods.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1761", "problem_id": "17610001", "content": "Unsupervised many-to-many image-to-image translation methods aim to produce a realistic image in the target domain that retains domain-invariant features from the source image and adopts domain-specific characteristics from a guidance image in the target domain. For instance, translating a female face to a male face should result in a male face with the same expression, pose, and hair color as the original female image, as well as the same hairstyle and other male-specific features as the guidance male image, as shown in Figure A. However, existing state-of-the-art methods, although capable of generating visually appealing images, have not been comprehensively evaluated for semantic correctness due to the lack of knowledge about domain-specific and domain-invariant attributes in most real-world datasets, and thus require further assessment, such as that proposed in [citation]. This paper introduces a set of benchmarks and metrics to assess the semantic correctness of these methods, and presents an in-depth analysis of current state-of-the-art UMMI2I translation methods, revealing that they struggle to distinguish between domain-specific and domain-invariant attributes from data and often rely on architectural inductive biases, as illustrated in Figure B, and discussed in [References].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1762", "problem_id": "17620001", "content": "Detecting camouflaged objects poses a significant challenge due to their similar textures to their environment. This paper introduces enhancements to highlight the subtle textural differences between camouflaged items and their backgrounds through the development of multiple texture-aware refinement modules, which are integrated into a deep convolutional neural network to learn texture-sensitive features. The texture-aware refinement module calculates covariance matrices of feature responses to derive texture information, creates an affinity loss to develop parameter maps that aid in distinguishing the textures of camouflaged objects from the background, and incorporates a boundary-consistency loss to investigate the structural details of objects. We assess our network on a standard dataset for camouflaged object detection using both qualitative and quantitative methods. The results demonstrate that our approach significantly surpasses a range of existing state-of-the-art techniques.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1763", "problem_id": "17630001", "content": "Clustering ensemble represents a significant development in unsupervised learning, where results from various clustering algorithms or multiple runs of the same algorithm on a dataset are merged using a consensus function. Numerous studies have demonstrated its effectiveness and precision. This paper first reviews existing clustering ensemble methods, all of which involve two key phases: ensemble generation and consensus function application. Subsequently, we propose integrating supervision into the clustering ensemble process to improve outcomes, either by employing semi-supervised algorithms during ensemble generation or incorporating feedback into the consensus function. Additionally, we present an adaptable weighting mechanism with two parameters: the first assesses dataset compatibility with semi-supervised clustering algorithms for generating base partitions, while the second incorporates user feedback on these partitions. These parameters are utilized in a \"relabeling and voting\" consensus function to derive the final clustering result.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1764", "problem_id": "17640001", "content": "Multivariate time series data, frequently observed in real-world scenarios, often exhibit significant inter-correlation. This paper introduces a deep learning structural time series model designed to (i) process correlated multivariate time series and (ii) predict the intended temporal sequence by explicitly identifying and extracting trend, seasonality, and event components. Trend learning is achieved through a hierarchical neural network incorporating 1D and 2D temporal CNNs and LSTMs. This CNN-LSTM architecture is capable of (i) naturally exploiting dependencies among multiple correlated time series, (ii) extracting weighted differencing features to enhance trend learning, and (iii) memorizing long-term sequential patterns. Seasonality is approximated using a non-linear function based on Fourier terms, while event components are learned via a linear function of regressors that encode event dates. The proposed model's performance is evaluated against state-of-the-art methods through extensive experiments on diverse time series datasets, including Amazon AWS Simple Storage Service (S3) and Elastic Compute Cloud (EC2) billing forecasts, and closing prices for corporate stocks within the same sector.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1765", "problem_id": "17650001", "content": "This study introduces a novel object detection approach that incorporates context awareness through a retrieve-and-transform scene layout model. The method begins by extracting a coarse scene layout from a predefined set of typical layout templates for a given input image. To accommodate significant variations in layout, a spatial transformer network variant is utilized to transform and refine the retrieved layout, yielding a set of feature maps that provide interpretable and semantically meaningful information regarding object locations and scales. This process is implemented within a Layout Transfer Network, which is integrated into Faster RCNN to facilitate joint object detection and scene layout estimation. As demonstrated through extensive experiments on three public datasets, Figure A, B, C, and as cited in References [citation], our method consistently outperforms state-of-the-art object detection baselines in various challenging tasks related to traffic surveillance and autonomous driving, according to the results shown in Figure A, B, C, and discussed in References [citation].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1766", "problem_id": "17660001", "content": "This paper introduces RIFE, a novel Real-time Intermediate Flow Estimation algorithm designed for Video Frame Interpolation (VFI) tasks. Unlike existing flow-based VFI methods that estimate bi-directional optical flows and subsequently scale and reverse them to approximate intermediate flows, often resulting in artifacts at motion boundaries, RIFE employs a neural network called IFNet. This network is capable of directly estimating intermediate flows in a coarse-to-fine manner, significantly enhancing speed. Furthermore, a privileged distillation scheme is developed for training the intermediate flow model, yielding substantial performance gains. Experimental results show that RIFE exhibits flexibility and achieves state-of-the-art performance on multiple public benchmarks, as seen in Figure A, B, C, and the implementation is made available at \\url.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1767", "problem_id": "17670001", "content": "The machine learning domain has seen significant progress recently, fueled by both academic and industrial investigations. Current research efforts have focused on employing generative models, particularly Generative Adversarial Networks, within computer vision and image classification tasks. Since its introduction, numerous adaptations of this framework have emerged. This paper examines Goodfellow et al.'s original model alongside its subsequent modifications, offering a comparative analysis of their performance.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1768", "problem_id": "17680001", "content": "Contemporary approaches to attributed network clustering commonly employ graph convolution to derive node embeddings while concurrently executing clustering assignments within the embedding space. This method proves efficient due to graph convolution's ability to integrate both structural and attributive data for node embedding acquisition. Nevertheless, these approaches are limited because graph convolution only integrates attribute data from the immediate vicinity of nodes, neglecting the mutual relationships between nodes and their attributes. To address this, we introduce a variational co-embedding learning model for attributed network clustering (VCLANC). VCLANC employs dual variational auto-encoders to simultaneously embed nodes and attributes, allowing the reconstruction of mutual affinity information between them from the embedding space. This reconstructed information then acts as supplementary self-supervised knowledge for representation learning. Concurrently, trainable Gaussian mixture models are utilized as priors to deduce node clustering assignments. To improve the quality of the resulting clusters, we implement a mutual distance loss on the Gaussian prior centers and a clustering assignment hardening loss on the node embeddings. Empirical evaluations conducted on four real-world attributed network datasets confirm the efficacy of the proposed VCLANC in attributed network clustering.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1769", "problem_id": "17690001", "content": "The significant advancements achieved by convolutional neural networks in various machine learning domains have led to a shift in focus towards automatically generating neural network architectures, as manually designing them can be costly and time-consuming. While research on auto-generated networks has shown promising results, it primarily focuses on selecting individual layers, such as convolution or pooling layers, sequentially. In contrast, hand-crafted neural networks, like GoogLeNet's Inception-block, residual networks' residual block, and dense convolutional networks' dense block, exhibit elegant and creative designs. Leveraging reinforcement learning and the strengths of these networks, we propose a novel approach to automatically design a multi-block neural network, comprising multiple block types, with the goal of exploring structure learning in deep neural networks and investigating the potential of combining different blocks to form a well-performing network. Our method utilizes a Q-learning agent, trained to sequentially select various block types, to create the optimal network. We validate our approach by conducting experiments on the MNIST, SVHN, and CIFAR-10 image classification tasks, using the auto-generated multi-block neural network, under restricted computational resources, and demonstrate that our method achieves comparable or superior performance to hand-crafted and advanced auto-generated neural networks.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1770", "problem_id": "17700001", "content": "Contemporary reinforcement learning algorithms generally aim to minimize a cumulative single-step cost throughout a trajectory. However, the resultant movements are frequently 'unnatural', characterized by abrupt accelerations that lead to energy inefficiency and unpredictability. In this study, we introduce an innovative approach for steering nonlinear systems through the minimization of the Koopman spectrum cost, which pertains to the Koopman operator governing the controlled dynamics. This approach enables a wider array of dynamical behaviors that progress along stable manifolds, such as nonlinear oscillators, closed loops, and smooth motions. We illustrate that certain dynamic realizations, which are unattainable using a cumulative cost, can be accomplished within this framework. Additionally, we present an online learning algorithm for our problem that is demonstrably efficient and achieves a sub-linear regret bound given specific structural assumptions.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1771", "problem_id": "17710001", "content": "Key information relevant to a particular subject within a document is frequently arranged in tabular form to aid readers in information retrieval and comparison, which can be challenging to express in natural language. However, extracting tabular data from unstructured digital formats, such as Portable Document Format (PDF) and images, is difficult to convert into a structured, machine-readable format due to their structural and stylistic complexity and variability. To enhance image-based table recognition using deep learning, we present the largest publicly accessible table recognition dataset, PubTabNet (https://github.com/ibm-aur-nlp/PubTabNet), which includes 568,000 table images alongside their structured HTML representations. This dataset was automatically crafted by aligning the XML and PDF versions of scientific articles available in the PubMed Central Open Access Subset (PMCOA). Additionally, we introduce an innovative attention-based encoder-dual-decoder (EDD) architecture designed to transform table images into HTML code. The model features a structure decoder that reconstructs the table layout and assists the cell decoder in identifying cell contents. Furthermore, we propose a novel Tree-Edit-Distance-based Similarity (TEDS) metric for table recognition that more effectively addresses multi-hop cell misalignment and OCR inaccuracies compared to existing metrics. Experimental results indicate that the EDD model can accurately interpret complex tables based solely on their image representations, achieving a TEDS score that surpasses the state-of-the-art by an absolute 9.7%.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1772", "problem_id": "17720001", "content": "This paper introduces a novel approach to representing scenes as an abstraction of 'things'. Building upon 'things' produced by contemporary object proposal methods, we examine their basic observable attributes—specifically position, size, aspect ratio, and color—while disregarding object identities, diverging from the field's focus on object recognition. Our work presents three key contributions. First, we analyze fundamental properties of 'things', termed things syntax. Second, we explore converting this syntax into linguistic abstract statements and evaluate their effectiveness in scene retrieval. Third, we investigate scene discrimination using abstract block illustrations derived directly from images, bypassing traditional attribute learning. Remarkably, despite relying solely on basic 'things' layout features without any prior learning, our method achieves competitive scene retrieval performance.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1773", "problem_id": "17730001", "content": "Short-range 3D pedestrian detection is adequate for emergency braking, but longer-range detection is essential for facilitating smoother braking and fostering confidence in autonomous vehicles. The current leading methods on the KITTI benchmark show subpar performance in locating pedestrians at extended distances. To address this, we introduce a method focused on long-range 3D pedestrian detection (LRPD), which utilizes the abundance of RGB data along with the accuracy of LiDAR. Our approach combines RGB instance segmentation with LiDAR point-based proposal generation, followed by a second stage that symmetrically integrates both sensors. This methodology results in a notable enhancement in mean Average Precision (mAP) at long ranges when compared to existing state-of-the-art techniques. The effectiveness of our LRPD method was assessed using pedestrian data from the KITTI benchmark.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1774", "problem_id": "17740001", "content": "The introduction of the transformer has led to a rapid expansion in the scale of language models, significantly surpassing advancements in hardware capabilities. Dense transformers are anticipated to achieve a trillion-parameter scale in the near future, necessitating the use of thousands or even tens of thousands of GPUs for training. We explore the difficulties associated with training at this magnitude and beyond on commercially available hardware. Specifically, we examine the minimum achievable training time for various distributed training configurations, utilizing empirical scaling laws for language models to gauge the optimal (critical) batch size. Contrary to widespread assumptions, we find no signs of a memory bottleneck; instead, we assert that the primary constraint—apart from cost—resides in the duration of training. Alongside this analysis, we present two novel methods, which collectively halve the shortest training time. These methods also minimize data transfer, reducing the network requirements to a level where a high-speed InfiniBand connection is unnecessary. This enhanced network efficiency outperforms existing approaches linked with the ZeRO optimizer, significantly decreasing memory consumption to a fraction of the total GPU memory available.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1775", "problem_id": "17750001", "content": "Traditional few-shot object segmentation techniques acquire segmentation knowledge from a limited number of labeled support images accompanied by highly labeled segmentation masks. Recent studies have demonstrated comparable performance using weaker supervision methods, such as scribbles and bounding boxes. Nevertheless, the area of few-shot object segmentation utilizing image-level supervision has not received much focus. We introduce an innovative multi-modal interaction module for few-shot object segmentation that integrates a co-attention mechanism leveraging both visual and word embeddings. This approach allows our model to achieve a 5.1% enhancement over previously established methods for image-level few-shot object segmentation. Our technique closely rivals state-of-the-art methods that depend on strong supervision, albeit our approach utilizes minimal supervision. Additionally, we present a new framework for few-shot weakly supervised video object segmentation (VOS) that relies on image-level labels for the initial frame. This framework employs weak annotations, contrasting with semi-supervised VOS approaches that rely on strongly labeled segmentation masks. It assesses the potential for generalizing to new classes within the VOS context, organizing the VOS data into various folds with distinct categories for each fold. This setup offers a promising framework to evaluate how few-shot object segmentation techniques can improve by leveraging additional object poses or interactions, which may not be present in static images as seen in the PASCAL-5i benchmark.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1776", "problem_id": "17760001", "content": "Bayesian Decision Trees are valued for their capacity to provide probabilistic interpretations; nevertheless, building them can be computationally expensive. This paper introduces a versatile Bayesian Decision Tree algorithm suitable for both regression and classification tasks. The algorithm avoids the use of Markov Chain Monte Carlo methods and eliminates the need for pruning. Although a weighted probability tree space can be developed, empirical results suggest that a single tree, specifically the greedy-modal tree (GMT), captures the majority of relevant information. The performance of this method appears to be comparable to that of Random Forests.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1777", "problem_id": "17770001", "content": "Notwithstanding the significant focus and resources devoted to clinical machine learning (CML) research, a substantial gap remains between the development of these technologies and their actual implementation in clinical settings. Although advancing the state-of-the-art through research is crucial, the translation of CML into practical applications is equally vital for realizing the potential of AI to improve patient care and meet the high expectations surrounding its use in healthcare. To gain a more comprehensive understanding of the challenges involved, we conducted a survey of researchers and practitioners with experience in developing CML for clinical deployment, gathering their firsthand insights and lessons learned. By analyzing these responses, we identified key categories of obstacles and challenges, which can inform the design and development of more effective clinical machine learning applications.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1778", "problem_id": "17780001", "content": "We introduce a tree-based methodology aimed at tackling classification and regression challenges within the realm of functional data analysis. This approach capitalizes on representation learning and employs various splitting rules at the node level, which aids in minimizing generalization error while ensuring that the tree remains interpretable. This is accomplished by constructing a weighted functional L^ space through constrained convex optimization, which facilitates the extraction of multiple weighted integral features from the input functions, subsequently guiding the binary split for each internal node. Our method is tailored to accommodate multiple functional inputs and/or outputs by establishing appropriate splitting criteria and loss functions that are adaptable to different problems and can also integrate scalar and categorical data, all while employing the conventional greedy CART algorithm for tree development. We concentrate on scalar-valued functional inputs situated in unidimensional domains and demonstrate the efficacy of our technique through both a simulation study and four practical applications in classification and regression scenarios.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1779", "problem_id": "17790001", "content": "The integration of reinforcement learning (RL) with deep neural networks has the potential to create efficient and powerful solvers for NP-hard problems in neural combinatorial optimization (CO), enabling the discovery of near-optimal solutions without requiring extensive domain expertise. To this end, we propose Policy Optimization with Multiple Optima (POMO), a comprehensive approach for developing heuristic solvers that can be applied to a broad range of CO problems by leveraging the symmetries inherent in the representation of CO solutions. By utilizing a modified REINFORCE algorithm that encourages diverse rollouts towards all optimal solutions, POMO facilitates fast and stable RL training, characterized by low variance and increased resistance to local minima compared to existing methods. Furthermore, we introduce an augmentation-based inference method that complements POMO effectively. The efficacy of POMO is demonstrated through its application to three renowned NP-hard problems: traveling salesman (TSP), capacitated vehicle routing (CVRP), and 0-1 knapsack (KP), with our POMO-based solver achieving significant performance improvements over recent learned heuristics, including an optimality gap of 0.14% for TSP100 while reducing inference time by over an order of magnitude.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1780", "problem_id": "17800001", "content": "This study tackles the issue of estimating 3D human poses from a single color image. While the end-to-end learning framework has shown overall success, the leading methods incorporate a two-step process: using a Convolutional Network (ConvNet) for 2D joint localization followed by an optimization phase to derive the 3D pose. We highlight the representation of 3D pose as a significant challenge in existing ConvNet methodologies and introduce two key contributions to demonstrate the efficacy of end-to-end learning for this application. First, we suggest a finely detailed discretization of the 3D space surrounding the subject and train a ConvNet to predict likelihoods for each joint at the voxel level. This creates a more natural 3D pose representation and significantly enhances performance compared to directly regressing joint coordinates. Second, we implement a coarse-to-fine prediction strategy to improve the initial estimates, which tackles the substantial increase in dimensionality and facilitates iterative refinement and repeated analysis of the image features. Our method surpasses all leading state-of-the-art techniques on standard benchmarks, achieving an average reduction in relative error exceeding 30%. Moreover, we explore the use of our volumetric representation in a related but suboptimal architecture that remains of practical significance, as it allows for training in the absence of corresponding 3D ground truth images and yields noteworthy results for images captured in real-world scenarios.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1781", "problem_id": "17810001", "content": "Optimizing traffic control remains a complex challenge globally, with most current methods concentrating on adaptive strategies for normal (recurrent) traffic scenarios. Addressing disruptions caused by severe incidents, particularly those impacting multiple lanes or entire intersections, continues to be an unresolved issue. This study introduces an innovative approach for adjusting traffic signal timings in urban intersections during non-recurrent incidents. To enable rapid and dependable decision-making, the proposed framework integrates efficient Machine Learning (ML) techniques with robust Genetic Algorithms (GA). Initially, a standard GA is implemented as a baseline, using phase duration as the decision variable to minimize total network travel time, with optimized parameters for crossover, mutation, and fitness evaluation. Subsequently, multiple ML regression models are trained to predict travel time, with the top-performing extreme-gradient decision-tree model selected and fine-tuned. The final contribution is the BGA-ML algorithm, merging GA with the optimized regressor within a unified framework. Experimental results demonstrate that BGA-ML significantly outperforms the original GA in speed and effectiveness under non-recurrent incident scenarios.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1782", "problem_id": "17820001", "content": "Predicting corn yields is advantageous as it offers essential insights into production and pricing before the harvest occurs. Access to publicly available, high-quality corn yield forecasts can mitigate emerging information asymmetry challenges and enhance price efficiency within futures markets. This study is the inaugural application of Long Short-Term Memory (LSTM), a specialized variant of Recurrent Neural Network (RNN), for forecasting corn yields. The dataset utilized comprises a cross-sectional time series of county-level corn yields paired with hourly weather data, creating a sufficiently extensive sample for deploying deep learning techniques. LSTM proves effective for time series predictions featuring intricate interrelations, making it well-suited for this endeavor. The findings derived from county-level data in Iowa demonstrate its promising predictive capabilities in comparison to traditional survey-based approaches.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1783", "problem_id": "17830001", "content": "This study investigates the automated learning of essential steps for task completion, using narrated instruction videos, with a focus on tasks like changing a car tire. The contributions are threefold: Initially, a novel unsupervised learning technique is introduced, exploiting the synergistic relationship between video content and accompanying narration. This technique addresses two interconnected clustering challenges—one for text and another for video—sequentially, using joint constraints to derive a unified and logical sequence of steps across both modalities. Secondly, a new, demanding dataset comprising real-world instructional videos sourced from the Internet was compiled and annotated. This dataset encompasses approximately 800,000 frames across five distinct tasks, featuring intricate interactions between individuals and objects, filmed in diverse environments. Thirdly, experimental results validate the capacity of the presented methodology to autonomously identify the primary steps required to accomplish the task and pinpoint their locations within the videos, all without supervision.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1784", "problem_id": "17840001", "content": "Conventional one-stage anchor-based detectors typically employ multiple priors at each spatial position to ensure high target box coverage, particularly in scene text detection. This study introduces a straightforward approach for multi-oriented text detection, where each feature map location corresponds to a single reference box. Inspired by the two-stage R-CNN framework, which predicts object locations of arbitrary shapes using learned proposals, our method adapts this mechanism for one-stage detection. By replacing the original anchor with a regression-derived learned anchor in the final predictions, our approach, built on RetinaNet, delivers competitive results on multiple benchmarks while maintaining real-time performance (26.5fps at 800p), outperforming existing anchor-based text detectors. Furthermore, by minimizing reliance on anchor design, our method can be readily extended to similar detection tasks. The code will be available at https://github.com/xhzdeng/stela.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1785", "problem_id": "17850001", "content": "To address the mode collapse issue in generative adversarial networks (GANs), we introduce a novel approach that utilizes multiple discriminators, each assigned a distinct subset of the minibatch, referred to as a microbatch. By introducing a diversity parameter \\alpha, we progressively modify the task of each discriminator from distinguishing real and fake samples to identifying whether samples originate from within or outside its designated microbatch. This encourages the generator to enhance diversity within each minibatch, making it more challenging for each discriminator to accurately discriminate its assigned microbatch. As a result, all models in our framework are incentivized to produce diverse generated sets, leading to reduced losses. Our experiments demonstrate that this approach effectively promotes sample diversity from the early stages of training across multiple datasets, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1786", "problem_id": "17860001", "content": "The estimation of discrete intervention effects from observational data has garnered significant attention, whereas the realm of continuous-valued interventions, such as dosage-dependent treatments, has received relatively scant consideration. This paper addresses this knowledge gap by proposing a modified generative adversarial networks (GANs) framework, termed SCIGAN, which can flexibly estimate counterfactual outcomes for multiple continuous interventions simultaneously. The core concept involves utilizing a substantially modified GAN model to generate counterfactual outcomes, subsequently employed to train an inference model via standard supervised methods, enabling the estimation of counterfactuals for novel samples. To overcome the challenges inherent to continuous interventions, a novel hierarchical discriminator architecture is introduced, capitalizing on the structural properties of the continuous intervention setting. Theoretical foundations are established to justify the use of the GAN framework and the hierarchical discriminator. The experimental section presents a new semi-synthetic data simulation tailored to the continuous intervention setting, showcasing the superiority of the proposed approach over existing benchmark models, as demonstrated in the experiments section, Figure A, B, C, and cited in References [citation].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1787", "problem_id": "17870001", "content": "Text images with low resolution frequently appear in natural environments, such as documents photographed using mobile devices. Recognizing such images is difficult due to the loss of fine-grained details, resulting in reduced accuracy. A common approach involves applying super-resolution (SR) methods as a preprocessing step. Yet, existing single image super-resolution (SISR) techniques are typically trained on artificially degraded images (e.g., Bicubic downsampling), which are simplistic and inadequate for real-world low-resolution text recognition. To address this, we introduce TextZoom, a dataset of real paired low- and high-resolution text images captured in varying focal lengths under natural conditions. As illustrated in Fig. 1, this dataset is more realistic and demanding than synthetic alternatives. We emphasize that enhancing recognition accuracy is the primary objective for Scene Text SR. Accordingly, we present TSRN, a Text Super-Resolution Network incorporating three innovative components: (1) a sequential residual block for capturing sequential text features, (2) a boundary-aware loss to enhance character edge clarity, and (3) a central alignment module to mitigate misalignment issues in TextZoom. Extensive testing on TextZoom shows that TSRN significantly boosts recognition accuracy, achieving over 13% improvement for CRNN and nearly 9.0% for ASTER and MORAN compared to synthetic SR data. Additionally, TSRN surpasses seven leading SR methods in enhancing recognition performance for low-resolution images in TextZoom, outperforming LapSRN by more than 5% and 8% for ASTER and CRNN, respectively. These findings indicate that real-world low-resolution text recognition remains an unsolved challenge, warranting further investigation.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1788", "problem_id": "17880001", "content": "To effectively collaborate with humans in real-world settings, robots must be capable of identifying and manipulating various tools. Currently, there is a lack of datasets and studies that tackle this challenge in realistic environments. This paper addresses this limitation by introducing the METU-ALET dataset, a comprehensive collection of images featuring tools from diverse domains, including farming, gardening, office, stonemasonry, vehicle, woodworking, and workshop settings, with scenes that include sophisticated environments and human interaction. These scenes pose significant challenges for object detection, such as tool miniaturization, articulation, occlusion, and inter-class similarities. We evaluate the performance of state-of-the-art deep object detectors, including Faster R-CNN, Cascade R-CNN, RepPoint, and RetinaNet, on our dataset and find that they struggle to detect small-scale tools or those with visual similarities to other tool components. This underscores the significance of our dataset and research, which provides a foundation for future investigations into tool detection and robotics applications, accompanied by the release of our dataset, code, and trained models.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1789", "problem_id": "17890001", "content": "Combinatorial optimization plays a crucial role in computer vision. For example, in tasks such as semantic segmentation, human pose estimation, and action recognition, methodologies are constructed to address inference in Conditional Random Fields (CRFs), aiming to yield a structured output that aligns with the image's visual features. Nonetheless, solving inference in CRFs is generally challenging, and approximation techniques are often resource-intensive and restricted to unary, pairwise, and manually designed higher order potentials. In this study, we demonstrate that it is possible to learn program heuristics, or policies, for efficiently addressing inference in higher order CRFs specifically for semantic segmentation by utilizing reinforcement learning. Our approach effectively tackles inference tasks without being restricted by the form of the potentials. We present strong results on the Pascal VOC and MOTS datasets.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1790", "problem_id": "17900001", "content": "A significant obstacle to the practical deployment of machine learning models is their inability to generalize effectively to new, unseen domains that exhibit different data distributions than those encountered during training, often due to the models learning features that are not generalizable and are instead spuriously correlated with the data labels. This limitation has sparked considerable interest in the development of models that can learn robust explanations which remain consistent across diverse domains, a concept known as Out-of-Distribution (OOD) Generalization. Essentially, the goal is to identify optimal solutions that are universally applicable across all training domains, akin to finding local or global minima in the loss landscape that are invariant across domains. This paper introduces a novel masking strategy that dynamically adjusts the update magnitude for each network edge based on the consistency of gradients flowing through it, thereby controlling the optimization process. Specifically, our \"Smoothed-AND (SAND)-masking\" technique not only verifies the alignment of gradient directions but also promotes consistency among their magnitudes, enhancing the discovery of domain-invariant features. The efficacy of SAND-mask is demonstrated on the Domainbed benchmark, where it achieves state-of-the-art accuracy on the Colored MNIST dataset and yields competitive performance on other domain generalization datasets, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1791", "problem_id": "17910001", "content": "Pre-trained deep neural networks have demonstrated exceptional generalizability and versatility across various related tasks, thanks to their robust feature representations. Fine-tuning, the most straightforward and widely adopted approach, aims to leverage and adapt these representations for new tasks with limited data. However, fine-tuning often falls short of optimal performance and requires meticulous optimization to avoid severe overfitting on small datasets, largely due to the vast number of parameters in typical deep convolutional neural networks. To mitigate these issues, we introduce a simple yet effective regularization technique for fine-tuning pre-trained deep networks, specifically designed for k-shot learning. Our approach involves clustering model parameters to ensure intra-cluster similarity and inter-cluster diversity, effectively regularizing the parameter search space dimensionality. We identify and group neurons within each layer that exhibit similar activation patterns, and when fine-tuning for a classification task with only k examples, we apply a single gradient update to all parameters within the same group. Since neuron activations depend on the input data distribution, we propose a reinforcement learning-based search strategy using recurrent networks to efficiently determine optimal group assignments for each layer. As shown in our experimental results, our method can be easily applied to several popular convolutional neural networks, outperforming other state-of-the-art fine-tuning-based k-shot learning strategies by over 10%.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1792", "problem_id": "17920001", "content": "Self-supervised learning has been shown to excel across various domains by extracting meaningful representations from unlabeled data. Nevertheless, traditional self-supervised techniques predominantly prioritize the analysis of inter-sample structures, neglecting the crucial intra-temporal structures that are vital for time series data. This paper introduces SelfTime, a comprehensive self-supervised framework for time series representation learning, which investigates both inter-sample and intra-temporal relationships to uncover the underlying structural features in unlabeled time series. Initially, we generate inter-sample relationships by selecting positive and negative samples relative to an anchor sample, while we derive intra-temporal relationships by sampling segments from that anchor. Subsequently, a shared feature extraction backbone, along with two distinct relation reasoning heads, is utilized to evaluate the relationships among sample pairs for inter-sample reasoning and among time segment pairs for intra-temporal reasoning. Ultimately, valuable representations of time series are obtained from the backbone, guided by the relation reasoning heads. Experimental evaluations on several real-world time series datasets for classification tasks confirm the method's efficacy. The code and data are accessible at https://haoyfan.github.io/.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1793", "problem_id": "17930001", "content": "Adapting a pre-trained neural network is often favored over training from scratch, especially when training data is limited, a model needs to handle multiple tasks, or incorporating prior knowledge is desired. Common adaptation techniques include fine-tuning and using the pre-trained network as a fixed feature extractor. This paper introduces \"side-tuning,\" an alternative approach that adapts a pre-trained network by training a small, auxiliary network. This \"side\" network is then integrated with the original, unaltered pre-trained network through summation. Side-tuning achieves performance comparable to or exceeding existing methods while addressing fundamental limitations of fine-tuning, fixed features, and other prevalent strategies. Specifically, side-tuning exhibits reduced overfitting, asymptotic consistency, and resilience to catastrophic forgetting in incremental learning scenarios. The effectiveness of side-tuning is demonstrated across various applications, such as incremental learning (iCIFAR, iTaskonomy), reinforcement learning, imitation learning (visual navigation in Habitat), NLP question-answering (SQuAD v2), and single-task transfer learning (Taskonomy), yielding consistently encouraging outcomes.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1794", "problem_id": "17940001", "content": "Unsupervised learning enables the utilization of extensive datasets without requiring labeled annotations. Within this framework, autoencoders built on deep learning have demonstrated significant promise in identifying anomalies in medical imaging. Nevertheless, current anomaly detection metrics predominantly rely on reconstruction error, which has two major limitations: it disregards the model's internal representations used for reconstruction and lacks formal guarantees or consistent sample comparisons. To overcome these issues, we introduce the Context-encoding Variational Autoencoder (ceVAE), integrating reconstruction-based and density-based anomaly scoring. This approach enhances performance at both the sample and pixel levels. Evaluated on the BraTS-2017 and ISLES-2015 segmentation benchmarks, the ceVAE attains unsupervised ROC-AUC scores of 0.95 and 0.89, respectively, surpassing existing methods by a substantial margin.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1795", "problem_id": "17950001", "content": "This study presents a novel approach to analyzing deep neural networks (DNNs) using a black box methodology, where a DNN is treated as a function that maps inputs to outputs, and its local robustness is examined for a specific input. By leveraging scenario optimization techniques from robust control design, the score difference function $f_i-f_\\ell$ is learned with respect to the target label $\\ell$ and attacking label $i$. A linear template is applied over the input pixels, and the corresponding coefficients of the score difference function are learned by reducing the problem to linear programming (LP) problems. To enhance scalability, optimizations such as component-based learning and focused learning are proposed. The learned function provides a probably approximately correct (PAC) guarantee for the robustness property, and as it approximates the local behavior of the DNN, it can be utilized to generate potential adversarial examples, which can then be verified by the original network. Furthermore, the input pixels with large absolute coefficients are identified and used to explain the attacking scenario. The proposed approach has been implemented in a prototype tool, DeepPAC, and experimental results demonstrate its ability to handle large neural networks, such as ResNet152 with 6.5M neurons, and generate adversarial examples that are often very close to the decision boundary.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1796", "problem_id": "17960001", "content": "The early classification of time series data has been thoroughly investigated to reduce prediction delays in time-critical fields such as healthcare and finance. A key objective of early classification is to categorize an incomplete time series as quickly as possible while maintaining a certain level of accuracy. In recent years, various strategies for early classification of time series have emerged. Given that different methodologies have addressed the early classification challenge from diverse perspectives, it is crucial to conduct a comprehensive review of the available solutions to ascertain the current landscape of this field. These methodologies have exhibited commendable effectiveness across a variety of applications, such as human activity recognition, gene expression-based health diagnostics, and industrial monitoring. In this paper, we offer a systematic assessment of the existing literature on early classification techniques for both univariate and multivariate time series. We categorize the different methods into four distinct groups based on their proposed strategies: prefix-based, shapelet-based, model-based, and miscellaneous approaches. Additionally, the authors explore the applications of early classification in several domains, including industrial monitoring, intelligent transportation, and healthcare. Lastly, we present a succinct overview of the current state of research along with potential directions for future investigation.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1797", "problem_id": "17970001", "content": "This study introduces proLab, a novel colour coordinate system that is obtained through a 3D projective transformation of CIE XYZ, offering improved performance over the commonly used CIELAB system, although it is outperformed by the more recent CAM16-UCS, as assessed by the STRESS metric in relation to the CIEDE2000 colour difference formula. Notably, proLab retains the linearity of manifolds due to the preservation of projective transformations, allowing for the utilization of angular errors of chromaticity estimation, which are typically applied in linear colour spaces. Furthermore, proLab normalizes angular errors across different hues in accordance with human colour discrimination thresholds, a characteristic not found in linear spaces. Additionally, the study reveals that proLab exhibits more homoscedastic shot noise compared to CAM16-UCS and other standard colour spaces, making it a suitable coordinate system for linear colour analysis.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1798", "problem_id": "17980001", "content": "Entropic regularization in Reinforcement Learning (RL) is a widely adopted technique to promote sufficient exploration of the state-space before converging to a locally optimal policy. While entropy is primarily employed to encourage exploration and resolve ambiguities among optimal policies, its theoretical implications remain incompletely explored. This work examines the broader regularized RL objective and, leveraging Fenchel duality, formulates a dual problem resembling an adversarial reward scenario. Specifically, we demonstrate that the optimal policy derived from a regularized objective corresponds to the optimal policy for an RL problem with a worst-case adversarial reward. This finding reinterprets entropic regularization as a robustness mechanism. Additionally, the generality of our analysis extends to other regularization methods, offering new insights into policy regularization and enhancing the understanding of exploration through robust reward structures.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1799", "problem_id": "17990001", "content": "To alleviate the computational costs associated with calculating the simplicial complexes of a large point cloud, a common strategy involves selecting a representative subset. This research explores an innovative method, which involves sampling critical points of a Morse function related to the point cloud, in order to approximate the Vietoris-Rips complex or the witness complex and subsequently calculate persistence homology. The efficacy of this new approach is evaluated in comparison to farthest point sampling, within the context of utilizing persistence homology to classify human face images into ethnic groups.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1800", "problem_id": "18000001", "content": "We introduce a reinforcement learning approach for a soccer dribbling challenge, where an agent must navigate from a starting point to a goal while maintaining ball control against an opponent attempting to intercept. The adversary follows a fixed policy, while the dribbler learns optimal actions at each step. By defining relevant state variables and incorporating domain knowledge through high-level macro-actions, we implement the reinforcement learning algorithm using CMAC for function approximation. Experimental results demonstrate that, post-training, the dribbler successfully completes the task against a skilled opponent approximately 58% of the time.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1801", "problem_id": "18010001", "content": "Contemporary large-scale machine learning applications necessitate the implementation of stochastic optimization algorithms on distributed computing systems. A major challenge faced by these systems is the communication overhead involved in sharing information among the workers, such as stochastic gradients. Among the various strategies proposed to address this challenge, the framework of compressed communication with error feedback (EF) has proven to be one of the most effective. EF is uniquely capable of managing the errors introduced by biased contractive compressors, such as Top-K. In this study, we introduce a novel and theoretically as well as practically superior alternative to EF for handling contractive compressors. Specifically, we present a methodology that can convert any contractive compressor into an induced unbiased compressor. This transformation allows the application of existing methods designed for unbiased compressors. Our findings demonstrate that this approach significantly surpasses EF, offering enhancements such as reduced memory needs, improved communication complexity guarantees, and fewer prerequisites. Furthermore, we expand our results to federated learning scenarios involving partial participation based on an arbitrary distribution across the nodes and highlight the advantages of this extension. We conduct several numerical experiments that support our theoretical conclusions.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1802", "problem_id": "18020001", "content": "Achieving generalization across different environments is essential for effectively applying reinforcement learning algorithms to practical issues. This paper addresses the challenge of developing abstractions that can generalize within block MDPs—groups of environments characterized by a common latent state space and dynamic structure, albeit with differing observations. We utilize concepts from causal inference to introduce a method of invariant prediction aimed at learning model-irrelevant state abstractions (MISA) that can generalize to new observations within a multi-environment context. We demonstrate that, for specific types of environments, this method is likely to yield a state abstraction aligned with the causal feature set concerning the return. Additionally, we present broader bounds on model error and generalization error in a multi-environment framework, illustrating the relationship between causal variable selection and the state abstraction paradigm for MDPs. Our empirical results indicate that our approaches are effective in both linear and nonlinear contexts, achieving better generalization when compared to single- and multi-task benchmarks.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1803", "problem_id": "18030001", "content": "We investigate the capacity of neural networks to guide or regulate the trajectories of continuous time non-linear dynamical systems represented on graphs through neural ordinary differential equations (neural ODEs). To achieve this, we propose a neural-ODE control (NODEC) framework and demonstrate its ability to learn control signals that direct graph dynamical systems toward specified target states. Although we employ loss functions that do not limit the control energy, our findings indicate that NODEC generates control signals with low energy consumption. Lastly, we highlight the efficacy and adaptability of NODEC by applying it to manage a system comprising over a thousand interconnected non-linear ODEs.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1804", "problem_id": "18040001", "content": "Empirical Risk Minimization (ERM) algorithms are prevalent in signal processing and machine learning for estimation and prediction. Although widely used, a comprehensive theoretical understanding of their statistical behavior in contemporary scenarios—where the number of measurements and unknown parameters are both substantial—is a recent development. This paper introduces a novel characterization of the statistical accuracy limits of convex ERM applied to inference in high-dimensional generalized linear models. Focusing on a setting with Gaussian features and proportionally growing problem dimensions, we establish precise performance characterizations and derive tight lower bounds on estimation and prediction error applicable to a broad range of loss functions and regularization parameter values. This detailed analysis provides several key insights. First, it offers a method for optimizing the loss function and regularization parameter. Second, it enables the precise quantification of the sub-optimality of common heuristic selections; for example, optimally-tuned least-squares is shown to be approximately optimal for standard logistic data, but its sub-optimality increases significantly with signal strength. Third, the bounds are used to accurately evaluate the effectiveness of ridge-regularization relative to the over-parameterization ratio. Notably, our bounds are formulated using the Fisher Information of random variables, which are simple functions of the data distribution, thereby connecting to corresponding bounds in classical statistics.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1805", "problem_id": "18050001", "content": "The degradation of insulation systems is often signaled by partial discharges, which can be detected through various methods, although their reliability and selectivity are hindered by background noise levels. Background noise can obscure the partial discharge pattern, thereby reducing the effectiveness of detection methods in identifying characteristic features associated with insulation system degradation. To address this challenge, this paper presents a deep learning-based framework that incorporates innovative frequency and phase attention layers to identify partial discharge patterns in insulated overhead conductors. The proposed phase and frequency attention layers are designed to pinpoint the distinctive regions in signal spectrograms that are indicative of partial discharge activity, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1806", "problem_id": "18060001", "content": "This study introduces a novel generative adversarial network designed for pose transfer, enabling the transposition of a specified individual's pose onto a desired target pose. The network's generator incorporates a series of Pose-Attentional Transfer Blocks, each responsible for transferring specific attended regions, thereby progressively generating the person image. In comparison to prior research, the person images produced by our method exhibit enhanced appearance and shape consistency with the input images, resulting in a significantly more realistic visual outcome. The effectiveness and efficiency of the proposed network are demonstrated through both qualitative and quantitative evaluations using the Market-1501 and DeepFashion datasets. Additionally, the proposed architecture can be utilized to create training images for person re-identification, addressing the challenge of limited data availability. Codes and models are available at: https://github.com/tengteng95/Pose-Transfer.git.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1807", "problem_id": "18070001", "content": "Developing facial analysis systems capable of handling extreme variations in lighting and expressions remains a significant challenge, which synthetic data may help address. To this end, we introduce LEGAN, a novel synthesis framework that uses perceptual quality assessments to jointly modify lighting and expressions in face images without relying on paired training data. LEGAN separates lighting and expression features, applies transformations in the feature space, and then upscales to generate the output. A perceptual quality estimation model, trained on crowd-sourced naturalness ratings of images produced by various synthesis techniques, is incorporated as an auxiliary discriminator to enhance synthetic image realism. Evaluated using FID and LPIPS metrics, LEGAN outperforms models like StarGAN and StarGAN-v2 in generating high-quality facial images with varied lighting and expressions. A perceptual study confirms the alignment between our quality estimation and visual fidelity. Additionally, LEGAN proves effective as a data augmentation tool for improving expression recognition and face verification tasks.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1808", "problem_id": "18080001", "content": "The advent of reinforcement learning (RL) has led to notable successes in complex sequential decision-making problems, yet a key challenge remains: enabling RL to effectively handle partial observability, a common phenomenon in real-world scenarios. Unlike existing RL methods, which often rely on enhanced memory representations or make strong assumptions about the nature of partial observability, our proposed approach offers a straightforward yet effective solution that can be integrated with a broad range of RL techniques. By gradually introducing partial observability during training, our method, termed partially observable guided reinforcement learning (PO-GRL), yields high-performance policies while leveraging full state information for policy optimization without sacrificing optimality. A thorough examination of PO-GRL in various benchmark problems, including discrete partially observable Markov decision process (POMDP) and continuous partially observable MuJoCo and OpenAI gym tasks, reveals significant performance improvements. Furthermore, the efficacy of PO-GRL is demonstrated in a real-world setting, namely the ball-in-the-cup task on a Barrett WAM robot, under conditions of partial observability.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1809", "problem_id": "18090001", "content": "Although there have been remarkable achievements in video games like Atari 2600, current artificial intelligence still has not managed to defeat human champions in real-time strategy (RTS) games. This challenge arises because RTS games are multi-agent environments, where traditional single-agent reinforcement learning approaches fall short, as they do not account for the dynamic nature of the game, which is not a stationary Markov Decision Process. In this paper, we introduce an initial approach to developing a game-theoretic solution for RTS games by utilizing Neural Fictitious Self-Play (NFSP), a method aimed at identifying Nash equilibria, and applying it to Mini-RTS, a compact yet complex RTS game available on the ELF platform. Specifically, we demonstrate that NFSP can be effectively integrated with policy gradient reinforcement learning techniques for implementation in Mini-RTS. Our experimental findings further indicate that the scalability of NFSP can be significantly enhanced through pretraining the models with straightforward self-play using policy gradients, which, despite lacking a theoretical convergence guarantee, produces a robust strategy.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1810", "problem_id": "18100001", "content": "In recent times, deep generative models have demonstrated the ability to 'envision' realistic high-dimensional data, including images, audio, and video, by learning directly from unprocessed information. This study explores how to conceive goal-oriented visual plans—plausible sequences of observations that facilitate the transition of a dynamical system from its existing state to a specific target state, which can subsequently serve as a reference trajectory for control purposes. Our emphasis is on systems characterized by high-dimensional observations, such as images, and we introduce an approach that seamlessly integrates representation learning with planning. Our framework develops a generative model for sequential observations, where the generative process is driven by transitions in a low-dimensional planning model along with some added noise. By maximizing the mutual information between the produced observations and the transitions in the planning model, we achieve a low-dimensional representation that most effectively captures the causal relationships in the data. We design the planning model to align with efficient planning algorithms and put forth several models that utilize either discrete or continuous states. Ultimately, to create a visual plan, we map the current and target observations onto their corresponding states within the planning model, formulate a trajectory, and then employ the generative model to convert this trajectory into a sequence of observations. We validate our technique by imagining realistic visual plans for rope manipulation tasks.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1811", "problem_id": "18110001", "content": "Neural networks have demonstrated effectiveness in time series prediction, showcasing a strong ability to approximate functions accurately. While many neural models in these applications employ activation functions with fixed parameters, the selection of activation functions significantly impacts the neural network's complexity and performance, yet only a limited range of such functions has been explored. This study introduces a family of asymmetric activation functions with free parameters for neural networks, proving that this family meets the criteria of the universal approximation theorem. A methodology is presented for the global optimization of both these free-parameter activation functions and the connection weights within the neural network. The core concept involves simultaneously optimizing the weights and the activation function within a multilayer perceptron (MLP) network, leveraging a hybrid approach combining simulated annealing, tabu search, and a local learning algorithm to enhance time series fitting and forecasting performance. Backpropagation with momentum (BPM) and Levenberg-Marquardt (LM) algorithms were selected as the learning algorithms.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1812", "problem_id": "18120001", "content": "Estimating optical flow in the presence of occlusions or substantial displacement remains difficult because corresponding pixels are absent between successive frames. This study reveals that this information loss stems from the disappearance of a large proportion (over 40%) of motion features, calculated from a discriminative cost volume, because of invalid sampling, which reduces optical flow learning efficiency. This issue is termed the Vanishing Cost Volume Problem. Drawing inspiration from the consistency of local motion within brief periods, an iterative Motion Feature Recovery (MFR) method is introduced to mitigate the vanishing cost volume by modeling motion consistency across multiple frames. Each MFR iteration identifies invalid entries from the original motion features using the current flow. Subsequently, a network adaptively learns motion correlations to restore invalid features, thereby recovering lost information. The final optical flow is decoded from these recovered motion features. Experimental results on Sintel and KITTI datasets demonstrate state-of-the-art performance, with MFR currently holding the second position on the Sintel public leaderboard.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1813", "problem_id": "18130001", "content": "Contemporary visual question answering techniques depend on extensively annotated datasets. Recognizing the challenges of manual annotation for videos—namely, its labor-intensive nature, high cost, and limitations to scalability—this study introduces a method to circumvent manual annotation by creating a large-scale video question answering training dataset using automated cross-modal supervision. A question generation transformer, pre-trained on textual data, is employed to produce question-answer pairs from video narration transcripts. This approach facilitates the automatic generation of the HowToVQA69M dataset, comprising 69 million video-question-answer triplets, from narrated videos. To address the open vocabulary challenge posed by the diverse answers within this dataset, we present a training regimen founded on a contrastive loss mechanism between a video-question multi-modal transformer and an answer transformer. We introduce the zero-shot VideoQA task and show excellent results, in particular for rare answers. Moreover, our method markedly surpasses current state-of-the-art performance on MSRVTT-QA, MSVD-QA, ActivityNet-QA and How2QA. For comprehensive assessment, we also introduce iVQA, a novel VideoQA dataset designed with diminished language biases and enriched with high-quality, redundant manual annotations. Our code, datasets and trained models are available at https://antoyang.github.io/just-ask.html.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1814", "problem_id": "18140001", "content": "Image inpainting, a vibrant and appealing research area, addresses the challenge of eliminating or rectifying flaws in digital images and videos. This technique finds diverse applications, including the restoration of scratched vintage photographs, the removal of unwanted text and logos, and the generation of stylized cartoon-like effects. In this paper, we introduce a novel and efficient approach for image restoration that leverages a non-linear diffusion tensor. The core principle involves accurately tracing the local geometric structure of the damaged image, thereby enabling diffusion solely along the direction of isophote curves. We present experimental results using both benchmark and authentic photographic color images to demonstrate the effectiveness and performance of our proposed method.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1815", "problem_id": "18150001", "content": "The Neural Tangent Kernel (NTK) describes the dynamics of infinitely wide neural networks optimized via gradient descent with least squares loss. Although significant, the super-quadratic computational cost of kernel methods restricts NTK's applicability to large-scale learning problems. To enhance the efficiency of NTK-based kernel machines, we introduce an algorithm with near-linear runtime in input sparsity, projecting data into a randomized low-dimensional feature space where inner products approximate NTK evaluations. This approach leverages sketching techniques on polynomial expansions of arc-cosine kernels. Additionally, we present a feature transformation for the convolutional NTK, enabling image processing in linear time relative to pixel count. Empirical results demonstrate that linear regression on our features surpasses trained neural networks and Nystrom-based NTK approximations in large-scale regression and classification tasks.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1816", "problem_id": "18160001", "content": "Transformers have shown remarkable capability in tasks related to computer vision. To mitigate the intensive computations associated with self-attention in high-resolution visual inputs, some newer Transformer architectures utilize a hierarchical approach, performing self-attention calculations solely within localized windows. While this strategy considerably enhances efficiency, it falls short in facilitating global feature reasoning during the initial phases. In this study, we introduce a multi-path structure for the Transformer that permits local-to-global reasoning across various levels of granularity at each stage. The proposed architecture is both computationally efficient and highly effective. With only a slight increase in computational requirements, our model exhibits significant advancements in image classification and semantic segmentation. Code is accessible at https://github.com/ljpadam/LG-Transformer.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1817", "problem_id": "18170001", "content": "Imaging phantoms serve as standardized test objects for assessing image quality in computed tomography (CT) scanners. The Mercury Phantom (Gammex) offers a versatile platform with patterns designed to evaluate task transfer function (TTF) and noise power spectrum (NPS), while also emulating various patient anatomies. Currently, experts must manually identify suitable image slices for analysis, as minor imperfections can render them unfit for evaluation. This study introduces an automated approach for classifying test patterns in phantom image sequences using deep learning. Leveraging transfer learning, we modify a VGG19-based convolutional neural network, initially trained on ImageNet, to create a specialized classifier. The model is trained and tested on a dataset of more than 3,500 phantom images collected from a university hospital. The network's input channels are effectively repurposed to encode relevant contextual details from phantom scans. Through ablation studies, we validate the classifier's design choices and assess its robustness across different training scenarios. By incorporating extensive image augmentation, our method achieves 98% accuracy in classifying standard phantom images and maintains 86% accuracy even with suboptimal imaging conditions.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1818", "problem_id": "18180001", "content": "Although deep learning has demonstrated groundbreaking results in 3D applications, the resilience of these models remains understudied. Current research on 3D adversarial attacks primarily targets local point modifications, often neglecting global geometric properties such as isometric robustness under linear transformations that preserve Euclidean distances. This study reveals that leading deep 3D models are highly susceptible to isometry-based attacks. Leveraging Thompson Sampling, we introduce a black-box attack achieving over 95% success on the ModelNet40 dataset. Additionally, by integrating the Restricted Isometry Property, we design a novel white-box attack framework using spectral norm perturbations. Unlike prior methods, our adversarial samples exhibit strong transferability. When tested on prominent 3D models, our white-box attack achieves success rates between 98.88% and 100%, maintaining over 95% effectiveness even within subtle rotation bounds [\\pm 2.81^].", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1819", "problem_id": "18190001", "content": "This paper introduces a new architecture designed for the automatic anonymization of facial images, ensuring the preservation of the original data distribution. The method achieves complete anonymization by generating images solely from privacy-protected information. Utilizing a conditional generative adversarial network, the model synthesizes images that account for the original pose and background, allowing for the creation of highly realistic faces that seamlessly integrate with the existing background. In addition, a comprehensive dataset of human faces is introduced, encompassing diverse poses, occlusions, and background variations. Experimental results demonstrate the model's ability to anonymize images while maintaining the data distribution, making the data appropriate for subsequent deep learning model training. To the best of our knowledge, this is the first solution that simultaneously guarantees facial anonymization and generates realistic images.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1820", "problem_id": "18200001", "content": "Sign language recognition (SLR) is essential for closing the communication divide between the hearing and vocally impaired community and the broader society. Word-level sign language recognition (WSLR) serves as a foundational step in comprehending and interpreting sign language. Nevertheless, the task of recognizing signs from videos presents challenges, as the interpretation of a word relies on an interplay of intricate body movements, hand positions, and other gestures. Recent pose-based frameworks for WSLR either address both spatial and temporal dependencies in different frames concurrently or focus solely on temporal dynamics without fully leveraging spatial elements. We approach the WSLR challenge through an innovative pose-based method that separates the capture of spatial and temporal information and employs late fusion. Our architecture leverages a Graph Convolutional Network (GCN) to effectively represent spatial interactions in the video, while the temporal relationships across frames are modeled using Bidirectional Encoder Representations from Transformers (BERT). Results from our experiments on WLASL, a well-established word-level sign language recognition dataset, demonstrate that our model surpasses previous state-of-the-art pose-based techniques, achieving up to a 5% increase in prediction accuracy.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1821", "problem_id": "18210001", "content": "This study addresses the challenge of identifying significant associations in networks derived from high-throughput molecular data, which has become a valuable tool for understanding biological pathways. While graphical models and networks are useful abstractions, current methods often rely on arbitrary thresholds when learning network structures. To address this limitation, we introduce a statistically principled method for determining significant associations. Our approach estimates a threshold by minimizing the L norm between the empirical cumulative distribution function (CDF) of observed edge confidences and its asymptotic counterpart. We validate the method's performance on synthetic datasets and experimental molecular data, specifically gene and protein expression profiles. Results demonstrate improved performance in terms of sensitivity, specificity, and accuracy across various synthetic datasets, sample sizes, and structure learning algorithms. Notably, the proposed approach consistently achieves high specificity and accuracy, with sensitivity increasing logarithmically with sample size. Furthermore, the estimated threshold outperforms ad-hoc thresholds in sensitivity while maintaining comparable specificity and accuracy. The reconstructed networks from experimental data align with findings reported in the original publications.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1822", "problem_id": "18220001", "content": "A crucial challenge in agricultural robotics is selective weeding, which requires a farm robot to accurately identify plants and differentiate between crops and weeds. Current state-of-the-art methods rely on appearance-based models trained on extensive annotated datasets, but creating such datasets with pixel-level annotations is a labor-intensive process that hinders the application of data-driven techniques. To address this issue, this paper proposes a novel approach that significantly reduces the need for human intervention in training detection and classification algorithms by generating large synthetic training datasets. This is achieved by procedurally randomizing key features of the target environment, such as crop and weed species, soil type, and light conditions, and leveraging real-world textures to render realistic views of artificial agricultural scenarios. The generated data can be used to train models or supplement real-world images, and its effectiveness is validated using a deep learning-based image segmentation architecture, with results comparing the use of real and synthetic images as training data, demonstrating the potential of this approach, as shown in Figure A, B, C (References [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1823", "problem_id": "18230001", "content": "Automated decision-making is increasingly prevalent in processes that affect our lives, with algorithms using feature vectors from applicants to determine outcomes. This paper introduces a straightforward concept that empowers applicants by offering alternative scenarios that would lead the decision algorithm to a different conclusion. Utilizing a formalization similar to evasion attack methodologies, the approach involves identifying subspaces where the classifier yields the preferred output. This method has been implemented for decision forests (ensemble methods based on decision trees), transforming the problem into an iterative enumeration of k-cliques.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1824", "problem_id": "18240001", "content": "Due to the widespread adoption of Knowledge Graphs (KGs), considerable research has been dedicated to link prediction for knowledge graph completion. However, most studies concentrate on binary relational data, representing facts as (head entity, relation, tail entity) triples, while overlooking the prevalence of n-ary relational facts. Current approaches typically decompose n-ary facts into triples, generating numerous virtual entities and supplementary triples, thereby complicating link prediction and potentially losing structural information. To address these limitations, this paper introduces a representation of n-ary relational facts as sets of role-role-value pairs. We propose a method, NaLP, for link prediction on n-ary relational data that explicitly models the relationships between all role-role-value pairs within a fact. NaLP is further enhanced by incorporating role and role-value type constraints without external type-specific supervision, alongside a refined negative sampling strategy. Empirical evaluations confirm the efficacy and advantages of the presented methodologies.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1825", "problem_id": "18250001", "content": "Semantic image segmentation represents a critical and challenging task in computer vision, as it involves both recognition and segmenting images. Typically approached as a structured output problem within a vast output space, it often employs a discrete probabilistic model. The optimal segmentation is determined by inferring the Maximum a-Posteriori (MAP) solution from the model's defined output distribution. However, due to optimization constraints, the complexity of the model is limited, resulting in a compromise: create a highly accurate model that reflects intricate high-order interactions among image components, which may lead to either inaccurate or intractable optimization, or use a manageable model that offers less precise MAP solutions but can yield high-quality outcomes from other distribution modes. This thesis explores the latter approach and introduces a two-stage method for semantic segmentation. In the first stage, a manageable segmentation model produces a collection of high-probability segmentations that are significantly different from one another rather than mere variations. Importantly, this stage yields a diverse array of viable solutions instead of a solitary one. The second stage employs a discriminatively trained re-ranking model to identify the best segmentation from this collection. The re-ranking phase can incorporate more intricate features than those feasible within the segmentation model, thereby enhancing the exploration of the solution space beyond simply retrieving the MAP solution. The framework is independent of the specific segmentation model (such as CRF, CNN, etc.) and optimization technique, making it versatile for various models and inference methods. Assessment of this approach on multiple benchmark datasets for semantic image segmentation demonstrates its advantages over the conventional MAP solution inference.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1826", "problem_id": "18260001", "content": "Multi-label classification of images and videos represents a fundamental yet complex area within computer vision. The primary difficulties stem from accurately capturing the spatial or temporal dependencies among labels and identifying the positions of distinguishing features for each class. To address these issues, we introduce a cross-modality attention mechanism combined with semantic graph embedding for multi-label classification. Utilizing the constructed label graph, we present an adjacency-based similarity graph embedding technique to derive semantic label embeddings that explicitly utilize the relationships between labels. Subsequently, our innovative cross-modality attention maps are created based on the learned label embeddings. Experiments conducted on two multi-label image classification datasets (MS-COCO and NUS-WIDE) indicate that our approach surpasses other existing leading techniques. Furthermore, we assess our method on a substantial multi-label video classification dataset (YouTube-8M Segments), and the results confirm the robustness and generalization of our approach.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1827", "problem_id": "18270001", "content": "In numerous domains, particularly those involving streaming applications, unlabelled data is prevalent, while labelled data is scarce, despite the abundance of available data. To tackle the challenges posed by such data, several approaches can be employed, including supervised learning, which relies solely on labelled data; semi-supervised learning, which utilizes both labelled and unlabelled data to improve performance; and active learning, which assumes that labels can be obtained on demand. However, each approach has its limitations: supervised learning is constrained by the limited amount of labelled data, semi-supervised learning requires identifying and exploiting the underlying data distribution characteristics, and active learning relies on timely label provision by an external agent. This survey focuses on semi-supervised methods that harness unlabelled data, and also explores the issue of delayed labelling, which affects both supervised and semi-supervised approaches, as seen in Figure A, B, C (References [citation]). A unified problem framework is proposed, and the learning guarantees, existing methods, and differences between related problem settings are discussed, as mentioned in [citation]. Additionally, current benchmarking practices are reviewed, and adaptations are proposed to enhance them.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1828", "problem_id": "18280001", "content": "Graph matching is a significant problem, particularly in computer vision, and has garnered considerable interest. Recent advanced techniques aim to combine graph matching with deep learning methodologies. However, existing research lacks an explanation regarding the specific function of the graph matching algorithm within these models. To address this, we introduce a method that integrates a mixed-integer linear programming (MILP) representation of the graph matching problem, which is solved to optimality, thus providing an inherent baseline. Furthermore, analogous methods are developed by relaxing the optimality constraint of the graph matching solver and by incorporating a quality level. This quality level regulates the caliber of solutions generated by the graph matching solver. Moreover, several relaxations of the graph matching problem are examined. The experimental results offer several theoretical perspectives and inform the development of deep graph matching techniques.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1829", "problem_id": "18290001", "content": "The estimation of parameters in latent graphical models is significantly more challenging than in non-latent models due to the non-concave nature of the corresponding log-likelihood caused by latent variables. Although expectation-maximization approaches are widely used in practice, they often converge to local optima. Recently, the method of moments has offered a new perspective on addressing the non-convexity issue, but its applicability is limited to a specific class of latent graphical models. This paper seeks to extend the scope of this method by broadening the class of latent graphical models it can handle. To achieve this, we propose two innovative concepts, marginalization and conditioning, which enable the reduction of learning a large graphical model to learning a smaller one. Notably, these concepts give rise to a sequential learning framework that incrementally expands the learnable portion of a given latent graphical model, thereby covering a substantially wider range of complex loopy latent graphical models, including convolutional and random regular models, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1830", "problem_id": "18300001", "content": "Recent studies indicate that convolutional neural networks tend to prioritize texture information over shape features in classification tasks. We introduce a novel distinction between shape and local image cues versus global image statistics. Our proposed technique, Permuted Adaptive Instance Normalization (pAdaIN), diminishes the influence of global statistics in classifier hidden layers. pAdaIN employs a random permutation \\(\\pi\\) to rearrange batch samples, followed by Adaptive Instance Normalization (AdaIN) between each sample \\(i\\) and its permuted counterpart \\(\\pi(i)\\), thereby exchanging statistical properties. This process disrupts global statistics, forcing the network to focus on alternative cues like shape or texture. By adjusting the probability \\(p\\) of applying the permutation—selected beforehand without test data influence—we regulate the method's intensity. With an optimal fixed \\(p\\), our approach consistently surpasses baselines across diverse scenarios. It enhances performance on CIFAR100 and ImageNet for various architectures in classification tasks, improves robustness on ImageNet-C and CIFAR-100-C, and achieves state-of-the-art results in domain adaptation and generalization, particularly in GTAV-to-Cityscapes transfer learning and the PACS benchmark.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1831", "problem_id": "18310001", "content": "Our team took part in Task 1: Lesion Segmentation as part of the ISIC Challenge 2018, which focuses on Skin Lesion Analysis for Melanoma Detection. This paper outlines our proposed methodology and presents the validation set outcomes achieved by our algorithm.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1832", "problem_id": "18320001", "content": "Inferring the structure of causal graphical models using both observational and interventional datasets is a crucial task across various scientific disciplines. Score-based methods utilizing continuous optimization offer a compelling approach, facilitating efficient, data-driven causal graph discovery. Nevertheless, current techniques either necessitate constrained optimization to ensure acyclicity or lack assurances of convergence. This paper introduces ENCO, an efficient structure learning method designed for directed, acyclic causal graphs, which capitalizes on both observational and interventional data. ENCO frames the graph search as an optimization problem involving independent edge likelihoods, where edge orientation is treated as a distinct parameter. This formulation allows us to provide convergence guarantees for ENCO under reasonable assumptions, without imposing acyclicity constraints on the score function. Experimental results demonstrate that ENCO can effectively reconstruct graphs containing hundreds of nodes, representing a significant improvement in scale compared to existing methods, while also accommodating deterministic variables and latent confounders.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1833", "problem_id": "18330001", "content": "This dataset offers high-definition images of authentic, completed bank checks featuring diverse intricate backgrounds, handwritten text, and signatures in designated fields, accompanied by pixel-level and patch-level segmentation masks for the signatures. The check images were sourced from multiple origins, such as existing public check datasets, online images, and scanned or photographed real checks. Signature segmentation masks were manually created as binary images using GIMP, followed by automated generation of patch-level masks. The dataset is designed to facilitate the training and evaluation of networks aimed at signature extraction from bank checks and comparable documents with highly complex backgrounds.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1834", "problem_id": "18340001", "content": "The conventional design of deep neural networks relies on extensive human-driven experimentation, demanding significant time, specialized knowledge, and computational resources. To mitigate this issue, we introduce a new algorithm for the automated optimization of deep network hyperparameters, specifically for medical image segmentation. Our method employs policy gradient reinforcement learning, where the reward function is defined by a segmentation evaluation metric, namely the dice index. The efficiency of our approach is demonstrated through its reduced computational demands compared to existing state-of-the-art medical image segmentation networks. Furthermore, we introduce a novel densely connected encoder-decoder CNN architecture, serving as a robust baseline for implementing our hyperparameter search algorithm. We apply this algorithm to each layer of the baseline architecture and evaluate its performance using cine cardiac MR images from the Automated Cardiac Diagnosis Challenge (ACDC) MICCAI 2017. The resulting network, derived from a baseline segmentation architecture, achieves state-of-the-art accuracy without the need for manual trial-and-error or intensive supervision of hyperparameter adjustments.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1835", "problem_id": "18350001", "content": "This study presents a method for predicting a driver's intent at a road intersection. Our approach employs a deep bidirectional Long Short-Term Memory (LSTM) model enhanced by an attention mechanism, which operates within a hybrid-state system (HSS) framework. Given that intersections are significant contributors to road accidents, accurately forecasting a driver's intentions in these scenarios is essential. Our technique utilizes sequence-to-sequence modeling combined with an attention mechanism to effectively harness temporal information from time-series vehicle data, such as speed and yaw-rate. The model anticipates whether the target vehicle or driver will proceed straight, stop, or make a right or left turn well in advance. We assess the effectiveness of our proposed method using a naturalistic driving dataset, which demonstrates that our approach not only achieves high accuracy but also surpasses other existing methods. This innovative solution shows great potential for integration into advanced driver assistance systems (ADAS) and as a component of the active safety systems in autonomous vehicles.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1836", "problem_id": "18360001", "content": "This research broadens the scope of composed image retrieval, a task that involves retrieving images based on an input query comprising an image and a concise textual description detailing the desired modifications. Current methodologies are confined to relatively simple images within specific domains, like fashion, which restricts the investigation of intricate visual reasoning within comprehensive image and language contexts. To overcome this limitation, we introduce the CIRR (Compose Image Retrieval on Real-life images) dataset, featuring over 36,000 pairs of open-domain images and corresponding human-authored text modifications sourced via crowdsourcing. To adapt existing approaches to the open domain, we present CIRPLANT, a transformer-based model that utilizes extensive pre-trained vision-and-language (V&L) knowledge to adjust visual features according to natural language instructions, followed by retrieval using nearest neighbor search on the adjusted features. Experimental results reveal that CIRPLANT, despite its comparatively uncomplicated architecture, surpasses current methods on open-domain images while maintaining state-of-the-art precision on existing constrained datasets, such as fashion. We anticipate that the release of CIRR, along with this work, will stimulate further research in the field of composed image retrieval.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1837", "problem_id": "18370001", "content": "The utilization of satellite imagery is crucial across various sectors like disaster management, law enforcement, and environmental oversight, all of which necessitate the identification of objects and infrastructure within the images. Due to the vast areas to be analyzed and a limited number of analysts, automated solutions are essential. However, conventional object detection and classification methods lack the necessary precision and dependability. Deep learning, a subset of machine learning techniques, presents a promising alternative for automating these tasks, demonstrating notable achievements in image analysis through convolutional neural networks. This study explores the application of deep learning to identify objects and facilities in high-resolution, multi-spectral satellite imagery. We present a deep learning framework designed to categorize objects and facilities from the IARPA Functional Map of the World (fMoW) dataset into 63 distinct categories. This framework comprises an ensemble of convolutional neural networks, complemented by additional neural networks that incorporate satellite metadata with image-derived features. Developed in Python using the Keras and TensorFlow deep learning libraries, the system operates on a Linux server equipped with an NVIDIA Titan X graphics card. Currently, the system holds the 2nd position in the fMoW TopCoder competition, achieving an overall accuracy of 83%, an F1 score of 0.797, and demonstrating classification accuracies of 95% or higher for 15 classes.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1838", "problem_id": "18380001", "content": "We introduce Matrix Nets (xNets), a novel deep learning framework designed for object detection. xNets organize objects of varying sizes and aspect ratios into distinct layers, ensuring uniformity in these dimensions within each layer. This design results in a structure that inherently accounts for scale and aspect ratio variations. By applying xNets to key-point-based detection, our model attains a 47.8 mAP on MS COCO, surpassing all single-shot detectors in performance. Additionally, it utilizes 50% fewer parameters and trains three times faster than the closest competing architecture.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1839", "problem_id": "18390001", "content": "Shot boundary detection constitutes a critical step in video data analysis. This paper introduces a novel shot boundary detection technique leveraging multiple video characteristics, including color histograms and object boundaries. The performance of the developed algorithm was evaluated using publicly available datasets, namely BBC Planet Earth [1] and RAI [2], alongside the MSU CC datasets, which are derived from videos utilized in codec comparisons at MSU. Furthermore, videos from the IBM set were incorporated into the evaluation. The resultant dataset employed for both developing and testing the algorithm surpassed the size of existing TRECVID datasets. Empirical results demonstrate that the proposed scene change detection algorithm exhibits superior performance compared to existing methods, achieving a final F-score of 0.9794.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1840", "problem_id": "18400001", "content": "Recent advancements in disparity estimation primarily rely on the 4D concatenation volume and employ a highly deep 3D convolutional neural network (CNN) for disparity regression, which is often inefficient in terms of memory usage and processing speed. In this study, we introduce EDNet, a novel network designed for efficient disparity estimation. To begin with, we create a combined volume that merges contextual information from the squeezed concatenation volume with feature similarity assessment from the correlation volume. This combined volume can then be processed using 2D convolutions, which are quicker and more memory-efficient than their 3D counterparts. Additionally, we present an attention-based spatial residual module that generates features with attention-aware residuals. The attention mechanism offers valuable spatial insights regarding inaccurate areas by utilizing error maps at various scales, thereby enhancing the efficiency of residual learning. Comprehensive experiments on the Scene Flow and KITTI datasets demonstrate that EDNet surpasses earlier 3D CNN-based methods and achieves leading performance while consuming significantly less memory and operating at a faster speed.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1841", "problem_id": "18410001", "content": "We introduce an approach for creating 3D shapes from point clouds. Our technique begins by constructing a kd-tree to spatially organize the points of a given point-cloud representation, ensuring consistent ordering across different shapes to establish reliable correspondences. Using PCA analysis, we develop a linear shape basis for these spatially arranged points and refine the point ordering by iteratively reducing PCA reconstruction errors. Despite spatial sorting, point clouds remain noisy, leading to a multi-modal distribution of shape coefficients. To address this, we employ neural networks within a generative-adversarial framework to model the distribution of these coefficients. Unlike voxel-based 3D shape generation models, our point-based approach is more efficient and scalable while maintaining high quality. It also surpasses simpler linear models like Probabilistic PCA, both in visual and numerical evaluations, across multiple ShapeNet categories. Additionally, our method seamlessly integrates extra point attributes such as normals and colors, offering an advantage over voxel-based representations.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1842", "problem_id": "18420001", "content": "Modern machine learning algorithms, while empirically successful across numerous applications, are vulnerable to minor, often imperceptible alterations in input data, known as adversarial attacks. Recent adversarial training methods aim to address this vulnerability. However, while these methods improve accuracy against adversarially manipulated inputs (robust accuracy), they frequently diminish accuracy on unaltered, natural inputs (standard accuracy). The impact of adversarial training on standard and robust accuracy is complex, varying significantly based on factors such as the type of perturbation used during training, data size and quality, and model overparameterization. This study examines binary classification problems with data generated from a mixture of two Gaussians with anisotropic covariance matrices, providing a detailed characterization of standard and robust accuracy for a class of minimax adversarially trained models. We analyze a general norm-based adversarial model, where perturbations of bounded \\ell_p norm can be added to each input, with p\\ge 1. Our analysis theoretically elucidates several empirical observations and offers insights into how various problem parameters affect standard and robust accuracies.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1843", "problem_id": "18430001", "content": "We present a framework designed to compare data manifolds, with a specific focus on evaluating deep generative models. This framework incorporates Cross-Barcode(P,Q), a new tool that monitors multiscale topological and spatial differences between manifolds where two high-dimensional distributions are concentrated. Utilizing the Cross-Barcode, we define the Manifold Topology Divergence score (MTop-Divergence) and use it to evaluate the effectiveness of deep generative models across diverse fields such as images, 3D shapes, and time series, and using various datasets, including MNIST, Fashion MNIST, SVHN, CIFAR10, FFHQ, chest X-ray images, market stock data, and ShapeNet. The MTop-Divergence is shown to precisely identify different levels of mode-dropping, intra-mode collapse, mode invention, and image disturbance. The algorithm exhibits favorable scaling properties, increasing approximately linearly with the dimensionality of the high-dimensional space. As a practical methodology grounded in Topological Data Analysis (TDA), it offers broad applicability across datasets of varying sizes and dimensions, including those used to train state-of-the-art GANs in the visual domain. Notably, the proposed method remains domain-agnostic and does not depend on pre-trained networks.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1844", "problem_id": "18440001", "content": "We examine the challenge of mapping a relation, depicted as a directed graph, into a Euclidean space. For three distinct embedding approaches inspired by recent research on knowledge graphs, we establish criteria for the types of relations they can represent, along with limits on the required dimensionality and numerical precision.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1845", "problem_id": "18450001", "content": "Imitation Learning focuses on learning an expert policy from observed demonstrations. Although inverse reinforcement learning methods are highly efficient in utilizing expert samples, they often depend on customized reward functions or specialized reward regularization for specific tasks. This study establishes a natural link between inverse reinforcement learning and Optimal Transport, allowing for more versatile reward functions with beneficial characteristics such as smoothness. Building on this insight, we introduce Wasserstein Adversarial Imitation Learning, which uses Kantorovich potentials as reward functions and employs regularized optimal transport to facilitate scalability. In various robotic experiments, our method surpasses baseline approaches in cumulative reward performance and demonstrates notable sample-efficiency, achieving strong results with only a single expert demonstration.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1846", "problem_id": "18460001", "content": "Breast cancer classification is difficult because of similarities between different cancer subtypes and variations within the same subtype. Current deep learning approaches address this by employing intricate nonlinear transformations. However, these methods often rely on global features from entire images, overlooking the importance of subtle details for identifying distinguishing characteristics. This paper introduces a new method, the Attention Model Enhanced Network (AMEN), designed with a multi-branch architecture incorporating a pixel-wise attention model and a classification submodular. AMEN's feature learning component creates pixel-wise attention maps, and the classification submodular classifies samples. To emphasize subtle details, the sample image is enhanced using the pixel-wise attention map from a previous branch. Additionally, a boosting strategy combines classification results from different branches to improve overall performance. Experiments on three benchmark datasets demonstrate the proposed method's superior performance in various situations.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1847", "problem_id": "18470001", "content": "This research investigates the multiple manifold problem, a binary classification task inspired by machine vision applications, where a deep fully-connected neural network is trained to distinguish between two low-dimensional submanifolds of the unit sphere. An analysis of the one-dimensional case reveals that for a simple manifold configuration, when the network depth is sufficiently large compared to the geometric and statistical properties of the data, a randomly-initialized gradient descent can achieve perfect classification with high probability, provided the network width grows polynomially with the depth and the number of i.i.d. samples is polynomial in the depth. The study highlights the benefits of depth and width in this context, demonstrating that depth serves as a fitting resource, allowing smoother networks to separate class manifolds more effectively, while width acts as a statistical resource, enabling concentration of the network and its gradients. By leveraging the neural tangent kernel, the analysis establishes essentially optimal rates of concentration for deep fully-connected networks, requiring a width of n \\gtrsim L\\,\\mathrm(d_0) to achieve uniform concentration over a d_0-dimensional submanifold of the unit sphere, and provides a nonasymptotic framework for generalization in the NTK regime with structured data, utilizing martingale concentration to optimally address statistical dependencies across layers of the initial random network, with potential applications to other network architectures, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1848", "problem_id": "18480001", "content": "Deep convolutional neural networks excel with images containing spatially invariant noise (synthetic noise); however, their efficacy diminishes with real noisy images and necessitates multi-stage network designs. To enhance the practicality of denoising algorithms, this study introduces a new single-stage blind real image denoising network (RIDNet) that utilizes a modular architecture. We implement a residual-on-residual structure to facilitate the transfer of low-frequency information and employ feature attention to leverage channel relations. Additionally, our evaluation, measured through quantitative metrics and visual quality across three synthetic and four real noisy datasets compared to 19 leading algorithms, illustrates the superiority of RIDNet.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1849", "problem_id": "18490001", "content": "Blur detection involves distinguishing between blurred and sharp areas within an image, a significant and challenging aspect of computer vision. In this study, we approach blur detection as an image segmentation issue. Drawing inspiration from the efficacy of the U-net model for image segmentation, we introduce a Multi-Scale Dilated convolutional neural network based on U-net, which we refer to as MSDU-net. This network employs a set of multi-scale feature extractors utilizing dilated convolutions to capture texture information across various scales. The U-shaped design of MSDU-net integrates these multi-scale texture features, producing a semantic representation that enhances performance in the blur detection task. Our results demonstrate that MSDU-net surpasses existing state-of-the-art blur detection techniques on two publicly accessible benchmarks.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1850", "problem_id": "18500001", "content": "Fingerprint recognition is widely utilized in various commercial applications, such as mobile phone authentication, and serves millions of users daily. The initial stage in most fingerprint processing algorithms involves segmentation, which separates the image into foreground (the relevant region) and background. Errors in this step can degrade recognition accuracy in two ways: genuine foreground may be incorrectly marked as background, leading to the loss of critical features like minutiae, or background may be misidentified as foreground, introducing false features. This paper presents three key contributions: first, we introduce a new factorized directional bandpass (FDB) segmentation technique, leveraging a directional Hilbert transform of a Butterworth bandpass (DHBB) filter combined with soft-thresholding for texture extraction. Second, we offer a manually annotated ground truth dataset of 10,560 images for benchmarking. Third, we perform a comprehensive evaluation comparing the FDB approach with four prominent fingerprint segmentation methods, demonstrating its superior performance. The FDB implementation and benchmark dataset are publicly accessible.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1851", "problem_id": "18510001", "content": "We present a novel virtual framework for simulating a card game called \"Big 2.\" This game accommodates four players and involves imperfect information, featuring a complex action space where players can play combinations of 1 to 5 cards from an initial hand of 13 cards. Consequently, this complexity presents a challenge for many existing reinforcement learning techniques. We apply the recently introduced \"Proximal Policy Optimization\" algorithm to train a deep neural network for playing the game, relying entirely on self-play. Our findings indicate that the network can achieve a level of performance that surpasses that of amateur human players after a relatively brief training period and without the necessity of exploring a tree of potential future game states.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1852", "problem_id": "18520001", "content": "Analog computing hardware, including Processing-in-memory (PIM) accelerators, has gained increasing interest for speeding up neural network operations. Nevertheless, these accelerators frequently encounter inherent noise in their physical components, hindering neural networks from matching their digital counterparts in performance. Prior approaches to addressing intrinsic noise relied on predefined noise models and necessitated model retraining. This work introduces a noise-agnostic technique to enhance neural network robustness across varying noise conditions. Our analysis reveals that performance decline stems from activation distribution shifts induced by noise. To address this, we develop a \"noise-aware\" batch normalization layer that adjusts activation distributions under analog hardware noise variations. The proposed method is straightforward, adaptable to diverse noise scenarios, and eliminates retraining requirements. Evaluations on vision tasks—classification, object detection, and semantic segmentation—confirm its efficacy, showing consistent performance across noise levels and outperforming existing solutions. This simple yet versatile approach could promote broader adoption of analog computing for neural networks.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1853", "problem_id": "18530001", "content": "It is widely believed that the performance constraints of Graph Convolutional Networks (GCNs) and the inability to stack additional layers for improved results, as commonly done in other deep learning approaches, stem from inherent limitations of GCN layers, such as restricted expressive power. However, if this were true, altering only the training process for a fixed architecture would unlikely reduce training challenges or enhance performance—yet this paper demonstrates that such improvements are not only feasible but achievable through multiple strategies. We first analyze GCN training difficulties by examining graph signal energy loss, revealing that backward-pass energy dissipation hinders learning in layers near the input. Subsequently, we introduce several techniques to address this issue by refining the GCN operator from an energy-based perspective. Empirical results confirm that these operator adjustments substantially ease training challenges and boost performance without modifying parameter composition. These findings suggest that training difficulty, rather than other factors, is the primary obstacle.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1854", "problem_id": "18540001", "content": "Recent advancements in image semantic segmentation have spurred progress in video semantic segmentation. However, per-frame segmentation remains impractical due to its high computational demands. Existing approaches often employ flow-based feature propagation to reuse features from prior frames, but inaccuracies in optical flow estimation lead to distorted propagated features. To address this, we introduce distortion-aware feature correction, enhancing video segmentation by rectifying these distortions. Specifically, we transfer distortion patterns from feature to image space to predict accurate distortion maps. Leveraging these maps, our Feature Correction Module (FCM) effectively adjusts propagated features in distorted regions. This approach substantially improves segmentation accuracy with minimal overhead. Experiments on Cityscapes and CamVid demonstrate our method's superiority over current state-of-the-art techniques.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1855", "problem_id": "18550001", "content": "Time series classification models have gained substantial attention within the research field, yet limited work exists on creating adversarial samples for these models, which could pose security risks. This paper introduces an adversarial transformation network (ATN) applied to a distilled model to target different time series classifiers. The attack strategy employs a distilled model as a surrogate, replicating the behavior of traditional time series classification models. Our approach is tested on 1-Nearest Neighbor Dynamic Time Warping (1-NN DTW), a Fully Connected Network, and a Fully Convolutional Network (FCN), all trained on 42 UCR datasets. The results demonstrate that both models were vulnerable to attacks across all datasets. To our knowledge, this marks the first such attack on time series classifiers. We advise future researchers to enhance model robustness by integrating adversarial samples into training datasets and including resilience as a key evaluation metric.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1856", "problem_id": "18560001", "content": "This paper introduces a novel approach to training neural network models using limited data, deviating from conventional methods that rely on fine-tuning pre-trained networks or utilizing domain-specific, hand-engineered features. By modularizing network layers or entire networks, we combine pre-trained and untrained modules to adapt to distribution shifts between datasets. The key advantage of this modular strategy lies in its ability to introduce new representations to the network, rather than overwriting existing ones through fine-tuning. As a result, our technique outperforms standard fine-tuning transfer learning methods and achieves substantial performance gains, particularly when working with smaller datasets, as shown in Figure A (Reference [1], Citation [2]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1857", "problem_id": "18570001", "content": "Casual photography frequently occurs under unpredictable lighting conditions, leading to poor-quality images that can hinder subsequent processing tasks. This work addresses the challenge of estimating surface normal and reflectance maps for human subjects under such suboptimal lighting by augmenting the visible light with a single near-infrared (NIR) source and camera, referred to as a \"dark flash image.\" The proposed approach processes a single color image taken under arbitrary visible illumination alongside a corresponding dark flash image captured with controlled front-lit NIR illumination from the same viewpoint, generating outputs that include a normal map, diffuse albedo map, and specular intensity map. Due to the difficulty of obtaining ground truth normal and reflectance data for faces, we introduce a training method that leverages two complementary and easily accessible data sources: stereo depth signals and photometric shading cues. The method is tested across various subjects and lighting scenarios, and we demonstrate its utility in two applications: refining stereo geometry and reconstructing shadowed regions in images.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1858", "problem_id": "18580001", "content": "This paper presents novel minimal solvers designed to simultaneously estimate lens distortion and affine rectification using images of coplanar features undergoing rigid transformations. Unlike existing methods, these solvers function effectively in scenes lacking straight lines and generally require fewer assumptions about the scene's composition. The proposed solvers leverage the affine invariant property that coplanar repetitions maintain a consistent scale within the rectified space. These solvers are divided into two categories, distinguished by their application of the equal scale invariant in rectified space to constrain lens undistortion and rectification parameters. A systematic methodology based on the Gr\\\"obner basis method is employed to generate stable minimal solvers, achieved through sampling viable monomial bases to optimize numerical stability. Experimental results using both synthetic and real images validate that the proposed solvers exhibit enhanced robustness to noise compared to current state-of-the-art techniques. The method's broad applicability is demonstrated through precise rectifications of images captured with lenses ranging from narrow to fisheye field-of-view. Furthermore, the process is fully automated.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1859", "problem_id": "18590001", "content": "The MEx dataset, a multi-sensor and multi-modal collection, has been developed to evaluate the performance of Human Activity Recognition (HAR) and multi-modal fusion algorithms, with a specific focus on assessing the quality of exercise execution to support individuals with Musculoskeletal Disorders (MSD). Motivated by the need to recognize and evaluate exercise performance, seven exercises commonly prescribed by physiotherapists for MSD patients were selected, and data was collected using a combination of four sensors: a pressure mat, a depth camera, and two accelerometers. The resulting dataset comprises three distinct data modalities - numerical time-series data, video data, and pressure sensor data - presenting intriguing research challenges for HAR and exercise quality assessment. This study presents an evaluation of the dataset using various standard classification algorithms for the HAR task, comparing different feature representation algorithms for each sensor to establish a baseline performance for individual sensors, highlighting their strengths and weaknesses. Additionally, the pressure mat data is visualized to explore its potential in capturing exercise performance quality, and the dataset is proposed as a suitable benchmark for not only HAR algorithms but also multi-modal fusion algorithms, leveraging the recent advancements in this field to integrate heterogeneous data types across multiple application domains, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1860", "problem_id": "18600001", "content": "As autonomous driving technologies advance, the significance of motion forecasting has risen as a vital aspect of planning. Interactive scenarios such as merges and unprotected turns are particularly crucial, as merely predicting the movements of individual objects is inadequate. Effective route planning necessitates joint predictions involving multiple objects. There exists a pressing demand for high-quality motion data that encompasses rich interactions and detailed annotations to support the development of motion planning models. In this study, we present the most varied interactive motion dataset known to us, complete with specific labels for interacting objects tailored for joint prediction model development. Our dataset comprises over 100,000 scenes, each lasting 20 seconds at a frequency of 10 Hz, accounting for more than 570 hours of distinct data across 1750 km of roadways. This data was gathered by identifying notable interactions among vehicles, pedestrians, and cyclists in six U.S. cities. We employ a precise 3D auto-labeling system to create high-quality 3D bounding boxes for each road agent, along with providing high-definition 3D maps for every scene. Additionally, we present a new set of metrics designed for a thorough assessment of both single-agent and joint-agent interaction motion forecasting models. Finally, we introduce robust baseline models for individual-agent and joint-prediction tasks. We anticipate that this extensive interactive motion dataset will open new avenues for the enhancement of motion forecasting models.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1861", "problem_id": "18610001", "content": "Medical imaging provides critical data for diagnosis and treatment planning. The process of examining (visual perception) and analyzing images to produce reports is a time-consuming clinical task for radiologists, where automation could significantly alleviate the workload. Although natural image captioning has advanced rapidly, automated medical image analysis and interpretation remain difficult, primarily due to insufficient high-quality annotated image-report datasets and specialized generative models capable of effectively extracting and utilizing localized semantic features, especially those linked to abnormalities. To address these issues, we introduce Vispi, an automated medical image interpretation system that first labels images by classifying and locating common thoracic diseases with visual aids, then generates reports using an attentive LSTM model. Evaluated on the open IU X-ray dataset, Vispi shows outstanding performance in disease classification, localization, and report generation, as measured by the automated metrics ROUGE and CIDEr.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1862", "problem_id": "18620001", "content": "The rise of the Internet of Things (IoT) has led to a growing adoption of energy harvesting techniques as alternatives or supplements to battery-powered sensors. To optimize their performance, these sensors must be configured based on application requirements, hardware specifications, and environmental factors. Currently, sensor configuration relies on manual adjustments or heuristic methods, demanding significant domain knowledge. While reinforcement learning (RL) offers a potential solution for automating configuration and scaling IoT systems efficiently, its practical implementation remains limited. Our work addresses this challenge by minimizing RL training duration to enable rapid sensor deployment and lowering computational demands for large-scale applications. Specifically, we examine the sampling rate configuration of indoor solar-powered energy harvesting sensors. Using a simulator built on three months of data from five sensor nodes exposed to varying lighting conditions, we demonstrate that RL can effectively adapt to energy availability trends and optimize sensor sampling rates to maximize data collection without depleting energy reserves. Our approach enables nodes to become operational within a single day and shows that a unified RL policy can be applied to nodes with comparable lighting conditions, reducing the number of required policies.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1863", "problem_id": "18630001", "content": "The application of semantic segmentation for the masking and cropping of input images has demonstrated notable effectiveness in enhancing medical imaging classification performance by reducing noise and variability in the training dataset. Nonetheless, employing this strategy with traditional methods presents challenges: obtaining a comprehensive segmentation is costly, and identifying the specific areas that are most relevant to the classification task beforehand is complicated. We introduce a new deep reinforcement learning framework for image augmentation based on joint training. In this framework, a segmentation network that is weakly supervised using policy gradient optimization operates as an agent, generating masks as actions from samples considered as states, aiming to maximize reward feedback from the classification network. This approach enables the segmentation network to focus on masking less critical imaging features. Our method, Adversarial Policy Gradient Augmentation (APGA), yields encouraging outcomes on Stanford's MURA dataset and a hip fracture classification task, demonstrating a global accuracy improvement of up to 7.33% and outperforming baseline methods in 9 out of 10 assessed tasks. We further elaborate on the wide applicability of our joint training approach across numerous medical imaging applications.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1864", "problem_id": "18640001", "content": "Optimizing deep neural networks is a crucial task in computer vision, but traditional training methods are often hindered by over-fitting. To address this, teacher-student optimization techniques have been proposed, which leverage pre-trained models to provide additional guidance, but these methods are typically time-consuming due to their sequential training pipeline, resulting in increased time complexity. This paper introduces snapshot distillation (SD), a novel framework that enables teacher-student optimization within a single generation. By extracting supervision signals from earlier epochs within the same generation, rather than relying on previous generations, SD prevents under-fitting by maintaining a sufficient difference between the teacher and student models. Implemented through a cyclic learning rate policy, where the last snapshot of each cycle serves as the teacher for the next cycle, SD provides richer information through smoothed teacher signals. The effectiveness of SD is demonstrated on standard image classification benchmarks, including CIFAR100 and ILSVRC2012, where it achieves consistent accuracy gains without incurring significant computational overheads, and its pre-trained models are shown to transfer well to object detection and semantic segmentation tasks on the PascalVOC dataset.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1865", "problem_id": "18650001", "content": "We propose a model compression technique for deep convolutional neural networks based on filter correlation, which involves iteratively selecting pairs of highly correlated filters and removing one filter from each pair. To minimize information loss, the model is re-optimized to maximize correlation between filters in each pair before removal, ensuring that the discarded filter has a negligible impact. Following filter removal, the model undergoes fine-tuning to mitigate any potential loss in performance. Through a series of experiments and ablation studies, our approach demonstrates state-of-the-art compression rates in terms of FLOPs on benchmarks like LeNet-5, VGG-16, and ResNet-50,56, while maintaining exceptional predictive accuracy on tasks such as object detection across various benchmark datasets.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1866", "problem_id": "18660001", "content": "Face morphing attacks present a significant security risk to current face recognition technologies. While some morphing detection techniques exist, reconstructing the facial image of the accomplice involved remains a difficult task. This paper introduces a Face De-morphing Generative Adversarial Network (FD-GAN) designed to restore the accomplice's face. The FD-GAN employs a symmetric dual network structure and a two-tiered loss function to isolate the accomplice's identity characteristics. By leveraging the facial image acquired by the face recognition system (containing the criminal's identity) and the morphed image stored in the e-passport system (containing identities of both the criminal and the accomplice), the FD-GAN effectively reconstructs the accomplice's facial image. Experimental evaluations and their subsequent analysis validate the efficacy of the proposed method, suggesting its potential for deployment in real-world identity verification scenarios to identify face morphing accomplices.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1867", "problem_id": "18670001", "content": "Despite significant advancements in traffic sign detection due to deep learning, challenges persist, particularly in complex real-world scenarios. These challenges primarily stem from the small size of traffic signs, making them harder to detect compared to larger objects, and the difficulty in differentiating genuine signs from similar false targets within intricate street views, especially without contextual understanding. To address these issues, we introduce a new end-to-end deep learning approach designed for traffic sign detection in challenging environments. Our contributions include: 1) A multi-resolution feature fusion network architecture employing densely connected deconvolution layers with skip connections to enhance the learning of effective features for small objects; 2) The formulation of traffic sign detection as a spatial sequence classification and regression problem, along with the introduction of a vertical spatial sequence attention (VSSA) module to incorporate contextual information for improved detection accuracy. We evaluate our method extensively using various traffic sign datasets, as well as a general object detection dataset, demonstrating the effectiveness of our proposed approach.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1868", "problem_id": "18680001", "content": "We present a series of algorithms (Het-node2vec) that enhance the original node2vec sampling technique for node neighborhoods to accommodate heterogeneous multigraphs, which are defined by various types of nodes and edges. The random walk samples generated from these algorithms effectively reflect both the structural features of the graph and the semantics associated with the different node and edge types. Furthermore, the algorithms are capable of prioritizing particular node or edge types, thus providing accurate representations for those underrepresented types that are significant to the prediction problem being studied. These detailed and targeted representations can enhance both unsupervised and supervised learning on heterogeneous graphs.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1869", "problem_id": "18690001", "content": "The effectiveness of super-resolution reconstruction (SRR) techniques, which aim to enhance the spatial resolution of images, has undergone substantial improvement with the incorporation of deep convolutional neural networks. Typically, these networks are trained on large datasets consisting of original images and their corresponding low-resolution versions, which are often generated using bicubic downsampling. This study examines the impact of the method used to obtain low-resolution training data on SRR performance, a factor that has not been previously investigated. Our comprehensive experiments reveal that the characteristics of the training data significantly affect the accuracy of reconstruction, and the conventional approach may not be optimal for satellite images, as evidenced by Figure A, B, C (References [citation]). Our findings suggest that refining the process of preparing training data could be crucial in adapting SRR for practical, real-world applications.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1870", "problem_id": "18700001", "content": "The World Health Organization (WHO) establishes guidelines for regulating Particulate Matter (PM) levels due to their adverse effects on human health when elevated. To manage PM levels effectively, accurate measurement procedures are essential. We employ Tapered Element Oscillating Microbalance (TEOM)-based PM sensors, which offer greater cost efficiency compared to Beta Attenuation Monitor (BAM)-based sensors. However, TEOM-based sensors are more prone to malfunctions than their BAM counterparts. This paper defines such malfunctions as anomalies and focuses on detecting them to ensure proper sensor maintenance. To address this objective, we introduce a new framework called Hypothesis Pruning Generative Adversarial Network (HP-GAN). Through experimental evaluation, we demonstrate the superior performance of our approach compared to existing anomaly detection methods.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1871", "problem_id": "18710001", "content": "In conventional convolutional layers, the filters learned during training remain static thereafter. Our proposed Dynamic Filter Network framework deviates from this approach by generating filters dynamically based on the input, thereby offering enhanced flexibility due to its adaptive characteristics without substantially increasing the model's parameter count. This architecture enables the learning of diverse filtering operations, including spatial transformations, selective blurring or deburring, and adaptive feature extraction. Furthermore, the dynamic filter layers can be stacked, such as in a recurrent setup, allowing for more complex representations. The efficacy of the Dynamic Filter Network is demonstrated through its state-of-the-art performance on video and stereo prediction tasks, notably on the moving MNIST dataset, where it achieves superior results with a significantly smaller model. Visualization of the learned filters reveals that the network has successfully captured flow information from unlabelled training data, implying its potential for unsupervised pretraining of networks for tasks like optical flow and depth estimation.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1872", "problem_id": "18720001", "content": "The deployment of machine learning models in real-world applications often involves training on one data distribution and testing on another, highlighting the need for models that can withstand distribution shifts. Existing approaches typically focus on either adversarial or interventional shifts, but have limitations: adversarial methods struggle to capture plausible shifts due to their joint distribution constraints, while interventional methods, although more expressive, yield overly conservative models due to their robustness to unbounded shifts. This work proposes a novel formulation, RISe, which leverages the strengths of both approaches to design models robust to a specific set of distribution shifts that combine adversarial and interventional shifts. By utilizing a distributionally robust optimization framework, the proposed objective is optimized in both supervised and reinforcement learning contexts. The effectiveness of the proposed approach is demonstrated through comprehensive experiments on synthetic and real-world healthcare datasets, as shown in Figure A, B, C (References [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1873", "problem_id": "18730001", "content": "In contrast to traditional frame-based sensors, event-based visual sensors provide data through spikes with a high temporal resolution. By focusing solely on variations in pixel intensity, these sensors offer a low-power, low-latency means of visual information acquisition. To apply this data for advanced sensory functions, such as object recognition and tracking, feature extraction and learning are crucial simplification steps. An ideal feature descriptor must exhibit resilience to (i) local transformations and (ii) the recurrence of a local event pattern. To address this, we introduce a new algorithm for learning spatiotemporal feature representations based on slow feature analysis (SFA). Through SFA, we learn smoothly varying linear projections that are resilient to local visual transformations. Feature point tracking tasks are employed to assess whether the features can achieve invariance against different visual transformations. Comprehensive experiments across two datasets demonstrate the spatiotemporal feature learner's flexibility in adapting to translation, scaling, and rotational changes of feature points. Most importantly, our findings indicate that the generated feature representations can effectively take advantage of the high temporal resolution offered by event-based cameras to produce improved feature tracks.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1874", "problem_id": "18740001", "content": "Recent progress in deep learning has shown that deep neural networks, when trained on extensive datasets, can outperform cardiologists in recognizing cardiac arrhythmias. While traditional ECG pattern recognition relied on explicit feature extraction, recent studies indicate that deep neural networks can autonomously extract relevant features directly from the data. However, the practical application of deep neural networks for ECG analysis is often hindered by the need for large training datasets, especially in independent studies. To address this limitation, this study explores the identification and classification of four ECG patterns using a transfer learning approach, leveraging knowledge gained from image classification to enhance ECG signal classification. The results demonstrate that feature maps learned by a deep neural network trained on large volumes of generic images can serve as effective general descriptors for ECG signal spectrograms, leading to features suitable for arrhythmia classification. The proposed method achieves an overall accuracy of 97.23 percent in classifying approximately 7000 instances using ten-fold cross validation.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1875", "problem_id": "18750001", "content": "Deep learning has achieved remarkable progress and widespread success across diverse applications, leading to its adoption in numerous safety-critical domains. Nevertheless, deep neural networks have been shown to be susceptible to carefully crafted inputs known as adversarial examples. These inputs, while indistinguishable to humans, can mislead neural networks during testing or deployment. This vulnerability poses a significant risk when deploying deep neural networks in high-stakes environments, prompting extensive research into adversarial attacks and defenses. This paper examines recent advancements in adversarial example research, categorizes the techniques used to generate such examples, and introduces a systematic taxonomy. Additionally, it explores practical applications of adversarial examples, discusses existing defense strategies, and highlights ongoing challenges along with potential solutions.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1876", "problem_id": "18760001", "content": "Anomaly detection involves determining whether a given input sample belongs to the distribution of a designated normal class or an anomaly class. Traditional methods based on generative adversarial networks (GANs) process entire images that include both foreground and background. However, these approaches often incorporate irrelevant regions unrelated to the normal class (such as extraneous background), which can lead to incorrect anomaly detections. To address this issue, this paper introduces an innovative two-stage network featuring an attention network and an anomaly detection GAN (ADGAN). The attention network produces an attention map that highlights the areas indicative of the normal class distribution. To refine the attention map, we suggest an attention loss and an adversarial anomaly loss derived from synthetic anomaly samples created through rigorous augmentation techniques. By applying this attention map to an image feature map, ADGAN learns the normal class distribution while excluding the irrelevant areas, significantly easing the anomaly detection challenge. Furthermore, the derived attention map can facilitate anomaly segmentation, enabling differentiation between normal and anomaly regions. Consequently, the proposed method demonstrates superior performance compared to the leading anomaly detection and segmentation techniques on commonly used datasets.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1877", "problem_id": "18770001", "content": "Object detection presents a significant challenge in the field of visual understanding, particularly under weak supervision. Recent advancements have explored deep neural networks as a promising solution to reduce reliance on costly human annotations. A novel cascaded network architecture is introduced to train a convolutional neural network (CNN) under these constraints. Two variants are proposed—a two-stage and a three-stage model—both trained end-to-end. The initial stage in both architectures employs a fully convolutional network to generate optimal class-specific region proposals. For the three-stage model, an intermediate stage enhances object segmentation using activation maps from the first stage. The final stage in both designs incorporates a CNN component that applies multiple instance learning to refine proposals from prior stages. Evaluations on PASCAL VOC 2007, 2010, 2012 and large-scale datasets like ILSVRC 2013 and 2014 demonstrate enhanced performance in weakly-supervised object detection, classification, and localization tasks.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1878", "problem_id": "18780001", "content": "Place recognition is a critical component in autonomous driving and robotic navigation systems. While numerous point cloud-based approaches have demonstrated encouraging performance, most overlook the varying scales of objects. Small objects such as pedestrians and cars require limited receptive fields to avoid irrelevant data, whereas large structures like buildings demand broader fields to capture full geometric details. This limitation of fixed receptive fields motivates our Adaptive Receptive Field Module (ARFM), which dynamically adjusts field sizes according to input point clouds. Additionally, we introduce TransLoc3D, a novel network architecture combining a 3D sparse convolutional module, ARFM, an external transformer for long-range dependencies, and a NetVLAD layer to generate distinctive global descriptors. Experimental results demonstrate superior performance over existing methods, achieving average recall@1 gains of 1.1% on the Oxford RobotCar dataset and 0.8% on the B.D. dataset.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1879", "problem_id": "18790001", "content": "Contemporary multi-object tracking (MOT) systems leveraging machine learning are gaining prominence for processing 3-D point clouds. Conventional tracking methodologies often employ filters, such as Kalman or particle filters, to forecast object positions across temporal sequences; however, these methods are susceptible to significant motion variations, including abrupt stops and turns. This paper introduces PointTrackNet, a unified end-to-end network for 3-D object detection and tracking, designed to produce foreground masks, 3-D bounding boxes, and point-level tracking association displacements for each identified object, requiring only two consecutive point-cloud frames as input. Empirical evaluations conducted on the KITTI tracking dataset demonstrate performance on par with state-of-the-art techniques, particularly in situations characterized by irregular and rapid changes.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1880", "problem_id": "18800001", "content": "Developing the ability to infer Bayesian posteriors from limited datasets is crucial for achieving robust meta-learning, as it acknowledges the inherent model uncertainty associated with this problem. This paper introduces a novel approach to Bayesian model-agnostic meta-learning, which integrates scalable gradient-based meta-learning with nonparametric variational inference within a probabilistically principled framework. The proposed method enables the learning of complex uncertainty structures during rapid adaptation, extending beyond simplistic point estimates or Gaussian approximations. Furthermore, it incorporates a robust Bayesian meta-update mechanism, facilitated by a novel meta-loss function that mitigates overfitting during the meta-update phase. While maintaining efficiency as a gradient-based meta-learner, the method remains model-agnostic and straightforward to implement. Experimental results, as seen in Figure A, B, C, demonstrate the proposed method's accuracy and robustness across a range of tasks, including sinusoidal regression, image classification, active learning, and reinforcement learning, as discussed in References [citation].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1881", "problem_id": "18810001", "content": "This study explores the application of sequence machine learning methods to analyze raw radio signal time-series data. Employing deep recurrent neural networks, we successfully differentiate between various application layer traffic types transmitted via constant envelope modulation, bypassing the need for specialized demodulation algorithms. The results demonstrate the capacity of this approach to learn intricate protocol sequences, which can then be utilized for both classification and generation purposes.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1882", "problem_id": "18820001", "content": "Ischemic heart disease ranks as the leading cause of global mortality each year, significantly impacting the lives of those affected as well as placing considerable pressure on public healthcare systems. To analyze the functions of both healthy and unhealthy hearts, medical professionals frequently utilize electrocardiogram (ECG) and blood pressure (BP) measurements. However, these approaches can be rather invasive, especially when it comes to obtaining continuous arterial blood pressure (ABP) readings, and they can also be quite expensive. In response, we propose a machine learning framework designed to infer ABP from a single optical photoplethysmogram (PPG) sensor. Our framework is trained using distributed models and data sources, simulating a large-scale collaborative learning experiment that could be deployed across low-cost wearable devices. Our time series-to-time series generative adversarial network (T2TGAN) demonstrates the ability to generate high-quality continuous ABP from a PPG signal, achieving a mean error of 2.54 mmHg and a standard deviation of 23.7 mmHg for estimating mean arterial pressure on a previously unseen, noisy, independent dataset. To our knowledge, this framework is the first instance of a GAN that accomplishes continuous ABP generation from an input PPG signal while employing a federated learning approach.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1883", "problem_id": "18830001", "content": "Humans can infer the dynamic narrative behind a still image, deducing past events, present intentions, and future outcomes from a single frame. For instance, observing a man struggling in water suggests he fell in earlier, currently aims to survive, and will soon require assistance to avoid being swept away. We introduce VisualComet, an innovative framework for visual commonsense reasoning tasks aimed at predicting prior events, subsequent actions, and present intents. To advance research in this domain, we present the first extensive dataset of Visual Commonsense Graphs, containing over 1.4 million annotated textual inferences across 60,000 diverse images, each supplemented with brief video summaries of preceding and following events. Additionally, we include person-grounding (co-reference links) connecting individuals in the images with those described in the textual inferences, enhancing image-text alignment. Our benchmarks show strong performance, proving that combining visual and textual commonsense reasoning outperforms non-integrative approaches.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1884", "problem_id": "18840001", "content": "Inferring structural patterns from continuous data streams poses a significant challenge for intelligent systems, and research suggests that the brain addresses this issue by segmenting sensorimotor information into concise event representations that facilitate anticipation and interpretation of environmental changes. This study proposes a novel approach, introducing the SUrprise-GAted Recurrent neural network (SUGAR), which incorporates a unique counterfactual regularization technique. The model is evaluated on a sequence prediction task characterized by hierarchical structures with alternating hidden graphs, demonstrating its ability to compress temporal dynamics into latent event-predictive representations and accurately anticipate event transitions based on noisy signals. The counterfactual regularization term enables seamless transitions between latent codes, yielding compositional properties. The developed mechanisms have far-reaching implications for various applications, including hierarchical reasoning, planning, and decision-making domains.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1885", "problem_id": "18850001", "content": "Deep neural networks (DNNs) have achieved notable success in various supervised learning applications, such as voice recognition, object detection, and image classification. Nevertheless, their growing complexity can lead to poor generalization performance, which complicates their implementation on edge devices. Quantization serves as an effective strategy for compressing DNNs to address these limitations. Employing a quasiconvex base function to create a binary quantizer facilitates the training of binary neural networks (BNNs), while incorporating noise into the input data or utilizing a specific regularization function can enhance generalization performance. In this context, we present the foothill function, an infinitely differentiable quasiconvex function. This regularizer is versatile enough to adapt to L_1 and L_2 penalties. Foothill can act as a binary quantizer, a regularization term, or a loss function. Specifically, we demonstrate that this regularizer diminishes the accuracy disparity between BNNs and their full-precision equivalents in image classification tasks on ImageNet.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1886", "problem_id": "18860001", "content": "We explore the challenge of Language-Based Image Editing (LBIE), which involves generating a target image from a source image and a corresponding natural language description by applying edits informed by that description. We introduce a versatile modeling framework tailored for two sub-tasks within LBIE: language-driven image segmentation and image colorization. This framework employs recurrent attentive models to integrate features from both images and language. Instead of adhering to a fixed step size, we implement a termination gate for each image region that dynamically assesses after each inference step whether to extract further information from the textual description. The framework's efficacy is demonstrated across three datasets. Initially, we present a synthetic dataset named CoSaL to assess the end-to-end performance of our LBIE system. Next, we illustrate that the framework achieves state-of-the-art results in image segmentation on the ReferIt dataset. Lastly, we showcase the first language-guided colorization outcomes on the Oxford-102 Flowers dataset.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1887", "problem_id": "18870001", "content": "Heterogeneous face recognition that involves the comparison of color images and depth images is a crucial capability for practical applications where shape information is considered essential in the gallery. In this study, we introduce a cross-modal deep learning approach as an effective solution to this challenge. We start by training two convolutional neural networks (CNNs) to extract face features from 2D and 2.5D images separately. Once these networks are trained, they function as pre-trained models for a two-way CNN that investigates the relationship between color and depth images for heterogeneous matching. Unlike most traditional cross-modal techniques, our method also performs precise depth image reconstruction from a single color image using Conditional Generative Adversarial Nets (cGAN), thereby improving recognition accuracy by integrating multi-modal matching results. Through both qualitative and quantitative assessments conducted on the benchmark FRGC 2D/3D face database, we demonstrate that our proposed approach surpasses the current state-of-the-art in heterogeneous face recognition while ensuring a highly efficient online processing stage.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1888", "problem_id": "18880001", "content": "This paper presents SynSE, a groundbreaking generative approach for Zero-Shot Learning (ZSL) that leverages syntactic guidance to learn refined embedding spaces across visual and language modalities. By imposing inter-modal constraints between action sequence embeddings and Part of Speech (PoS) tagged word embeddings in action descriptions, SynSE enables end-to-end learning of compositional generalization, allowing it to recognize action sequences with unseen words during training. The proposed approach is applied to skeleton-based action sequence recognition and further extended to Generalized Zero-Shot Learning (GZSL) using a confidence-based gating mechanism. Notably, SynSE achieves state-of-the-art performance in both ZSL and GZSL settings on the large-scale NTU-60 and NTU-120 datasets, with results reported on multiple splits, and the code and pre-trained models are made available at https://github.com/skelemoa/synse-zsl.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1889", "problem_id": "18890001", "content": "Artificial Neural Networks (ANNs) perform a particular type of multi-variate extrapolation and can produce outputs for input patterns, even in the absence of analogous training data. However, such extrapolations may not always be reliable, and for systems that are critical to safety, it is essential for these systems to indicate the uncertainty related to their outputs and the training samples. While some might regard this as a familiar concern already addressed by fundamental pattern recognition principles, we will clarify how this perception is misleading and demonstrate that the traditional (Likelihood estimate of) conditional probability of classification fails to accurately evaluate this uncertainty. Our discussion includes the standard interpretations of this issue and presents how a quantitative methodology grounded in well-established techniques can be effectively utilized. This approach is exemplified in the context of early diagnosis of dementia-related diseases through Magnetic Resonance Imaging.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1890", "problem_id": "18900001", "content": "We introduce a comprehensive modeling and inference framework that integrates probabilistic graphical models with deep learning techniques, harnessing the advantages of both. Our approach enhances graphical structure within latent variables by incorporating neural network observation models. For the inference process, we adapt variational autoencoders to utilize distributions from graphical models through recognition networks that produce conjugate potentials. All elements of these models are jointly optimized with a unified objective, resulting in a scalable algorithm that employs stochastic variational inference, natural gradients, message passing in graphical models, and the reparameterization trick. We demonstrate this framework through various example models and a case study on mouse behavioral phenotyping.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1891", "problem_id": "18910001", "content": "The free energy functional has emerged as a variational principle for modeling bounded rational decision-making, as it naturally balances utility and information processing costs in a manner that can be axiomatically justified. In this work, we extend the application of the free energy principle to encompass general decision trees incorporating both adversarial and stochastic environments. We establish generalized sequential optimality equations, which encompass the standard Bellman optimality equations as a special case and yield established decision-rules such as Expectimax, Minimax, and Expectiminimax. We demonstrate that these decision-rules can be derived from a unified free energy principle by assigning a resource parameter to each node within the decision tree. These resource parameters quantify the computational cost, specifically the number of samples required from the distribution associated with each node. Consequently, the free energy principle offers a normative foundation for generalized optimality equations that account for both adversarial and stochastic conditions.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1892", "problem_id": "18920001", "content": "Reconstructing the 3D geometry of an object from a single image is inherently difficult because the problem is ill-posed. A potential solution involves leveraging large collections of images from the same object category to learn a robust 3D shape prior. Wu et al. (2020) successfully demonstrated this approach by training unsupervised 3D reconstruction networks, though their method is limited to symmetric objects. Our work removes this symmetry constraint by introducing a new unsupervised algorithm capable of learning from multi-image datasets, making it more versatile while still accommodating symmetric cases as a subset. Additionally, we introduce an innovative albedo loss function that enhances detail and realism in reconstructions. Experimental results across diverse datasets—including single-view, multi-view, image collections, and videos—demonstrate that our approach outperforms prior methods in both quality and robustness.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1893", "problem_id": "18930001", "content": "Ground-based sky-cameras equipped with fish-eye lenses capture sky/cloud images with an extensive field of view, yet the sky's luminance dynamic range exceeds the capabilities of standard cameras. Consequently, single-exposure images often fail to preserve details across the entire scene, with the circumsolar area appearing overexposed and the horizon regions underexposed, complicating cloud segmentation. This paper introduces HDRCloudSeg, a novel approach leveraging High-Dynamic-Range (HDR) imaging through multi-exposure fusion to enhance cloud segmentation. We detail the HDR image creation process and provide a publicly available dataset for evaluation. As the first method utilizing HDR radiance maps for cloud segmentation, our technique delivers highly accurate outcomes.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1894", "problem_id": "18940001", "content": "This study introduces LocGAN, a localization method utilizing geo-referenced aerial imagery and LiDAR grid maps. While existing self-localization techniques often rely on sensor observations matched to pre-existing maps, such data may be unavailable or sensor-specific. Although Global Navigation Satellite Systems (GNSS) offer an alternative, their reliability is compromised in urban environments due to multi-path interference and signal obstructions. To address this, aerial imagery serves as prior information. Conditional Generative Adversarial Networks (cGANs) align aerial images with grid maps, while a localization network (LocNet) estimates the transformation between predicted and measured grid maps. This enables vehicle pose estimation through geo-referenced aerial image transformations. Testing on data from Karlsruhe, Germany, demonstrates that LocGAN delivers accurate global localization outcomes.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1895", "problem_id": "18950001", "content": "The current leading technique for automated white matter bundle segmentation in diffusion-weighted MRI combines tractography with streamline cluster selection. However, this approach involves lengthy, computationally intensive processing chains that are difficult to configure and require extensive quality control. Alternative direct bundle segmentation methods frame the task as a conventional image segmentation problem, offering potential solutions to these challenges, though previous implementations have not matched state-of-the-art performance. We introduce a new supervised method for direct tract segmentation that significantly improves results, utilizing a stacked U-Net architecture trained on manual bundle segmentations from Human Connectome Project data. Our approach is validated on the ISMRM 2015 Tractography Challenge phantom dataset, demonstrating performance comparable to human segmentation and substantial improvements over prior methods. Additionally, we illustrate how the model's learned spatial priors effectively maintain segmentation accuracy even with reduced image quality.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1896", "problem_id": "18960001", "content": "The understanding of molecular structure-property relationships is crucial for advancing materials and drug discovery through molecular engineering. Recent advancements in deep learning have introduced a promising approach to directly derive these relationships from chemical data. This study demonstrates that the predictive capabilities of graph convolutional networks (GCNs) for molecular properties can be enhanced through the integration of attention and gate mechanisms. By incorporating attention, GCNs can distinguish between atoms in varying environments, while the gated skip-connection optimizes feature map updates. The resulting augmented GCN model exhibits improved extraction of structural features pertinent to specific molecular properties, including solubility, polarity, synthetic accessibility, and photovoltaic efficiency, outperforming the standard GCN. Notably, the model identifies distinct molecular substructures that are critical for high photovoltaic efficiency, which correspond to the donor and acceptor orbital regions involved in charge-transfer excitations. Ultimately, this enhanced model enables accurate predictions of molecular properties and effectively clusters molecules with similar properties within a well-trained latent space, a key requirement for successful molecular engineering, as shown in Figure A (Reference [1], Citation [2]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1897", "problem_id": "18970001", "content": "Feature importance ranking (FIR) is receiving increased focus, especially with the extraction of thousands of features for intelligent diagnosis and personalized medicine. Despite the abundance of proposed FIR methods, few are integrated for comparative analysis and practical use. This study introduces a MATLAB toolbox comprising 30 algorithms, which is then assessed using a database of 163 ultrasound images. For each breast mass lesion, 15 features are extracted. To identify the best feature subset for classification, all feature combinations are tested, and a linear support vector machine is employed to predict the malignancy of lesions annotated in the ultrasound images. Ultimately, the efficacy of FIR is analyzed through performance comparison. The toolbox is available online (https://github.com/NicoYuCN/matFIR). Future work will incorporate additional FIR methods, feature selection techniques, and machine learning classifiers.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1898", "problem_id": "18980001", "content": "This study empirically evaluates a contemporary category of Generative Adversarial Networks (GANs) employing Integral Probability Metrics (IPM) and their effectiveness in semi-supervised learning. IPM-based GANs, such as Wasserstein GAN, Fisher GAN, and Sobolev GAN, offer advantages including theoretical interpretability, stable training dynamics, and a well-defined loss function. The research explores the impact of critic (or discriminator) architecture on semi-supervised learning performance, identifying three critical elements for optimal SSL results: (1) the K+1 formulation, (2) the omission of batch normalization within the critic, and (3) the avoidance of gradient penalty constraints applied to the classification layer.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1899", "problem_id": "18990001", "content": "This study tackles the challenge of recognizing scene text characters with varying scales by introducing a novel scale-aware feature encoder, termed SAFE. The proposed SAFE architecture consists of a multi-scale convolutional encoder, which extracts character features at multiple scales, and a scale attention network, responsible for selecting the most relevant scale features. Compared to traditional single-CNN encoders used in current state-of-the-art text recognizers, SAFE offers two key advantages: it explicitly addresses the scale issue by extracting scale-invariant features, allowing the recognizer to focus on other challenges such as view distortion and poor image quality, and it enables the transfer of feature encoding knowledge across different character scales, which is crucial when dealing with unbalanced scale distributions in the training dataset. The effectiveness of SAFE is evaluated through a simple text recognizer, named scale-spatial attention network (S-SAN), which utilizes SAFE as its feature encoder, and experimental results on six public benchmarks demonstrate that S-SAN achieves state-of-the-art or highly competitive performance without requiring post-processing, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1900", "problem_id": "19000001", "content": "This paper details our contribution to the lesion classification task (Part 3) of the 2017 International Skin Imaging Collaboration's Skin Lesion Analysis Towards Melanoma Detection competition. The challenge involved two binary image classification problems: first, differentiating melanoma from nevus and seborrheic keratosis, and second, differentiating seborrheic keratosis from nevus and melanoma. Our methodology employed a deep neural network with transfer learning, utilizing a pre-trained Inception V3 network both as a feature extractor for a multi-layer perceptron and for fine-tuning an augmented Inception network. This method achieved Area Under the Curve (AUC) scores of 0.84 and 0.76 on the second and first tasks, respectively, resulting in an average AUC of 0.80 on the validation set. Despite our late entry into the competition, we anticipate further improvements to these results.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1901", "problem_id": "19010001", "content": "Time series prediction is a critical challenge in various fields, such as estimating solar energy production, forecasting electricity usage, and anticipating traffic congestion. This paper addresses these forecasting tasks using the Transformer model [1]. While initial experiments demonstrated promising results, we identified two key limitations: (1) locality insensitivity—the standard Transformer’s point-wise self-attention fails to adequately capture local patterns, increasing vulnerability to anomalies in time series data; (2) memory constraints—the quadratic space complexity relative to sequence length L restricts the model’s ability to handle long sequences efficiently. To mitigate these issues, we introduce convolutional self-attention, which employs causal convolution to generate queries and keys, enhancing local context integration. Additionally, we present the LogSparse Transformer, which reduces memory overhead to O(L(\\log L)^), enabling accurate predictions for high-resolution time series with long-range dependencies under memory limitations. Evaluations on synthetic and real-world datasets confirm the model’s superior performance compared to existing methods.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1902", "problem_id": "19020001", "content": "The growing fascination with transformers has highlighted their potential as versatile \"universal\" models for various computer vision applications, including classification, detection, and segmentation. While prior research primarily focuses on discriminative models, we investigate transformers for more challenging vision tasks, such as generative adversarial networks (GANs). This study pioneers the development of a convolution-free GAN, relying solely on transformer-based architectures. Our proposed model, TransGAN, features a memory-efficient transformer generator that incrementally enhances feature resolution, paired with a multi-scale discriminator to capture both high-level semantics and fine-grained textures. To address memory constraints, we introduce grid self-attention, enabling high-resolution generation. Additionally, we devise a specialized training approach incorporating techniques like data augmentation, adjusted normalization, and relative position encoding to stabilize TransGAN training. Our top-performing architecture rivals leading convolutional GANs, achieving a record inception score of 10.43 and FID of 18.28 on STL-10, surpassing StyleGAN-V2. For higher resolutions (e.g., 256 x 256) on datasets like CelebA-HQ and LSUN-Church, TransGAN generates diverse, high-quality images with rich details. We further analyze transformer-based generation behavior through training dynamics visualization, contrasting it with convolutional approaches. The code is available at https://github.com/VITA-Group/TransGAN.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1903", "problem_id": "19030001", "content": "Wireframes, or line-based models, are commonly employed in architecture and computer-aided design as foundational 3D representations for rapid design iterations and evaluations. Unlike complete design files, wireframes omit key details such as textures, materials, and precise shapes, which traditional renderers require to produce 2D visualizations. This paper addresses this limitation by introducing a method to synthesize photorealistic indoor scene renderings from wireframe models using an image translation framework. Current image synthesis techniques, while effective for objects like faces or birds, often fail to maintain structural elements inherent to wireframes, such as junctions, parallel lines, and planar surfaces. To overcome this, we present a novel approach that leverages a joint structure-appearance representation, trained on both images and wireframes, ensuring structural constraints are preserved through a shared encoder network. Evaluations on a wireframe-scene dataset demonstrate that our model surpasses existing methods in both visual fidelity and structural accuracy of the generated images.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1904", "problem_id": "19040001", "content": "This chapter provides a summary of our prior research, which leverages the minimal path framework and the Eikonal partial differential equation (PDE) to tackle various image analysis challenges. By devising suitable Riemannian and Randers geodesic metrics, we demonstrate that minimal paths can be effectively employed to solve a wide range of active contour problems and the Euler-Mumford elastica problem, thereby combining the benefits of minimal geodesic paths with traditional approaches, including active contours and elastica curves. Our proposed models, based on minimal paths, can be applied to a diverse array of image analysis tasks, such as detecting boundaries, segmenting images, and extracting tubular structures. Furthermore, the computational efficiency of minimal path calculations is ensured by the use of Eikonal solvers, including the Finsler variant of the fast marching method, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1905", "problem_id": "19050001", "content": "Direct approaches are commonly employed for aligning models with images because of their precision, as they reduce errors within the measurement noise domain. These methods have utilized least squares minimization for straightforward and efficient variational optimization since Lucas & Kanade's foundational 1981 study, as well as normalized cross correlation (NCC) for resilience to brightness changes since at least 1972. Although these two established techniques offer complementary advantages, they have not been successfully integrated to handle localized intensity fluctuations. Consequently, numerous improvised NCC frameworks, suboptimal least squares approaches, and image transformation techniques have been developed, each with inherent drawbacks. This study demonstrates that optimizing NCC using least squares without approximation is not only feasible but also simple and computationally efficient. A robust, locally normalized formulation is presented to address local brightness variations and partial occlusions. Additionally, oriented patches with sparse features are suggested to enhance efficiency. The proposed framework is easy to implement, computationally fast, and resilient to local intensity changes. Evaluated on image alignment tasks, it outperforms existing illumination-invariant methods in both convergence speed and processing time.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1906", "problem_id": "19060001", "content": "We investigate a reinforcement learning (RL) framework where the learner obtains binary feedback solely at the conclusion of an episode. While this may represent a challenging scenario for theoretical analysis, it can be seen as more indicative of real-world situations compared to the conventional RL assumption that feedback is given at every time step. In various practical cases of reinforcement learning, such as in autonomous vehicles and robotic systems, it is often simpler to assess whether an entire trajectory is \"good\" or \"bad,\" rather than to assign a reward at each individual step. To demonstrate the feasibility of learning in this more demanding context, we examine a scenario where trajectory labels are produced by an unknown parametric model and present an algorithm that is both statistically and computationally efficient, achieving sub-linear regret.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1907", "problem_id": "19070001", "content": "Learning invariant representations is a critical challenge in machine learning and pattern recognition. In this study, we introduce an innovative framework for transformation-invariant feature learning by integrating linear transformations into feature learning algorithms. For instance, we propose the transformation-invariant restricted Boltzmann machine, which efficiently encodes data through its weights and their transformations, achieving invariance in feature representation via probabilistic max pooling. Moreover, we demonstrate that our transformation-invariant feature learning framework can be adapted to other unsupervised learning techniques, including autoencoders and sparse coding. We evaluate our approach on multiple image classification benchmark datasets, including variations of MNIST, CIFAR-10, and STL-10, revealing competitive or superior classification performance relative to existing state-of-the-art methods. Additionally, our method excels in phone classification tasks using the TIMIT dataset, illustrating the broad applicability of our proposed algorithms across diverse domains.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1908", "problem_id": "19080001", "content": "Advances in image editing tools enable users to produce artistic creations, though manipulated areas may appear inconsistent with their surroundings. Detecting these discordant regions presents an interesting yet difficult challenge. Recognizing that this task demands efficient integration of multi-scale contextual data while filtering out unnecessary details, we introduce innovative Bi-directional Feature Integration (BFI) and Global-context Guided Decoder (GGD) blocks to merge multi-scale features in the encoder and decoder stages. Additionally, a Mask-guided Dual Attention (MDA) block is implemented between the encoder and decoder to minimize redundant information. Testing on the image harmonization dataset shows our approach delivers strong performance in identifying inconsistent regions. The source code is accessible at https://github.com/bcmi/DIRL.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1909", "problem_id": "19090001", "content": "Recent interest in crowd counting from a single image has grown, yet many prominent approaches remain suboptimal, particularly in dense environments. In this study, we introduce the Hierarchical Attention-based Crowd Counting Network (HA-CCN), which utilizes attention mechanisms at different levels to selectively improve the network's features. Built on the VGG16 architecture, our method includes a spatial attention module (SAM) that enhances low-level features by integrating spatial segmentation data, along with a series of global attention modules (GAM) that concentrate on improving channel-wise information in the upper layers. The framework proposed allows for single-step training, is straightforward to implement, and achieves cutting-edge results across various datasets. Additionally, we enhance the counting network by presenting a new configuration that enables the network to adapt to diverse scenes and datasets through weak supervision using image-level labels. This innovative setup alleviates the need for labor-intensive point-wise annotations for new datasets while enhancing performance across different datasets.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1910", "problem_id": "19100001", "content": "The generation of labeled graphs has become a topic of increasing interest within the Deep Learning community, posing a significant challenge due to the sparse and discrete characteristics of graph spaces. Various methods have been proposed to address this issue, with many involving the transformation of graphs into sequential representations that capture their structural and labeling information, which are then modeled using auto-regressive generative approaches. This study focuses on the GraphGen model, which converts graphs into unique sequences, known as Depth-First Search (DFS) codes, allowing isomorphic graphs to be assigned identical codes. Each element in a DFS code corresponds to a graph edge, represented as a quintuple containing node identifiers, node labels, and an edge label. Although GraphGen effectively generates these sequences in an auto-regressive manner, modeling each quintuple component independently, its assumption of independence is overly simplistic, failing to accurately capture the intricate label dependencies present in real-world graphs. To address this limitation, a novel graph preprocessing technique is introduced, enabling the joint processing of node and edge labeling information. The resulting model, termed GraphGen-Redux, demonstrates improved generative performance across a diverse range of chemical and social graph datasets, while requiring approximately 78% fewer parameters and 50% fewer training epochs than the original GraphGen model, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1911", "problem_id": "19110001", "content": "This study introduces the application of Train-Tensor (TT) networks to develop a condensed representation of the traditional Multilayer Perceptron, achieving a coefficient reduction of up to 95%. A performance comparison between the tensor-based approach and conventional multilayer neural networks is conducted using the Mackey-Glass noisy chaotic time series and NASDAQ index as prediction benchmarks. The results demonstrate that TT networks effectively learn the weights of a multidimensional regression model while exhibiting greater resilience to variations in coefficient initialization and hyper-parameter selection. Additionally, an efficient alternating least squares-based algorithm is introduced to approximate TT-format weights with reduced computational complexity, delivering significantly faster convergence compared to standard adaptive learning methods commonly used in neural network optimization.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1912", "problem_id": "19120001", "content": "Enhancing sample efficiency remains a critical challenge in reinforcement learning (RL), where CURL employs contrastive learning to derive high-level features from raw pixel data in individual video frames, proving to be an effective approach~\\citep. We note that successive frames in a game exhibit strong correlations, yet CURL processes them in isolation. To boost data efficiency further, we introduce a novel algorithm—masked contrastive representation learning for RL—that accounts for dependencies between consecutive inputs. Beyond the CNN encoder and policy network used in CURL, our approach incorporates an auxiliary Transformer module to exploit inter-frame relationships. During training, we randomly mask features of certain frames and employ the CNN encoder and Transformer to reconstruct them using surrounding context. These components are jointly optimized via contrastive learning, ensuring reconstructed features align with ground-truth data while diverging from unrelated samples. At inference time, only the CNN encoder and policy network are retained for decision-making, while the Transformer is omitted. Our method demonstrates consistent performance gains over CURL in 14 of 16 DMControl environments and 21 of 26 Atari 2600 Games. The implementation is accessible at https://github.com/teslacool/m-curl.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1913", "problem_id": "19130001", "content": "Artificial neural networks (ANNs) with a computer science orientation have demonstrated remarkable capabilities in various applications, leveraging their robust feature extraction and precise data processing abilities. Nevertheless, ANNs are often hindered by their high resource requirements and associated costs. Conversely, spiking neural networks (SNNs) inspired by neuroscience offer a promising approach to energy-efficient information processing, thanks to their event-driven spike activities, although their effectiveness in complex real-world tasks remains to be fully established. The integration of these two neural network families to harness their respective advantages poses an intriguing open question. Two key hurdles must be overcome: the scarcity of benchmark datasets that incorporate both ANN-oriented (frame-based) and SNN-oriented (spike-based) signal resources, and the difficulty of jointly processing the synchronous activations from ANNs and the event-driven spikes from SNNs. This study proposes a novel hybrid paradigm, termed DashNet, which combines the strengths of ANNs and SNNs within a single model. To address the aforementioned challenges, a simulator and a benchmark dataset (NFS-DAVIS) are developed, along with a temporal complementary filter (TCF) and an attention module. The results show that DashNet achieves a record-breaking speed of 2083FPS on neuromorphic chips and superior tracking performance on the NFS-DAVIS and PRED18 datasets, as seen in Figure A, B, C. According to our knowledge, DashNet is the first framework to successfully integrate and process ANNs and SNNs in a hybrid paradigm, providing an innovative solution for achieving both effectiveness and efficiency in high-speed object tracking, as discussed in References [citation].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1914", "problem_id": "19140001", "content": "Many real-world processes, including sunlight distribution in forests, mineral concentrations, and stock valuations, display nonstationary dynamics, where variations change with location. These dynamics present significant theoretical and practical obstacles for statistical machine learning algorithms aiming to accurately model the evolution of such processes. Nonstationary Gaussian Process models (NGPS) are commonly used, employing local latent dynamics parameterization to represent these dynamics. A recent approach using the most likely induced latent dynamics representation has garnered attention, but its application to large-scale problems is limited by the computational intractability of maximizing the marginal likelihood of observed real dynamics as the number of induced latent points increases. We have identified a direct correlation between the informativeness of induced latent dynamics and the marginal likelihood of observed real dynamics. This allows for the indirect maximization of the marginal likelihood of observed real dynamics by near-optimally maximizing entropy or mutual information gain on the induced latent dynamics using greedy algorithms. Consequently, for efficient and accurate inference, we introduce LISAL, a novel algorithm that adaptively maximizes entropy or mutual information on the induced latent dynamics and marginal likelihood of observed real dynamics iteratively. The effectiveness of LISAL is demonstrated using real-world datasets.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1915", "problem_id": "19150001", "content": "In contrast to image-based methods, point cloud-based retrieval for place recognition remains an understudied and unresolved challenge, primarily due to the complexity of deriving local feature descriptors from point clouds that can be aggregated into a global descriptor for retrieval. This paper introduces PointNetVLAD, a novel approach that builds upon recent advancements in deep learning to address point cloud-based place recognition. Our method integrates and adapts PointNet and NetVLAD, enabling end-to-end training and inference to generate global descriptors directly from 3D point clouds. Additionally, we present \"lazy triplet and quadruplet\" loss functions to enhance the discriminative power and generalizability of these descriptors. We establish benchmark datasets for evaluating point cloud-based retrieval, and experimental results demonstrate the effectiveness of PointNetVLAD. The code and benchmark datasets are accessible at our project website: http://github.com/mikacuy/pointnetvlad/.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1916", "problem_id": "19160001", "content": "In this study, we demonstrate that Generative Adversarial Networks (GANs) experience catastrophic forgetting, even when designed to mirror a singular target distribution. We illustrate that the training of GANs represents a continual learning challenge, where the evolving model distributions correspond to a series of tasks for the discriminator. The degree of task mismatch in this sequence directly influences the extent of forgetting. Catastrophic forgetting is closely linked to mode collapse, which can impede the convergence of GAN training. We explore the output landscape of the discriminator across various GAN variants and observe that, when a GAN achieves a favorable equilibrium, genuine training data points correspond to extensive local maxima of the discriminator. Our empirical analysis reveals a connection between the sharpness of these local maxima and the phenomena of mode collapse and generalization in GANs. Furthermore, we illustrate how catastrophic forgetting hampers the discriminator's ability to classify real data points as local maxima, leading to non-convergence. Lastly, we examine strategies to mitigate catastrophic forgetting in GANs.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1917", "problem_id": "19170001", "content": "We introduce an innovative algorithm designed to address the expectation propagation relaxation in Bayesian inference for continuous-variable graphical models. Unlike many existing algorithms, our approach guarantees convergence. By integrating convergent EP concepts from (Opper&Winther 05) with covariance decoupling methods (Wipf&Nagarajan 08, Nickisch&Seeger 09), it operates at least ten times quicker than the widely used EP solvers.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1918", "problem_id": "19180001", "content": "Various deep learning architectures have been successfully employed for medical image segmentation, demonstrating strong performance. Medical images from the same imaging modality typically exhibit consistent patterns, with normal anatomical structures or tissues appearing in similar locations across different scans. This study explores integrating such domain-specific prior knowledge into neural network architectures to enhance segmentation accuracy. We introduce a knowledge-based fully convolutional network (KFCN), specifically designed for medical image segmentation, and analyze its segmentation capabilities and associated error. Our theoretical analysis reveals that KFCN possesses an asymptotically stable region, a feature absent in conventional FCNs. Experimental results confirm the effectiveness of embedding prior knowledge into KFCN's convolutional kernels, demonstrating both accurate segmentation and reliable performance.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1919", "problem_id": "19190001", "content": "Partial multi-label learning (PML) addresses the challenge of training multi-label prediction models using instances with incomplete and noisy annotations, an area that has recently attracted increasing research interest. This paper introduces PML-GAN, an adversarial learning approach within a generalized encoder-decoder framework designed for PML. The model employs a disambiguation network to filter out noisy labels and a multi-label prediction network to associate training instances with refined label vectors, complemented by a generative adversarial network that inversely maps label vectors back to input feature space samples. The training process involves a minimax adversarial game, strengthening the bidirectional alignment between input features and output labels. Comprehensive evaluations across multiple datasets confirm that PML-GAN achieves state-of-the-art performance in partial multi-label learning.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1920", "problem_id": "19200001", "content": "This paper presents two novel self-supervised learning strategies for enhancing the performance and self-learning capabilities of Graph Convolutional Networks (GCNs): randomly removed links with a fixed step at one region (RRLFSOR) and randomly removing links with a fixed step at some blocks (RRLFSSB), which addresses the issue of adjacent nodes lacking a selected step. The effectiveness of these strategies is demonstrated through experiments on transductive link prediction tasks, where they consistently outperform baseline models by up to 21.34% in terms of accuracy across three benchmark datasets.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1921", "problem_id": "19210001", "content": "The examination of adversarial examples and their activation patterns has become a crucial aspect of developing secure and robust deep neural networks (DNNs). This paper presents a novel perspective on adversarial examples, focusing on two key characteristics related to channel-wise activation: firstly, adversarial examples exhibit higher activation magnitudes compared to natural examples; and secondly, adversarial examples tend to activate channels more uniformly than natural examples. Our analysis reveals that while state-of-the-art defense methods, such as adversarial training, have successfully mitigated the issue of high activation magnitudes, the problem of uniform activation persists. To address this, we propose a Channel-wise Activation Suppressing (CAS) strategy, designed to suppress unnecessary activation triggered by adversarial perturbations. By incorporating CAS, we demonstrate that it is possible to train models that inherently resist adversarial activation, and that this approach can be seamlessly integrated with existing defense methods to enhance their robustness, thereby providing a straightforward yet effective training strategy for robustifying intermediate layer activation in DNNs.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1922", "problem_id": "19220001", "content": "Statistical operations like density estimation and approximate Bayesian inference frequently deal with distributions that have unknown normalizing constants. Score-based approaches, such as score matching, are widely used because they avoid the need for normalizing constants. While these methods have theoretical support, they exhibit practical limitations when applied to unnormalized distributions with isolated components—failing to detect such components or accurately determine mixing proportions. We illustrate these shortcomings using straightforward examples and propose heuristic solutions to mitigate them. Our goal is to highlight these challenges for both researchers and practitioners working on algorithm development and applications.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1923", "problem_id": "19230001", "content": "Vision Transformers (ViTs) and MLP-based architectures represent ongoing advancements in substituting manually designed features or inductive biases with versatile neural structures. Current approaches enhance these models through extensive data utilization, including large-scale pretraining and intensive data augmentation, yet they still encounter optimization challenges like sensitivity to initialization and learning rates. This study examines ViTs and MLP-Mixers through the perspective of loss geometry, aiming to enhance their training efficiency and generalization performance. Analysis of visualization and Hessian matrices indicates that converged models exhibit highly sharp local minima. By applying a sharpness-aware optimizer to encourage smoother loss landscapes, significant improvements in accuracy and robustness are achieved across tasks such as supervised, adversarial, contrastive, and transfer learning (e.g., +5.3% and +11.0% top-1 accuracy on ImageNet for ViT-B/16 and Mixer-B/16, respectively, using basic Inception-style preprocessing). Enhanced smoothness is linked to fewer active neurons in early layers. The refined ViTs surpass similarly sized ResNets in performance and speed when trained from scratch on ImageNet without relying on extensive pretraining or aggressive data augmentation, while also displaying more insightful attention maps.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1924", "problem_id": "19240001", "content": "Autosomal Dominant Polycystic Kidney Disease (ADPKD), marked by the progressive proliferation of renal cysts, is the most common and potentially fatal monogenic kidney disorder, impacting approximately one in every 500-100 individuals. Total Kidney Volume (TKV), assessed from Computed Tomography images, has been recognized as a critical prognostic indicator of renal function deterioration. However, the significant variation in kidney morphology and size in ADPKD diminishes the accuracy of current TKV computation methods—including those utilizing 2D convolutional neural networks—making them inadequate for clinical application. This study presents a solution using multi-task 3D Convolutional Neural Networks for ADPKD segmentation, resulting in a mean DICE score of 0.95 and a mean absolute percentage TKV error of 3.86. Furthermore, to address the issue of class imbalance, we suggest a straightforward approach of bootstrapping cross-entropy loss and evaluate its effectiveness against the increasingly popular dice loss within the medical image segmentation field.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1925", "problem_id": "19250001", "content": "This study addresses the problem of determining the non-zero entries in the sparse inverse covariance matrix of a zero-mean Gaussian random vector using independent and identically distributed samples, which is equivalent to uncovering the graph structure of a sparse Gaussian Markov Random Field (GMRF). We introduce two new greedy algorithms for this task. The first method globally estimates the non-zero entries of the inverse covariance matrix through iterative forward and backward greedy steps, while the second independently estimates each node’s neighborhood using similar greedy steps and combines these to form a complete estimate. Our key contribution is a thorough theoretical analysis demonstrating sparsistency—consistent recovery of the inverse covariance matrix’s sparsity pattern. Notably, we prove that both greedy approaches accurately reconstruct the model’s structure with high probability using only O(d\\log(p)) samples, outperforming the O(d^2\\log(p)) samples required by existing ℓ₁-regularized Gaussian MLE (Graphical Lasso) techniques. Additionally, our methods impose weaker restricted eigenvalue and smoothness conditions compared to the stringent irrepresentable conditions demanded by ℓ₁-regularization. Extensive simulations validate our findings, comparing our greedy methods against ℓ₁-regularized Gaussian MLE and nodewise ℓ₁-regularized linear regression (Neighborhood Lasso).", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1926", "problem_id": "19260001", "content": "The objective of best group subset selection is to identify a small number of non-overlapping groups that provide the greatest interpretability for the response variable. Although this approach holds practical value for selecting group variables, it has not garnered significant attention due to challenges associated with computational inefficiency in high-dimensional contexts. To address the gap in effective algorithms for best group subset selection, this paper presents a group-splicing algorithm that systematically identifies useful groups while eliminating ineffective ones. Additionally, through the integration of an innovative Bayesian group information criterion, we introduce an adaptive algorithm to ascertain the accurate size of the group subset. Our algorithms are designed to reliably find the optimal group subset in polynomial time under reasonable conditions. We validate the efficacy and precision of our method by comparing it with leading algorithms on both synthetic and real-world datasets.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1927", "problem_id": "19270001", "content": "Image inpainting inherently permits multiple plausible solutions when reconstructing incomplete images without additional constraints. Recent approaches for generating diverse inpainting results have demonstrated promise but often struggle with maintaining quality, frequently producing distorted structures or blurry textures. To address this, we introduce a two-stage model: the first stage produces multiple coarse outputs with varying structures, while the second stage refines each by enhancing texture details. Our design draws from the hierarchical vector quantized variational auto-encoder (VQ-VAE), which separates structural and textural information through its layered architecture. The vector quantization in VQ-VAE facilitates autoregressive modeling of discrete structural distributions, enabling diverse and high-quality structure generation in the first stage. For the second stage, we incorporate a structural attention module within the texture generation network to leverage structural cues for capturing long-range dependencies. Additionally, we reuse VQ-VAE to compute two feature losses, enhancing structural coherence and texture realism. Evaluations on CelebA-HQ, Places2, and ImageNet confirm that our approach not only increases solution diversity but also elevates the visual quality of generated images. Code and models are accessible at: https://github.com/USTC-JialunPeng/Diverse-Structure-Inpainting.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1928", "problem_id": "19280001", "content": "In the past five years, convolutional neural networks (CNNs) have achieved significant success in semantic segmentation, a fundamental task in various applications including autonomous driving and augmented reality. However, training CNNs necessitates a substantial volume of data, which is often challenging to obtain and tedious to annotate. Recent developments in computer graphics allow for the training of CNNs using photo-realistic synthetic images accompanied by computer-generated annotations. Nevertheless, the performance of these models is compromised by the domain discrepancy between real images and synthetic data. To address this issue, we introduce a curriculum-style learning methodology aimed at reducing the domain gap in urban scene semantic segmentation. This approach prioritizes easier tasks to extract essential characteristics of the target domain, starting with the initial task of learning global label distributions across images and local distributions among landmark superpixels, which are straightforward to estimate due to the distinctive features of urban scenes, such as the dimensions and spatial arrangements of buildings, streets, and vehicles. Subsequently, we train a segmentation network, ensuring that its predictions in the target domain align with these inferred characteristics. Our experiments demonstrate that our method surpasses baseline performance on two datasets and across two backbone networks, and we also provide comprehensive ablation studies regarding our approach.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1929", "problem_id": "19290001", "content": "Transfer learning enhances generalization in machine learning by leveraging parameters pre-trained on related tasks. In image recognition, deep neural networks often achieve better results with transfer learning, even with scarce training data. This study introduces a two-stage feature transfer approach tailored for textural medical image recognition, where a model undergoes sequential training using large natural image datasets, followed by textural images and the target dataset. The method was tested on classifying X-ray computed tomography images of diffuse lung diseases, outperforming both traditional single-stage transfer learning and training from scratch. Experiments also demonstrated its superior robustness with varying dataset sizes. Additionally, feature visualization revealed that the two-stage approach captures both edge and textural features of lung diseases, a capability absent in single-stage transfer models.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1930", "problem_id": "19300001", "content": "Recent advancements in data-driven image inpainting techniques have significantly influenced essential image editing tasks, including object elimination and restoration of damaged images. These novel methods outperform traditional ones; however, they are constrained by memory limitations, restricting them to low-resolution inputs, generally below 1K. In contrast, images taken with mobile devices can reach resolutions up to 8K. Simply enlarging the low-resolution inpainted output results in images that are large but lack clarity. On the other hand, superimposing a high-frequency residual image over the blurry output can produce a detailed and sharp image. Inspired by this approach, we introduce a Contextual Residual Aggregation (CRA) mechanism that generates high-frequency residuals by weighted aggregation from surrounding patches, thus necessitating only low-resolution predictions from the network. This design allows convolutional layers of the neural network to work with low-resolution data, effectively minimizing memory and computing power requirements and reducing dependence on high-resolution training datasets. Our experiments demonstrate that we can train the proposed model on smaller 512x512 resolution images and run inference on high-resolution images, resulting in impressive inpainting quality. Our method is capable of inpainting images up to 8K with sizeable gaps, which is a challenge for previous learning-based methods. Additionally, we detail the lightweight design of the network architecture, achieving real-time performance on 2K images using a GTX 1080 Ti GPU. Codes are available at: Atlas200dk/sample-imageinpainting-HiFill.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1931", "problem_id": "19310001", "content": "The protection of user data confidentiality is increasingly becoming a significant challenge in contemporary deep learning research. Traditional data-driven model compression methods are at a greater risk of performance decline when access to data is restricted. Recent studies have suggested generating images from a specific pretrained model to create training data. However, the inversion process relies on biased feature statistics from a single model, transitioning from low-dimensional to high-dimensional representations. This approach inevitably faces challenges related to generalizability and inaccurate inversion, resulting in subpar performance. To tackle these issues, we introduce MixMix, which is founded on two straightforward yet powerful strategies: (1) Feature Mixing: it employs multiple models to establish a universal feature space that facilitates generalized inversion; (2) Data Mixing: it combines synthesized images and labels to produce accurate label information. We demonstrate the efficacy of MixMix from both theoretical and empirical standpoints. Comprehensive experiments reveal that MixMix surpasses existing methods in key compression tasks, including quantization, knowledge distillation, and pruning. Notably, MixMix achieves improvements of up to 4% in quantization accuracy and 20% in pruning, compared to other data-free compression techniques.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1932", "problem_id": "19320001", "content": "Identifying causal relationships among variables is a core challenge across various fields. Yet, most advanced techniques often overlook the common issue of missing values in observational datasets, which can severely degrade causal discovery algorithms or cause them to fail entirely. This paper introduces a method for uncovering causal structures from incomplete data by employing a specialized encoder and reinforcement learning (RL). The encoder is tailored for both imputing missing values and extracting meaningful features, learning to transform available (yet incomplete) data into a resilient representation that guides the search for optimal causal graphs. Integrated within an RL framework optimized via the actor-critic algorithm, our approach processes incomplete observational data to produce a causal structure graph. Tests on synthetic and real-world datasets confirm that our method reliably constructs causal models from incomplete data, outperforming conventional combinations of imputation and causal discovery techniques by up to 43.2%.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1933", "problem_id": "19330001", "content": "This research introduces a new computer-aided diagnosis (CADx) framework designed to enhance the interpretability of breast mass classification. While deep learning has shown promise in medical image analysis, including CADx, existing approaches often lack the ability to explain their diagnostic decisions, hindering their deployment in real-world clinical settings where justified explanations are essential. To address this, we explore interpretability in CADx by presenting an interpretable CADx (ICADx) framework. This framework utilizes a generative adversarial network composed of an interpretable diagnosis network and a synthetic lesion generative network, designed to learn the association between malignancy and a standardized description (BI-RADS). Through adversarial learning, the lesion generative network and the interpretable diagnosis network are iteratively refined. Validation on a public mammogram database demonstrates the effectiveness of the proposed method. Experimental results indicate that the ICADx framework effectively provides mass interpretability alongside accurate mass classification, primarily due to its training in discerning the relationship between malignancy and interpretations via adversarial learning. These findings suggest that the proposed ICADx framework holds significant potential for advancing CADx system development.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1934", "problem_id": "19340001", "content": "Numerous obstacles currently hinder non-experts from effectively utilizing machine learning solutions, including a lack of understanding of statistical learning methods and the complexities of hyperparameter optimization. These challenges have fueled a significant interest in automated machine learning (AutoML), where a ready-to-use system can manage many processes for end-users without requiring machine learning expertise. This paper introduces Ensemble Squared (Ensemble^2), an AutoML system that combines the outputs of cutting-edge open-source AutoML solutions. Ensemble^2 takes advantage of the diversity among existing AutoML systems by utilizing the variations in their model search spaces and heuristics. Our empirical findings indicate that the diversity present in each AutoML system is adequate to support ensembling at the AutoML system level. In the course of this exploration, we also achieve new state-of-the-art results in AutoML on the OpenML tabular classification benchmark.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1935", "problem_id": "19350001", "content": "Constructing accurate generative models is crucial for comprehending the spatial arrangement of cells and tissues, and this paper presents a novel approach to building such models for electron microscope images with densely annotated cell membranes and mitochondria. We propose a two-stage methodology utilizing Generative Adversarial Networks (GANs) in a supervised manner to generate realistic images. Initially, a synthetic label image is produced from a noise image, which subsequently serves as supervision for synthesizing EM images in the second stage, resulting in the natural generation of label-image pairs. The efficacy of our model is evaluated through assessments of shape features, global statistics, segmentation accuracies, and user studies, demonstrating the production of accurate synthetic EM images. Furthermore, we enhance our model by incorporating a reconstruction loss on intermediate synthetic labels, thereby integrating the two stages into a single end-to-end framework, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1936", "problem_id": "19360001", "content": "We introduce a novel framework for dimension reduction, feature extraction, and moment reconstruction in dynamical systems, which operates on probability measure spaces induced by system observables rather than the original observable data space. By leveraging the fact that system orbits induce probability measures over the measurable space of (partial) observations, we endow the space of these measures with a divergence, or distance between distributions, to define a kernel integral operator. The eigenfunctions of this operator form an orthonormal basis capturing various timescales of the system. A key finding reveals that the evolution of dynamics-dependent probability measure moments relates to a time-averaging operator on the original system, enabling moment expansion in the eigenfunction basis and facilitating nonparametric forecasting. Furthermore, if the probability measure collection forms a manifold, we can equip the statistical manifold with a Riemannian metric and apply information geometry techniques. Applications to ergodic systems on the 2-torus and Lorenz 63 system demonstrate the framework's efficacy, while a real-world example shows that a limited number of eigenvectors can accurately reconstruct moments (specifically, the first four moments) of an atmospheric time series, such as the multivariate Madden-Julian oscillation index.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "llama-33-70b-instruct-turbo-1937", "problem_id": "19370001", "content": "Reliably predicting events from multivariate time series data with irregular sampling and noisy observations is hindered by missing data, which poses a significant challenge. Traditional imputation methods used to complete the data before event prediction lack a systematic approach to addressing the uncertainty associated with missing values. In contrast, current joint modeling techniques can model longitudinal and event data jointly, calculating event probabilities based on longitudinal observations, but these methods rely on strong parametric assumptions and struggle to scale to large multivariate datasets. To address these limitations, we propose a novel approach that features several key advancements, including the development of a scalable and flexible joint model utilizing sparse multiple-output Gaussian processes, which can effectively handle complex structures and non-Gaussian noise. Additionally, we derive an optimal event prediction policy that balances the costs of delayed detection and incorrect assessments, and abstains from making predictions when the estimated event probability does not meet the derived confidence criteria, as demonstrated through experiments on a large dataset, which show that our framework substantially outperforms existing state-of-the-art techniques in event prediction.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1938", "problem_id": "19380001", "content": "Multi-scale techniques are commonly employed in blind image and video deblurring to achieve high performance in both traditional and modern deep learning methods. Typically, bicubic down-sampling is used to reduce spatial dimensions after applying a fixed kernel filter in multi-scale approaches. However, this fixed kernel may be suboptimal, potentially losing critical information, such as strong edges, which are essential for accurate deblurring. We introduce convolutional neural network (CNN)-based down-scaling methods for multi-scale deep-learning-based non-uniform single image deblurring. We posit that our CNN-based down-scaling effectively reduces the image's spatial dimension, while learned multi-channel kernels better preserve the necessary details for deblurring. At each scale, we utilize Residual Channel Attention Networks (RCAN) as a backbone to enhance performance. Our method significantly outperformed existing state-of-the-art techniques on the GoPro dataset, achieving a 2.59dB higher PSNR than Tao's method. The CNN-based down-scaling was crucial to this performance improvement, as its removal resulted in a 1.98dB decrease. Furthermore, when trained on the GoPro dataset and evaluated on the large-scale Su dataset, our method surpassed Tao's method by 1.15dB in PSNR. Qualitative results on the Lai dataset also corroborated the superior performance of our method compared to other state-of-the-art approaches.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1939", "problem_id": "19390001", "content": "The notable efficacy of deep neural networks (DNNs) in diverse applications is juxtaposed with their well-documented susceptibility to adversarial perturbations, a phenomenon that has garnered substantial interest within the machine learning community. A particularly surprising aspect of this vulnerability is the presence of universal adversarial perturbations (UAPs), which refer to a single perturbation capable of deceiving a target DNN for a majority of images, offering the advantage of being generated prior to the attack and applied in real-time. This survey provides an overview of recent advancements in universal adversarial attacks, with a focus on deep classifiers, examining the challenges posed from both offensive and defensive perspectives, as well as exploring the underlying reasons for the existence of UAPs, and further extending the discussion to universal attacks in various applications beyond deep classification.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1940", "problem_id": "19400001", "content": "Dynamic Time Warping (DTW) is extensively employed for processing temporal data. Nevertheless, current techniques are unable to learn class-specific discriminative prototypes or utilize them for subsequent analysis. We introduce Discriminative Prototype DTW (DP-DTW), an innovative approach designed to learn discriminative prototypes specific to each class for temporal recognition tasks. DP-DTW outperforms traditional DTWs in time series classification benchmarks. When integrated with end-to-end deep learning, DP-DTW effectively addresses complex weakly supervised action segmentation issues, achieving state-of-the-art performance on standard benchmarks. Furthermore, the learned action prototypes provide detailed insights into the input video, allowing for action-based video summarization through the alignment of the input sequence with these prototypes.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1941", "problem_id": "19410001", "content": "Recent advancements in neural networks have enabled them to effectively participate in dialogues, but existing methods are limited to verbal text and neglect the nuances of in-person conversations. To address this, we present a neural conversation model designed to interpret and generate facial gestures in conjunction with text, allowing it to adjust its responses according to the conversation's emotional tone. Our approach utilizes an RNN encoder-decoder that leverages both facial muscle movements and verbal dialogue. The decoder features a two-layer architecture, where the first layer generates verbal responses and coarse facial expressions, and the second layer refines these outputs by adding subtle gestures, resulting in more natural and smooth interactions. Trained on a dataset of 250 movies, our neural network is evaluated through automated metrics and a human study, demonstrating its capability to produce more realistic conversations. An example application of our face-text model is showcased through a face-to-face chatting avatar, highlighting its potential in generating more engaging and human-like interactions, as seen in Figure A, B, C (citations remain unchanged).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1942", "problem_id": "19420001", "content": "Differentiable neural architecture search (DNAS) excels at automatically designing high-performance neural networks, but its memory demands escalate rapidly with larger search spaces, often exceeding the capacity of even high-end GPUs. In contrast, reinforcement learning (RL) approaches are memory-efficient but require excessive computation time. To address these limitations, this work introduces RADARS, a scalable hybrid framework that integrates RL and DNAS to efficiently explore extensive search spaces. RADARS employs RL to progressively eliminate suboptimal architecture candidates and narrows the search to a promising subspace for DNAS optimization. Evaluations on CIFAR-10 and ImageNet using a 12 GB GPU workstation demonstrate that RADARS achieves up to 3.41% improved accuracy while reducing search time by 2.5X compared to leading RL-based methods, whereas standard DNAS baselines fail due to memory or time constraints. To the authors' knowledge, RADARS represents the first DNAS framework capable of managing large search spaces within fixed memory limits.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1943", "problem_id": "19430001", "content": "Recently, recurrent neural networks (RNNs) have been successfully applied to electronic medical records (EMRs), which encompass patients' diagnostic histories, medications, and various events, to forecast their current and future health states. Although RNNs demonstrate strong performance, understanding the rationale behind specific predictions often proves difficult for users. This opaque nature of RNNs can hinder their broader implementation in clinical settings. Additionally, there are no established methods for interactively integrating users' domain knowledge and expertise as inputs to guide the model. Therefore, our design study seeks to create a visual analytics solution aimed at enhancing the interpretability and interactivity of RNNs through collaboration among medical professionals, AI researchers, and visual analytics experts. Through an iterative design process with these specialists, we developed, implemented, and assessed RetainVis, a visual analytics tool that integrates an enhanced, interpretable, and interactive RNN model named RetainEX with visualizations that facilitate users' exploration of EMR data in prediction contexts. Our findings illustrate how RetainVis effectively reveals the influence of specific medical codes on risk predictions, using EMR data from patients with heart failure and cataract symptoms. Moreover, we highlight significant modifications made to the advanced RNN model RETAIN to utilize temporal data and enhance interactivity. This research serves as a valuable resource for those aiming to create interpretable and interactive visual analytics tools for RNNs.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1944", "problem_id": "19440001", "content": "This research investigates deep reinforcement learning (RL) methods in scenarios where rewards are delayed. Real-world applications frequently lack immediate rewards or may not define them right after an agent's actions. We begin by formally characterizing environments with delayed rewards and analyzing the challenges posed by their non-Markovian properties. Next, we propose a versatile off-policy RL framework featuring a novel Q-function formulation designed to manage delayed rewards while ensuring theoretical convergence. To address high-dimensional state spaces in practical settings, we incorporate the HC-decomposition rule within our framework, enabling an approximation technique that enhances training efficiency and stability. Comprehensive experiments validate the outperformance of our algorithms compared to existing approaches and their variants.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1945", "problem_id": "19450001", "content": "Top-down saliency models generate probability distributions highlighting task-specific target locations, such as object detection, typically trained with pixel-level supervision. We introduce a weakly supervised framework that relies solely on binary labels indicating object presence or absence in an image. By analyzing the probabilistic influence of image regions on a CNN classifier’s confidence through backtracking, we derive top-down saliency. Among multiple bottom-up saliency maps generated for an image, we identify the most task-relevant one and merge it with the top-down saliency. High-saliency features are then used to train a linear SVM for feature saliency estimation, which is integrated with the combined saliency and refined via multi-scale superpixel averaging. Our weakly supervised method achieves performance comparable to fully supervised techniques, as demonstrated through evaluations on seven challenging datasets, with quantitative comparisons against 40 related approaches across four applications.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1946", "problem_id": "19460001", "content": "Numerous natural language understanding tasks involve analyzing relationships between pairs of sequences, including natural language inference, paraphrasing, and entailment. While these tasks share fundamental similarities, they are typically addressed through separate models. Transferring knowledge between related tasks can enhance performance, but indiscriminate knowledge transfer may introduce irrelevant information, resulting in sub-optimal outcomes due to negative transfer. This study explores the transferability of both instances and parameters across natural language understanding tasks by introducing an ensemble-based transfer learning technique. The key innovation lies in integrating instance and parameter transfer to enhance neural network adaptability, termed as the proposed method. We introduce a simple yet effective strategy for adapting source networks to target tasks in few-shot learning, which minimizes negative transfer by employing a decaying parameter adjusted based on slope variations in a smoothed spline error curve during training. The method is evaluated against hard and soft parameter sharing approaches in few-shot scenarios, as well as fully supervised models trained on target tasks. This adaptation yields superior transfer learning performance, achieving results comparable to state-of-the-art methods while utilizing significantly less target task data.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1947", "problem_id": "19470001", "content": "Recent advancements in single-image super-resolution (SISR) networks, which can customize their parameters based on specific input images, have yielded encouraging outcomes by leveraging both the intrinsic information within the input data and extensive external datasets. However, the application of these self-supervised SISR techniques to video processing remains unexplored. In response, we introduce a novel learning algorithm enabling traditional video super-resolution (VSR) networks to adjust their parameters for test video frames without relying on ground-truth datasets. By utilizing numerous self-similar patches over both spatial and temporal dimensions, we enhance the effectiveness of fully pre-trained VSR networks and achieve temporally coherent video frames. Additionally, we propose a test-time knowledge distillation approach that improves adaptation speed while requiring fewer hardware resources. Our experiments illustrate that this innovative learning algorithm can effectively fine-tune cutting-edge VSR networks, significantly boosting performance across various benchmark datasets.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1948", "problem_id": "19480001", "content": "All reinforcement learning algorithms must negotiate the balance between exploration and exploitation. Numerous advanced deep reinforcement learning techniques apply noise to action selection, such as employing Gaussian noise in policy gradient approaches or implementing \\epsilon-greedy strategies in Q-learning. Although these techniques are attractive due to their straightforwardness, they lack a systematic exploration of the state space. We propose a novel method that utilizes a model to generate reward bonuses, serving as a form of intrinsic motivation to enhance model-free reinforcement learning. A fundamental insight of our approach is that this dynamics model can be learned within the latent feature space of a value function, encapsulating the interactions between the agent and the environment. This strategy is both supported by theory and computationally efficient, enabling the effective application of Bayesian information-theoretic methods in high-dimensional state environments. We assess our method across various continuous control tasks, with an emphasis on optimizing exploration.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1949", "problem_id": "19490001", "content": "Image captioning, a prominent research area in vision and language, has seen considerable advancement recently. Nevertheless, current methodologies often produce generic captions comprised of common terms, leading to imprecise and indistinguishable descriptions (see Figure 1). This is mainly caused by (i) the cautious nature of conventional training objectives, which leads the model to create accurate but uninformative captions for similar images, and (ii) the skewed word distribution in the ground-truth captions, which favors the generation of frequent words while suppressing less common but more specific terms. In this paper, we introduce a new global-local discriminative objective, built upon a reference model, to promote the generation of detailed descriptive captions. Globally, we develop a novel global discriminative constraint that guides the generated sentence to more effectively distinguish the corresponding image from all others in the dataset. Locally, a local discriminative constraint is introduced to enhance attention, emphasizing less frequent but more concrete words, thereby improving the generation of captions that better capture the visual details of the images. Evaluations on the MS-COCO dataset demonstrate that our approach significantly surpasses baseline methods and achieves competitive results compared to state-of-the-art techniques. Furthermore, self-retrieval experiments confirm the discriminative power of our method.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1950", "problem_id": "19500001", "content": "A significant obstacle in materials science involves navigating the extensive chemical design space to identify materials with targeted characteristics. A promising approach involves creating sampling techniques that leverage both explicit chemical insights and implicit compositional patterns found within extensive materials databases. This study introduces MatGAN, a generative machine learning framework utilizing a generative adversarial network (GAN) to produce novel inorganic materials efficiently. When trained on the ICSD database, the GAN model generates previously unseen hypothetical materials, achieving a novelty rate of 92.53% across 2 million generated samples. Remarkably, 84.5% of these samples meet chemical validity criteria (charge neutrality and electronegativity balance) despite the absence of explicit chemical constraints in the model, demonstrating its ability to infer implicit composition rules. This method holds potential for accelerating inverse design and computational screening of inorganic materials.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1951", "problem_id": "19510001", "content": "The Vision-language Navigation (VLN) task involves guiding an agent to navigate through a series of steps based on visual observations and natural language instructions, which is complicated by large data bias resulting from the significant disparity between the limited data scale and vast navigation space. While prior research has explored various data augmentation techniques to mitigate this issue, these methods fail to explicitly address the data bias that arises from differences between distinct house scenes, leading to overfitting in seen scenes and subpar performance in unseen ones. To address this limitation, we introduce the Random Environmental Mixup (REM) approach, which generates augmented data by creating cross-connected house scenes through environmental mixup. This process entails selecting key viewpoints from each scene's room connection graph, cross-connecting key views from different scenes to form new scenes, and ultimately generating augmented instruction-path pairs within these cross-connected environments. As evidenced by experiments on benchmark datasets, our REM-based augmentation strategy enables the agent to narrow the performance gap between seen and unseen environments, ultimately yielding improved overall performance and establishing our model as the state-of-the-art approach on the standard VLN benchmark, as shown in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1952", "problem_id": "19520001", "content": "Semantic image segmentation poses significant challenges in the field of computer vision. In this study, we introduce a highly integrated convolutional network comprising three components: feature downsampling, combined feature upsampling, and multiple predictions. Our approach employs a strategy that incorporates several stages of upsampling and integrated feature maps within pooling layers alongside their associated unpooling layers. We generate multiple preliminary outputs, with each one arising from an unpooling layer through a single step of upsampling. Ultimately, we merge these preliminary outputs to form the final result. Consequently, our proposed network effectively leverages feature information by fusing and reusing feature maps. Furthermore, during the training process, we implement multiple soft cost functions for both the preliminary and final outputs, which helps minimize loss reduction during backpropagation. We assess our model on three prominent segmentation datasets: CamVid, PASCAL VOC, and ADE20K, achieving state-of-the-art performance on the CamVid dataset, alongside notable enhancements on the PASCAL VOC and ADE20K datasets.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "llama-33-70b-instruct-turbo-1953", "problem_id": "19530001", "content": "The widespread adoption of AI has raised significant concerns regarding algorithmic bias, discrimination, and fairness in recent years. Unlike traditional forms of human-induced bias, AI-generated algorithmic bias is more complex and less intuitive, making it more challenging to address and explain. A notable knowledge gap exists in the current literature regarding the evaluation and mitigation of bias in pruned neural networks. This study aims to address the complex issues of evaluating, mitigating, and explaining bias induced in pruned neural networks. We make three key contributions: firstly, we introduce two novel metrics, Combined Error Variance (CEV) and Symmetric Distance Error (SDE), to quantitatively assess the bias prevention quality of pruned models; secondly, we show that knowledge distillation can effectively mitigate induced bias in pruned neural networks, even when dealing with unbalanced datasets; and thirdly, we discover a strong correlation between model similarity and pruning-induced bias, providing a valuable explanation for the occurrence of bias in pruned neural networks, with our code available at https://github.com/codestar12/pruning-distilation-bias.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gemini20-1954", "problem_id": "19540001", "content": "This paper introduces a new stochastic network model, the Fractal Gaussian Network (FGN), designed to incorporate fractal structures that are both well-defined and analytically manageable. These structures have been found in various real-world applications. FGNs provide a continuous spectrum between purely random geometric graphs (also known as Poisson Boolean networks) and random graphs exhibiting stronger fractal characteristics. They constitute a parametric family of sparse random geometric graphs, controlled by a fractality parameter \\nu that determines the intensity of the fractal structure. FGNs are based on the underlying spatial geometry of Gaussian Multiplicative Chaos (GMC), which is itself a standard model for fractality. We derive asymptotic expressions for the expected number of edges and triangles in FGNs. Furthermore, we investigate the detection of fractality and the estimation of parameters from observed network data, along with the fundamental properties of FGNs as a random graph model. Finally, we analyze fractality within community structures by introducing a stochastic block model within the FGN framework.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1955", "problem_id": "19550001", "content": "In recent years, deep neural networks have emerged as effective tools for solving signal recognition problems, particularly in recognizing the nonlinear characteristics of signals. Nonetheless, the efficacy of many deep learning approaches is significantly contingent upon having a large volume of training data; classical neural networks tend to perform poorly when faced with limited training samples or unseen data during testing. This situation necessitates a more sophisticated approach known as model-agnostic meta-learning (MAML), which seeks to identify invariant representations of data samples or signals. In this work, motivated by the unique structure of signals, specifically the presence of real and imaginary components in practical time-series data, we introduce the Complex-valued Attentional MEta Learner (CAMEL) to tackle the challenge of few-shot signal recognition by integrating attention mechanisms and meta-learning within the complex domain. To our knowledge, this represents the first complex-valued implementation of MAML capable of identifying first-order stationary points for general nonconvex problems, backed by theoretical convergence assurances. Comprehensive experimental results demonstrate the advantages of the proposed CAMEL in comparison to leading methodologies.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1956", "problem_id": "19560001", "content": "This study investigates the relationship between human movement paths and head orientations, proposing that trajectory and head pose prediction can be addressed as a unified task. While existing methods for trajectory forecasting primarily utilize short-term pedestrian trajectories (tracklets) alongside social factors like intended destinations or interpersonal dynamics, we introduce MiXing-LSTM (MX-LSTM) to model the interaction between spatial positions and head orientations (vislets) through joint optimization of covariance matrices during LSTM backpropagation. The framework also treats head orientations as indicators of visual attention in social interaction modeling. MX-LSTM enhances conventional trajectory prediction by simultaneously forecasting future pedestrian locations and head poses, demonstrating superior performance across multiple public benchmarks compared to existing methods. The model excels particularly in low-speed scenarios, which pose significant challenges to other approaches, while also achieving reliable predictions over extended time horizons.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1957", "problem_id": "19570001", "content": "In the context of big data, diverse real-world applications produce extensive time-series datasets, many of which exhibit notable or latent periodic trends, such as those in weather and financial domains. Efficiently detecting these underlying periodic patterns and enabling precise forecasting from vast time-series data is essential. This paper introduces the Periodicity-based Parallel Time Series Prediction (PPTSP) algorithm, designed for large-scale time-series analysis and implemented within the Apache Spark cloud framework. To manage extensive historical datasets effectively, the Time Series Data Compression and Abstraction (TSDCA) algorithm is introduced, minimizing data volume while preserving key features. Building on this, the Multi-layer Time Series Periodic Pattern Recognition (MTSPPR) algorithm employs Fourier Spectrum Analysis (FSA) for pattern detection. Additionally, the Periodicity-based Time Series Prediction (PTSP) algorithm is proposed, forecasting future data by leveraging prior periodic models and incorporating a time decay factor to adjust the influence of historical periods. To enhance computational efficiency, a parallelized approach is implemented using Apache Spark’s Streaming module, utilizing Distributed Streams (DStreams) and Resilient Distributed Datasets (RDDs) for scalable data storage and processing. Comprehensive experiments demonstrate that the PPTSP algorithm outperforms existing methods in both prediction accuracy and computational performance.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1958", "problem_id": "19580001", "content": "Predicting the toxicity of chemical compounds remains a major scientific challenge. While recent advancements have improved accuracy, they often rely on extensive feature sets, complex black-box approaches like deep neural networks, and substantial computational power. This paper advocates for simpler machine learning models that are computationally efficient yet highly accurate. We present a single-task toxicity prediction framework utilizing only 2D features to minimize computational demands. By employing a decision tree, we efficiently select an optimal subset from thousands of features and integrate it with a shallow neural network, jointly optimizing both model parameters and input features. Our approach trains in just one minute on a single CPU, compared to existing deep learning methods requiring approximately 10 minutes on an NVidia Tesla K40 GPU, while achieving comparable or superior performance across multiple toxicity benchmarks. Additionally, we introduce a cumulative feature ranking technique to identify key features that assist chemists in preliminary toxicity screening.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1959", "problem_id": "19590001", "content": "Social media platforms are widely used for the rapid dissemination of information. A significant challenge for platforms such as Twitter is establishing the credibility of shared news in the absence of a formal verification system. Moreover, the rapid nature of social media makes the prompt identification of rumors a complex problem. This paper introduces an ensemble model that employs majority voting on predictions generated by deep neural networks. These networks utilize time-series vector representations of Twitter data to facilitate timely rumor detection. Experimental results using the PHEME dataset demonstrate that the integration of our proposed data pre-processing technique with the ensemble model enhances rumor detection performance. Specifically, the classification performance is improved by 7.9% in terms of the micro F1 score when compared to the baselines.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1960", "problem_id": "19600001", "content": "Recent advancements in vision-related tasks have largely focused on enhancing deep learning models through techniques like architectural innovations (e.g., Residual Networks) and regularization methods such as Batch Normalization. These approaches primarily aim to improve model conditioning, enabling the training of increasingly deeper networks. This paper explores enhanced conditioning for Generative Adversarial Networks (GANs) in unsupervised learning by integrating the discriminative power of decision forests into the GAN discriminator. Our approach yields a more stable and well-conditioned model, with empirical results demonstrating substantial qualitative and quantitative improvements over existing GAN methods on the Oxford Flowers and Aligned Celebrity Faces datasets.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1961", "problem_id": "19610001", "content": "Recent developments in drone applications equipped with cameras have heightened the demand for vision-based object detection algorithms applied to aerial imagery. The object detection process is fundamentally a complex challenge within the broader context of computer vision. However, the application of these algorithms in UAVs (or drones) presents additional difficulties, primarily due to several factors: (i) the scarcity of extensive datasets featuring a wide range of objects captured by drones, (ii) significant variance in orientation and scale of drone images compared to terrestrial images, and (iii) differences in texture and shape characteristics between ground and aerial visuals. Deep learning-based object detection methods can be categorized into two primary types: (a) single-stage detectors and (b) multi-stage detectors, each with its unique advantages and disadvantages. However, combining the strengths of both approaches could produce a more effective solution than either can offer independently. In this study, we introduce an ensemble network, SyNet, which integrates a multi-stage technique with a single-stage one, aiming to reduce the high false-negative rate associated with multi-stage detectors while enhancing the quality of proposals from single-stage detectors. Using CenterNet and Cascade R-CNN with pretrained feature extractors as foundational elements and employing an ensembling strategy, we present state-of-the-art results achieved through our method on two distinct datasets: MS-COCO, where we attained a mean Average Precision (mAP) of 52.1% on the val2017 dataset, and visDrone, where we achieved a mAP of 26.2% on the test set.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1962", "problem_id": "19620001", "content": "We demonstrate that pre-trained Generative Adversarial Networks (GANs), such as StyleGAN, can serve as a latent repository to enhance the restoration quality of high-factor image super-resolution (SR). Unlike most current SR techniques that seek to create realistic textures using adversarial loss, our approach, Generative LatEnt bANk (GLEAN), surpasses traditional methods by directly utilizing the abundant and varied priors contained in a pre-trained GAN. In contrast to widely used GAN inversion techniques, which necessitate costly image-specific optimization during execution, our method requires only a singular forward pass to produce the upscaled image. GLEAN can be readily integrated into a straightforward encoder-bank-decoder framework with multi-resolution skip connections. Altering the bank enables the method to accommodate images from various categories, such as cats, buildings, human faces, and cars. Images enhanced by GLEAN exhibit significant advancements in fidelity and texture authenticity compared to existing approaches.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1963", "problem_id": "19630001", "content": "Deep Neural Networks (DNNs) have made significant advancements in numerous practical applications, particularly when large-scale training data is available. Nevertheless, data isolation poses a critical challenge in current scenarios. Prior research addresses privacy-preserving DNN models through either algorithmic or cryptographic approaches. The algorithmic method partitions the DNN computation graph among data holders or between data holders and a server, offering scalability but compromising accuracy and privacy. Conversely, cryptographic techniques provide robust privacy guarantees but suffer from inefficiency and limited scalability. This paper introduces SPNN—a Scalable and Privacy-preserving deep Neural Network framework—combining both algorithmic and cryptographic perspectives. Algorithmically, SPNN divides the DNN computation graph, assigning private data-related tasks to data holders and offloading intensive computations to a high-performance server. Cryptographically, it employs secret sharing and homomorphic encryption to enable secure, collaborative computations among isolated data holders. Additionally, SPNN is implemented in a decentralized environment with user-friendly APIs. Real-world dataset experiments validate the framework's effectiveness.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1964", "problem_id": "19640001", "content": "Established optimization-based techniques can provide an optimal trajectory for a brief optimization horizon, generally not exceeding a few seconds. Consequently, while the selected optimal trajectory for this short duration may lead to a sub-optimal long-term outcome, the resulting short-term trajectories facilitate effective, comfortable, and verifiably safe maneuvers in a dynamic traffic setting. This study tackles the challenge of ensuring an optimal long-term driving strategy while preserving the advantages of traditional trajectory planning. We propose a Reinforcement Learning-based method that, in conjunction with a trajectory planner, acquires an optimal long-term decision-making strategy for highway driving. By generating locally optimal maneuvers as actions in real time, we achieve a balance between the endless low-level continuous action space and the restricted flexibility of a fixed set of predefined standard lane-change actions. Our evaluation of the method using realistic scenarios in the open-source traffic simulator SUMO demonstrated superior performance compared to four benchmark approaches, including a random action selection agent, a greedy agent, a high-level discrete actions agent, and an IDM-based SUMO-controlled agent.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1965", "problem_id": "19650001", "content": "Detecting text in scenes with arbitrary shapes has presented significant challenges in recent years. This paper introduces a novel text detection approach, SAST, featuring a segmentation-based design that utilizes a context-attended multi-task learning framework grounded in a Fully Convolutional Network (FCN) to learn diverse geometric properties essential for reconstructing polygonal representations of text regions. By considering the sequential nature of text, we incorporate a Context Attention Block to effectively capture long-range dependencies in pixel data, leading to more accurate segmentation. For post-processing, we propose a Point-to-Quad assignment method that clusters pixels into distinct text instances by merging high-level object knowledge with low-level pixel information in a single step. Additionally, the proposed geometric properties enable the extraction of polygonal representations of text with arbitrary shapes more efficiently. Experimental results on various benchmarks, including ICDAR2015, ICDAR2017-MLT, SCUT-CTW1500, and Total-Text, indicate that SAST provides equal or superior performance in accuracy. The algorithm operates at 27.63 FPS on SCUT-CTW1500, achieving an Hmean of 81.0% using a single NVIDIA Titan Xp graphics card, outperforming many existing segmentation-based methods.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1966", "problem_id": "19660001", "content": "This study explores user modeling through the analysis of photo and video collections on mobile devices. A new engine is introduced for predicting user preferences, utilizing scene recognition, object detection, and facial analysis techniques. Initially, facial clustering is performed on the gallery, and private photos and videos containing faces from large clusters are processed offline on the embedded system. Remaining photos are potentially analyzed remotely using deep learning models. Visual features extracted from scene recognition and object detection models are then consolidated into a unified user descriptor using a neural attention mechanism. The described pipeline is implemented on the Android platform. Experiments conducted using a subset of the Photo Event Collection, Web Image Dataset for Event Recognition, and Amazon Fashion datasets show that images can be processed efficiently with minimal loss of accuracy. The Android mobile application source code is accessible at https://github.com/HSE-asavchenko/mobile-visual-preferences.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1967", "problem_id": "19670001", "content": "The swift advancement of high-throughput technologies has facilitated the creation of extensive biological and disease-related datasets, encompassing multiple layers such as genomic, proteomic, and metabolomic data, and originating from diverse sources like disease subtypes or experimental conditions. In this study, we introduce a broad statistical framework, grounded in Gaussian graphical models, for the horizontal (across conditions or subtypes) and vertical (across molecular compartment data layers) integration of information within these datasets. Our approach begins by decomposing the multi-layer problem into a series of two-layer problems. For each two-layer instance, we model the outcomes at a lower-layer node as contingent on other nodes within that layer, as well as all nodes in the upper layer. We employ a combination of neighborhood selection and group-penalized regression to derive sparse estimates for all model parameters. Subsequently, we develop a debiasing method and asymptotic distributions of inter-layer directed edge weights, leveraging pre-computed neighborhood selection coefficients for nodes in the upper layer. We then establish global and simultaneous testing procedures for these edge weights. The efficacy of the proposed methodology is assessed using both synthetic and real data.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1968", "problem_id": "19680001", "content": "The leading techniques in image segmentation currently create dense representations by jointly processing color, shape, and texture information within a deep CNN. However, this approach may not be optimal due to the distinct nature of the information important for recognition. We introduce a novel two-stream CNN architecture for semantic segmentation that distinctly manages shape information through a dedicated processing branch, known as the shape stream, which operates alongside the traditional stream. A critical aspect of this architecture is the introduction of new gates that interconnect the intermediate layers of both streams. These gates utilize higher-level activations from the classical stream to regulate lower-level activations in the shape stream, thereby filtering out noise and directing the shape stream to concentrate on relevant boundary-related data. This design allows for a lightweight architecture for the shape stream, which functions at the image-level resolution. Our experimental results indicate that this approach produces a highly effective model, yielding sharper predictions near object boundaries and significantly enhancing performance on smaller and thinner objects. Our method achieves state-of-the-art results on the Cityscapes benchmark, with improvements of 2% and 4% in mask (mIoU) and boundary (F-score) quality, respectively, compared to strong baseline models.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1969", "problem_id": "19690001", "content": "In recent years, non-linear spectral decompositions of images utilizing one-homogeneous functionals like total variation have attracted significant interest. These decompositions facilitate the extraction of spectral components associated with varying sizes and contrasts of objects, thus enabling applications such as filtering, feature transfer, and image fusion. Nonetheless, the process of obtaining this decomposition necessitates addressing multiple non-smooth optimization challenges, making it highly computationally demanding. In this paper, we introduce a neural network-based approximation for a non-linear spectral decomposition, achieving processing speed improvements of up to four orders of magnitude (× 10,000) for mega-pixel images compared to traditional GPU implementations. Our proposed network, TVSpecNET, can implicitly learn the underlying partial differential equations and, although fully data-driven, retains the invariances of the model-based transformation. To our knowledge, this represents the first effort to learn a non-linear spectral decomposition for images. This method not only provides remarkable computational benefits but also paves the way for exploring neural networks capable of decomposing images into user-defined spectral components instead of relying on manually designed functionals.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1970", "problem_id": "19700001", "content": "Few/zero-shot learning presents a significant challenge in classification tasks, particularly when a classifier must identify instances from classes with minimal or no training examples. This difficulty escalates in multi-label classification, where each instance is associated with multiple classes. We propose a straightforward multi-graph aggregation model that combines knowledge from various label graphs representing diverse semantic relationships to investigate how aggregated knowledge enhances multi-label few/zero-shot document classification. The model leverages three types of semantic information: pre-trained word embeddings, label descriptions, and predefined label relations. Tests conducted on two extensive clinical datasets (i.e., MIMIC-II and MIMIC-III) and the EU legislation dataset demonstrate that approaches incorporating multi-graph knowledge aggregation yield notable performance gains across nearly all metrics for few/zero-shot labels.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1971", "problem_id": "19710001", "content": "Scalable access to high-quality education is hindered by the challenge of offering meaningful feedback on open-ended tasks in structured fields such as programming, graphics, and short-answer questions. This issue has been particularly challenging: human graders face extensive manual effort, while automated systems have historically struggled to match human-level precision. This paper introduces generative grading, an innovative computational method for large-scale feedback that delivers accurate assessments and detailed, interpretable responses. The approach leverages probabilistic programs to model student cognition, generating vast datasets of labeled solutions, which then enable feedback inference for actual student submissions. We evaluate our method in three contexts: block-based programming, where feedback accuracy surpasses prior benchmarks by 50%, exceeding human performance; graphical tasks and short-text responses, where improvements over existing methods reach 4x and 1.5x, nearing human-level precision; and a classroom trial demonstrating doubled grading accuracy alongside a 50% reduction in time when augmenting human graders with our system.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "deepseekv3-1972", "problem_id": "19720001", "content": "SigNet represents a cutting-edge model for feature extraction in handwritten signature verification (HSV), employing a Deep Convolutional Neural Network (DCNN) with 2048-dimensional representations. When mapped to a dissimilarity space via dichotomy transformation (DT) under a writer-independent (WI) framework, these features may exhibit redundancy. This study examines overfitting risks when employing Binary Particle Swarm Optimization (BPSO) for wrapper-based feature selection and introduces a global validation approach with an external archive to mitigate overfitting while identifying the most discriminative features. Additionally, the research explores the applicability of selected features in transfer learning scenarios. Evaluations conducted on CEDAR, MCYT, and GPDS datasets under a WI paradigm demonstrate that optimization without validation leads to overfitting, whereas the proposed validation strategy enhances performance. The reduced feature space also proves effective for transfer learning applications.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1973", "problem_id": "19730001", "content": "Considerable research attention has been devoted to examining human brain connectomes, which are inferred from various imaging modalities, and investigating their connections to human characteristics, such as cognitive abilities. Typically, brain connectomes are represented as complex networks, where nodes denote distinct regions of interest (ROIs) and edges signify the strengths of connections between these ROIs. However, due to the high-dimensional and non-Euclidean properties of these networks, visualizing their distribution across populations and establishing links to human traits poses significant challenges. Existing methods often rely on predefined topological features or principal components analysis (PCA) to summarize networks. Leveraging recent breakthroughs in deep learning, this study introduces a novel nonlinear latent factor model, termed Graph AuTo-Encoding (GATE), to capture the population distribution of brain graphs and elucidate the relationships between brain structural connectomes and human traits. The efficacy of GATE was evaluated using two extensive brain imaging datasets, the Adolescent Brain Cognitive Development (ABCD) study and the Human Connectome Project (HCP) for adults, to gain insights into the structural brain connectome and its cognitive correlations. The results show that GATE outperforms competing methods in terms of predictive accuracy, statistical inference, and computational efficiency, revealing a stronger association between structural connectomes and a broad range of human cognitive traits than previously acknowledged, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1974", "problem_id": "19740001", "content": "A depth map can be expressed as a combination of learned basis functions and can be resolved efficiently via a closed-form solution. However, a challenge with this approach is the potential for artifacts to arise when color boundaries do not align with depth boundaries, a situation frequently encountered in natural images. To overcome this challenge, we adopt a more rigorous approach in depth recovery by utilizing a piece-wise planar model. Specifically, we articulate the desired depth map as a series of 3D planes, and we frame the reconstruction task as an optimization of the parameters defining these planes. This problem can be approached as a continuous CRF optimization problem and resolved using a particle-based method (MP-PBP) \\cite. Comprehensive experiments on the KITTI visual odometry dataset demonstrate that our proposed methods exhibit strong resistance to false object boundaries and can produce effective and aesthetically pleasing 3D point clouds.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1975", "problem_id": "19750001", "content": "Machine learning models operating as black boxes are increasingly employed in critical decision-making processes across various sectors, including healthcare and criminal justice, leading to significant challenges. While some believe that developing techniques to explain these opaque models could mitigate such issues, relying on explanations rather than building inherently interpretable models may reinforce poor practices and pose severe societal risks. A viable solution lies in constructing models that are transparent by design. This paper highlights the distinction between explaining black box models and employing intrinsically interpretable ones, discusses key reasons to avoid explainable black boxes in high-stakes scenarios, outlines obstacles in interpretable machine learning, and presents practical examples where transparent models could replace black boxes in fields like criminal justice, healthcare, and computer vision.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1976", "problem_id": "19760001", "content": "The application of deep neural networks to image segmentation tasks yields highly accurate results, but is hindered by the need for extensive manually annotated datasets for supervised training. To mitigate this limitation, few-shot learning techniques have been developed, enabling the learning of new classes from a limited number of annotated support examples. This study presents a novel few-shot learning framework designed for the segmentation of volumetric medical images, leveraging only a small number of annotated slices. Unlike related works in computer vision, medical image segmentation poses unique challenges, including the lack of pre-trained networks and the volumetric nature of medical scans. To address these challenges, a new architecture is proposed, incorporating 'squeeze & excite' blocks, which consists of two arms: a conditioner arm that generates a task-specific representation from the annotated support input, and a segmenter arm that utilizes this representation to segment the query image. The introduction of 'channel squeeze & spatial excitation' blocks facilitates efficient interaction between the two arms, enabling effective segmentation without relying on pre-trained models, which are often unavailable for medical scans. Additionally, an efficient strategy for volumetric segmentation is proposed, involving the optimal pairing of support volume slices with query volume slices. Experimental results on whole-body contrast-enhanced CT scans from the Visceral Dataset demonstrate the superiority of the proposed model, outperforming multiple baselines and existing approaches in terms of segmentation accuracy, with the source code available at https://github.com/abhi4ssj/few-shot-segmentation.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1977", "problem_id": "19770001", "content": "This study introduces a technique for visual text recognition that does not rely on paired supervisory data. We conceptualize the text recognition problem as aligning the conditional distribution of strings predicted from given text images with lexically valid strings obtained from target corpora. This approach facilitates fully automated and unsupervised learning from merely line-level text images and unassociated text string samples, eliminating the requirement for extensive aligned datasets. We provide a comprehensive analysis covering several facets of the proposed technique, including: (1) the effect of training sequence length on convergence, (2) the connection between character frequencies and the order of learning, (3) the generalization capability of our recognition network for inputs of varying lengths, and (4) the influence of different text corpora on recognition accuracy. Ultimately, we showcase impressive text recognition performance on both synthetically generated text images and scanned images of actual printed books, without using any labeled training examples.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1978", "problem_id": "19780001", "content": "A significant challenge lies in learning embeddings from extensive networks. While numerous methods exist, effectively leveraging network structure for generalization to new nodes, edges, or graphs remains unclear. This study addresses the problem of generating inductive network embeddings in large networks lacking node/edge attributes specific to a domain. We introduce a learning algorithm that utilizes a set of fundamental, predefined local encodings. Specifically, we examine degree frequencies at varying distances from a node, enabling efficient computation for short distances and numerous nodes. Notably, the generated embeddings exhibit strong generalization capabilities across unseen or distant network regions, both in unsupervised contexts when integrated with language model learning, and in supervised tasks as supplementary features within a neural network. Despite its simplicity, this approach attains state-of-the-art results in tasks like role detection, link prediction, and node classification, presenting an inductive network embedding method directly applicable to large, unattributed networks.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1979", "problem_id": "19790001", "content": "It is widely assumed that model-based Reinforcement Learning (RL) exhibits greater sample efficiency compared to model-free RL; however, this is not always the case in practice due to significant model errors. In intricate and noisy scenarios, model-based RL struggles to utilize the model effectively if it lacks certainty about its reliability. This study reveals that improved model utilization can greatly influence outcomes. We demonstrate theoretically that by confining the use of model-generated data to state-action pairs with minimal model error, the disparity in performance between model- and real-generated rollouts can be diminished. This insight drives the approach to employ model rollouts solely when the model displays confidence in its forecasts. We introduce the Masked Model-based Actor-Critic (M2AC), an innovative policy optimization algorithm that enhances a model-based lower bound of the genuine value function. M2AC incorporates a masking mechanism informed by the model's uncertainty to determine whether to utilize its predictions. As a result, the new algorithm generally leads to reliable policy enhancements. Experiments conducted on continuous control benchmarks reveal that M2AC performs exceptionally well, even with extended model rollouts in highly noisy conditions, and it considerably surpasses previous leading methods.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1980", "problem_id": "19800001", "content": "This paper examines high-dimensional estimation from truncated samples, concentrating on two key classical issues: (i) the inference of sparse Gaussian graphical models and (ii) the recovery of support in sparse linear models. In the context of Gaussian graphical models, we consider d-dimensional samples generated from a Gaussian N(\\mu,\\Sigma), which are only observed if they fall within a subset S \\subseteq \\mathbb^d. Our findings demonstrate that both parameters can be estimated with an error of \\epsilon in the Frobenius norm, utilizing \\tilde\\left(\\frac(^)}\\right) samples from the truncated distribution and access to a membership oracle for S, where S is assumed to have a non-negligible measure under the unknown distribution but can vary arbitrarily. For the sparse linear regression scenario, we analyze samples (,y) generated such that y = ^\\top^*} + (0,1), which are observed only when y is within a truncation set S \\subseteq \\mathbb. We assume that ^* is sparse, with its support set comprising k elements. Our primary result delineates the exact requirements related to the problem dimension d, the support size k, the number of observations n, and the characteristics of the samples and truncation necessary for support recovery of ^*. Notably, we establish that with some mild assumptions, only O(k^2 \\log d) samples suffice to estimate ^* in the \\ell_\\infty-norm with a bounded error. In both cases, our estimator minimizes the combination of the finite population negative log-likelihood function and an \\ell_1-regularization component.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1981", "problem_id": "19810001", "content": "Predicting potential crowd flow at new transportation sites is crucial for urban planning. This prediction can be inferred by analyzing nearby locations. However, differences in transportation modes between neighboring sites (e.g., bus versus subway stations) can lead to significant data limitations. To address this, we introduce MOHER, a data-driven method for predicting mode-specific crowd flow at new sites. Our approach first identifies relevant neighboring regions based on geographic proximity and urban functional similarity. Then, to integrate these diverse relationships, we develop a cross-mode relational GCN, a novel relation-specific transformation model, that captures both the correlations and distinctions between various transportation modes. Subsequently, we create an aggregator for inductive potential flow representation. Finally, an LSTM module is employed for sequential flow prediction. Experiments using real-world datasets show that MOHER outperforms existing state-of-the-art algorithms.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gemini20-1982", "problem_id": "19820001", "content": "This paper introduces a new Attention-based Multi-Reference Super-resolution network (AMRSR) designed to generate a high-resolution image from a low-resolution input by intelligently transferring pertinent textures from several reference images, ensuring spatial consistency. Utilizing multiple reference images in conjunction with attention-based sampling demonstrably enhances performance compared to current state-of-the-art reference super-resolution methods across several benchmark datasets. Reference super-resolution techniques have emerged as a means to address the inherent ambiguity of image super-resolution by incorporating supplementary information from a high-resolution reference image. Multi-reference super-resolution broadens this concept by offering a wider range of image characteristics to compensate for the intrinsic lack of information, all while conserving memory. A novel hierarchical attention-based sampling technique is presented, which learns the similarity between low-resolution image features and multiple reference images, guided by a perceptual loss. Ablation studies confirm that both multi-reference utilization and hierarchical attention-based sampling contribute to the enhanced overall performance. Perceptual and quantitative evaluations against ground truth reveal a substantial improvement in performance, even when the reference images differ considerably from the target image. The project website can be found at https://marcopesavento.github.io/AMRSR/", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1983", "problem_id": "19830001", "content": "Monocular depth estimation is a widely explored challenge in computer vision with numerous practical uses. Techniques based on deep learning have shown potential in both supervised and unsupervised depth prediction from single images. Current methods typically frame depth estimation as a regression task using localized pixel-wise loss functions. This study introduces an advancement by employing adversarial training to derive a non-local, context-sensitive loss function. By evaluating predicted depth at the patch level rather than individual pixels, the network can leverage broader contextual information. Within this setup, the generator maps RGB images to depth maps, while the discriminator differentiates between predicted and ground truth depth-RGB pairs. Spectral normalization stabilizes this conditional GAN framework, mitigating mode collapse when processing varied datasets. Experiments with diverse generators, including U-Net and CNN-CRF hybrids, demonstrate the method's effectiveness. Evaluations on NYUv2, Make3D, and KITTI datasets show significant error reduction, yielding state-of-the-art results through adversarial training.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1984", "problem_id": "19840001", "content": "A type of adversarial attack on deep neural networks, known as a Trojan or backdoor attack, involves an attacker providing a model that has been trained or retrained on malicious data to unsuspecting victims. This backdoor is triggered when a specific pattern, referred to as a trigger, is applied to a normal input, resulting in misclassification. While existing Trojan attacks often utilize simple triggers, such as patches or objects in the input space or basic transformations like Instagram filters, these triggers can be detected by recent backdoor detection algorithms. In this study, we introduce a novel Trojan attack that operates in the deep feature space, possessing five key characteristics: effectiveness, stealthiness, controllability, robustness, and a reliance on deep features. Through comprehensive experiments conducted on nine image classifiers across various datasets, including ImageNet, we demonstrate the properties of our proposed attack and show its ability to evade state-of-the-art defensive measures.", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1985", "problem_id": "19850001", "content": "Sparse representation, a robust statistical approach for image modeling, has proven effective in numerous image restoration tasks. Its effectiveness stems from advancements in l1-norm optimization and the inherent sparsity of natural images in certain domains. The restoration quality hinges on the chosen sparse domain's ability to accurately represent the target image. Given the substantial variation in content across images or even within patches of a single image, we suggest learning multiple basis sets from a dataset of sample patches and adaptively selecting the most suitable basis for each patch to define its local sparse domain. Our framework incorporates two adaptive regularization components: first, autoregressive (AR) models are derived from the dataset, with the best-fitting models adaptively applied to regularize local structures; second, non-local self-similarity is integrated as an additional regularization term. Furthermore, the sparsity parameter is adaptively tuned to enhance restoration outcomes. Comprehensive tests on image deblurring and super-resolution demonstrate that our adaptive sparse domain selection and regularization approach outperforms leading methods in both PSNR and visual quality.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1986", "problem_id": "19860001", "content": "Predicting pedestrian trajectories is difficult because of inherent uncertainty and multiple possible paths. Generative adversarial networks, although capable of learning the distribution of potential future trajectories, often forecast unrealistic samples when these trajectories represent a combination of distinct and separate modes. To overcome this, we introduce a multi-generator model for pedestrian trajectory prediction. In this model, each generator is specifically trained to learn the distribution of trajectories directed toward a primary mode within the environment. Simultaneously, another network learns a categorical distribution across these generators, based on dynamic factors and the surrounding scene. This design facilitates efficient sampling from specialized generators, substantially decreasing the occurrence of out-of-distribution samples relative to methods employing a single generator.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-1987", "problem_id": "19870001", "content": "This study introduces a framework for extracting parametric wireframes from high-density point clouds. The method analyzes a scalar distance field indicating closeness to sharp feature curves, followed by corner detection, curve segmentation, and the creation of a topological graph aligned with the wireframe structure. The final output consists of editable parametric spline curves with arbitrary sampling capabilities. The approach is tested on 50 intricate 3D models and outperforms a recent deep learning-based method in terms of quality.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "llama-33-70b-instruct-turbo-1988", "problem_id": "19880001", "content": "The field of biometrics has seen the emergence of cross-spectral iris recognition as a viable method for identifying individuals, but it is hindered by significant performance decline when matching iris images across different spectral bands, such as visible and near-infrared, compared to matching within the same band. To address this, the present study explores the application of deep convolutional generative adversarial networks (DCGANs) to enhance the accuracy of cross-spectral iris recognition, with a particular focus on introducing resolution differences into the matching problem. Two novel techniques are proposed, both leveraging the conditional generative adversarial network (cGAN) as the foundation: the first technique utilizes a cGAN to concurrently resolve cross-resolution and cross-spectral discrepancies by translating images to a uniform resolution and spectrum, while the second technique employs a coupled generative adversarial network (cpGAN) architecture, comprising paired cGAN modules that project visible and near-infrared iris images into a shared low-dimensional embedding space to maximize similarity between feature vectors from the same subject, as seen in Figure A, B, C (References: [citation]).", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "gpt-4o-mini-1989", "problem_id": "19890001", "content": "Enhancing data efficiency by transferring knowledge across tasks remains a significant challenge in the domain of global black-box optimization. Typically, available algorithms are created to serve as universal optimizers, which can lead to suboptimal performance for specific tasks. We introduce an innovative transfer learning approach to develop tailored optimizers within the established framework of Bayesian optimization, enabling our algorithm to leverage the effective generalization properties of Gaussian processes. By employing reinforcement learning to meta-train an acquisition function (AF) on a group of related tasks, the proposed method learns to derive implicit structural information and utilize it for enhanced data efficiency. We provide experimental results from a simulation-to-real transfer task, as well as multiple synthetic functions and two hyperparameter optimization challenges. The findings indicate that our algorithm (1) autonomously detects structural characteristics of objective functions from existing source tasks or simulations, (2) performs effectively in scenarios with both limited and abundant source data, and (3) defaults to the performance level of general AFs when no specific structure is identified.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "deepseekv3-1990", "problem_id": "19900001", "content": "Recent advancements have drawn greater attention to semi-supervised classification methods that incorporate graph-based information. Emerging learning models now fundamentally depend on classifying data following an initial graph convolution step. To evaluate this approach, we analyze the classification of Gaussian mixtures, where node attributes in a stochastic block model represent the data. Our findings demonstrate that graph convolution enhances linear separability by approximately a factor of 1/D, with D denoting the expected node degree, compared to using the mixture model data alone. Additionally, we observe that the linear classifier derived by minimizing cross-entropy loss post-convolution generalizes effectively to out-of-distribution data, even when the unseen data exhibits varying intra- and inter-class edge probabilities relative to the training set.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gpt-4o-mini-1991", "problem_id": "19910001", "content": "We introduce Spectral Inference Networks, a framework designed for acquiring eigenfunctions of linear operators through stochastic optimization. This approach extends Slow Feature Analysis to a broader class of symmetric operators and has strong ties to Variational Monte Carlo techniques utilized in computational physics. Consequently, it serves as a potent instrument for unsupervised representation learning from video or graph-based data. We reformulate the training of Spectral Inference Networks as a bilevel optimization challenge, enabling the online acquisition of multiple eigenfunctions. Our findings include training Spectral Inference Networks on quantum mechanics problems and video feature learning with synthetic datasets. The results indicate that Spectral Inference Networks effectively retrieve eigenfunctions of linear operators and can uncover interpretable representations from video data in a fully unsupervised fashion.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1992", "problem_id": "19920001", "content": "The identification of disease genes, which are genes linked to specific diseases, is crucial for comprehending and addressing health conditions. In this study, we introduce a biomedical knowledge graph tailored for this challenge, and we propose an innovative machine learning approach that utilizes recent advancements in network biology and graph representation learning to identify disease genes within such graphs. We analyze the impact of different relation types on the accuracy of predictions and provide empirical evidence that our methods surpass the nearest state-of-the-art competitor in disease gene identification by 24.1%. Additionally, we demonstrate that our approach achieves greater precision than Open Targets, the premier initiative in target identification, regarding the prediction of drug targets in clinical trials for Parkinson's disease.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1993", "problem_id": "19930001", "content": "This paper introduces a new deep learning method for completing and reconstructing 3D shapes from incomplete scan data. Unlike supervised learning techniques that train on labeled completion tasks and then apply the learned model to a test shape, our network is optimized from the ground up for each individual test shape. This allows the network to adapt specifically to the shape's characteristics and complete missing data by leveraging contextual information from the existing regions. This capability of an untrained neural network to complete missing data is known as the deep prior. We analyze the deep prior using the neural tangent kernel (NTK) framework, demonstrating that the shape patches completed by the trained CNN are inherently similar to existing patches due to their proximity in the kernel feature space defined by the NTK. This interpretation guides the design of more effective network structures and learning processes for shape completion and reconstruction. Our approach, which exhibits a greater sensitivity to structural regularities compared to traditional and other unsupervised reconstruction methods, accurately completes large missing areas with plausible shapes. Furthermore, it complements supervised learning methods that rely on database priors by eliminating the need for additional training datasets and providing adaptable performance on individual shape instances.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "llama-33-70b-instruct-turbo-1994", "problem_id": "19940001", "content": "The GATR system, developed by Lockheed Martin, is a globally scalable software solution designed for automated target recognition, enabling real-time object detection and classification in satellite imagery worldwide. Leveraging GPU-accelerated deep learning algorithms, GATR rapidly searches extensive geographic areas, achieving a processing rate of over 16 square kilometers per second (or more than 10 million pixels per second) on a single GPU, allowing it to scan the entire state of Pennsylvania for gas fracking wells in just two hours. The system's search time and processing rate scale linearly with geographic area and GPU count, respectively. With a modular, cloud-based architecture utilizing the Maxar GBDX platform, GATR offers automated target recognition analytics as a service, supporting applications such as broad area search, monitoring of ports and airfields, and site characterization, using deep learning models like RetinaNet and Faster R-CNN, as seen in Figure A, with results demonstrating recalls exceeding 90% for aircraft and fracking well detection, even in unseen regions, and is adaptable to new targets, including cars and ships, as well as radar and infrared imagery, as shown in Figure B, and referenced in [1].", "label": "human + AI", "label_detailed": "human + Llama-3.3-70B-Instruct-Turbo"}
{"solution_id": "deepseekv3-1995", "problem_id": "19950001", "content": "Process mining focuses on deriving insights from logs of business process executions. Conventional techniques, such as process model discovery or conformance analysis, operate on a limited set of features, typically including only case IDs, activity types, and timestamps for each event. Modern machine learning, however, leverages comprehensive data inputs and automatically constructs feature hierarchies during training. This paper presents ProcK (Process & Knowledge), an innovative framework designed to develop predictive models for business processes by integrating sequential event log data with structured semantic knowledge from graph-based repositories. This hybrid methodology allows ProcK to dynamically utilize all organizational database information. The framework includes modules for extracting interconnected event logs and knowledge graphs from relational databases. We evaluate ProcK’s effectiveness using the OULAD e-learning dataset, achieving top-tier results in predicting student course dropout and academic success. Additionally, we test the approach on other tasks, such as forecasting exam scores and making early predictions based solely on data from the initial weeks of courses.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
{"solution_id": "gemini20-1996", "problem_id": "19960001", "content": "Despite significant advancements in automatic emotion recognition from facial expressions and speech, the area of emotion recognition based on body gestures remains relatively underexplored. Given the diverse range of body language used to convey emotions, and the challenges in cataloging all such gestures and acquiring sufficient data for each, the ability to recognize novel emotional body gestures is crucial for enhancing our understanding of human emotion. Current methodologies, however, struggle to accurately assign new body gestures to their corresponding emotional states. To address this limitation, we present a Generalized Zero-Shot Learning (GZSL) framework comprising three branches designed to infer the emotional state of new body gestures using only their semantic descriptions. The first branch, a Prototype-Based Detector (PBD), identifies whether a sample belongs to a known body gesture category and provides prediction results for samples from these seen categories. The second branch, a Stacked AutoEncoder (StAE) with manifold regularization, leverages semantic representations to predict samples from unseen categories. Both of these branches focus on body gesture recognition. Furthermore, we incorporate an emotion classifier with a softmax layer as the third branch to improve feature representation learning for emotion classification. All three branches receive input features learned by a shared feature extraction network, specifically a Bidirectional Long Short-Term Memory Networks (BLSTM) with a self-attention module. We employ multi-task learning strategies to jointly train these three branches as subtasks. Experimental results on an emotion recognition dataset demonstrate that our framework significantly outperforms traditional emotion classification methods and existing state-of-the-art zero-shot learning techniques.", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "gpt-4o-mini-1997", "problem_id": "19970001", "content": "This study explores multi-objective reinforcement learning, where the objectives are prioritized based on preferences. In real-world scenarios, these preferences are frequently presented adversarially; for instance, customers may exhibit selective behavior in various applications. We frame this issue as an episodic learning challenge within a Markov decision process, characterized by unknown transitions and a reward function defined as the inner product of a preference vector with predetermined multi-objective reward functions. We examine two scenarios. In the online context, the agent encounters a (adversarial) preference in each episode and suggests policies for environmental interaction. We introduce a model-based algorithm that attains a nearly minimax optimal regret bound \\widetilde}\\bigl(\\cdot H^2 SAK}\\bigr), where d denotes the number of objectives, S the number of states, A the number of actions, H the horizon length, and K the number of episodes. Additionally, we address preference-free exploration, whereby the agent initially engages with the environment without defining any preferences, subsequently allowing it to adapt to an arbitrary preference vector with up to \\epsilon error. Our proposed algorithm is demonstrably efficient, achieving a near-optimal trajectory complexity \\widetilde}\\bigl(\\cdot H^3 SA}/\\bigr). This finding contributes to addressing an unresolved issue identified by \\citet.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gpt-4o-mini-1998", "problem_id": "19980001", "content": "Acquiring discriminative and robust representations is essential for the effectiveness of machine learning systems. A significant challenge in representation learning is to achieve invariance against arbitrary nuisance or sensitive attributes while maintaining strong performance on designated tasks. This challenge is predominantly addressed by eliminating sensitive information from the learned representations. In this study, we introduce a novel disentanglement strategy for the invariant representation issue. Our method separates meaningful representations from sensitive ones by imposing orthogonality constraints as a proxy for independence. We ensure that the meaningful representation remains unaffected by sensitive information through entropy maximization. The proposed approach is assessed using five publicly accessible datasets and benchmarked against leading methods in learning fairness and invariance, demonstrating state-of-the-art performance on three datasets and comparable results on the remaining ones. Additionally, we conduct an ablative analysis to investigate the impact of each component.", "label": "human + AI", "label_detailed": "human + GPT-4o-mini"}
{"solution_id": "gemini20-1999", "problem_id": "19990001", "content": "Single-image 3D photography allows users to observe a static image from different perspectives. Contemporary methods integrate monocular depth and inpainting networks to generate visually appealing outcomes. However, these methods often employ rigid depth layering, which fails to accurately represent subtle visual elements like fine hair strands. This paper introduces SLIDE, a cohesive and adaptable system for single-image 3D photography that employs a straightforward yet robust soft layering technique to enhance the preservation of visual details in new views. Furthermore, a novel depth-aware training methodology is proposed for the inpainting module, optimized for 3D photography applications. The modular design of SLIDE facilitates the integration of other elements, such as segmentation and matting, to refine layering. Simultaneously, SLIDE utilizes an efficient layered depth formulation, requiring only a single pass through the component networks to generate high-quality 3D photographs. Comprehensive experimental evaluations across three view-synthesis datasets, along with user studies conducted on diverse image collections, reveal that SLIDE outperforms existing baseline methods while maintaining a significantly simpler conceptual framework. Project page: https://varunjampani.github.io/slide", "label": "human + AI", "label_detailed": "human + Gemini 2.0"}
{"solution_id": "deepseekv3-2000", "problem_id": "20000001", "content": "Significant progress has been made in unsupervised representation learning, particularly through contrastive learning, which treats each image and its augmentations as distinct classes without accounting for semantic relationships between images. This work introduces Center-wise Local Image Mixture (CLIM), a novel data augmentation technique designed to broaden an image's neighborhood space. CLIM promotes both local similarity and global clustering by identifying locally similar samples and selecting those nearest to their respective cluster centers, termed center-wise local selection. This approach ensures representations gradually converge toward clusters without disrupting local relationships. Additionally, image mixing serves as a smoothing regularizer to prevent overconfidence in selected samples. Multi-resolution augmentation is also incorporated to enhance scale invariance. Combining these augmentations improves feature learning across multiple unsupervised benchmarks, achieving 75.5% top-1 accuracy in linear evaluation with ResNet-50 and 59.3% top-1 accuracy with only 1% labeled data during fine-tuning, while consistently surpassing supervised pretraining in various downstream transfer tasks.", "label": "human + AI", "label_detailed": "human + Deepseek V3"}
